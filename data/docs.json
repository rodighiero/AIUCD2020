[
	{
		"id": 1,
		"title": "EcoDigit-Ecosistema Digitale per la fruizione e la valorizzazione dei beni e delle attività culturali del Lazio",
		"abstract": {
			"it": "EcoDigit è uno dei progetti del 'Centro di Eccellenza DTC Lazio', che ha l'obiettivo di aggregare e integrare varie competenze nell'ambito del patrimonio culturale digitale. EcoDigit intende aricchire il sistema dell'Anagrafe delle Competenze, altro progetto del DTC Lazio, con una piattaforma middleware che faciliti l'integrazione di nuove risorse di dati e permetta la pubblicazione e riuso di servizi per la valorizzazione e fruizione del patrimonio culturale nella regione Lazio. Il progetto è finalizzato all'integrazione dei dati censiti attraverso formati aperti e tecnologie semantiche basate sul modello Linked data.",
			"en": "EcoDigit is one of the projects of the 'Centro di Eccellenza DTC Lazio', which intends to aggregate and integrate expertises in the field of technologies applied to cultural heritage. EcoDigit aims at enriching the Anagrafe delle Competenze, another project of the DTC Lazio, with a middleware platform that is able to facilitate the integration of new data sources and to allow the publication and reuse of services for the enhancement and fruition of the cultural heritage in the Lazio region. The project is designed to integrate the collected data through open formats and semantic technologies based on Linked Data model."
		},
		"authors": [
			"Luigi Asprino",
			"Antonio Budano",
			"Marco Canciani",
			"Luisa Carbone",
			"Miguel Ceriani",
			"Ludovica Marinucci",
			"Massimo Mecella",
			"Federico Meschini",
			"Marialuisa Mongelli",
			"Andrea Giovanni Nuzzolese",
			"Valentina Presutti",
			"Marco Puccini",
			"Mauro Saccone"
		],
		"body": "Introduzione Il progetto Ecosistema Digitale per la fruizione e la valorizzazione dei beni e delle attività culturali del Lazio (EcoDigit)1 è una delle iniziative del Centro di Eccellenza del Distretto Tecnologico per i beni e le attività Culturali (DTC)2, costituito dalle cinque università statali del Lazio - Sapienza, Tor Vergata, Roma Tre, Cassino e Tuscia - in rete con CNR, ENEA e INFN per aggregare e integrare competenze nel settore delle tecnologie per i beni e le attività culturali. Il progetto (luglio 2018 - gennaio 2020) si rivolge alle organizzazioni operanti nel settore della cultura le quali producono e mantengono basi di dati anche di grandi dimensioni archiviati usando formati, modelli e processi diversi. Questo fenomeno comporta la necessità di identificare processi, tecnologie e modelli di integrazione semplici e sostenibili che consentano la fruizione del patrimonio in modo globale e collegato. Le tecnologie semantiche, e in particolare i Linked Open Data (LOD), sono state ampiamente sfruttate con successo nel campo del patrimonio culturale al fine di migliorare l'accesso e l'esperienza di fruizione dei beni culturali da parte dei cittadini, così come di facilitare la reperibilità, l'integrazione e l'arricchimento dei dati (Dijkshoorn et al., 2018; Daquino et al., 2017; Lodi et al., 2017). Infatti, il paradigma dei LOD è utilizzato per collegare dati provenienti da diverse istituzioni culturali, aumentando così la possibilità di raggiungere i dati culturali disponibili nel Web of Data. L'interconnessione dei contenuti delle organizzazioni collaboratrici ha anche contribuito ad arricchire le informazioni in modo efficace e finalizzato alla valorizzazione del patrimonio culturale (Hyv\u00F6nen, 2009). La collaborazione tra organizzazioni culturali ha portato anche allo sviluppo collaborativo di ontologie che descrivono il patrimonio culturale a livello internazionale, ad esempio CIDOC-Conceptual Reference Model (CRM) (Doerr, 2003)3, in modo tale che i requisiti di interoperabilità semantica potessero essere soddisfatti all'interno dei loro sistemi. Inoltre, l'uso di ontologie comuni ha facilitato lo scambio di dati e la creazione di enormi biblioteche digitali, ad esempio l'Europeana Data Model (EDM) (Isaac and Haslhofer, 2013)4. EcoDigit, inoltre, non solo ha preso ispirazione da esperienze già consolidate sia degli enti partner e non del progetto sia maturate nell'ambito delle Pubbliche Amministrazioni (PA), ma intende anche instaurare collaborazioni con tali realtà. Ad esempio, il progetto ReCAP5 dell'Università Sapienza ha creato una rete di condivisione di conoscenze, strumenti e sperimentazioni che permette di elaborare linee guida e modelli per la costruzione di processi conservativi del patrimonio digitale. Inoltre, il Sacher Project6 sta affrontando problematiche simili, ma non a livello di progettazione di una piattaforma regionale. Molte istituzioni regionali e nazionali che gestiscono il nostro patrimonio culturale stanno adottando il modello dei dati aperti seguendo le linee guida dell' Agenzia per l'Italia Digitale (AgID); tuttavia per le organizzazioni pubbliche e private non è semplice adeguarsi per la mancanza di una piattaforma che ne faciliti il compito. La missione del progetto Data & Analytics Framework (DAF)7, di cui il network di ontologie OntoPia8 è uno dei risultati, è analoga a quella di EcoDigit, ma insiste sull'intero territorio nazionale con focus sull'interoperabilità tra dati tra PA. In tale contesto, EcoDigit può contribuire a rendere efficace e coordinata l'integrazione con i sistemi nazionali. 2 Obiettivi EcoDigit ha l'obiettivo di arricchire il sistema Anagrafe delle Competenze9 con una piattaforma middleware che faciliti l'integrazione di nuove sorgenti di dati e consenta la pubblicazione e il riuso di servizi per la valorizzazione e la fruizione del patrimonio culturale del Lazio. Nello specifico, EcoDigit fornisce (i) l'architettura di riferimento per l'integrazione di servizi modulari e per la loro pubblicazione e il riuso; (ii) una componente software sviluppata in forma prototipale per l'orchestrazione dei servizi, l'integrazione 1http:\/\/ecodigit.dtclazio.it\/ 2https:\/\/dtclazio.it\/ 3http:\/\/cidoc-crm.org\/ 4https:\/\/pro.europeana.eu\/page\/edm-documentation 5http:\/\/digilab.uniroma1.it\/attivit\/recap 6http:\/\/www.sacherproject.com\/progetto 7https:\/\/teamdigitale.governo.it\/it\/projects\/daf.htm 8https:\/\/github.com\/italia\/daf-ontologie-vocabolari-controllati 9http:\/\/dtc.si.cnr.it\/anagrafe-delle-competenze 2 \fFigure 1: Schema architetturale in cui si inserisce il Middleware di EcoDigit. e l'aggregazione delle interfacce e dei dati; (iii) la versione prototipale di servizi orientati alla fruizione e valorizzazione del patrimonio culturale. Con queste caratteristiche EcoDigit si configura come un progetto di ricerca e trasferimento tecnologico collegato in maniera significativa agli altri progetti del DTC Lazio. Con Anagrafe è in relazione fornendo un livello software intermedio, detto middleware, che consente al sistema di aggregare nuove sorgenti di dati, servizi, strumenti innovativi sia industriali che accademici. In questa prospettiva EcoDigit estende il sistema Anagrafe e gli conferisce la capacità di evolvere ed essere esteso. Per gli altri progetti di ricerca, EcoDigit svolge il ruolo di mediatore nei confronti del sistema Anagrafe. Ciò significa che i risultati dei vari progetti potranno essere integrati grazie all'interfacciamento con il middleware di EcoDigit. In Figura 1 è rappresentato uno schema architetturale di alto livello in cui si inserisce il middleware di competenza di EcoDigit. Sulla sinistra sono rappresentate le risorse, ovvero le banche dati (ciascuna col il proprio formato sintattico e modello concettuale di rappresentazione dei dati) e i servizi messi a disposizione dai vari enti operanti sul territorio che, però, mancano di un punto di accesso unico. Ciò ostacola potenziali utenti interessati a consultare le risorse o creare applicativi basati su esse. Il sistema, a cui stanno lavorando congiuntamente i gruppi di EcoDigit e Anagrafe, ha l'obiettivo di superare questa frammentarietà. Esso è composto da vari livelli: subito al di sopra del Data Layer, c'è (i) l'Acquisition Layer, a cui lavorano congiuntamente Anagrafe ed EcoDigit, che si occupa di creare dei flussi di dati i quali saranno acquisiti, uniformati secondo uno schema comune e memorizzati dal sistema. Questi flussi dati verranno creati dal (ii) Middleware, che offre alle applicazioni client delle funzionalità per l'elaborazione semantica dei contenuti, facilities per il mashup dei servizi indicizzati dal sistema, oltre a definire le linee guida che servizi e dataset esterni dovranno seguire per garantire l'interoperabilità con il sistema. Nella parte più alta, (iii) il Client Application rappresenta una qualunque applicazione che intende utilizzare i dati acquisiti e offerti dal sistema. Essa potrà, attraverso il Middleware, interrogare il sistema per raccogliere dati, recuperare i metadati di dataset esterni o descrizioni dei servizi disponibili. 3 Metodologia Gli obiettivi sovraesposti sono stati realizzati attraverso l'esecuzione di attività svolte in maniera collaborativa e sinergica tra i partner del progetto e sintetizzabili in tre fasi: 1. Censimento delle risorse relative ai beni culturali presenti nella regione Lazio (cf. Sezione 3.1); 2. Elaborazione del modello di integrazione delle sorgenti nel Middleware (cf. Sezione 3.2); 3. Realizzazione di prototipi volti a verificare la validità dei risultati e dell'approccio seguito (cf. Sezione 3.3). Tali attività sono state svolte considerando una prospettiva almeno quinquennale di sostenibilità. 3 \f3.1 Attività di censimento delle risorse presenti nel Lazio Fondamentali nel corso del progetto sono state tre tipi di attività preliminari di censimento consistenti in: (a) un'analisi dei requisiti del sistema, con relativa ricognizione degli attori e dei casi d'uso, e dello stato dell'arte su tecnologie middleware e architetture per l'integrazione di servizi; (b) l'individuazione delle sorgenti dei dati presenti nel Lazio, e dei modelli ontologici e tecniche per la loro integrazione e standardizzazione basata su formati aperti e semantici. In questo contesto, tramite la formalizzazione e disseminazione è stato possibile anche individuare potenziali stakeholder del progetto; (c) una valutazione dei tool allo stato dell'arte per la creazione e la gestione di ambienti virtuali 2D\/3D. 3.2 Modello di integrazione di una sorgente Il modello di dati e metadati, che le sorgenti devono rispettare per poter essere acquisite dal middleware EcoDigit, si basa su metodi e tecniche di 'metadatazione' e di Semantic Web, comprendendo anche tecnologie proprie delle openAPI e tutto ciò che viene comunemente classificato come Open Data. Per la sua elaborazione, si è proceduto a un'analisi delle sorgenti censite nel Lazio al fine di evidenziare tanto i domini di conoscenza coperti dalle sorgenti quanto il dettaglio dei vari campi che il modello deve rappresentare. Per ognuno di essi sono stati ricercati gli schemi concettuali considerati standard di riferimento per la modellazione dei dati inerenti a un certo dominio, quali FOAF10, DOAP11, Org Ontology12, OntoPia network, SPAR Ontologies13, ArCo14, ecc. Il modello deve essere pensato come l'unione di standard tecnologici, schemi concettuali e di metadatazione allo stato dell'arte per i vari domini di conoscenza, relativi in particolare all'ambito dei beni culturali. Successivamente, quando i modelli esistenti allo stato dell'arte sono stati ritenuti non in grado di rappresentare semanticamente campi peculiari presenti nei dataset di input, è stata effettuata una modellazione ex novo, utilizzando una metodologia di ingegneria ontologica (Blomqvist et al., 2016), basata su un'estensione di eXtreme Design (XD) (Blomqvist et al., 2010). XD è un metodo di progettazione agile di ontologie che si basa sul riuso di Ontology Design Patterns (ODP) (Gangemi and Presutti, 2009) al fine di risolvere problemi di modellazione ontologica noti e ricorrenti. Il workspace del progetto EcoDigit è disponibile su Github15. Di seguito, si elencano le ontologie definite ex-novo nel corso del progetto: \u2022 Ontologia delle Organizzazioni16 lo scopo di definire un vocabolario condiviso ditermini per la descrizione delle organizzazioni che partecipano al Centro di Eccellenza-DTC Lazio. Essa estende: OntoPiA-COV, FOAF, W3C's Organization Ontology. \u2022 Ontologia delle Valutazioni17 ha lo scopo di definire un vocabolario di termini per la descrizione di qualsiasi cosa abbiamo una valutazione associata che è espressa rispettouna certa scala. \u2022 Ontologia delle Esperienze e Competenze18 ha lo scopo di definire un vocabolario condiviso di termini per la descrizione delle esperienze e competenze di una persona. Estende: OntoPiA-CPV, OntoPiA-COV, FOAF, BIBO; importa l'Ontologia delle Valutazioni. 10http:\/\/xmlns.com\/foaf\/spec\/ 11http:\/\/usefulinc.com\/ns\/doap# 12https:\/\/www.w3.org\/TR\/vocab-org\/ 13http:\/\/www.sparontologies.net\/ontologies 14http:\/\/w3id.org\/arco 15https:\/\/github.com\/ecodigit\/workspace 16https:\/\/w3id.org\/ecodigit\/ontology\/organization 17https:\/\/w3id.org\/ecodigit\/ontology\/grade 18https:\/\/w3id.org\/ecodigit\/ontology\/eas\/ 4 \f3.3 Prototipi A seguito delle attività sovraesposte di censimento dei dataset, degli schemi concettuali e dei tool allo stato dell'arte per la fruizione del patrimonio culturale, sono in fase di elaborazione due prototipi: \u2022 una Proof-of-Concept, volta a mostrare la validità del modello attraverso l'integrazione di alcune delle sorgenti identificate nel task di censimento. L'obiettivo principale è quello di creare una best practice che mostri sia la semplicità dell'approccio di integrazione, e la sua scalabità nel tempo, sia gli strumenti hardware e software che una sorgente deve adottare per aderire al modello di ingresso di EcoDigit; \u2022 un prototipo di un servizio avanzato per la fruizione dei beni culturali nel dominio della formazione. Si sta lavorando ad una soluzione che permetta di raggiungere e fruire da una singola interfaccia i dati acquisiti dal sistema. I partner del progetto, sulla base delle loro expertise, hanno collaborato alla definizione di una tassonomia di categorie che popolino l'interfaccia utente a fini didattici. Anche questo prototipo fa uso di tecnologie semantiche per migliorare la ricerca delle risorse e collegarle ai contenuti di carte tematiche (GIS based) e ricostruzioni 3D per permettere la fruizione in ambiente virtuale dei beni mostrati dall'educatore. In particolare, sarà possibile gestire completamente modelli 3D attraverso l'implementazione ad hoc (cfr. il Workspace GitHub di EcoDigit19) del software 3DHOP20 e di mostrare dati cartografici tematizzati grazie all'uso della libreria Openlayer21. I due prototipi utilizzano e rappresentano due viste diverse sugli stessi contenuti ai quali applicare le tecniche semantiche di integrazione dei dati, indicandone la provenienza: (i) il dataset della S&TDLScience & Technology Digital Library del CNR; (ii) il dataset della Sapienza Digital Library; (iii) il modello 3D di Porta Latina (Mura Aureliane), implementato da Roma Tre; (iv) la mappatura GIS con modelli 2D e 3D di alcune chiese della città di Viterbo, curata dalla Tuscia; (v) la ricostruzione 3D del Trono Corsini e del busto in Terracotta di Alessandro VII Chigi, fornita da ENEA; (vi) i Linked Open Data del progetto Arco-Architettura della Conoscenza22, progettati dal CNR-ISTC in collaborazione con il MiBAC-ICCD. 4 Avanzamento tecnologico e impatto del progetto Il modello di integrazione delle sorgenti nel quale riveste un ruolo fondamentale lo sviluppo delle tecniche di interoperabilità semantica dei vari dataset censiti nel Lazio consente l'arricchimento delle informazioni dei dati esistenti secondo una fruizione integrata. I risultati di questo studio possono essere applicati a qualsiasi progetto che potenzialmente abbia necessità di integrare sorgenti eterogenee. Data la rilevante diversità dei tipi di dati rinvenuti finora, la riusabilità delle tecniche ideate e sperimentate in EcoDigit rappresenta uno degli aspetti principali di avanzamento tecnologico prodotto. Ciò comporta la possibile partecipazione di stakeholder eterogenei alla fruizione ed evoluzione di servizi e contenuti, dando supporto all'inclusione di patrimoni già esistenti e alla loro filiera di gestione. L'utilizzo di formati e strumenti aperti garantisce tanto la sostenibilità nel tempo quanto la diffusione massiva dei contenuti. Inoltre, il progetto stimola la creazione di nuove figure professionali e la conseguente richiesta formativa sull'uso e sulla divulgazione delle tecnologie semantiche basate sul modello dei Linked Data per la fruizione e la valorizzazione del patrimonio culturale. Ringraziamenti Si ringrazia i seguenti enti per il loro supporto istituzionale: la Galleria Corsini, l'Associazione CIVITA, l'Istituto per il Catalogo e la Documentazione (ICCD) del MiBAC, la Sovrintendenza xx. 19https:\/\/github.com\/ecodigit\/3dhop-react 20http:\/\/vcg.isti.cnr.it\/3dhop\/ 21https:\/\/openlayers.org\/en\/latest\/doc\/ 22http:\/\/dati.beniculturali.it\/arco\/; http:\/\/dati.beniculturali.it\/"
	},
	{
		"id": 2,
		"title": "Encoding the critical apparatus by domain specific languages: The case of the Hebrew book of Qohelet",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Gianfranco Bandini",
			"Andrea Mangiatordi"
		],
		"body": "Introduction1 The work of the textual philologist consists of two main parts: 1. the gathering and the systematic analysis of all the available documents (the witnesses) of a literary work (recensio); 2. the removing of all the errors due to the textual transmission process (emendatio) (Timpanaro 2005). During the recensio phase, the scholar proceeds to a comparison of the witnesses with the purpose of detecting textual differences (the variant readings or simply variants). This procedure, named collatio, is one of the most important and delicate phase within the workflow of the text-critical praxis and represents a preliminary step to the preparation of a critical edition. The variants are presented in the critical apparatus, an instrument devised to show the reader the results of both the recensio and the emendatio by means of a conventional and formalized language, specific for the domain of textual philology (Domain Specific Language, cf. section 3). One of the tasks of the digital philologist consists in encoding variant readings and conjectural emendations. The encoding enables the creation of dynamic critical apparatuses: as with databases, the user can decide which data to extract and present, to combine the result of different queries, to transform the philological data into numerical format suitable for quantitative analysis and, finally, to prepare a digital 1Even if both authors contributed equally to this work, L. Bambaci is responsible for sections 1-3 and 5-6 and F. Boschetti is responsible for section 4. 7 \fversion of the work. Unlike traditional, printed critical apparatuses, where the information is stored in a predefined, static way, a digital apparatus allows to retrieve, from the same encoded file, different types of information according to different research purposes and needs (Driscoll and Pierazzo 2016). The guidelines provided by the Text Encoding Initiative (TEI)2 are among the best practices in the domain of the digital philology. TEI markup schemes pursue interoperability and reusability, making available for digital philologists a common interchange language covering a large set of text-critical phenomena.3 TEI digital framework, moreover, is flexible enough to enable the user to add new tags and attributes, thus allowing to shape customized encoding vocabularies suitable for specific text-critical problems and for different literary traditions. The verbosity and complexity of XML language, however, combined with the necessity of being adherent to standards, is at risk of distracting the traditional philologist from his or her critical activity. Goal of this paper is to show how it is possible to encode variant readings by exploiting the nature of DSL which characterizes the language of the critical apparatus, without requiring from the philologist to deal with TEI technicalities and with problems of conformity to standards. Our case studies are represented by a sample digital collation of one book of the Hebrew Bible, the book of Qohelet also known as Ecclesiastes, conventionally dated to V-III BC, and by the digital version of the collation of Hebrew Medieval manuscripts of the same book, carried out by Benjamin Kennicott at the end of the XVIII century (Kennicott 1776). Both the collations are part of a forthcoming doctoral dissertation, which aims to prepare a digital critical edition of the literary work. 2 Background As we have already discussed in Bambaci et al. 2018, 2019, many tools for encoding critical apparatuses are already available or currently developing.4 The strategies adopted by these tools to avoid or facilitate the encoding process are mainly based on ad hoc graphical user interfaces or on annotation systems through abbreviated markers. Tools that allow to create digital TEI apparatuses directly from printed, traditional ones are lacking or not fully developed, such as in the case of the Classical Text Editor (Hagel 2007). The methodology we propose is implemented on Euporia, a web annotation system based on DSLs developed at the CoPhiLab of the CNR-ILC. Euporia has been formerly used for interpretative tasks, such as the identification of ritual frames in the ancient Greek tragedies (Mugelli et al. 2016), and also for educational purposes, namely in teaching Ancient Greek both in secondary school (Liceo Classico) and with BA students in Classics (first year students) at the University of Pisa. 3 Methodology As stated in the introduction, the critical apparatus is the part of a critical edition or collation devoted to the collection of textual variants and conjectural emendations. There are no fixed guidelines for compiling a critical apparatus. The different methodologies are indeed the result of different traditions of study and research practices, which depend not only on the literary domain (a critical edition of a classical text will necessarily be different from a critical edition of a medieval text, Varvaro 1970), but also on internal developments and trends within each discipline: even different editions of the same text may vary in the choice of vocabulary or typographical standards, according to the different scholarly orientations or the editor's critical insights. Despite this extreme degree of variability, the critical apparatuses, however different the shapes they may assume, all strive for the same goal: to overcome the verbosity of the natural language and present the information in a way which is as concise as possible. The result of this process of departure from the natural language is an artificial (or planned) language (Blanke 2011, Libert 2018), specific to the domain of the textual philology (Domain Specific Language, DSL). Unlike a General Purpose Language (GPL), which is applicable across domains, a DSL is a language of limited 2https:\/\/tei-c.org\/ 3Cf. in particular chapter 12 of the TEI Guidelines, devoted to the encoding of the critical apparatus: TEI Consortium, eds. '12 Critical Apparatus.' TEI P5: Guidelines for Electronic Text Encoding and Interchange. [3.5.0.]. [16th July 2019]. TEI Consortium. https:\/\/tei-c.org\/release\/doc\/tei-p5-doc\/en\/html\/TC.html ([29\/11\/2019]). 4Cf. https:\/\/wiki.tei-c.org\/index.php\/Category:Editing_tools and https:\/\/wiki.tei-c.org\/ index. php\/Editors for a list of the main editing tools. 8 \fexpressiveness optimized for a particular domain of knowledge or domain of application (Fowler 2010).5 Figure 1: A sample apparatus entry from Qohelet 3:17 Let's take an example of apparatus entry from the third chapter of Qohelet, verse 17 (Fig. 1). Concepts such as 'location', 'lemma', 'witness', 'reading' and 'variant reading' are encoded here by means of: 1. numbers (chapters and verses); 2. strings (the Latin sigla indicating the witnesses; the words of the readings); 3. characters (separators such as the square bracket ']', found after the lemma; a vertical line '|' which divides the readings and a double vertical line '||' which marks the end of a reading group). The function of the apparatus components is determined by their position within the sentence (the first reading group shows the readings supporting the lemma; the following groups contain the variants).6 Similarly, in the apparatus entry of Qohelet 5:1 in Kennicott's collation (Fig. 2) we find integers for the Figure 2: A sample apparatus entry of Qohelet 5:1 from Kennicott's collation identification of the location ('verse 1'), of the manuscripts' sigla and of the word occurrence in the reference text (the numero sign following the roman number after the reading, e. g. '1\u00B0'); Hebrew words identifying lemmas and readings; special symbols for describing the variant typology (e. g. the symbol '\u2038' standing for omission); annotations concerning the source description ('primo' for 'first copyist's hand, 'nunc' for 'second copyist's hand') and finally special separators for discriminating reading groups ('\u2014') and apparatus entries ('.'). A language of this sort, in which all the constituents are characterized in a concise, non-redundant and unambiguous way, is a formal language. From a computational point of view, a formal language is a language whose structure (its syntax) and meaning (its semantics) is clearly defined (Grishin 1989). A computer, therefore, is able to check that sentences are grammatically correct (well-formed) and to recognize their meaning and function (Reghizzi 2009). 4 4.1 Workflow The Context Free Grammar In order to allow the interpreter (or compiler) to parse the apparatus written in our DSL, we wrote the Context Free Grammar (CFG). A CFG is a formal grammar consisting of a set of rules describing a formal language (Parr 2010). An example of CFG suitable for analysing the lemma is shown in Fig. 3. Thanks to the parsing system provided by ANTLR software (Parr 2012), the CFG allows to tokenize and parse the whole apparatus entry, identifying the vocabulary symbols (token rules) and the syntactic structure (parser rules). The token rules (in the example, the rules in capital letters) allow to tokenize integers (NUM), alphabetic characters (ALPHA_SEQ), Hebrew words (HEBW) and separators (R_BRACKET). The parser rules allow to check the syntax: the lemma (lem), for example, is encoded as a sequence of Hebrew words (w+), witnesses sigla (wit) and separators (LemSep). The result of the parsing of the whole apparatus entry is an Abstract Syntax Tree (AST), as shown in Fig. 4 below. The AST attaches 5Examples of DSL can be considered the language of algebra for stating numerical relationships, the language of boolean logic for propositional calculus and, more in general, any kind of notation systems that allows, within a particular community of practice, the description of problems and solutions in a specific area of interest. In computer science, examples of DSLs are HTML for web pages, SQL for relational databases, LaTeX for text processing, XSLT for XML transformations and so forth. In software engineering, these languages can be called, more properly, Domain Specific Programming Language (DSPL), as opposed to General Purpose Programming Language (GPPL), such as Java, C, etc. 6Both vocabulary and structure of the critical apparatus of the sample edition have been shaped following the more recent critical edition of the book (Goldman 2004). 9 \f(a) CFG for the sample edition of Qohelet (b) CFG for Kennicott's collation Figure 3: Examples of Context Free Grammars (a) AST from the sample edition (b) AST from Kennicott's collation Figure 4: Examples of syntactic trees (AST) 10 \fto the textual items (the nodes) a label which remind the function assumed in the context and shows the syntactic hierarchical relationships existing between them (the branches). 4.2 The Visitor In order to convert any DSL in XML, we wrote a software component, named 'AstToXmlVisitor', which generates an XML file structured on the AST. The Visitor passes through the tree nodes and slavishly translates the parser rules into XML markers (Fig. 5). The result is a structured, well-formed XML file, whose elements take the name from the parser rules and the hierarchical structure from the AST. The Visitor has been implemented in Java language, through the set of tools available in ANTLR4 software. (a) XML output of Qoh. 3:17 from the sample edition (b) XML output of Qoh. 5:1 from Kennicott Figure 5: Visitor's XML outputs 4.3 From XML to TEI-XML A final conversion from XML code to a TEI compliant critical apparatus has been carried out through an XSLT stylesheet (Fig. 6). During the transformation phase from XML to TEI-XML, the philologist can choose which elements represent in the encoding and which to rule out (punctuation, separators and so forth). The encoding of both the apparatuses rely on the TEI model of critical apparatus: the apparatus of the digital edition of Qohelet has been encoded with the parallel segmentation method, while the encoding of Kennicott's collation follows the location-referenced method. 5 Results So far, the first three out of twelve chapters of Qohelet have been collated. Kennicott's collation has been totally digitalized and automatically encoded. The critical apparatus of both the collations hosted in Euporia have been successfully converted into a compliant TEI file. Using XSLT stylesheets, it is possible to re-convert the TEI file back to our DSL, without loss of information. TEI encoding schemes and our DSL are therefore isomorphic. The implementation of AstToXmlVisitor in JavaScript language represents an important point of the work-flow. It allows to automatically create a well-formed XML file from any AST and is therefore applicable to different DSLs. It is written once and for all by the computer scientist and needs no further customization according to the input file. Such a division of tasks meets the needs and habits of digital philologists, who are generally more accustomed to manipulating XML code, rather than working with general-purpose programming languages. 11 \f(a) TEI apparatus of Qoh. 3:17 (b) TEI apparatus of Qoh. 5:1 Figure 6: TEI compliant XML encoding 6 Discussion There are several advantages in using a DSL for ecdotic purposes. First of all, the compactness of the DSL respect to TEI encoding. The annotation through a DSL is significantly less verbose than TEI annotation, as it can be seen by comparing the number of characters of the traditional apparatus shown in Fig. 1 and the TEI counterpart of Fig. 4 (on the right). Compactness is an important feature: the verbosity of XML language may compromise human readability and make the encoding difficult to handle, especially for traditional scholars not accustomed with long in-line encoded files. Manually encoding is a time-consuming task. Markup vocabularies require a long apprentice time to be mastered; once the encoding is complete, moreover, the encoder must check whether it is internally coherent, perfectly TEI conformant and in line with best practices. The cognitive stress derived from such a mixture of disciplinary content and cross-disciplinary formalism may stray away the world of humanistic academic research from the potentialities of computational technologies, thus contributing to increase the gap between the respective communities of scholars. A DSL-based approach, on the contrary, is entirely domain-centered: the scholar is not compelled to acquire skills which fall outside his or her cultural background, nor to make his or her research practices adhere to external, technology-conditioned standards. It is up to the digital philologist, who best knows how to organize the data according to standards, to create a perfectly conformant TEI encoding from the results of the XML general exporter. Finally, the DSL may represent a good way to exercise tighter control not only on transcriptional errors, but also on semantic errors. Thanks to the tokenization, indeed, the parser is able to assign a semantic value to each apparatus component directly from the data type to which it belongs: so, for example, tokens such as 'omit' (abridgement for Latin 'omittit', non-capital character), 'K1' (capital alphabetic character + integer), 'McNeile(1904)' (string of alphabetic characters, integers and punctuation), will always be parsed differently and automatically assigned to different XML tags or attributes (In TEI encoding, respectively, @ana, @wit, @resp). In a manual encoding, on the other hand, the encoder must decide, each time, which tags or attributes are more suitable for expressing his or her interpretations of textual phenomena: this may often lead to an incoherent or erroneous choice of markers and increase the possibility of semantic errors, which are very difficult to be detected. In a DSL-based approach, on the contrary, the choice of markers is not entrusted to human decision, but it is determined by the form of the apparatus components and automatically performed by the Visitor."
	},
	{
		"id": 3,
		"title": "600 maestri raccontano la loro vita professionale in video: un progetto di (fully searchable) open data",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Gianfranco Bandini",
			"Andrea Mangiatordi"
		],
		"body": "1 Tematiche e obiettivi della ricerca Il progetto 'memorie di scuola' è basato sul Content Management System 'WordPress' (https:\/\/wordpress.org), una piattaforma sulla quale si basa \u2013 secondo le stime dei suoi autori e sviluppatori \u2013 circa un terzo dei siti web mondiali. La nostra implementazione, utilizzabile attraverso qualsiasi web browser, è stata adattata a esigenze molto specifiche, in modo da consentire la costruzione di una memoria collettiva della vita scolastica nella scuola primaria in Italia. Al momento raccoglie oltre 600 interviste a maestre e maestri in pensione (o prossimi ad essa) che raccontano con molti dettagli e grande passione la loro vita professionale, a partire dagli anni \u201840. Il nostro intento è quello di migliorare la piena disponibilità dei video (che costituiscono un ampio insieme di open data) e la loro completa fruibilità attraverso un sistema di indicizzazione analitico. I video, così come le attività formative e didattiche connesse, sono indirizzati in primo luogo agli insegnanti in servizio e in formazione, ma anche a un più ampio pubblico interessato agli aspetti educativi della nostra storia sociale. In questa sede presentiamo quindi l'implementazione di una particolare feature del sito web che consente di effettuare una ricerca full-text all'interno di tutte le parole pronunciate in tutti i video che compongono il progetto (e di collegarsi ad essi nel preciso istante durante il quale il soggetto pronuncia la parola cercata). 1  Gli autori hanno lavorato alla stesura del testo confrontandosi e concordando ogni sua parte. Gianfranco Bandini si è dedicato in particolare alle sezioni 1, 2 e 4; Andrea Mangiatordi ha curato in particolare le sezioni 3 e 3.1.  14  \f2 Quadro teorico di riferimento Nel corso del Novecento la storia orale ha affermato, non senza contrasti e opposizioni, la sua legittimità e utilità, soffermandosi soprattutto sulla storia dal basso, degli esclusi dalla storia tradizionale (Gardner, & LaPaglia, 2006). Il settore di studi che si occupa della storia della scuola, all'interno della storia dell'educazione (McCulloch, 2011), ha utilizzato con sempre maggiore convinzione fonti non testuali, come le fotografie o i dipinti. Una piccola parte di queste ricerche ha inoltre scoperto, anche se con un certo ritardo, l'utilizzo delle fonti orali, cioè la raccolta delle testimonianze (Gardner, 2003). La memoria personale ha consentito di spostare l'accento degli studi sulle percezioni e sui sentimenti delle persone, sugli aspetti interiori e comunitari della vita sociale. La storia dell'educazione e la storia orale, in questa forma congiunta, hanno trovato un campo di nuova e eccezionale sperimentazione nell'ambito della digital public history, nel quale il presente progetto si colloca (Bandini, 2017). Nel contesto digitale, l'incrocio di queste diverse tradizioni di ricerca dà la possibilità di potenziare la comune aspirazione a un maggiore contatto tra il mondo accademico e la società, soprattutto per quanto riguarda il rapporto con le professioni educative e di cura (cfr. Depaepe, 2001; Linné, 2001; Vinovskis, 2015). La trasformazione delle classiche fonti storiche in open data risponde proprio a questo ambizioso obiettivo. Il progetto consente, oltre alla piena e completa disponibilità online delle testimonianze, di superare una delle principali limitazioni che qualsiasi tradizionale ricerca che fa uso di storia orale si trova a affrontare (cfr. Ritchie, 2011): la numerosità delle testimonianze e la gestione della massa dei dati che possono esserne ricavati (intesi anche come momenti di titubanza del testimone, silenzi, salti temporali, ecc.). A questo proposito, uno dei punti di discussione metodologica consiste, ad esempio, nel riflettere sulla questione del numero ottimale di testimonianze. In alcuni casi (cfr. l'ampio studio di Johnson & Reuband, 2008), in abbinamento a ricerche di tipo quantitativo, si è stabilito un vero e proprio piano di campionamento, come di consueto nelle ricerche statistiche. In generale, tuttavia, nelle ricerche storiche si sostiene che la ricerca di nuove testimonianze si può arrestare nel momento in cui si satura il campo concettuale che stiamo indagando, cioè quando nuove testimonianze non porterebbero più nulla di nuovo, o insignificanti dettagli, rispetto a quanto già detto. Nel contesto della storia digitale il panorama risulta ampiamente modificato perché la raccolta delle testimonianze può essere espansa senza eccessiva fatica e scarso dispendio di risorse (cfr. Thomson, 2007). Questo aspetto ci consente di progettare una raccolta di testimonianze aperta, nel senso che può essere incrementata nel tempo e offrire sempre nuove opportunità di conoscenza e formazione. La cassetta degli attrezzi dello storico e l'insieme delle sue fonti vengono così trasportate da uno spazio privato a uno spazio pubblico. Nell'ottica della public history, il contesto digitale può quindi essere progettato non soltanto per la collocazione online di fonti primarie (come sono le interviste), ma per consentire lo sviluppo delle interazioni tra gli utenti. Per superare l'impostazione tradizionale, che trasforma le fonti online in musei statici (per quanto digitali e più facilmente accessibili) c'è bisogno di alcuni strumenti di base, che sono volti a aumentare la significatività delle fonti. La trascrizione automatica dei video, in questo senso, rappresenta un tassello fondamentale della strategia comunicativa, a vantaggio della piena fruibilità da parte degli utenti.  3 Metodologia La raccolta delle testimonianze video presenta delle particolari caratteristiche rispetto alla raccolta di documenti testuali (per esempio diari o autobiografie). Il repository 'memorie di scuola' attualmente supera le 5.000 ore di filmato e rende oggettivamente molto difficile sia l'ascolto integrale, sia una completa attività di indicizzazione manuale basata sull'inserimento di tags (per esempio quelli contenuti in TESE, Thesaurus Europeo dei Sistemi Educativi). Nella prospettiva della public history il numero delle ore è inoltre destinato a crescere ancora: in questa situazione la messa a disposizione del pubblico, con una piccola serie di tags di indicizzazione, di fatto non consente l'accesso alla ricchezza documentaria contenuta nei video che diventa quasi casuale. Mantenendo la prospettiva fin qui esposta, il progetto si è indirizzato a cercare di produrre un archivio video liberamente accessibile attraverso l'uso di risorse sostenibili \u2013 per lo più software Open Source \u2013 e l'automazione di una serie di operazioni che permettono di facilitare la ricerca all'interno della grande mole di contenuti raccolti. Il servizio online YouTube (https:\/\/www.youtube.com), famoso per essere tra i principali  15  \frepository gratuiti di contenuti video online, consente la trascrizione automatica del parlato dalla lingua italiana al fine di produrre sottotitoli. La quantità di errori di trascrizione è altamente variabile e dipendente da molteplici fattori, primo tra tutti la qualità della traccia audio (cfr. Alberti e altri, 2009). Tuttavia, un sistema in grado di estrarre i sottotitoli creati automaticamente dal servizio e di inserirli in un database permette di rendere le interviste esplorabili e ricercabili in modo simile a un corpus testuale. Questo sistema, pur non presentando un'innovazione dal punto di vista degli algoritmi e del software, consente di ottenere un significativo vantaggio rispetto ai sistemi attuali e offre un modello di funzionamento facilmente replicabile e applicabile a grandi masse di dati. Bisogna ricordare che quando sono in gioco grandi quantità di interviste, la trascrizione umana (ovviamente più accurata dei sistemi automatici) solo in rarissimi casi riesce a raggiungere il risultato: è il caso, quasi unico nel suo genere, della grande raccolta di testimonianze condotta dalla Shoah Foundation2.  3.1  Struttura del Software e flussi di lavoro  L'architettura della soluzione software messa in atto per sostenere il progetto Memorie di Scuola consta di servizi e applicazioni Open Source che si interfacciano con il servizio proprietario YouTube per l'hosting dei materiali video e per la produzione di trascrizioni rese disponibili nella forma di sottotitoli e in formati diversi. In particolare, lo stack software utilizzato è basato su: \u2022  Un web server dotato in grado di eseguire codice scritto in linguaggio PHP \u2013 nel caso specifico del progetto è stato utilizzato il software NginX (https:\/\/www.nginx.com\/), ma a questo livello non ci sono requisiti particolarmente stringenti;  \u2022  Un database MySQL (https:\/\/www.mysql.com\/);  \u2022  Il CMS WordPress (https:\/\/wordpress.org);  \u2022  Il plugin Meks Video Importer per WordPress (https:\/\/wordpress.org\/plugins\/ meks-video-importer\/), che permette la ricerca di video disponibili pubblicamente su YouTube e l'importazione automatica del contenuto testuale della loro descrizione;  \u2022  Il plugin Advanced Custom Fields per WordPress (https:\/\/wordpress.org\/plugins\/ advanced-custom-fields\/), che facilita l'inserimento di metadati ai contenuti dei post;  \u2022  Il software YouTube Transcript\/Subtitle API (https:\/\/github.com\/jdepoix\/youtubetranscript-api), uno script in grado di estrarre i sottotitoli da video YouTube pubblicamente disponibili;  \u2022  Lo script 'YouTube transcript to WordPress' (https:\/\/github.com\/andreamangia\/ youtube-transcript-to-wp), progettato specificamente per automatizzare le operazioni.  Il software si inserisce nel workflow descritto di seguito e lo sostiene, minimizzando la necessità di intervento umano ma rendendola comunque possibile in una fase successiva di revisione dei contenuti: \u2022  L'intervistatore effettua l'intervista e esegue l'upload su YouTube, aggiungendo al video una semplice descrizione testuale; \u2022 Un operatore del sito web www.memoriediscuola.it importa il video, nella forma di un nuovo post WordPress, con l'aggiunta di tag ed eventuali altri termini tassonomici, la verifica della congruenza tematica e l'indicazione di dati quali il nome dell'intervistatore, il luogo; \u2022 L'operatore del sito web esegue lo script 'YouTube transcript to WordPress' indicando il numero identificativo del post WordPress generato al punto precedente. Lo script si occupa di: \u25E6 Estrarre in formato JSON la trascrizione dell'audio; La Shoah Foundation gestisce il Visual History Archive, disponibile all'indirizzo https:\/\/ sfi.usc.edu\/vha, con lo scopo di documentare e rinforzare l'empatia verso le memorie di persone che hanno vissuto genocidi e altri drammi. 2  16  \f\u25E6  \u2022 \u2022  Trasformare ciascun frammento della trascrizione (in genere, ma non sistematicamente, corrispondente a una breve frase) in un elemento HTML contenente l'indicazione del momento temporale in cui il testo viene pronunciato nel video; \u25E6 Salvare l'intera trascrizione come valore per un campo personalizzato di WordPress associato al post contenente il video. L'utente effettua una ricerca libera all'interno del repository, che è stato opportunamente configurato per consentire la ricerca anche all'interno dei campi personalizzati, normalmente ignorati dal motore di ricerca interno di WordPress; Il sito web elenca tutti i video che contengono la parola ricercata. Accedendo alla pagina di ciascun video è possibile raggiungere il momento in cui una frase è pronunciata attraverso un click sulla porzione di trascrizione corrispondente.  La selezione degli strumenti operativi che rendono possibile il procedimento è dunque stata pensata per favorire la replicabilità totale dell'esperienza in qualunque sistema basato su WordPress, indipendentemente da elementi quali il tema grafico utilizzato o dalla necessità di modificare l'architettura del database sul quale poggia il sistema. Non è previsto supporto in questa fase per altri CMS, ma non è da escludere che le stesse funzionalità e la stessa logica di lavoro possano essere applicate anche altrove, data la presenza di diversi layer basati su tecnologie standard e interoperabili.  4 Risultati attesi e interventi futuri Il progetto ha come obiettivo principale la costruzione di un set di open data costituiti da testimonianze video, liberamente accessibili sul web e esplorabili in profondità attraverso gli strumenti di ricerca del parlato sopra descritti. Questo obiettivo, del resto, è propedeutico a molte azioni formative che possono essere svolte proprio grazie alla possibilità di trovare all'interno dei video esattamente ciò che stiamo cercando, siano esse parole generiche o identificatori di luogo o di persona. L'insieme delle trascrizioni si presta inoltre a successive analisi e esplorazioni con software di data mining (per esempio T-Lab) o di categorizzazione dei testi (per esempio Nvivo), all'interno del paradigma di ricerca della Grounded Theory. Per quanto l'accuratezza delle trascrizioni possa essere migliorata, già allo stato attuale i testi dei video appaiono ben comprensibili e di grande aiuto per effettuare delle analisi approfondite. Nel caso di uno studio volto alla categorizzazione dei testi, un ulteriore passaggio manuale di correzione potrebbe portare a un corpus assestato e corretto in tempi ragionevolmente brevi. Tenendo conto che i software di analisi dei testi sono di fatto degli strumenti semi-automatici, questo tipo di operazione risulta essere di carattere ordinario. Bisogna inoltre considerare che il presente progetto, in modo del tutto automatico (attraverso il citato plugin Meks Video Importer per WordPress), si gioverà dei miglioramenti nel riconoscimento del parlato che verranno implementati nella piattaforma YouTube. Lo sviluppo dei sistemi di Intelligenza Artificiale ha dato prova, in questi ultimi anni, di consentire dei progressivi e tangibili miglioramenti nella comprensione del parlato, a partire dalla lingua inglese (riconosciuta nelle sue molte varianti di pronuncia). L'insieme delle trascrizioni, infine, può costituire la base per una piattaforma partecipativa che permetta di dare avvio a progetti di crowdsourcing di correzioni e integrazioni alle trascrizioni automatiche, aumentando ulteriormente la sostenibilità del progetto; inoltre, in linea con l'approccio di public history fin qui adottato, sarà possibile dare la possibilità agli utenti di apporre commenti ai video, commenti che a loro volta verranno trascritti e inseriti in una mappa esplorabile per concetti chiave. Dal punto di vista tecnico è possibile pensare in tempi brevi all'integrazione di altre due tecnologie Open Source. La prima di queste è la piattaforma di ricerca Apache Solr (https:\/\/lucene.apache.org\/ solr\/): questa mette a disposizione un motore di indicizzazione più raffinata rispetto alla semplice ricerca testuale disponibile nel CMS WordPress e permetterebbe di avere risultati di ricerca rispondenti a query di tipo diverso (con operatori booleani, stemming, proximity search). La seconda tecnologia potenzialmente utilizzabile è Hypothes.is (https:\/\/web.hypothes.is\/), software Open Source di annotazione di pagine web, che renderebbe possibile appunto l'annotazione dei contenuti, in modalità aperta e collaborativa da parte di ricercatori diversi.  17  \fRingraziamenti Si ringrazia sentitamente il prof. Gianfranco Crupi per gli utili consigli e suggerimenti; Inclusive Cloud S.r.l.s. per aver messo a disposizione competenze e infrastrutture informatiche; si ringraziano vivamente tutti i maestri che hanno cortesemente messo a disposizione il racconto della loro vita professionale e tutti gli studenti del corso di laurea in scienze della formazione primaria (università di Firenze) che hanno raccolto, con pazienza e serietà, le testimonianze in video."
	},
	{
		"id": 4,
		"title": "Ripensare i dati come risorse digitali: Un processo difficile?",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Nicola Barbuti"
		],
		"body": "Introduzione  L'art. 2 delle Conclusioni del Consiglio dell'UE del 21 maggio 2014 sul patrimonio culturale come risorsa strategica per un'Europa sostenibile (2014 \/ C 183\/08) recita1: Il patrimonio culturale è costituito dalle risorse ereditate dal passato in tutte le sue forme e aspetti \u2013 tangibile, intangibile e digitale (nato digitale e digitalizzato), inclusi monumenti, siti, paesaggi, abilità, pratiche, conoscenze ed espressioni della creatività umana, nonché raccolte conservate e gestite da enti pubblici e privati come musei, biblioteche e archivi. Ha origine dall'interazione tra persone e luoghi nel tempo ed è in continua evoluzione. Queste risorse sono di grande valore per la società dal punto di vista culturale, ambientale, sociale ed economico e quindi la loro gestione sostenibile costituisce una scelta strategica per il 21\u00B0 secolo. Partendo da questo presupposto, dobbiamo necessariamente cambiare il nostro approccio al digitale e alla digitalizzazione iniziando a considerarli rappresentazioni qualificanti l'età contemporanea e la digital transformation che la connota. Ciò implica l'urgenza di individuare e classificare tra le risorse digitali prodotte fino a oggi e in produzione, siano esse singoli oggetti, o complesse digital libraries, o sistemi 3D, quelle che possono essere identificate come il nuovo Digital Cultural Heritage (DCH), distinguendole da quelle prodotte per mera semplificazione di processi gestionali o per la fruizione estemporanea e immediata di scadenti rappresentazioni relative a entità tangibili e intangibili. https:\/\/eur-lex.europa.eu\/legal-content\/EN\/TXT\/?uri=CELEX%3A52014XG0614%2808%29  1  19  \fDa diversi anni, la digitalizzazione e la qualità e preservazione delle risorse digitali sono riconosciute tra le principali emergenze da affrontare in tutto il mondo. Nel 2012 l'UNESCO ha tenuto la sua conferenza a Vancouver con il significativo titolo The Memory of the World in the Digital Age: Digitization and Preservation (Duranti and Shaffer [ed. by], 2012), nel cui ambito è stata redatta ed emanata la Vancouver Declaration on Digitisation and Preservation2, con l'IFLA e l'International Council of Archives (ICA) tra i principali responsabili. Da allora, la situazione non sembra essere molto cambiata, nonostante gli sforzi intrapresi per accelerare l'elaborazione di soluzioni a criticità di una complessità che, forse, non hanno precedenti storici. Gli attuali approcci ai processi di creazione delle risorse digitali, infatti, sembrano non recepire l'evoluzione che negli ultimi anni ha riguardato la digitalizzazione, ancora oggi associata semplicisticamente alla riproduzione fotografica, mentre, invece, è diventata un processo complesso guidato da regole definite e condivise. Anche l'importanza della qualità dei dati digitali è del tutto sottovalutata nel relegarne la funzione a meri strumenti di mediazione per la fruizione del il reale in forma virtuale, sebbene da più parti si riconosca che questi dovrebbero rispondere a requisiti di intellegibilità, affidabilità, pertinenza, persistenza, e registrare le trasformazioni delle funzioni legate al loro riutilizzo nel tempo. L'interpretazione strumentale, infatti, ancora orienta e condiziona negativamente soprattutto la strutturazione degli schemi di metadati con cui indicizzare gli oggetti digitali prodotti e la composizione delle descrizioni loro associate, formulate per essere meri codici funzionali esclusi-vamente alla ricerca e al recupero dei dati in rete. Proprio i metadati e i contenuti descrittivi, invece, dovrebbero essere oggetto di particolare attenzione, in quanto sono la sola possibilità di registrare e rappresentare in modo intellegibile i processi di digitalizzazione, creazione e trasformazione che caratterizzano il ciclo di vita dei dati, e di conservare così le informazioni necessarie a conoscerli e a qualificarli come risorse digitali con funzioni anche culturali. Il tema della funzione essenziale dei metadati nel management e nella fruizione dei dati digitali è il focus dei FAIR Guiding Principles for Scientific Data Management and Stewardship3, le linee guida per il management dei dati scientifici pubblicate nel 2016, da qualche tempo uno dei temi di maggior interesse nell'ambito del più ampio dibattito sulle possibilità di applicare le metodologie del data science alla creazione e gestione dei data humanities4. A riguardo, nella CIDOC Conference 2018 si è tenuto un workshop sull'effettiva efficacia dei FAIR Principles rispetto agli scenari che oggi la digitalizzazione propone, e soprattutto rispetto a quelli che già si preannunciano imminenti5. Il presente lavoro sintetizza alcune riflessioni maturate da quel proficuo confronto, relative alla necessità di provvedere a un ampliamento del requisito R: Reusable nei requisiti R5: Reusable, Readable, Relevant, Reliable and Resilient, finalizzato a facilitare l'applicabilità dei FAIR Principles ai data humanities e, conseguentemente, l'identificazione e certificazione come DCH dei dati rispondenti a tali requisiti nell'informe magma digitale in cui oggi fluttuiamo.  2  Verso un ampliamento dei FAIR da R a R5  L'assunto di partenza per avviare la riflessione è che i dati digitali non possono più essere creati finalizzandoli alla mera funzione di strumenti di mediazione per una fruizione della realtà alternativa a quella fisica: è necessario ripensarli quali risorse digitali che si qualificano come record, entità dinamiche e diacroniche che registrano e conservano nelle descrizioni i processi di digitalizzazione che li hanno creati e quelli che hanno caratterizzato il loro successivo ciclo di vita.  http:\/\/www.unesco.org\/new\/fileadmin\/MULTIMEDIA\/HQ\/CI\/CI\/pdf\/mow\/ unesco_ubc_vancouver_declaration_en.pdf 2  https:\/\/www.go-fair.org\/fair-principles\/ https:\/\/www.rd-alliance.org\/open-consultation-fair-data-humanities-until-15thjuly-2019; https:\/\/www.gofair.org\/implementation-networks\/overview\/co-operas\/; https:\/\/ operas.hypotheses.org\/ 5 http:\/\/www.cidoc2018.com\/sites\/default\/files\/CIDOC2018-BookOfAbstracts-Final-v-1-2.pdf 3  4  20  \fI metadati descrittivi diventano perciò fondamentali e inscindibili dagli oggetti digitali, in quanto sono proprio l'accuratezza e la qualità delle descrizioni a qualificarli come record e, quindi, a renderli risorse digitali pensate e strutturate per essere fruite diacronicamente dagli utenti del futuro, che devono comprendere cosa il dato rappresenta alla pari degli utenti contemporanei. L'adozione dei FAIR Principles lascia aperte alcune questioni. Innanzitutto, non siamo del tutto persuasi che Ricercabilità (Findable), Accessibilità (Accessible, che assolutamente non è identificabile con Open) e Interoperabilità (Interoperable) siano requisiti idonei a qualificare i dati come record e risorse digitali, conferendogli funzioni nuove e più evolute da quelle strumentali attualmente riconosciute. Un dato che sia ricercabile, accessibile e interoperabile con altri non fornisce alcuna garanzia di qualità, sufficienza e affidabilità dei contenuti informativi che contiene. Inoltre, la ricercabilità e, di conseguenza, l'accessibilità e interoperabilità che sono a essa vincolate hanno senso nella misura in cui un dato sia oggetto di interesse da parte dei fruitori. E l'interesse per un dato è legato strettamente non alla sua mera funzione di chiave d'accesso a un oggetto digitale semplice o complesso, ma all'essere risorsa informativa e cognitiva per la quantità e qualità dei contenuti descrittivi che mette a disposizione dell'utente già in fase di lettura dei suoi metadati. Siamo perciò del parere che il requisito che conferisce significato e senso ai primi tre, e dal quale questi dipendono indissolubilmente, sia la Riutilizzabilità (Reusable). L'utilizzo e, soprattutto il riutilizzo dei dati, infatti, sono secondo noi i fattori che ne garantiscono la sostenibilità nel tempo e, quindi, la sopravvivenza, in quanto requisiti caratterizzati da dinamismo e diacronia che, quasi sem-pre, implicano trasformazioni nelle funzioni delle entità che ne sono oggetto: per avere un'idea uti-lizzando un paradigma analogico, si pensi al Colosseo e al suo ciclo di vita. Le registrazioni descrittive dei metadati sono perciò fondamentali per garantire qualità e persistenza delle risorse digitali, qualora siano improntate a equilibrate soluzioni quantitative\/qualitative e rispondano a ulteriori requisiti che, secondo noi, sono altrettanto essenziali quanto la riutilizzabilità. Anche la Reusability, infatti, di per sé non costituisce una garanzia di qualità del dato e del suo valore quale risorsa informativa e cognitiva. Anzi: proprio le variabili cui una risorsa è soggetta perché riutilizzabile possono essere fonte di distorsione e difformità dei contenuti, il cui valore informativo e cognitivo può perciò non essere più certificabile come affidabile. La R di Reusable andrebbe perciò, secondo noi, ampliata in R5 con i seguenti requisiti: -  -  -  -  Readability: da intendersi non nell'accezione semantica di leggibilità, ma in quella concettuale di intellegibilità della risorsa digitale per tutte i possibili target di utenti interessati a fruirne; è requisito fondamentale per conferire ai metadati la funzione informativa e cognitiva necessaria a qualificarli come risorsa culturale, e si basa sull'equilibrato rapporto quantitativo\/ qualitativo dei contenuti descrittivi e sull'accuratezza formale, stilistica e linguistica dei contenuti; Relevance: la persistenza nel tempo è legata all'interesse degli utenti per i contenuti informativi e cognitivi registrati nella risorsa; essa è strettamente legata al suo riutilizzo e alle possibili trasformazioni di funzione registrate nelle descrizioni; è, quindi, requisito indispensabile affinché la risorsa, di solito creata con funzioni e scopi non necessariamente culturali, possa essere identificabile e riconoscibile nella sua struttura formale e descrittiva anche se varia nel tempo le proprie funzioni evolvendosi in fonte di conoscenza sui processi che registra e, quindi, in risorsa culturale digitale; Reliablility: l'affidabilità è la certificazione e validazione della qualità della risorsa digitale rilevabili dalle registrazioni delle sue descrizioni durante tutto il suo ciclo di vita, in relazione a tutte le possibili trasformazioni ed evoluzioni funzionali cui può essere stata soggetta; è, dunque, strettamente connessa alla capacità dell'entità digitale di registrare e preservare gli elementi qualificanti la qualità informativa e cognitiva dei suoi contenuti descrittivi, anche nell'evoluzione delle funzioni e nelle variazioni di forme e funzioni nel tempo; Resilience: come l'intellegibilità, anche la resilienza applicata ai dati e, soprattutto, ai metadati è requisito fondamentale per conferire alle risorse digitali la nuova dimensione culturale;  21  \fchiosando la definizione comunemente in uso in ambito informatico6, essa va intesa come la capacità di una risorsa digitale di adattarsi alle condizioni di utilizzo e riutilizzo, di resistere all'usura, di essere duttile nelle trasformazioni e nell'evoluzione delle sue funzioni, al fine di garantire la disponibilità del proprio potenziale cognitivo e informativo nello spazio e nel tempo; è, quindi, indispensabile per garantire la sostenibilità e il riutilizzo delle risorse digitali nel medio-lungo termine, provvedendo a preservare sia le informazioni utili a conoscere i processi della loro creazione, sia quelle sulla loro funzione originale, sia, infine, le registrazioni delle trasformazioni ed evoluzioni funzionali che ne hanno caratterizzato il ciclo di vita.  3  Conclusioni  Tirando le conclusioni su quanto sopra sinteticamente delineato, è nostra opinione che l'adozione dei requisiti FAIR con la R ampliata in R5 sia prerequisito indispensabile nel processo di creazione dei dati digitali e, soprattutto, dei metadati che li descrivono, in quanto gli conferirebbero le funzioni di potenziale DCH, rendendoli sostenibili, permanenti, affidabili e, nel contempo, storicizzandoli come fonti di conoscenza dei processi e delle complessità che caratterizzano la rapidissima evoluzione della digital transformation. Non il dato in sé, infatti, ma l'interesse degli utenti presenti e futuri per la fruizione del dato in quanto risorsa informativa e cognitiva deve diventare il prerequisito su cui fondare l'intero processo di creazione, pubblicazione e preservazione di risorse digitali. L'applicazione dei requisiti R5, dun-que, deve diventare oggetto di attenzione fin dalla fase di analisi e progettazione dei processi sia di digitalizzazione che di creazione di qualsiasi schema di metadati con cui descrivere e gestire gli oggetti digitali in produzione. Solo così si potrà dare un serio inizio, nel medio termine, all'individuazione di quanto possa essere identificato come DCH nella massa di dati che oggi sovrabbonda nel web e, nel contempo, si potranno definire linee guida omogenee e condivise che presiedano alla creazione di nuove risorse avendo chiaro fin dal principio se gli si voglia conferire il potenziale valore di entità culturali. In questo modo, nel giro di pochi anni le Conclusioni EU del 2014 potranno finalmente essere sostanziate con un nuovo DCH ufficialmente riconosciuto. Diversamente, continueremo a considerare digitalizzazione e digitale solo come un modo diverso e accattivante di fruire il tangibile, perdendo di vista quanto invece tutto ciò sia già oggi l'humus identitario che, pur a livelli diversi, identifica l'era digitale contemporanea."
	},
	{
		"id": 5,
		"title": "Verso il riconoscimento delle Digital Humanities come area scientifica: Il catalogo online condiviso delle pubblicazioni dell’AIUCD",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Nicola Barbuti",
			"Maurizio Lana",
			"Vittore Casarosa"
		],
		"body": "Introduzione  Nell'ambito della Conferenza AIUCD 2019, uno dei temi di maggior confronto, anche in seno all'Assemblea annuale dei soci, è stata la necessità ormai non più rinviabile di attivare iniziative finalizzate a riconoscere alle Digital Humanities \u2013 o Humanities Computing che si vogliano ridefinire \u2013 la dignità di Area Scientifica, emancipandole definitivamente dalla dimensione di come nebulosa in cui fluttuano in modo frammentario e caotico ricercatori e studiosi rinvenienti da una pletora di SSD tradizionali delle Humanities, in cui sono considerati delle entità scientifiche ancipiti. L'analisi dello stato dell'arte della disciplina ha evidenziato la consistenza decisamente ampia di contributi scientifici che, pur riferendosi ad ambiti umanistici riconducibili a singoli SSD tradiziona-li, sono confluenti nella comune prospettiva di ricerca su digitale e computazionale applicati alle humanities e, perciò, assolutamente associabili in un un'unica area di contenimento. Pertanto, al fine di sostanziare l'istanza di riconoscimento delle DH come Area con una sua digni-tà scientifica, si è scelto un indirizzo operativo 'dal basso': produrre un catalogo digitale delle pub-blicazioni nazionali di settore che possa essere riconosciuto dall'AIUCD quale proprio riferimento ufficiale e, in prospettiva, possa diventare nodo di un più ampio e condiviso catalogo internazionale di pubblicazioni sulle DH. Il Gruppo di Lavoro individuato per occuparsi di progettare e strutturare il catalogo è composto dai soci AIUCD Maurizio Lana, Vittore Casarosa e Nicola Barbuti. Il GdL ha intrapreso le attività immediatamente dopo la chiusura del Convegno 2019 e attualmen-te sta provvedendo agli ultimi passaggi per la realizzazione esecutiva di quanto progettato e proposto all'Associazione agli inizi dell'estate 2019. 24  \f2  Il Catalogo delle DH AIUCD  Primo tema di riflessione è stato definire i limiti geografici e cronologici delle pubblicazioni da inserire in prima istanza. Si è deciso di limitare inizialmente l'inserimento agli autori italiani con priorità per i soci AIUCD, con un orizzonte temporale non superiore agli ultimi 10 anni. \u00C8 stata ipotizzata la creazione dello spazio online con credenziali di accesso da condividere con tutti i soci in modo che, per quanto possibile, ognuno possa inserire da sé i record bibliografici relativi alle proprie pubblicazioni, e di consentire anche l'associazione dei pdf ai record, ove legalmente disponibili. Tuttavia, per conferire al catalogo veste ufficiale, pur consentendo a ciascuno di inserire i dati direttamente, sembra opportuno stabilire regole definite e condivise, nominando un organismo deputato a eseguire un controllo annuo sulla coerenza delle nuove risorse bibliografiche caricate per evitare l'insorgere di situazioni caotiche. Dal momento che una delle principali criticità connesse con la multiforme produzione scientifica delle DH è proprio l'elevata varietà di fonti e quindi di formati citazionali bibliografici, si è passati ad analizzare software open source per la gestione e la fruizione di record bibliografici digitali che avessero le caratteristiche necessarie a favorire un import di risorse di diversa tipologia e struttura. Primi a essere presi in considerazione sono stati Zotero, in considerazione sia del fatto che l'AIUCD utilizzando questo software aveva già attivato un repository per l'allocazione di risorse digitali relative alle DH sia dell'ampio uso che se ne sta facendo per la creazione di bibliografie on line con pubblicazioni relative ad altri ambiti di ricerca scientifica, e Zenodo per il repository delle fonti non altrimenti online. Zotero nacque proprio come strumento per la gestione di bibliografie di area storica (Cohen, 2008) ed è attualmente usato, per esempio, dall'associazione tedesca di Digital Humanities1 per uno scopo simile a quello cui stiamo pensando anche per AIUCD o dalla American School of Classical Studies at Athens (ASCSA) per catalogare e gestire i metadati di tutte le pubblicazioni della Scuola stessa (libri e articoli)2. Tuttavia, quest'ultimo è risultato un open repository per \"prodotti della ricerca\" generici ed ha evidenziato il limite non secondario che ogni oggetto caricato deve essere linkato a mano al record corrispondente. Zotero invece è mirato a collezioni di articoli e bibliografie di varia natura e i dati caricati in uno suo spazio on line non sono soggetti al problema rilevato per Zenodo (O'Donnell, Manola, Manghi, Porter, Esau, Viejou, Rosselli Del Turco, and Singh, 2018; Peters, Kraeker, Lex, Gumpenberger and Gorriaz, 2017). \u00C8 sufficiente, infatti, che il responsabile del caricamento tagghi la pubblicazione con l'indicazione del\/dei SSD in cui si colloca e con le keywords che egli ritiene ne identifichino correttamente il contenuto per rendere il record interrogabile anche in questo modo oltre che con i consueti criteri di autore, titolo, etc. Inoltre, altre caratteristiche interessanti di Zotero sono la separazione dei (meta)dati dalla loro presentazione secondo uno stile citazionale piuttosto che un altro, la possibilità di esportare in RDF e altri formati open il database della bibliografia. Altro punto a favore di Zotero è la possibilità di definire modalità di recupero delle informazioni bibliografiche incrociando i seguenti dati: - dati dei titoli; - dati delle keywords inserite ufficialmente nei lavori o, in mancanza, indicate espressamente dagli autori rilevando parole poi riscontrabili nel testo; - dati che possono essere estratti dagli abstract o dai full text dei contributi inseriti; - SSD degli autori. Queste rilevazioni possono essere utilizzate per produrre report annui sullo stato dell'arte della ricerca scientifica sulle DH da ufficializzare per rimarcare la fertilità produttiva del settore. Per iniziare a popolare il catalogo, si è concordato di proporre ai soci (e non, purché italiani) vari modi per inserire i dati nella bibliografia (Vahdati, Arndt, Auer and Lange, 2016): https:\/\/www.zotero.org\/groups\/372575\/dhd_ag_publikationen https:\/\/www.zotero.org\/groups\/80651 american_school_of_classical_studies_at_athens  1  2  25  \f-  coloro che usano un Bibliographic Reference Software (BRS: Zotero, Mendeley, Endnote, Bibref, Refworks, etc.), possono esportare i (meta)dati citazionali delle loro pubblicazioni completi di URL a ciascuna pubblicazione, quindi inserirli in Zotero e dare l'accesso pubblico per consentire la fruizione diretta delle risorse caricate; - coloro che possono utilizzare l'ISBN per le monografie o il DOI per gli articoli possono inserire direttamente nella bibliografia online, utilizzando Zotero, i dati delle loro pubblicazioni, completandoli con i tag che indicano il SSD e quant'altro può essere necessario al recupero del record. Per tutte le forme di pubblicazione grigia (presentazioni, abstract, raccolte di dati, etc.) Zotero non può gestire in modo ottimale i dati perché gestisce per lo più risorse bibliografiche. Giunge utile a questo punto Zenodo, poiché assegna automaticamente un DOI alle pubblicazioni o alle fonti in genere che non lo hanno già, e quindi permette di salvare nel suo repository aperto anche altri prodotti della ricerca, come dati e software (Potter and Smith, 2015), oltre alle classiche pubblicazioni. Si è dunque concluso di creare il catalogo sfruttando al massimo le diverse opportunità offerte dai due software, creando una soluzione che, non comportando attività di sviluppo software ad hoc, dia ragionevoli prospettive di sostenibilità e permetta agevoli importazione, esportazione e migrazione dei dati. Di seguito riportiamo l'articolazione dell'ipotesi progettuale, che si articola nelle seguenti fasi. Base di partenza sarà il catalogo AIUCD già esistente in Zotero, sebbene esso presenti alcuni fisiologici punti di debolezza (duplicazioni, item incompleti, assenza di rimando con link alle fonti dove disponibili in OA, etc.). I cataloghi Zotero sono accessibili in lettura-scrittura per gli editors, in sola lettura per tutti quelli che hanno il link. Per facilitare l'acquisizione e l'inserimento dei dati si partirà dalle pubblicazioni che hanno già il DOI, quindi seguiranno, in ordine progressivo, quelle che hanno ISBN, poi quelle con ISSN, infine le pubblicazioni i cui dati devono essere raccolti e inseriti manualmente. Le pubblicazioni cosiddette 'grigie' prive di DOI (a es.: presentazioni) saranno invece preliminarmente caricate in Zenodo, di modo che il sistema le renda accessibili assegnandovi un DOI. Scegliere di gestire le risorse digitali associando Zotero e Zenodo, oltre a presentare il vantaggio di rendere le pubblicazioni inserite facilmente ricercabili utilizzando chiavi di accesso uniformi, consente di collocare il catalogo in un contesto di ricerca aperta. Ciascun socio\/autore, quindi, potrà provvedere all'inserimento dei propri dati nel modo che segue: - chi ha pubblicazioni in OA ma senza DOI le carica in Zenodo per ottenere il DOI; chi ha pubblicazioni già provviste di DOI comunica la lista dei DOI, uno per riga, mandando un messaggio all'indirizzo email dedicato pubblicazioni@aiucd.it; se la pubblicazione non ha keywords internamente, si possono indicare fino a un massimo di 5 keyword (parole o espressioni) accanto al DOI, separate da virgole; a riguardo, una buona chiave identificativa può essere il (o i) SSD in cui l'autore ritiene si collochi la sua pubblicazione nello spazio cloud di zotero; - chi ha libri manda gli ISBN; - chi ha articoli senza DOI manda l'URL. Le pubblicazioni provviste di DOI saranno caricare in Zotero utilizzando il codice. Il DOI dovrà essere presente e ben visibile nel catalogo pubblico, in quanto è la chiave di accesso principale alla pubblicazione inserita. Relativamente alle necessità di gestione del catalogo, si prospettano le seguenti soluzioni. Sarà necessario ridefinire chi avrà accesso in scrittura al catalogo sia per inserire i DOI in Zotero e costruire l'elenco, sia per intervenire a correggere eventuali errori nei dati inseriti. Un accesso indiscriminato, infatti, creerebbe rischi di rumore notevole e soluzioni caotiche e difformi nell'organizzazione dei dati. Ogni inizio d'anno, i soci saranno invitati a inviare all'indirizzo sopra indicato i DOI e gli ISBN delle nuove pubblicazioni dell'anno precedente.  26  \fDal momento che esiste un problema concreto di autodefinizione sulla pertinenza delle pubblicazione al campo DH, al fine di evitare l'afflusso nel catalogo di pubblicazioni che poco hanno a che fare con il settore sarà necessario che il Direttivo AIUCD definisca delle linee guida sui temi di ricerca coerenti con esso. Una volta analizzato l'impatto del catalogo nell'ambito della ricerca scientifica (Sample, 2011), si potranno prendere in considerazione altri aspetti che renderanno necessarie opportune implementazioni: si pensi, nello specifico, alle pubblicazioni senza codici, che sono in linea di massima quelle più datate e richiedono notevole lavoro per essere inserite nel catalogo. Resta da definire come agire operativamente, cioè chi si occuperà di inserire i DOI in Zotero per costruire l'elenco: potrebbe essere un'attività laboratoriale per studenti di biblioteconomia\/scienze biblioteconomiche e dell'informazione o per i futuri digital librarians?  3  Conclusioni  Tirando le conclusioni sul catalogo progettato, siamo del parere che un'integrazione Zotero-Zenodo in un'unica soluzione on line, sfruttando al massimo l'assegnazione DOI e, in prospettiva, la funzione di repository del secondo, prende il massimo da due mondi open source. Ciò apre opportunità di ana-lisi della ricerca del settore prima impossibili da attuare (Winslow, Rains, Skripsky and Kelly, 2016), e si configura come un elemento qualificante il riconoscimento delle DH come settore di primo livel-lo nella ricerca italiana."
	},
	{
		"id": 6,
		"title": "Il trattamento automatico del linguaggio applicato all'italiano volgare. La redazione di un formario tratto dalle prime dieci Lettere di Alessandra M. Strozzi",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Ottavia Bersano",
			"Nadezda Okinina"
		],
		"body": "1 Introduzione Il presente contributo si propone di coniugare le tecniche offerte dal Trattamento automatico del Linguaggio (TAL) allo studio dell'italiano volgare, e nella fattispecie a testi di origine fiorentina del sec. XV. Esso nasce in seno ad alcune difficoltà emerse durante la selezione dei dati testuali di una tesi di dottorato \u2013 ancora in corso di stesura \u2013 tesa a fornire una nuova edizione dell'Epistolario di Alessandra Macinghi Strozzi (14061471): un documento assai conosciuto e apprezzato da chiunque si occupi della civiltà italiana del Quattrocento e che siamo ancora costretti a leggere nell'edizione curata da Cesare Guasti nel 1877 (Guasti, 1877). Una nuova edizione e una nuova analisi linguistico-interpretativa, condotta con gli strumenti più aggiornati, porterà indiscutibili vantaggi anche agli studi di ambito storico e letterario.  2 Le Lettere di Alessandra Macinghi Strozzi Le Lettere \u2013 in tutto settantatré \u2013 furono scritte in un arco temporale compreso tra il 1447 e il 1470 e indirizzate, da Alessandra Macinghi \u2013 vedova di Matteo Strozzi \u2013 ai figli Filippo, Lorenzo e Matteo. Questi, raggiunta la maggiore età, ereditarono l'esilio paterno e furono costretti a lasciare Firenze: un provvedimento legislativo, infatti, stabiliva che tutti i figli maschi degli esuli, al compimento del diciottesimo anno, ne dovessero ereditare la condizione; lasciata dunque Firenze ed esercitando il mestiere della mercatura, Filippo, Lorenzo e Matteo viaggiarono per tutta Europa, trovandosi a soggiornare nei maggiori centri politicocommerciali del tempo, in particolare a Palermo, Napoli, Barcellona, Madrid, Londra e Bruges. Il presente Epistolario \u2013 che nasce da una naturale quanto necessaria esigenza comunicativa, quella che intercorre tra una madre e i suoi figli \u2013 costituisce un raro esempio di scrittura femminile privata del sec. XV e rappresenta non solo una delle prime testimonianze in lingua volgare, ma anche una delle prime testimonianze scritte da una donna laica, le cui possibilità di scolarizzazione, com'è noto, erano al tempo limitatissime.  28  \f2.1 La rilevanza linguistica dell'Epistolario La figura di Alessandra Macinghi Strozzi desta interesse non solo perché dimostra una certa dimestichezza nel padroneggiare la penna, ma anche perché riesce con disinvoltura a cimentarsi in partite e ragioni: un ambito lessicalmente e storicamente maschile. La prosa di Alessandra si contraddistingue per autenticità, schiettezza e onestà di valori pedagogici, tanto che Contini (1970) la definì la \"paradigmatica\" tra le prose domestiche del Quattrocento; la sua rilevanza linguistica è data anzitutto dal genere letterario cui va ascritta, ovverosia il genere epistolografico: \u00ABLe settantatré Lettere pervenuteci rispondono a un bisogno immediato, urgente di comunicazione, non hanno né preoccupazione né destinazione letteraria [...]. Sono scritti assolutamente privati, in cui la stessa grammatica è quella parlata; avvisi, massime, ricordi, notizie, resoconti [...] di fatti, avvenimenti, azioni, proposte relative al nucleo familiare, all'intimità della casa, segreti che non devono andare oltre il mittente e il destinatario [...]\u00BB (Doglio, 1984, p. 487). Per quanto concerne il lessico delle Lettere, si intende redigere \u2013 in appendice e a completamento della nuova edizione \u2013 un Glossario, che recepirà e metterà prontamente a disposizione del LEI \u2013 Lessico etimologico italiano (Pfister, 1979-) \u2013 numerosi termini specialistici, fra cui primeggiano quelli relativi all'attività commerciale e finanziaria, che ebbe nell'Italia del tardo Medio Evo e del Rinascimento uno straordinario sviluppo. E poiché taluni di questi termini si sono trasmessi anche alle altre lingue europee (si pensi alla fortuna di termini come banco, banchiere, capitale, credito, debito, polizza, assicurazione, lettera di cambio, ecc.), ne deriverà un contributo molto utile alla storia della terminologia economica sovranazionale tuttora in uso. Come messo in evidenza più volte dallo Stussi (2000), i documenti mercantili sono una fonte di inestimabile valore per la conoscenza della storia della lingua e offrono talvolta la possibilità di retrodatare parole o, addirittura, intere espressioni. Una nuova edizione delle Lettere della Macinghi Strozzi metterà finalmente a disposizione degli studiosi un testo filologicamente corretto e linguisticamente affidabile, che potrà sostituire l'edizione curata da Cesare Guasti nel 1877, di cui più volte ne sono stati segnalati i limiti (Trifone, 1989). Verrà così per la prima volta alla luce la grafia realmente utilizzata dall'autrice, sistematicamente modernizzata dal Guasti, e si potranno recuperare i diversi caratteri relativi ai suoni e alle forme, che non risultano nella precedente edizione. Le Lettere si prestano inoltre a interessanti rilievi di ordine sociolinguistico, essendo la scrivente una donna di ceto mercantile, assimilabile alla classe dei cosiddetti \"semicolti\".  3 L'applicazione del TAL per la redazione del formario Per il presente contributo è stato realizzato un primo modello di formario tratto dalle prime dieci Lettere di Alessandra M. Strozzi, teso a mettere in luce i tratti grafici, fonetici e morfologici caratterizzanti la lingua della scrivente, autentica espressione del fiorentino argenteo (cfr. Manni, 1979); trattandosi di un archetipo, lo studio si è limitato all'analisi delle prime dieci lettere dell'Epistolario: ciò ha consentito di eseguire un lavoro accurato, ma soprattutto di riflettere sui benefici e sulle criticità date dall'utilizzo degli strumenti offerti dal TAL. Alcune rese grafiche dell'originale (del tipo x per s, y\/j per i, cha per ca, cho per co, chu per cu) sono state anzitutto normalizzate conformemente alla grafia moderna; è stata successivamente realizzata una tokenizzazione attraverso l'impiego del modello TreeTagger per l'italiano contemporaneo elaborato da Achim Stein (Schmid, 1994). Si è quindi proceduto con l'assegnazione di un POS per ogni forma delle prime dieci Lettere, il cui corpus consta, in tutto, di 13.782 occorrenze. Il tagset impiegato, elaborato sulla base del tagset ELRA, è stato semplificato e adattato alle esigenze dell'analisi linguistica eseguita, della quale vengono esposti gli esiti al \u00A7 4; il tagset adottato è illustrato nella tabella sottostante, la quale pone inoltre in evidenza le corrispondenze tra le etichette impiegate da chi scrive e quelle proprie degli analizzatori adoperati. Come si evince dalla tabella, è stata operata una distinzione tra articolo determinativo e indeterminativo; per quel che riguarda le congiunzioni, invece, si è preferito non introdurre alcuna distinzione tra congiunzione subordinante e coordinante, dato che nella lingua del tempo la congiunzione assume una funzione che sovente non è possibile classificare con certezza, giacché polivalente. Le preposizioni, ripartite in semplici e articolate, constano di una terza etichetta, \"prep.\", sotto la quale sono state fatte confluire le preposizioni improprie.  29  \fSPIEGAZIONE  ETICHETTA  CORRISPONDENZE ELRA  CORRISPONDENZE TREETAGGER  CORRISPONDENZE TINT  aggettivo  agg.  AS, AP,AN, DP, DN, DS  ADJ  A, AP, DI, PI  antroponimo  antrop.  SPN  NPR  SP  art.det.f.  RS, RP  DET:def  RD  art.det.m.  RS, RP  DET:def  RD  art.indet.f.  RS, RP  DET:indef  RI  art.indet.m.  RS, RP  DET:indef  RI  avverbio  avv.  B  ADV  B  congiunzione  cong.  C*  CON  CC, CS  interiezione  int.  I  INT  I  non verbale  x  X*  SENT, SYM, PON, FW, LS  FB, FF, FS  numerale  num.  N  NUM  NO  preposizione  prep.  E  PRE  E  preposizione articolata  prep.art.  ES, EP  PRE:det  E+RD  preposizione semplice  prep.sempl.  E  PRE  E  pronome  pron.  P*, Q*  PRO*  PC, PD, PE, DQ  pronome femminile  pron.f.  P*, Q*  PRO*  PC, PD, PE, DQ  pronome maschile  pron.m.  P*, Q*  PRO*  PC, PD, PE, DQ  sostantivo femminile  s.f.  SS, SP, SN  NOM  S  sostantivo maschile  s.m.  SS, SP, SN  NOM  S  toponimo  top.  SPN  NPR  SP  verbo  v.  V*  VER*  V*  articolo determinativo femminile articolo determinativo maschile articolo indeterminativo femminile articolo indeterminativo maschile  Tabella 1. Corrispondenze delle etichette grammaticali. Si segnalano mediante l'asterisco (*) le etichette trascritte nella forma base, senza i dettagli offerti dai tools. L'etichetta TreeTagger \"PRO:demo\", per esempio, è stata riportata \"PRO*\". Dal momento che nessun POS tagger dispone di un modello per l'italiano volgare, il processo di POS tagging è risultato piuttosto articolato e, per ovviare all'assenza del modello, sono state sperimentate due diverse strategie: la prima è consistita nell'applicare il modello POS tagger addestrato sul corpus D(h)ante (Basile e Sangati, 2016), la seconda nel normalizzare il corpus delle Lettere, così da poter impiegare un POS tagger predisposto per l'italiano contemporaneo. Al fine di eseguire un valido confronto fra le due strategie, sono state manualmente etichettate le prime 1.000 parole del corpus delle Lettere, le cui assegnazioni sono state comparate con gli esiti dati dai due processi sopra descritti. Impiegando dunque TreeTagger e il parser Stanford CoreNLP \u2013 addestrati da A. Basile e F. Sangati sul corpus D(h)ante \u2013 sono state assegnate le parti del discorso. TreeTagger ha dato un risultato migliore rispetto al parser Stanford: il primo ha dato infatti il 59% dei tag corretti contro il 54% del secondo. Prima di approdare alla seconda strategia, il testo delle Lettere è stato normalizzato utilizzando il correttore ortografico GNU Aspell (http:\/\/aspell.net) e quindi nuovamente etichettato mediante il modello TreeTagger elaborato da Achim Stein e addestrato sull'italiano contemporaneo, il cui impiego ha consentito di approdare a una percentuale di tag corretti pari al 69%. I migliori risultati di POS tagging, tuttavia, sono stati ottenuti utilizzando il tagger per l'italiano contemporaneo Tint (Aprosio e Moretti, 2018) che, una volta applicato al testo precedentemente normalizzato, ha restituito il 72% delle etichette corrette. 30  \fProgramma  Stanford D(h)ante  TreeTagger D(h)ante  Aspell+TreeTagger (Stein)  Aspell + Tint  Accuratezza  54%  59%  69%  72%  Tabella 2. Descrizione dell'accuratezza dei programmi impiegati nell'assegnazione dei tag alle prime 1000 parole del corpus delle Lettere. Nell'attribuzione delle etichette le parti del discorso che hanno presentato maggiori difficoltà sono state aggettivi e pronomi, indipendentemente dal programma impiegato. L'etichettatore Stanford è inoltre risultato particolarmente debole e impreciso nel riconoscimento dei verbi, attribuendo erroneamente tale etichetta a molte altre parti del discorso. Al fine di incrementare la percentuale di tag corretti, il sistema è stato perfezionato attraverso l'impiego dei dati derivanti dal dizionario TLIO (Tesoro della Lingua italiana delle Origini, http:\/\/ tlio.ovi.cnr.it\/TLIO\/), che ha consentito di attribuire le etichette grammaticali a 5.194 forme e grazie al quale è stato possibile ricavare, inoltre, una puntuale distinzione di genere per 606 sostantivi. Per i sostantivi restanti, che non hanno trovato riscontro all'interno del dizionario TLIO, sono state elaborate alcune regole, basate su una serie chiusa di articoli e aggettivi, a seconda che questi accompagnino sostantivi femminili o maschili; tali regole \u2013 per l'elaborazione delle quali sono state manualmente redatte delle liste, comprensive di serie di articoli e aggettivi \u2013 hanno consentito di assegnare ad altri 120 sostantivi la distinzione di genere, precedentemente mancante. Prossimamente si ritiene opportuno impiegare un analizzatore morfologico per l'identificazione automatica del genere delle parole, così da verificarne il grado di correttezza; si intende inoltre sperimentare una terza strategia \u2013 simile a quella applicata per il POS tagging del corpus MIDIA (Iacobini et al., 2014) \u2013, finalizzata a perfezionare il lessico di TreeTagger per l'italiano contemporaneo attraverso le voci provenienti dal dizionario TLIO. Per favorire il riconoscimento di antroponimi e toponimi, inoltre, sono stati impiegati i dati ricavati dal Glossario del Libro dei debitori, creditori e ricordi che Alessandra Macinghi Strozzi tenne tra il 1453 e il 1473, un testo dunque coevo all'Epistolario oggetto del presente studio e di mano della stessa Alessandra Macinghi Strozzi (Bersano, 2015-16, pp. 271-294). Dal corpus annotato è stato tratto automaticamente un formario, strutturato in ordine alfabetico; sulla base di queste due fonti è stata eseguita una breve quanto esaustiva analisi linguistica, di cui si riportano gli esiti più significativi nel paragrafo successivo.  4 Esiti Per garantire la completa affidabilità dei risultati, è stato necessario confrontarsi costantemente con la trascrizione originale \u2013 specie nella fase iniziale del TAL \u2013 onde evitare errori nel processo iniziale di trasmissione dei dati \u2013 input \u2013, e poter essere certi dell'attendibilità dei dati in uscita, output. Il formario tratto dal testo delle Lettere e realizzato grazie al processo sinora descritto, ha consentito a colpo d'occhio di cogliere i fenomeni e i tratti grafici, fonetici e morfologici tipici del fiorentino argenteo, così come di porre in luce quelli che ne esulano, presentandosi inaspettatamente 'controcorrente'. Qui di s\u00E9guito una breve illustrazione degli esiti linguistici grafici e morfologici più interessanti ricavati dal corpus annotato e dal formario. GRAFIA: si rileva un'oscillazione costante per la resa della l palatale, con il primeggiare del tipo gl, anche dinanzi a i (seppure risultino consistenti le rese grafiche lgl, rare invece quelle in li); meno incerta sembrerebbe la resa grafica della n palatale, per la quale primeggia il tipo ngn (38 occorrenze), più sparute le rese ngni, gn; rarissima gni (con 3 sole occorrenze); dinanzi a i risulta nuovamente schiacciante il tipo ngn (16 occorrenze) rispetto a gn (2 sole occorrenze). Ancora, è da evidenziare l'uso della grafia k per l'occlusiva velare sorda dinanzi ad a nella voce Karisimo; tale resa grafica per l'occlusiva velare è fatto notevole: essa risulta infatti del tutto assente nel Libro dei debitori, creditori e ricordi di Alessandra Macinghi Strozzi (Bersano, 2015-16, p. 166). Scorrendo gli item presenti alla lettera h, spicca la scrizione etimologica homo, impiegata due volte in queste prime dieci Lettere (8 attestazioni in tutto, invece, per uomo, conforme alla grafia moderna). 31  \fMORFOLOGIA: notevoli sono i plurali in -(l)gli < -li; alcuni esempi: begli (-lgli) con 3 occorrenze e fanciugli (-lgli) con 2 occorrenze, senza esiti contrari. Per l'art. det. masch. sing. prevale la forma il, sebbene risulti considerevole anche la presenza della variante el (si contano in tutto, rispettivamente, 88 e 13 occorrenze), penetrata nel fiorentino intorno alla seconda metà del sec. XIV per influsso dei dialetti occidentali e meridionali (Manni,1979). Per il plurale dell'art. det. è attestata la forma e in luogo di i, il sistema ha tuttavia etichettato tutte le e presenti (550 occorrenze) come congiunzioni; occorrerà senz'altro ovviare all'errore, insegnando alla macchina che e può essere anche articolo se posta dinanzi a un sostantivo masch. pl., così che il sistema offra la possibilità di scegliere fra le due etichette: cong. oppure art.det.m. Per l'uso di mie, tuo, suo invariabili (del tipo, mie bisongni) e mia, tua, sua pl. masch. e femm. (del tipo, mia figluoli), è risultato maggiormente utile consultare il file contenente il testo delle Lettere etichettato automaticamente dal sistema piuttosto che il formario, poiché quest'ultimo è privo dei contesti, essenziali al fine di verificare a quale sostantivo pronomi e aggettivi si accordino e dunque comprendere, come nel caso presente, l'uso dei possessivi sopraccitati. Molto agevole è stata la ricerca delle occorrenze per i numerali duo (prevalente) e dua in luogo di due: le 12 occorrenze di duo sono state erroneamente etichettate come sostantivi; le 3 occorrenze di dua sono state correttamente etichettate come num. (numerale) secondo il tagset elaborato da chi scrive; le 7 occorrenze di due sono state invece etichettate come aggettivi. VERBI: è attestata una volta soltanto la forma sete per siete; anche in questo caso, è stato essenziale consultare il file contenente il testo delle Lettere etichettato automaticamente dal sistema piuttosto che il formario, così da verificare il contesto e sciogliere ogni perplessità rispetto al valore semantico della parola (sete sostantivo vs. sete verbo). Schiacciante è la presenza dei tipi arò, arei per avrò, avrei, che non presentano esempi contrari; tutte le attestazioni, inoltre, sono state correttamente etichettate come verbi, a eccezione dei tipi aresti (4 occorrenze in tutto) e arebe (un'occorrenza) classificati erroneamente come aggettivi. Non sono presenti esempi contrari ai tipi dia e stia; il primo, che consta di 18 occorrenze in tutto, risulta due sole volte erroneamente etichettato come sostantivo; il tipo stia occorre una volta soltanto ed etichettato regolarmente. Infine, a riprova della conformità della lingua di Alessandra al fiorentino argenteo, compaiono senza esempi contrari i tipi fussi per fossi e fusti per fosti. Per quel che concerne le desinenze verbali, non è possibile in questa sede offrirne una panoramica esaustiva; basti dire, tuttavia, che grazie al supporto informatico, l'analisi linguistica tesa a individuare le forme nonché il numero di occorrenze per ciascuna desinenza verbale è stata di facile esecuzione, oltre che rapida e accurata. Si segnala una desinenza atipica per la lingua di Alessandra, riscontrata nel formario e verificata nell'originale: il tipo preghiamo per la 1\u00B0 pers. pl. del pres. ind.; tale occorrenza è un unicum in tutto il testo delle Lettere, poiché la scrivente adotta uniformemente la desinenza -no per la 1\u00B0 pers. pl. (del tipo noi laviano), tanto nelle Lettere quanto nel Libro dei debitori, creditori e ricordi di Alessandra Macinghi Strozzi (cfr. Bersano, 201516, p. 233). Si riscontrano 3 attestazioni per la forma metatetica drento (Manni, 1979) \u2013 di cui una con raddoppiamento dell'occlusiva postconsonantica: drentto \u2013 in luogo di dentro e un'attestazione per la forma metatetica grillanda, tipiche del fiorentino argenteo. Sono inoltre attestate le forme metatetiche adrieto, adrietro (con mancata dissimilazione r-r in r-\u00F8), indrieto e, più rara per questo periodo, dirieto, derivanti dall'influsso esercitato da altri dialetti toscani (Manni, 1979, pp. 167-168). Significativa è ancora l'attestazione di sun in luogo di su nel tipo in sun un, con l'inserzione della n eufonica in sun.1 Per quel che concerne antroponimi e toponimi, ne sono stati riconosciuti in tutto 346, grazie ai dati ricavati dal Glossario del Libro di debitori, creditori e ricordi di Alessandra (Bersano, 2015-16); 127, invece, non sono stati riconosciuti secondo le etichette prestabilite ('antrop.' e 'top.'); di questi 127, tuttavia, 43 sono stati riconosciuti ed etichettati come nomi propri; 112 sono stati invece etichettati più genericamente come sostantivi e solo 4, erroneamente, come verbi.  1  Non si hanno esempi, in queste prime dieci Lettere, per il tipo sur, che trae origine da sun per dissimilazione (in sun un > in sur un). 32  \f5 Conclusioni Il corpus annotato, da cui è stato tratto il formario impiegato per l'analisi linguistica, costituisce non solo una proficua base di lavoro per la redazione del futuro Glossario delle Lettere di Alessandra Macinghi Strozzi, ma anche un primo quanto fondamentale strumento di analisi; esso ha infatti consentito di selezionare celermente i dati più significativi che andranno opportunamente registrati nel Glossario finale, che sarà posto in appendice alla nuova edizione delle Lettere di Alessandra Macinghi Strozzi. Il lavoro di revisione effettuato a mano è stato senz'altro ingente; si ritiene tuttavia che il supporto informatico sia stato essenziale al fine di porre in evidenza taluni tratti, così come la quantità di occorrenze per ogni forma riscontrata: fattore indispensabile, quest'ultimo, per chiarire quanto un fenomeno, o un tratto, fosse frequente nella lingua dell'epoca."
	},
	{
		"id": 7,
		"title": "Annotazione semantica e visualizzazione di un corpus di corrispondenze di guerra",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Beatrice Dal Bo",
			"Francesca Frontini",
			"Giancarlo Luxardo"
		],
		"body": "Introduzione  Il progetto Corpus 14, iniziato in concomitanza con il centenario della Grande Guerra, nasce dalla volontà di studiare la lingua delle persone comuni all'inizio del XX secolo, ed in particolare degli scriventi peu lettr\u00E9s, che potremmo tradurre, seguendo la letteratura italiana, con semicolti (D'Achille, 1994). In questo contesto si tratta dei Poilus, soldati francesi della Grande Guerra, spesso provenienti dalle campagne e da contesti rurali, ancora in parte dialettofoni, che si confrontano spesso per la prima volta con il testo scritto. Se lo studio delle competenze linguistiche e pragmalinguistiche è alla base della raccolta delle loro corrispondenze, tali documenti si dimostrano essere una fonte interessante anche per altre discipline, con interessanti informazioni di carattere storico, geografico e culturale. In particolare l'interesse si è focalizzato su due ambiti: \u2022 lo studio della Grande Guerra e della sua eredità in termini di memoria sociale, e delle trasformazioni da essa prodotte, \u2022 l'evoluzione degli usi linguistici, in particolare per quanto riguarda l'influenza delle varietà regionali (in particolare per le zone come il Sud della Francia o la Bretagna, caratterizzate da diglossia), o lo sviluppo di un socioletto comune, il cosiddetto argot des poilus . Utilizzando materiale proveniente da archivi pubblici, nonché documenti donati da eredi al progetto, Corpus 14 si compone ad oggi (versione 2.01) di 37 scriventi, provenienti da 11 regioni diverse, per un totale di 1.797 lettere e circa 500.000 parole. I criteri di selezione del corpus sono stati i seguenti: \u2022 la selezione di scriventi che non hanno completato la formazione elementare, \u2022 la preferenza per le corrispondenze complete, o che per lo meno permettessero di seguire gli scriventi su un arco temporale lungo, e che potessero dunque dare luogo a reti di corrispondenze complesse. Al momento Corpus 14 è costituito di 11 reti di corrispondenze, raggruppate per zona geografica e nominate secondo i luoghi di origine (si veda Figura 1)  34  \fFigure 1: Localizzazione dei luoghi di origine dei soldati di Corpus 14. Tali criteri di selezione fanno dei fondi di Corpus 14 una collezione unica nel suo genere. Tuttavia la sua realizzazione si ispira anche a progetti faro nella comunità delle edizioni digitali di corrispondenze, molti dei quali dedicati agli epistolari di personaggi illustri 2, con poche eccezioni, come: \"Digitising experiences of migration: the development of interconnected letter collections\" di Moreton e Nesi, 20132014 3. Inoltre, per la tematica il progetto può essere accostato ad altri omologhi sviluppati in diversi paesi europei in occasione del centenario della Prima Guerra Mondiale, come l'italiano \"Voci della Grande Guerra\"4 ed il britannico 'Letters from the First World War\"5.  2  L'edizione digitale  L'edizione digitale si è avvalsa di pratiche già ben stabilite, come la trascrizione diplomatica del testo, l'allineamento tra i facsimile delle cartoline o delle lettere e la loro codifica precisa (con precisazioni sulla leggibilità del testo). Per quanto riguarda la codifica, si è fatto appello allo standard della Text Encoding Initiative (TEI6). In particolare le trascrizioni sono state effettuate in modo da permettere la descrizione della struttura logica del testo, nonché delle caratteristiche di leggibilità del supporto fisico. Per ogni lettera sono state realizzate due versioni (fedele e normalizzata all'ortografia corrente). L'applicazione di questo schema di annotazione XML alla tipologia testuale in oggetto è stato facilitato dall'esistenza di un gruppo di lavoro sulle corrispondenze in seno alla comunità TEI7. In particolare si è fatto ricorso agli elementi TEIheader, correspDesc e CorrespAction (introdotti nella versione 2.8.0 delle specifiche TEI P5). Per quanto riguarda la distribuzione, Corpus 14 è reso disponibile in diverse modalità di accesso che garantiscono la fruizione da parte di tipologie di utenti diverse. Da una parte si è voluto fornire un'interfaccia di esplorazione8 ed analisi del testo attraverso la piattaforma di testometria TXM (si 1https:\/\/hdl.handle.net\/11403\/corpus14 2Uno dei progetti più noti in questo senso è Mapping the Republic of Letters, http:\/\/ republicofletters.stanford. edu\/; per una ricognizione più completa di tali progetti si veda (Stadler et al., 2016). 3http:\/\/lettersofmigration.blogspot.com; per ulteriori informazioni si veda (Moreton et al., 2014; Moreton, 2016) 4http:\/\/www.vocidellagrandeguerra.it\/ 5https:\/\/www.nationalarchives.gov.uk\/education\/resources\/letters-first-worldwar-1915\/ 6https:\/\/www.tei-c.org 7Special Interest Group della TEI sulle corrispondenze https:\/\/tei-c.org\/activities\/sig\/ correspondence\/ 35 8http:\/\/textometrie.univ-montp3.fr\/  \fveda la Figura 2)9. Allo stesso tempo i sorgenti TEI sono scaricabili dalla piattaforma Ortolang10, che garantisce l'interoperabilità dei dati, la loro preservazione e la loro reperibilità (tramite il protocollo OAI-PMH).  Figure 2: L'interfaccia di esplorazione del corpus TXM.  3 L'indicizzazione semantica dei testi Una volta realizzata la prima versione dell'edizione digitale, si è posto il problema di arricchire e indicizzare i testi, ed in particolare di creare indici a persone, luoghi, organizzazioni citate. Tali indici, collegati ai riferimenti nel testo, dovranno poi essere arricchiti e collegati con l'informazione corrispondente disponibile. L'indicizzazione dei testi, che per ora esiste solo su due reti di corrispondenze (Chazeaux e Le Soulié), è stata condotta secondo le buone pratiche della codifica in TEI, che si sono delineate anche nel contesto di gruppi di lavoro francesi facenti riferimento al consorzio CAHIER11, come il progetto Testaments de poilus12. In particolare le menzioni di luoghi, persone e organizzazioni sono state dapprima annotate nel testo di ogni lettera (sia nei metadati della corrispondenza che nel corpo della lettera), utilizzando gli elementi TEI persName, placeName, orgName. Si è inoltre scelto di annotare oltre ai nomi propri anche stringhe di testo aventi nel contesto della lettera dei referenti univoci, usando l'elemento rs. L'annotazione è stata effettuata in maniera ricorsiva, dunque un'espressione come \"les cousins de Cicignan\" è stata annotata come una rs, contenente un placeName. In seguito ogni menzione è stata referenziata con l'attributo ref e un codice univoco. Tale codice rinvia a tre indici, file separati contenenti delle liste di persone, luoghi, organizzazioni (listPerson, listPlace, listOrg). Per il referenziamento a DBpedia si è utilizzato il sistema di riconoscimento automatico di entità nominate REDEN Online (R\u00E9solution et D\u00E9sambigu\u00EFsation d'Entit\u00E9s Nomm\u00E9es) (Frontini et al., 2016), con postcorrezione manuale. Infine, tali liste sono state dove possibile arricchite con informazioni addizionali in nostro possesso (come le date e i luoghi di nascita e morte delle persone, scriventi o solo menzionate, il loro grado di 9TXM è uno strumento per l'esplorazione e l'analisi statistica di corpora testuali, sviluppato dall'ENS di Lione. Permette tra le altre cose l'import di testi annotati in TEI. Si veda http:\/\/textometrie.ens-lyon.fr. 10ORTOLANG, Outils et Ressources pour un Traitement Optimisé de la LANGue è la piattaforma francese per la pubblicazione delle risorse linguistiche, ora integrata all'infrastruttura CLARIN ERIC. https:\/\/www.ortolang.fr\/ 11CAHIER, Corpus d'Autueur pour les Humanit\u00E9s Num\u00E9riques, è un consorzio di progetti affiliati all'infrastruttura HumaNum, che si occupa di edizioni digitali principalmente in TEI. Si veda https:\/\/cahier.hypotheses.org\/ 12https:\/\/testaments-de-poilus.huma-num.fr\/  36  \fparentela, ecc.). Per quanto riguarda i luoghi si è fatto ricorso alla georeferenziazione e all'aggiunta di link al database geografico esterno GeoNames, oltre a quanto già referenziato su DBpedia. In alcuni casi, toponimi non presenti nelle basi sono stati individuati e localizzati. In alcuni casi, toponimi non presenti nelle basi sono stati individuati e localizzati.  4  Visualizzazione  Attualmente in corso è lo sviluppo di una piattaforma di visualizzazione, che permetterà di esplorare le corrispondenze in maniera geolocalizzata13. Come si può vedere dalla Figura 3, l'interfaccia permette di selezionare gli scambi epistolari di una stessa rete familiare per data, proiettando sulla carta ad esempio la lettera di un soldato e la risposta della moglie. Nella visualizzazione i segnaposto indicano il luogo di invio della lettera, mentre le bandierine indicano i luoghi citati nella lettera. La visualizzazione è realizzata in modo da sfruttare al massimo lo standard TEI recuperando i placeName con interrogazioni basate su XQuery (sia all'interno dei metadati correspDesc che nel corpo della lettera) e utilizzando la geocodifica degli indici. In questo modo, una volta terminata, la piattaforma potrà essere riutilizzata come base per altri progetti con lo stesso formato di annotazione14.  Figure 3: L'interfaccia di visualizzazione e geolocalizzazione delle corrispondenze.  5 Analisi Numerose analisi sono state condotte su Corpus 14, si cita in particolare il volume collettivo curato da Agn\u00E8s Steuckardt (Steuckardt, 2015a), nel quale sono analizzati vari aspetti linguistici di queste corrispondenze, fra cui la punteggiatura (Steuckardt, 2015b), l'ortografia (Pellat, 2015), il lessico (Luxardo, 2015) e la lingua regionale (G\u00E9a, 2015). Ricordiamo inoltre altri studi riguardanti aspetti morfosintattici (Steuckardt and Dal Bo, 2018) o discorsivi (Dal Bo and Wionet, 2018). Una tesi di dottorato è attualmente in corso di completamento (Dal Bo, 2019)). 13La rappresentazione delle reti di corrispondenze attraverso visualizzazioni è una componente tipica di questo tipo di progetti. Si vedano ad esempio (O'Leary and Moreton, 2017); Visual Correspondence, http:\/\/ www.correspondence.ie; Mapping the Republic of Letters, http:\/\/republicofletters.stanford.edu\/; Early Modern Letters Online, http:\/\/emlo.bodleian.ox.ac.uk\/home; Clavius on the Web, http:\/\/ claviusontheweb.it\/. 14Il progetto di interfaccia è stato realizzato da studenti del corso di laurea specialistica in informatica dell'Università di Genova, sotto la supervisione della prof. Marina Ribaudo.  37  \fPer quanto riguarda gli aspetti spaziali, l'analisi dei luoghi citati nelle corrispondenze dei soldati ha permesso di mettere in evidenza il fatto che questi evochino nelle lettere in maniera prevalente luoghi legati alla loro vita familiare, alla casa e agli affetti, e molto meno luoghi legati alla guerra e al fronte (come già messo in evidenza da (Dal Bo and Wionet, 2018; Gibelli, 2016)). La proiezione su una carta geografica delle informazioni geografiche e temporali delle corrispondenze permette inoltre di seguire gli spostamenti dei soldati al fronte e delle donne rimaste all'interno del Paese. Gli spostamenti di quest'ultime, più raramente studiati, potranno essere inoltre paragonati a quelli delle donne appartenenti a classi sociali superiori durante lo stesso periodo storico."
	},
	{
		"id": 8,
		"title": "The use of parallel corpora for a contrastive (Russian-Italian) description of resource markers: new instruments compared to traditional lexicography",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Anna Bonola",
			"Valentina Noseda"
		],
		"body": "1 Parallel corpora2 and linguistic research Despite the skepticism of early corpus linguists, who refused to use translated texts to draw conclusions about the functioning of a language3, nowadays the scientific community has produced countless works that demonstrate how the use of parallel corpora (PC) can have a greater impact in several areas4: 1) in linguistic research (contrastive, but not only) PC provide a rather solid empirical basis for comparing two or more languages (Johansson, 2003); moreover, the \u2018translation method' allows to deepen the semantics and functions of a given linguistic structure (No\u00EBl, 2003)5; 2) in Translation Studies, since Baker's work (1993), PC have become a fundamental tool for the study of translated texts, treated as a linguistic variety in its own right, worthy of analysis; 3) finally, PC have allowed computational linguistics to make progress in the programming of translation software and, more generally, they have favored the development of NLP (Calzolari and Lenci, 2004). However, these 3 points must be integrated with a further aspect: PC are in fact very useful for the heuristic phase of a contrastive analysis on polyfunctional linguistic elements that are strongly influenced by the context.  1 This paper is the result of a research in which the authors have equally contributed; however, Valentina Noseda is the author of  sections 1, 1.1, 3.1 and 4, Anna Bonola of sections 2, 3 and 3.2. A parallel corpus consists of texts in a language A, aligned (usually at the sentence level) with the corresponding translations in a language B. If bidirectional, the corpus will also contain language B originals alongside translations in language A. 3 The reasons for this skepticism can be traced to the generally recognized existence of the so-called universals of translations, to the influence that the source text often exerts on translators and their final product, and to the freedom with which a translator can interpret the source text while transferring its contents into a target text (Olohan, 2004; von Waldenfels, 2012; Zanettin, 2012). 4 Another field where parallel corpora have proved to be useful is the teaching of second languages, since bilingual corpora, first of all, allow students to grasp equivalences and differences between L1 and L2, thus acquiring greater awareness of the structures of a studied language (Granger, 2003), and secondly help them learn unknown words (Bernardini, 2004). 5 No\u00EBl (2003) was among the first to promote the use of PC not only for contrastive analysis but also to deepen the semantic investigation of one of the two aligned languages. In Russian studies, many works have been carried out following No\u00EBl's example, including (Zaliznjak, 2015; Levontina and Denissova, 2017; Zaliznjak, Denissova and Mikeljan, 2018). 2  39  \fIn this paper we will show an example of such further use of PC, applying it to the contrastive study of two DMs.  1.1 The Italian-Russian parallel corpus of the Russian National Corpus In Russian studies, the active use of language corpora fell slightly behind the spread of Corpus Linguistics around the world and it has been directly linked to the creation of the Russian National Corpus (Nacional'nyj korpus russkogo jazyka, from now on: NKRJa) in 2004. With its 500 million words, its numerous specialized sub-corpora and a highly sophisticated search engine, NKRJa has quickly become an essential tool for the study of Russian6. In 2005 NKRJa already presented a section dedicated to PC, although for the Russian-Italian pair there was only a small pilot corpus, not very balanced and almost useless for any type of research. A first expanded version \u2013 resulting from the collaboration between Catholic University of Milan (Università Cattolica del Sacro Cuore di Milano), the University of Bologna (Università di Bologna) and the Russian Language Institute in Moscow (Institut Russkogo Jazyka imeni V.V. Vinogradova) \u2013 became available in 2015. Now the ItalianRussian PC (it-ru PC) exceeds 4 million words and has become a sufficiently large tool allowing to conduct scientifically valid and statistically relevant research. The corpus, compiled according to precise criteria, has the following features7: i) is bidirectional: it contains Russian originals translated into Italian and vice versa; ii) it includes several literary works and essays (from 19th, 20th and 21st centuries) as well as some newspaper articles written in the last decade (and this variety distinguishes it from other parallel corpora in NKRJa); iii) like all the other sections of NKRJa, it has three types of annotation: metatextual8, morphological and semantic.  2. The use of parallel corpora for the analysis of discourse markers A field in which parallel corpus linguistics seems to have great potential, especially if compared to more traditional research methods, is that of discourse markers (DMs), i.e. multi-functional linguistic elements of various origins (adverbs, verbs, particles, etc.) that can operate at a textual, discursive, interactive, modal, social and contextual level9. DMs have come to the attention of researchers especially during the eighties, as a result of a new pragmatic direction in language studies, and since then considerable progress has been made in this area10. However, the use of electronic corpora in the description of DMs is still in its initial phase. The difficulty of producing a fully automatic tool for the analysis of DM is due to the fact that these are procedural and multifunctional elements expressing pragmatic and discursive functions which are clarified only in relation to the context or to the communicative situation, whose automatic annotation is still developing11. Moreover, syntactically, DMs are optional (can be removed), relatively mobile in the utterance and come from diverse grammatical classes, on which depends their syntactical integration (Crible, 1917: 106). Therefore, the discussion on the automatic processing of DMs is currently still focused on the 'need for functional paradigmatic studies that include every kind of DMs, possibly in multifunctional approaches for better generalization' in order to 'provide a solid basis for comparative or contrastive analysis between languages and frameworks' (Crible, 2017: 100). Some recent experiments for the identification and annotation of DMs are worth noting, like for example (Bolly et al., 2017), even though the empirical method they present is still matching manual and automatic annotation. For a fully automatic cross-linguistic analysis of DMs, which takes into account not only syntaxis 6 A detailed description of the corpus and its sub-corpora (including all the information about the available annotations) can be found on the corpus website (www.ruscorpora.ru). See also (Aa.Vv 2005) and (Plungjan 2009). 7 For a description of the Russian-Italian PC and its design criteria, see (Noseda, 2018). 8 It provides various pieces of information about a text: author, date, genre, number of words, etc. 9 This list of functional areas summarizes the results of the debate on the classification of DMs \u2013 for a review see (Schiffrin 2001; Frediani and Sansò 2017); for the discussion in Italy see (Bazzanella 2001: 41-42) \u2013 although we avoid entering into the discussion on labels, whose boundaries are subject to change and have a graduated character (Molinelli, 2018: 277). 10  For a review of the features of the DMs highlighted by the research, up to the most recent studies see (Frediani and Sansò, 2017). As far as Russian is concerned, in the NKRJa only the multimodal sub-corpus (4 million words) is pragmatically annotated: the search engine can be interrogated on the basis of specific contexts (at the doctor's, at the restaurant, etc..) and linguistic acts (complaint, prohibition, apology, etc..). Among Italian corpora, we can name the AVIP corpus (http:\/\/www.parlaritaliano.it\/index.php\/it\/corporadi-parlato\/673-corpus-avip-api) and PraTID (http:\/\/www.parlaritaliano.it\/index.php\/it\/ progetti\/35-pratid-un-sistema-di-annotazione-pragmatica-di-dialoghi-task-oriented), which are fully or partially annotated at a pragmatic level. 11  40  \f(Cinque 1999) but also semantics and pragmatics, the annotation of PC, according to Crible (2017: 107), should consider the following levels: ideational (the relation between real-world events), rhetorical (the relation between epistemic and speech-act events), sequential (the shaping of discourse segments) and interpersonal (speaker-hearer relationship). Therefore, the large amount of data that can be consulted today through electronic corpora, as far as DMs are concerned, has yet to find a way to be processed employing a targeted annotation. More specifically, concerning the automatic analysis of DMs in Russian, some supracorpora databases (SCDB), resulting from the processing of some bilingual parallel corpora within NKRJa, have recently been developed (Zatsman, Inkova, Kruzhkov and Popkova, 2016). Their aim is to increase the functionality of parallel corpora for goal-oriented cross-linguistic research on various linguistic elements. For the moment, there is one SCDB for French-Russian contrastive analysis of verbs (Zatsman and Buntman, 2015) and one for textual connectors (In'kova, 2018). The it-ru PC used for our research does not have an annotation that takes into account pragmatic and discursive parameters; moreover, we still do not have Russian-Italian SCDB for such particular linguistics elements as DMs. Therefore, for the moment, we tested the effectiveness of it-ru PC as a tool for linguistic analysis in the heuristic phase, as it provides a significant number of examples in a short time, allowing researchers to clarify and adjust their intuition regarding a given research question (corpus-based approach) or to formulate new hypotheses (corpus-driven approach) (Mikhailov and Cooper, 2016: 15-16). If this is generally helpful, it is even more useful for DMs, i.e. linguistic elements that both in Italian and Russian have been developing textual, discursive, modal and pragmatic functions that make them multifunctional and often language-specific, but frequently still lacking an adequate description (Proietti, 2000: 227) (Benigni and Nuzzo, 2019: 152\u2013154)12, especially if we consider current lexicography. As we will show in this paper, the effectiveness of our PC (in its current form) for the heuristic phase of a contrastive corpus-driven or corpus-based approach \u2013 well described in (Crible, 2017) \u2013 lies in the fact that: 1) it makes the multi-functionality of DMs easily emerge, clarifying it by contrast with another language, as description through linguistic comparison, 'rend le dispositif d'analyse plus puissant: elle peut sugg\u00E9rer, d'une part, de nouvelles hypoth\u00E8ses pour les faits constat\u00E9s; elle peut, d'autre part, inciter à r\u00E9examiner des hypoth\u00E8ses existantes' (Lamiroy, 1984: 224); 2) if a given DM presents recurrent functional equivalences in the language compared, it is possible to determine if in the L2 there are DMs associated with specific functions as well; 3) finally, analyzing quantitative data (even with a relatively small number of examples), we can see the preferential strategies of each language to express certain functions, and in some cases, as illustrated in section 3, it is also possible to make some assumptions about possible structural differences between the two compared languages. In section 3 we will exemplify the abovementioned points by analyzing Italian allora and Russian ved', two of the most frequently used DMs in the respective languages.  3. The DMs allora and ved' Concerning the pragmatic-textual multi-functionality of DMs (section 2, point 1), both Russian and Italian lexicographic descriptions are particularly poor and often do not distinguish contextual elements from the functional core meaning of the DM under investigation. For example, as far as allora is concerned, DISC 2008, among the several dictionaries that we have consulted, is the only one providing some clear categories about the discursive use of this word, which can be a temporal adverb, a conjunction or an actual DM. According to DISC, allora, as DM, refers to shared knowledge in dialogues (Allora?) or in exhortative, imperative and interrogative sentences (e allora sei pronto?). This brief description, although correct, is rather uncomplete and it uses contextual categories, such as sentence or text type, without specifying how their role interacts with the functionality of the DM. As for traditional Russian lexicography, the description of DMs is not better: in both traditional (U\u0161akov, 1935) and recent dictionaries (Kuznecov, 2000; Efremova, 2001; O\u017Eegov and \u0160vedova 2003) the particleconjunction ved', whose various meanings are summarized in (Morozov, 2014: 259), is defined as follows: 1) conjunction in those sentences that indicate the cause or the motivation of a previous statement; 2) concessive conjunction; 3) it expresses a hypothetical or possible state; 4) particle that underlines or contradicts what has  12  In particular, in the article, dedicated to the use of corpora for teaching DMs, the authors underline how even in this field has emerged so far 'a lack of contextualization of pragmatic phenomena and a shortage of natural conversational models, exemplifying the real use of language' (Benigni and Nuzzo, 2019: 154). 41  \fbeen said; 5) it emphasizes adversative conjunctions such as no [but]13, \u0430 [but, and], da\u017Ee [even]; 6) in conditional clauses it means togda [then], v takom slu\u010Dae [in this case]; 7) it indicates a statement from which a conclusion will be drawn; 8) it gives emotional color to spoken language; 9) in questions and exclamations it means neu\u017Eeli ne?, razve ne [really\/indeed]. Such a functional heterogeneity, as well as the variety of aspects involved, shows that the core meaning of ved' provided by lexicographic descriptions is quite vague and even confused. Moreover, as in the case of allora, the problem of distinguishing the function of connector from that of DM remains. Thanks to our corpus-driven analysis in the it-ru PC, a much more precise and complex description has surfaced.  3.1 Allora Our analysis took into account the first 200 occurrences of allora automatically extracted from the corpus (100 in Italian originals and 100 in texts translated from Russian)14. Firstly, we considered Russian DMs corresponding to allora both in Russian translations and in Russian original texts; secondly, we examined their different functions. Our goal was, on the one hand, to clarify the multi-functionality of allora by contrast with Russian, and on the other to compare our results with the descriptions of this DM provided by traditional lexicography and some linguistic research works. This allowed us to verify if our PC, even in its current form, can be useful to integrate these resources towards a more precise description. As Allora is highly polysemic (it combines temporal, logical and pragmatic values) and multifunctional (it can be an adverb, a connector or a DM), we found out that it does not have full functional equivalents in Russian; in fact, quite frequently (25 occurrences) allora does not show any equivalent at all: either it is omitted in the Russian translation or it is inserted in the Italian translation without a corresponding DM in the Russian original; its adverbial and connective values are rather carried out by different and thus highly specialized markers with metatextual\/metanarrative, interactive and pragmatic functions (this distinction is provided in Bazzanella, 2001) (see section 2, point 2). More precisely: \u2013 togda [then] mostly conveys adverbial and connective meanings; \u2013 zna\u010Dit and vychodit [so] (connectives) express conclusion by inference or deduction; \u2013 (i) tut [and then] often expresses temporal correlation and is combined with the metanarrative function typical of allora, that marks the different phases of narration. \u2013 Tak\/itak [so] add two pragmatic functions to the basic consequential meaning: i) interactional function (beginning or end of the interaction, and turn-taking in a conversation); ii) metanarrative function (restarting the narration or marking the narrative phases); \u2013 Nu and \u017Ee [well] never have temporal meaning, but they carry out pragmatic functions, emphasizing the interactional process as well as turn-taking. Nu and \u017Ee do not seem to express any consequential component. These results are summarized in Figure 1, which shows, in addition, quantitative data. In this regard we must point out that we had to leave out some examples due to translation errors, omissions etc.; as a result, the number of examples that we could actually take into account amounts to 164 (including the 25 cases of zero correspondence which are not showed in Figure 1):  13  The translations that we provide in brackets are approximate since even in English there is never a single equivalent for these words. This bidirectional approach allows determining if the behavior of a given linguistic unit differs according to text type (i.e. original versions vs translations). In this sense, a bidirectional parallel corpus has proved to be an extremely helpful resource. 14  42  \fFigure 1: Data resulting from a corpus-driven analysis of DM allora  3.2 Ved' Biagini and Bonola (2019, in press) have recently applied to ved' a similar heuristic method of investigation using the it-ru PC. They examined the first 100 occurrences automatically extracted from the corpus (both in originals and translated texts). In this case, the analysis was carried out considering first of all the contexts of occurrence. The primary goal was to identify the core meaning of ved', in order to distinguish it from other peripheral values. The results of the analysis are summarized in Figure 2, followed by a brief explanation: Figure 2: Data resulting from a corpus-driven analysis of DM ved'  43  \f- unlike what is stated in dictionaries, Biagini and Bonola would not attribute to ved' the encoding function of clause linking, even though in our corpus the group of contexts which exhibit an interphrastic relation is the second in terms of entity: ved' in fact occurs in sentences that express very different kinds of relations (such as adversative and causal), which, nevertheless, in almost all the examples are codified by conjunctions or are inferable from the propositional content of the statements, instead of directly depending on ved'. - secondly, in more than 50% of the analyzed contexts, ved' occurs in the presence of two sentences, the second of which expresses a \u2018reason to say' (i.e. a reason why something was previously said) instead of a mere causal relation. Strengthening the illocutionary force of the second sentence by referring to a shared background that the speaker wants to recall, ved' provides the listener with useful hints to overcome the inferential process. In these contexts, ved' realizes the macro-functions of expressing textual cohesion (discourse marker), social cohesion and personal attitude (pragmatic marker). - thirdly, the semantic core of ved' (if used when referring to a shared knowledge) persists in particular when it functions as a pragmatic marker that manages social cohesion and modulates illocutionary force (in questions and \u2018reasons to say') or as an element which favors the inferential process (in \u2018reasons to say'). When, on the other hand, it carries out the role of discourse marker favoring textual cohesion, it still refers to shared knowledge, but apart from this, nothing else is presupposed. If the results described above exemplify points 1 and 2 of section 2, concerning the multi-functionality of allora and ved' or the specialization of their equivalents in Russian (for allora) and in Italian (for ved'), for point 3 \u2013 i.e. the preferential strategies of Russian and Italian regarding the expression of certain discursive functions \u2013 it was very useful to analyze the asymmetries emerged from the it-ru PC, i.e. the cases of omission or addition of allora and ved' in target texts compared to the originals. This analysis showed that both DMs are sometimes omitted in translation or they are added in the absence of a correspondent marker in the original. In addition to this, neither of the two DMs has a perfect functional equivalent in the target language, but they distribute their many functions on partial equivalents. This demonstrates a certain language-specificity of both DMs (on the relationship between the number of translation variants and language-specificity of DMs see Inkova, 2017). Moreover, as far as allora is concerned, we observed that using this marker we tend to give a logical (consequential) interpretation to the temporal relationship between two circumstances: 'in that moment\/that circumstance' can, in fact, be interpreted through allora also as a consequential relationship. Here we can see the preference of Italian for logical cohesion in the text. Russian, on the contrary, often simplifies this temporalconsequential relation in a strictly temporal sense, translating allora with temporal adverbs or adverbial phrases (on this difference between Russian and Italian, a consequence of Latin syntax, see Govorucho, 2007).  4 Conclusions: a hypothesis on the differences between Italian and Russian regarding the use of DMs Our conclusions regard, firstly, the evaluation of the tool we adopted for our corpus-based contrastive analysis of DMs, i.e. the Russian-Italian bidirectional parallel Corpus of NKRJa. At the moment we can say that this corpus is suitable for the heuristic phase, but it does not yet provide sufficient data to draw general conclusions from a systemic or typological point of view. Any assumption about possible structural differences related to the use of DMs in Russian and Italian should be supported by a larger number of data. Nevertheless, a heuristic analysis allowed us to formulate some preliminary hypotheses. More precisely, we were able to register the tendency of Russian to express purely pragmatic functions, both cognitive and interactive15, through an ancient group of primitive particles, such as ved', nu, \u017Ee, which are more specialized if compared to DMs of more recent origin, such as togda, which maintains an adverbial and connective function as well. On the contrary, Italian tends to form multifunctional DMs of verbal or adverbial origin which combine their pragmatic features with the task of guaranteeing logical cohesion in the text and interphrastic relations. This is a broad \u2013 and according to us new \u2013 observation on a structural difference between the two languages, which deserves to be further explored by investigating \u2013 both from a diachronic and a synchronic point of view \u2013 more Russian and Italian DMs. All this demonstrates how a heuristic corpus-driven study allows, on the one hand, to quickly obtain linguistic descriptions on the functioning of DMs that are more precise than those provided by traditional tools and, on the other, to open up new hypotheses for wide-ranging research. 15"
	},
	{
		"id": 9,
		"title": "PhiloEditor: simplified HTML markup for interpretative pathways over literary collections",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Claudia Bonsi",
			"Angelo Di Iorio",
			"Paola Italia",
			"Francesca Tomasi",
			"Fabio Vitali",
			"Ersilia Russo"
		],
		"body": "Introduction  Descriptive markup has been introduced three decades ago as the main mechanism to provide structured annotations over arbitrarily organized text documents. The rigidity of SGML and XML-based languages and the complexity of their editing tools made HTML an attractive markup language for text representations, as well as web pages, academic articles, structured documents and literary collections. However, the syntax of HTML is too flexible and its vocabulary too rich and unspecific. So the idea of channeling and restricting this richness into smaller and simplified subsets of the language was tried: Scholarly HTML, RASH and ADF, for instance, are simple and rigid subsets of HTML5 provided with similar approaches towards a restricted vocabulary and syntax. PhiloEditor\u00AE1 started as a web-based tool that identifies variants and versions of literary texts using such descriptive richness as a mechanism for creating and perusing complex, multiform and coexisting interpretative pathways over literary collections. It has been used for three years now at the Departments of Italian Studies of the University of Rome and at the University of Bologna, in order to study and characterize the differences between earlier and later versions of the same literary texts. PhiloEditor\u00AE was originally meant to provide a visualization of the differences between two versions of the same text, as a display of the output of a diff tool over two text documents. To ease the development of this tool, a simplified form of HTML5 immediately displayable in a browser window was used. Over time, in addition to the mere display of differences, a few additional interpretative tools were added to the interface to provide for highlighting of some semantically relevant textual features, rendered with different colors of the text. Finally, a few 1  A demo is available at http:\/\/site1705.web.cs.unibo.it\/phed6.2\/#. 47  \fextractors of data were created to collect and graphically represent some statistics of features in the annotations and in the very text, exploiting the regularities in the controlled use of simplified HTML markup. The purpose of the tool, therefore, has shifted from the merely philological characterization of literary texts (i.e., to represent their temporal evolution) to a critical and hermeneutic approach to them, by providing support for custom, discipline-specific pathways over complex, multidimensional and temporally complex collections of (literary) documents. At the same time, the minimality of the markup language of the tool (relying exclusively on a well-behaved and extremely simplified subset of HTML) allows an extreme homogeneity and control over markup without the necessity of giving up in richness of features. Texts annotated through this simple format are extremely regular and can be exported to a well-known literary XML vocabulary for literary texts such as XML\/TEI.  2  Markup patterns in simplified HTML  The pattern theory of documents (Di Iorio et al., 2014) has been created to provide a generative approach to useful patterns in document structures. The advantages in restricting elements to patterns are to achieve orthogonality, as each pattern has a specific goal, and fits a specific context with no overlap with other patterns, and assemblability, as it is clearly associated to precise nesting rules. By limiting the possible choices, patterns prevent the spread of arbitrary structures and allow authors to create unambiguous, manageable and well-structured markup languages and, consequently, documents which drive reusability and homogeneity (Di Iorio et al., 2012; Di Iorio et al., 2013). In our theory of patterns applied to PhiloEditor\u00AE, elements are divided into four categories according to two axes: text\/no-text content and element\/no element content. This creates a basic scaffolding of four categories: marker is the class of elements that can contain neither text nor other elements (i.e., empty elements), flats are the elements that can only contain text, buckets are the elements that can only contain other elements, and mixed is the class of elements that can contain both text and other elements. Cannot contain text  Can contain text  Cannot contain elements  Empty element (Marker)  Plain text element (Flat)  Can contain elements  Plain structural element (Bucket)  Both elements and text (Mixed)  Figure 1. Main categories of our pattern theory of documents. We now switch to consider where these elements can be used, that is the context model that can accommodate them. Since only two categories can contain other elements, we have exactly eight patterns of elements. Marker  Flat  Bucket  Mixed  Marker  -  -  -  -  Flat  -  -  -  -  Bucket  Meta  Field  Container  Block  Mixed  Milestone  Atom  Popup  Inline  Figure 2. The basic set of patterns.  \u2022  According to this model: A meta element is a marker (i.e., an empty element) placed within a bucket, i.e., never close to text fragments. Usually their real position is not relevant, and only its existence has some relevance: it is the  48  \f\u2022  \u2022  \u2022  \u2022  \u2022  \u2022  \u2022  perfect candidate for metadata structures, hence its name. Element <meta> in HTML is clearly and rightly a meta element. A milestone is a marker (i.e., an empty element) placed within a mixed, i.e., close to the text fragments. Its location in the document is often its most important contribution, i.e. it represents a special position within of the document, hence its name. Elements <br> and <img> in HTML are milestones. A field is a flat element (text allowed, elements not allowed) placed within a bucket, i.e., never close to text fragments. It is just a container of data whose position is not particularly relevant. Its proper use is as a field of a record, hence the name. Element <title> in HTML is a field. An atom is a flat element (text allowed, elements not allowed) placed within a mixed, i.e., close to text fragments. Its location in the document is often its most important contribution. Since it allows no element content, it is an atomic container of text, hence its name. A container is a bucket (text not allowed, elements allowed) placed within a bucket. It is a container of elements and it provides the fundamental scaffolding of the document structure. Containers are arguably the most important pattern of the eighth. For instance, elements <html> and <table> in HTML are containers, but there are many others. A popup is a bucket (text not allowed, elements allowed) placed within a mixed element. It provides an interruption of the usual flow of inline and text elements within a mixed element, creating a separate context for buckets such as containers. It acts as a frontier element between mixed and buckets, and lets the contained elements jump out of the constraints of mixed elements, hence its name. Element <figure> in HTML is a popup. A block is a mixed element (text and elements are both allowed) placed within a bucket. It is the earliest element in a hierarchy of containment of buckets that can contain text, and therefore acts as a frontier element between buckets and mixed. The most important type of blocks is the paragraph, i.e. a basic, independent container of text and inline elements of a document, a block of text vertically separated from the others of the same type, hence its name. Element <p> in HTML is therefore a block. An inline is a mixed (text and elements are both allowed) placed within a mixed. An inline element characterizes text fragments according to several criteria, such as style, typography, semantics, etc. Since it does not break the paragraph structure, it stays on the same line as the text, hence its name. Elements <b> or <a> in HTML are inlines, as well as many others. These eight patterns represent by construction the complete set of possible element types, and no others can exist without loosen the rules. However we have identified some specific regularities within containers, which are very important and complex elements. Rather than creating independent patterns, we identified sub-patterns of containers, inheriting all their characteristics and adding others: record, table, hierarchical container (hcontainer). This theory shows that although HTML does not recognize nor use patterns in a systematic way, it is possible to restrict HTML by selecting a reasonable subset of elements expressive enough to capture the typical components of domain-specific documents while being also well-designed, easy to reuse and robust.  3  Simplified HTML for scholarly, technical and literary texts  The discussion to adopt HTML as the markup language of choice even for specialized domaindependent documents started in the academic publishing domain: through the unification of data formats we can homogenize the process of drafting, submitting and publishing documents. HTML easily supports the embedding of semantic annotations to improve the sharing of communication results, thanks to already existing W3C standards such as RDFa (Sporny, 2015) or JSON-LD (Sporny et al., 2014), so that discoverability, interactivity, openness and usability of the scientific works are increased (Shotton et al., 2009). The HTML language has been widely used as a full-fledged markup language to encode texts, not only plain Web pages but also academic articles, technical documentation, literary artefacts and data reports. The ability to embed semantics within HTML pages, for instance by using RDFa (Herman et al., 2015), is a key factor for this success since it allows designers to express rich 49  \finformation about any domain and to overcome the limitations of a language developed and used for long time for other purposes. HTML has recently gained importance as a markup language for writing technical documentation as well. ADF (Caponi et al., 2018) is a pattern-based subset of HTML that allows designers to express information about the authoring process such as change-tracking data, templates and reusable fragments and authorship attribution data. The format can be manipulated by a Web editor that relies on the pattern-based structure of the language and its ability to express finegrained semantic information on each piece of content.  4  PhiloEditor\u00AE for the scholarly markup of literary documents  PhiloEditor\u00AE, as a web-based environment for reading and annotating variants, is aimed to be a valuable support for scholars in the criticism of variants. Its main characteristics are the display of differences (deltas) between two versions of the same document and the classification of semantically relevant fragments of the text (colouring), according to a parametric set of features provided by a scholar in a specified domain. PhiloEditor\u00AE's data model is based on a highly simplified version of HTML5 that is fully based on the pattern model described in section 2. PhiloEditor\u00AE's features make it particularly suitable for the study of literary texts in multiple versions, in particular those resulting from the shifts of volition of the author (authorial philology). In fact, it has been tested first on the two printed editions of I Promessi Sposi by Alessandro Manzoni (Bonsi et al., 2015), then on the internationally-known Pinocchio by Carlo Collodi (Gargano and Italia, 2018), comparing the version published periodically between 1881 and 1883 on \"Giornale per i bambini\" magazine with the printed edition published by Paggi in 1883.  Figure 3. Color coding of the edits in I Promessi Sposi. For versions and variants to be comparable, it's necessary for a delta to be created. Many algorithms exist that perform such tasks, but the kind of delta is also relevant for our purpose. Most algorithms meant for computer code assume that the relevant atomic entity to be differentiated is the individual line, since most programming languages use lines as their atomic semantic unit of production. In other types of documents, atomic entities may be single characters or structural nodes (e.g., a paragraph or a whole chapter). These are not particularly appropriate for literary texts, where long paragraphs with many differences (as in the case of our documents) would make the understanding of the variants difficult. The best choice for literary texts in our opinion is word-based diffing, which identifies whole words as the basic perceivable and understandable difference between variants. We thoroughly tested two Javascript libraries for word-based diffing, Javascript diff algorithm (Resig, 2005) and wikEd diff (Cacycle, 2017). The final choice was the latter, which is more precise and better parameterized. This algorithm reads two text files and outputs a document comparing them by placing the differences as spans of an HTML document. The output needs to be further synthetized and qualified: in particular, 50  \fwikEd diff generates a simple list of insertions and deletions. Literary variants do in fact contain relatively few simple deletions and insertions, and mostly replacements (where an old fragment is substituted by a new fragment). A replacement is seen by the diff algorithm as a deletion of the old text followed, in the same position, by an insertion of the new text. Correctly identifying and characterizing replacements is important for a better description of the actual editing operation occurred and this is generated by appropriately grouping the list of insertions and deletions provided by the diff algorithm before the result is displayed to the user. PhiloEditor\u00AE provides different ways to display variants: the user can choose an exclusive, a synoptic or a layered representation of the text. In these views, all modifications are characterized as replacements. Complex diff issues can be handled manually: the segment of the variants automatically created can be modified thanks to a simple interface operation that makes possible to divide or combine groups of elements according to the user's needs. Word-based diffing doesn't work with macro-variants though, so for now PhiloEditor\u00AE can only manage microvariants. The increased familiarity with the temporal evolution of the text brought a growth in curiosity towards the types of modifications. For this reason, we devised a two-layer model of modification types (Italia, 2018): 'categories of corrections' to identify the actual methodology of correction (i.e. shifts of textual passages, additions, deletions, inversions of words, etc.) and 'linguistic categories' to identify the linguistic categories that drove the correction. In order to describe the text in these terms, a colorbased coding schema of the fragments of text affected by each phenomenon was created. Since the two classes could overlap, two separate types of styles were created using the color of the text (categories of correction) and the color of the background (linguistic categories). Displaying these changes visually enables us to reach a more complex perception of linguistic features of literary texts and facilitates not only the representation of important syntactic structures but also their hermeneutic implications. The variety of textual features that need to be expressed in the markup of documents edited inside PhiloEditor\u00AE is limited: one needs to support the hierarchical structure of the document (chapters, paragraphs, text), very little typography (just a few words in italic here and there), plus the applicationspecific requirements of describing replacements (e.g., the edits generated by the diff engine as insertions and deletions and converted by an internal algorithm) and the colors, i.e., the linguistic categories described by scholars.  Figure 4. The HTML source of I Promessi Sposi. Correspondingly, we have created an extreme simplification of the HTML5 used by the application: <section>, <p> and <i> are used for the document, and <span> is used for both modifications and color coding (since most of the colors are applied to modifications anyway). HTML classes are created and 51  \fused both to provide semantics and rendering characteristics, and an additional data-* attribute is used to provide basic provenance attribution. Given that the same span can be associated to multiple classes, clashes in categorization are impossible. On the other hand, the HTML that is being created is rigorously well-formed and homogeneous, and generating a correct and valid XML\/TEI source is immediate and straightforward. Using HTML class names for semantic characterization has both advantages and issues. Of course, classes make the association of special rendering to fragments easy. On the other hand, it is extremely difficult to impose constraints on the list of classes that can be used, e.g., to specify within a schema that elements of the class \"replace\" must first contain an element with the class attribute containing \"new\" and then an element with the class attribute containing \"old\".  5  PhiloEditor\u00AE and Scholarly Editions: markup tools for literary texts  Most of the available online editions of literary texts are based on full-text transcriptions of original texts into electronic form (Franzini, 2012), typically using the XML\/TEI model, where the sources (witnesses in philology jargon) are traditional primary sources. The infrastructure is generally based on standard web programming languages, both client- and server-side. Users have limited access to the digital sources without any awareness of the backend software employed. Most of the editions, as said, are XML\/TEI documents, written by hand with a stand-alone application such as Oxygen, and converted as needed into HTML\/CSS documents using XSLT stylesheets. All projects in this domain are built on the same basic structural components: they consist of a set of files (assets) stored inside an information architecture such as a database or file system (structure) where they can be accessed (services) and shown on a browser (use\/display) (Druker at al., 2014). Different phases (assets, structure, services, use\/display) mean different tools. Thus, the DiRT Directory is a registry of digital research tools for scholarly use, while the Taxonomy of Digital Research Activities in the Humanities (TaDiRAH) 'breaks down the research lifecycle into high-level \u2018goals', each with a set of \u2018methods' '. In the field of literary texts, editing is the first important step of the process: editors need to use simple applications in order to describe their documents and their features. TextGrid, for instance, is a real research environment with all the necessary tools and services to support the entire research process, especially in digital scholarly editing. The downloadable 'TextGridLaboratory' is an editor for XML\/TEI markup with a view on the source code and a traditional visualization mechanism. Catma (Computer Assisted Text Markup and Analysis) is an online environment that manages analysis, manual and automatic annotations, and visualizations of documents. Juxta is an open-source tool for comparing and collating multiple witnesses of a text work, useful for an analytic visualization of textual variants. Tapas is meant for visualizing, storing and sharing XML\/TEI documents, while EVT (Edition Visualization Technology) is a downloadable open source tool that creates digital editions from XMLencoded texts. The final styling of documents is entrusted to CSS style-sheets and is easily customizable. As shown, XML\/TEI is the fundamental data model for all documents, and it is used as a work format rather than as an output format, requiring scholars to learn it in depth. Simplified HTML could be an alternative that, while maintaining complete exportability to XML\/TEI, allows application designers to rely on web technologies much easier to work with, and avoiding exposing literary scholars to angle brackets altogether.  6  Conclusions  PhiloEditor\u00AE is but one of the activities in which we are pushing for the use of a simple HTML instead of a full-fledged custom XML vocabulary, although striving to identify a well-formed, well-behaved, totally descriptive and specialized subset of HTML. In PhiloEditor\u00AE two specific markup needs, the description of modifications and the coloring of semantic characterization of the texts, have been expressed within a standard and very simple subset of HTML. Similar activities, such as RASH or ADF, are used in other completely different, but still very specialized, domains. Overall the response to the features and the flexibility of PhiloEditor\u00AE has been overwhelmingly positive. The objective of the next 52  \ffuture is to slowly convert PhiloEditor\u00AE into a full-scale environment for all the needs of the collection, digitization and publication of scholarly editions of literary texts."
	},
	{
		"id": 10,
		"title": "An empirical study of versioning in Digital Scholarly Editions",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Martina Bürgermeister"
		],
		"body": "Digital publishing and a textuality that is dynamic, collaborative, distributed and interdependent lead to the digital scholarly edition (DSE) facing additional technological challenges in contrast to print editions. The MLA Committee von Scholarly Editions takes care of this and recommends the use of technologies appropriate to the goals of the edition, 'in recognition of the fact that technologies and methods are interrelated in that no technical decisions are innocent of methodological implications and vice versa (MLA 2016, 1).' The committee also suggests a design of the DSE that will be as durable and sustainable as possible. This applies not only to DSE, but also to the entire scientific practice. In the 'Guidelines of good scientific practice', like the one from the University of Parma, it says: 'I ricercatori dell'Ateneo devono aver cura che tutti i dati, primari e secondari, generati dalle loro attività di ricerca siano archiviati e conservati in modo corretto ed appropriato, garantendone la sicurezza e l'accessibilità [\u2026]' Besides being responsible for data safety and access, researchers should support a prompt data exchange and reuse: 'Con l'intento di rendere la ricerca più aperta, globale e collaborativa e garantirne un controllo di qualità, i dati dovrebbero essere messi a disposizione dei colleghi che vogliano replicare lo studio o elaborare nuove ricerche a partire da essi [\u2026] (Università di Parma, 2018)'. The reuse of data is a matter of trust in data itself and the IT infrastructure as maintainer and provider. In this case, transparency is a key factor. In the following it will be argued, that versioning is more than a measure to guarantee data authenticity, integrity and accessibility. Versioning as an integrated part of the DSE creates transparency in an ongoing scholarly discourse. This objective is achieved, firstly, when new versions of the DSE or components of it are created if changes are made to it. Second, if what has been changed is also communicated; and third, if previous versions are made available. These three criteria form the basis of my analysis of versioning DSEs. Using an empirical approach, I would like to find out why versioning plays a role in these projects and how it is implemented. The implementation of versioning in a DSE is not yet state-of-the-art. In order to find out which DSEs have a versioning strategy and what it looks like, the \"Catalogue of digital editions\" (\u00D6AW) was used as a finding aid. Since the list is very extensive, the analysis includes edition projects mainly from the past 15 years, which have developed over a relatively longer period of time. Since there are far more DSEs without versioning than with it, the empirical basis was mainly provided by Bleier's preliminary work (2019). He has evaluated 100 DSEs on criteria such as citation recommendation, permalinks and versioning. Finally, those cases that assign version numbers, but neither convey what has changed, nor make past versions available, were excluded from further consideration. Their exclusion from this analysis is justified by the fact that they fulfil only one of three conditions for versioning DSEs discussed here. The review has shown that so far three different versioning strategies have been applied, which meet at least two of the three conditions formulated above. An overview of the discussed strategies is provided in section 5 (Table 1).  55  \f1 Versioning as documentation DSE projects of this type implement the assignment of sequential version numbers for changes and document what has been changed. A list of past version numbers with descriptions of the revisions and where they were made is provided. In these projects, the DSE is seen as an ongoing process, which is documented. The strategy is pursued in the DSE \"Auchinleck Manuscript\", edited by David Burnley and Allison Wiggins. The project started in 2000 and was launched in July 2003 with version 1. The edition has a citation recommendation with version number indication. The editors recommend citing version 1.1 because it corresponds to the current version (March 15, 2004). The version number applies to the entire project. There is a version documentation with the title 'Archive of site updates'. It consists of three lists: One with version number and date, another with \"Corrections to texts and to textual notes\" and an extra column \"Record of other changes\", which lists changes to the bibliography and the side menu. Allison Wiggins explains the version documentation theme as follows: All changes made to the content of this site are recorded here in the archive of site updates. Each time a batch of updates is added, the site is designated a new version number. This system ensure that users can keep track of any changes made and can reference the site materials accurately (Burnley and Wiggins, 2019). At about the same time (March 2003), the DSE \"The Old Bailey Proceedings Online\" was published. It is an editorial long-term project that presents London processes from the period 1675-1913 and Tim Hitchcock, Robert Shoemaker, Clive Emsley, Sharon Howard and Jamie McLaughlin are responsible for the edition. A version documentation can be found in the \"What's New Archive\". As in the previous example, an introductory text refers to the non-final, process-like state of the DSE. It is followed by a listing and description of what has changed in previous versions: the current status is March 2018 with version 8. A version documentation is also available in the edition project \"The Online Froissart\". The editors Peter Ainsworth and Godfried Croenen cite another reason for documenting versions: The Online Froissart is a collaborative, interdisciplinary and incremental project. Given the sheer size of Froissart's Chroniques, the number of surviving manuscripts and their dispersal across many libraries in several different countries, it has been decided not to delay publication of available transcriptions until all of these materials have been transcribed and annotated. The Online Froissart, therefore, publishes all currently available transcriptions and other materials produced by the project team, plus updates, and augments the website and its underlying datasets on a regular basis (annually from 2012 onwards). (Ainsworth and Croenen 2018) Due to the large amount of material and its scattered provenance, an intentional decision was made that the publication of the DSE should be as early as possible, but the edition will be regularly updated.  2 Versioning as version management Versioning is implemented in the same way in the DSE as it can be found in software development. Version management systems save all changes to text documents as versions. All versions can be restored, and these systems are also designed to handle collaborative writing processes. Examples of DSEs that implement this versioning strategy are \"Papyri.info\" and \"The Devonshire Manuscript\". 'The Devonshire Manuscript' is a Wikibook edition, whose main editor is the Devonshire Manuscript Group. The aim is to discuss the edited sources as widely as possible and to change the role of the scientific editor from the sole authority for the text to that of a moderator: 'The social edition is a work that brings communities together to engage in conversation around a text formed and reformed through an ongoing, iterative, public editorial process.' (Wikibooks, 2014) One of the main contributors to this project is Ray Siemens (2012, 453), whose motivation to support the DSE with the help of social media is as follows: Such tools facilitate a model of textual interaction and intervention that encourage us to see the scholarly text as a process rather than a product, and the initial, primary editor as a facilitator, rather than a progenitor, of textual knowledge creation. (ibid.) It is therefore about the editorial process as an iterative and collaborative activity. In this respect, it is essential for the progress of the project to keep all iterations available in the form of commented versions. Like all pages of Wikipedia, all pages of a Wikibook are based on the same software MediaWiki (since version 1.5) and have a revision history (under the tab View history). The revision history in the form of a table contains all edits of a page in the wiki. Each change to a page creates a change line that contains information about the person who made the edit, the time when the edit was made, and a reference to the new wiki text in the text table. Elements of the revision table are preserved permanently, unless the page is deleted. 56  \fThe versions of Papyri.info are accessible in a similar way, but via a different interface. Papyri.info aggregates papyrological resources from different databases and makes them available for editing. This DSE has been in existence since 2006 and interested editors can still add or change data today. A peer review of the revisions ensures the quality of the content. The implementation of the strongly social and collaborative project approach is made possible by a version management software that manages different users and their contributions. A method that was developed to facilitate the software development process is used here to manage the editorial processes. The DSE is stored in a Git repository. All editorial processes are recorded, versioned and recoverable. A look at the repository in Github shows 100 contributors1 and more than 100,000 commits2. Git is a version control system that is used for collaborative software development. As already mentioned, the changes to the files are tracked. These programs provide access to any version of the file so that any changes can be undone. Each version has a timestamp and an author. It is always possible to see who changed what and when. In the case of Papyri.info, the edited texts are saved as XML files in Git. Via the platform Github the repository can be viewed and the different versions of the texts can be displayed. The version comparison is done line by line and any changes to the file will be recognized by the software and automatically saved as a new version. It makes no difference which version management software is used, the understanding of what a version is remains the same in the software development domain. In the examples mentioned so far, which have version documentation, the same version number stood for a whole series of revisions. If versioning takes place via version control systems, every saved change becomes a new version, which has no further semantic meaning. It makes no difference for the system if you make many changes or just fix a typo, it is always a new version. This strategy certainly has its advantages especially in a collaborative editing process.  3. Versioning as retrievable milestone versions In contrast to the open DSEs mentioned above, which are geared towards a high frequency of changes, DSEs of this type are updated at intervals under new version names. These versions can also be called milestone versions, because the question of when to publish the next update is a project specific decision. Former versions are findable via a permalink and can be retrieved. A version name or number applies to the entire edition. This versioning strategy is evident in teams of editors who deliberately publish changes, revisions, and enhancements collectively. Every single editing step is not shown. What is desired as a research process in the case of Wikibooks is not part of the intention to publish an edition in this case. This group includes the DSE \"Der Sturm. Digital Source Edition on the History of the International AvantGarde\", developed and edited by Marjam Trautmann and Torsten Schrade since 2018. This edition project is still a \"work in progress\": the team of editors will gradually open up new sources and publish them promptly. The motive for this approach is explained as follows: Dies kommt den interessierten Forschungscommunities zugute, da somit ein schneller Zugriff auf eine best\u00E4ndig wachsende Gesamtedition gew\u00E4hrleistet ist. Ein weiterer Vorteil dieses iterativen Vorgehens ist, dass sich somit auch das Forschungsdatenmodell und die ben\u00F6tigten Softwarekomponenten kontinuierlich und in Einklang mit den hinzukommenden Quellen und ihren jeweiligen Spezifika weiterentwickeln k\u00F6nnen. (https:\/\/sturm-edition.de\/projekt\/methodik.html) All developed sources have several permalinks that represent the versions. The permalinks are constructed in such a way that the identifier ends with the version information: Version 1: https:\/\/sturm-edition.de\/id\/Q.01.19140115.FMA.01\/1 Version 2: https:\/\/sturm-edition.de\/id\/Q.01.19140115.FMA.01\/2 'Humboldt Digital' also integrates this type of versioning. The edition \"Humboldt Digital\" is a publication of the Academy Project \"Alexander von Humboldt auf Reisen \u2013 Wissenschaft aus der Bewegung\" at the BerlinBrandenburg Academy of Sciences and Humanities. In this project, each entry offers the possibility to view past milestone versions. The specific version number is inserted after the domain name. E.g. 'https:\/\/ edition-humboldt.de\/v4\/H0002656'. 1  Github Glossary: 'A collaborator is a person with read and write access to a repository, who has been invited to contribute by the repository owner.' 2 Github Glossary: 'A commit, or \"revision\", is an individual change to a file (or set of files). It's like when you save a file, except with Git, every time you save it creates a unique ID (a.k.a. the \"SHA\" or \"hash\") that allows you to keep record of what changes were made when and by who. Commits usually contain a commit message which is a brief description of what changes were made.' 57  \f4. Mix type: Versioning as retrievable, documented milestone versions If milestone versions are also documented, then all three of the above criteria for versioning DSEs are met: changes will be published at intervals as new versions. Old versions remain viewable and what the version stands for and what has been changed are documented. An example of this is the 'August Wilhelm Schlegel Edition'. This DSE is about bringing together August Wilhelm Schlegel's correspondence. The project runs until 2020 under the direction of Jochen Strobel and Claudia Bamberg. The first beta version was published on 2 June 2014; version-07-19 was published on July 1, 2019. Specific versions can be addressed by entering the name after the domain. For example, a letter in the up-to-date version has a Permalink like this: 'https:\/\/august-wilhelm-schlegel.de\/version-07-19\/briefid\/1599'. Under the menu item 'Versions' is stated that every three months a new version with numerous resources will be published, while all previous versions remain fully accessible in the version archive. The DSE \"Johann Wolfgang Goethe: Faust\", edited by Anne Bohnenkamp, Silke Henke and Fotis Jannidis, also has a version archive. The entire project is versioned at regular intervals and the current version is 1.2. In this case, the version is specified in the subdomain: 'http:\/\/v1-2.faustedition.net\/document?sigil=B.a&page=59&view=print&section=5#l813'  5. Overview of applied versioning strategies version name  work in progress  X  X  X  X  Versioning as retrievable milestone versions  X  X  Mix type: Versioning as retrievable, documented milestone versions  X  X  Versioning as documentation  Versioning version management  as  collaborative  documented changes  former versions available  X  X  X  X  X  X  X  X  Table 1. Overview of applied versioning strategies. X means: is provided.  6. Conclusion The empirical analysis of DSE has shown that integrating versioning is still relatively rare in DSE projects. This is the case despite the fact that there are manifold practical motives for implementing a versioning strategy. Such as the idea that the editorial process should be shared with the public; the project has a long duration and editors would like to publish interim results; the material to be edited is too extensive and will be made accessible in publishable stages; or the editorial team and the collections are so scattered that it would make sense and be beneficial for the overall project to publish partial results. For whatever reason, all projects intend to communicate changes and to be transparent in such a way that makes the DSE more reliable and trustworthy. All projects presented here clearly pursue the strategy of making the DSE available to the public, although the editions are work in progress. The most technologically undemanding practice to communicate the changes is to maintain a version documentation. Many more ways to make changes transparent are given when the edition is linked to a version control system. In these cases, it is also easy to make a version comparison and 58  \fto understand the contributions of individual editors. Each resource has its own version history and can be retrieved. In the presented projects of the type \u2018versioning as retrievable milestone version' this is instead not the aim of the editors. The version number refers to the complete edition. Individual contributions can be searched under the milestone version number. A version control system is not absolutely necessary for the technical implementation, but it is an advantage. It is part of the workflow to have a published version and at the same time to have a new version in processing. The encapsulated data storage in the system is important for the addressability and availability of past versions. In addition, the information resources gain in quality if the traceability of the changes is also collectively available as documentation. By this means, versioning works like an apparatus in a broader sense, and one which verifies editorial decisions by making the work in progress transparent to the users."
	},
	{
		"id": 11,
		"title": "ELA: fasi del progetto, bilanci e prospettive",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Emmanuela Carbé",
			"Nicola Giannelli"
		],
		"body": "Introduzione  Tra le cinque fasi che dovrebbero caratterizzare ogni progetto, ovvero avvio, pianificazione, esecuzione, controllo e chiusura, l'ultima viene talvolta sottovalutata o non messa in atto, dimenticando che un progetto per definirsi tale deve avere le caratteristiche di unicità e durata limitata, con la produzione di un risultato il più possibile in linea con il piano di costi, tempo e qualità stabiliti. Ne consegue il rischio di trasformare un lavoro in un pericoloso work-in-progress, che può subire improvvisi arresti per mancanza di copertura finanziaria, per lo scioglimento del gruppo di lavoro, o per altre cause interne ed esterne. La qualità di un progetto andrebbe dunque valutata anche nella sua capacità di arrivare a una conclusione con la formalizzazione delle lezioni apprese (Mastrofini, 2017), incluse quelle meno positive (Dombrowski, 2019), al fine di costruire un patrimonio comune di conoscenze non meno importante del progetto stesso. Questo patrimonio, se conservato e condiviso, può essere utile per realizzazioni future e per la costruzione di nuove reti di collaborazione. Il presente contributo si propone di fare il punto su un progetto di start-up, DAS-MeMo1, avviato a marzo 2018 dall'Università di Siena sotto la direzione di Francesco Stella e in collaborazione con l'azienda QuestIT e la casa editrice Pacini. Questa fase del progetto prevede la realizzazione della piattaforma ELAEurasian Latin Archive, che raccoglie documenti in lingua latina e multilingua dal XII al XVIII secolo riguardanti l'Estremo Oriente. Il progetto è inserito all'interno di un contesto più ampio, caratterizzato da metodologie e strumenti di lavoro già collaudati grazie a numerose esperienze del PI e del suo gruppo di lavoro nell'ambito dei progetti digitali. Viene dunque tracciata la storia del progetto in tutte le sue fasi, dall'avvio del lavoro alla sua chiusura, prevista per febbraio 2020, e presentato un bilancio di ciò che è stato realizzato, delle lezioni apprese, e degli sviluppi futuri con l'avvio della fase di consolidamento e implementazione.  1  DAS-MeMo (Data Mining e analisi statistica su fonti testuali storiche del periodo medievale e moderno, www.dasmemo.unisi.it) ha ricevuto il contributo di Regione Toscana per un assegno di ricerca cofinanziato con le risorse POR FSE 2014-2020 nell'ambito del progetto Giovanisì. 61  \f2  Metodologia di progetto  Nella fase di avvio è stata elaborata la documentazione per il piano del progetto, basato su obiettivi, tempi e risorse. Nel corso della pianificazione sono stati individuati i passi da realizzare per il raggiungimento degli obiettivi, sono stati assegnati i ruoli e decise le tempistiche delle consegne. \u00C8 stata condotta un'analisi SWOT per identificare fin da subito le possibili problematiche e i fattori di rischio (si veda, a titolo di esempio, l'analisi applicata al caso BEIC di Consonni e Weston, 2015). La fase di start-up, della durata di 24 mesi, aveva i seguenti obiettivi: 1. Creazione di un modello e di un workflow di lavoro, includendo momenti di revisione e di controllo della qualità. 2. Definizione e creazione del corpus, con relativa codifica. 3. Progettazione, analisi dei requisiti e realizzazione, in collaborazione con l'azienda partner del progetto, di un prototipo della piattaforma, con: a. pagine informative gestibili tramite un comune CMS; b. digital library; c. tool di analisi linguistiche e semantiche; d. backend per la gestione della Digital Library. 4. Indagine preliminare di una parte del corpus con primi risultati, pubblicati in e-book grazie all'editore partner del progetto. 5. Disseminazione e comunicazione dei risultati su più livelli; 6. Realizzazione di un piano di sostenibilità. Dalla pianificazione del progetto è nata una lista di specifiche basate su metodo MoSCoW (vd. par. 3). Il progetto è stato monitorato sia internamente, con un controllo costante dello stato di avanzamento dei lavori anche per eventuali modifiche del cronoprogramma, sia esternamente, con relazioni intermedie inviate ai soggetti cofinanziatori del progetto. In questa ultima fase, ormai prossima alla chiusura dei lavori, rimane dunque da compiere una revisione generale e una valutazione di ciò che è stato fatto. In un intervento sul sito del King's Digital Lab, Arianna Ciula (2019a) spiega le motivazioni che hanno portato alla realizzazione della Checklist for Digital Outputs Assessment (Ciula, 2019b), una guida pubblicata per facilitare la revisione dei progetti in vista della prossima valutazione REF (Research Excellence Framework), utilizzata nel Regno Unito per monitorare la qualità della ricerca. Ciula rileva che se in altre discipline una valutazione e autovalutazione del lavoro parte da basi piuttosto definite, quando si opera nel contesto di progetti digitali stabilire dei criteri condivisi comporta criticità e incertezze. La checklist propone dodici punti tematici (con relativi esempi) per la valutazione di prodotti digitali, che abbiamo deciso di adottare in questa fase finale. I punti servono per verificare vari aspetti del progetto: i credits, con la corretta attribuzione dei lavori svolti (incluse le realizzazioni dei data model e dell'architettura) e degli enti che hanno partecipato e\/o finanziato il progetto, che devono essere correttamente menzionati con i loro loghi; il controllo delle licenze e copyright, che devono essere esplicite e il più possibile conformi ai principi FAIR; la messa a disposizione della documentazione relativa al progetto, che includa esplicitamente una riflessione sul valore aggiunto dei risultati conseguiti; l'attenzione all'accessibilità, alla user experience e alla funzionalità dell'interfaccia; controllo delle versioni del prodotto, con la conservazione delle eventuali versioni precedenti; la presenza di indicazioni su come citare, di identificatori persistenti e DOI; il piano di sostenibilità e accessibilità e delle informazioni sull'utilizzo del prodotto.  3  Analisi del corpus e creazione della piattaforma  Dopo una valutazione dei casi di studio, e in particolare di ALIM \u2013 Archivio della Latinità Italiana del Medioevo (Russo, 2005; Ferrarini, 2017; Manos, 2018), si è proceduto alla definizione del corpus attraverso un censimento di trecento testi, effettuato sulla base di alcuni repertori cartacei e online (tra questi: Bibliotheca Sinica 2.0 e CCT-Christian Texts Database). Il censimento è stato allestito e arricchito con l'utilizzo di OpenRefine (Hooland, Verborgh and De Vilde, 2013; Williamson, 2017), includendo i riferimenti bibliografici di ogni record, l'eventuale presenza di digitalizzazioni online e i diritti di utilizzo della risorsa. In corso d'opera sono stati aggregati metadati relativi allo stato di avanzamento del progetto: se un testo è preso in carico dal gruppo di lavoro, sono aggiunte informazioni circa il software utilizzato per l'eventuale digitalizzazione o trattamento degli OCR, il responsabile della trascrizione e codifica (con relativo ORCID), i tempi di realizzazione, la licenza di pubblicazione, il grado di attendibilità della risorsa. \u00C8 stato poi creato un modello di lavoro per la realizzazione della piattaforma, basato sulla MoSCoW analysis, che ha permesso di focalizzare meglio gli obiettivi prioritari (Must have), i desiderabili ma non essenziali (Should have), i desiderabili ma non strettamente necessari (Could Have), e quelli che possono 62  \fessere pianificati per il futuro (Would have). Si dà qui conto dei contenuti essenziali: 1. Must have: una piattaforma che raccoglie testi latini contenenti inserti multilingua (cinese, giapponese, coreano, ma anche pinyin); un solido motore di ricerca, in grado di realizzare ricerche mirate anche con filtri e indicizzazioni in base a una lista predefinita di metadati; un modello di codifica in XML TEI; possibilità di visualizzare il testo e scaricarlo in più formati (TXT; PDF; XML); un backend per il caricamento e la gestione dei documenti; un identificativo persistente per ciascun documento; un set base di tool per analizzare i documenti (indici di parole, lemmi, type, stopwords; frequenze assolute e relative; concordanze; Type\/Token Ratio, N-grams; numero totale di parole per documento e altre analisi quantitative di semplice acquisizione); definizione della user policy; messa a punto di metodi di lavoro collaborativi per future implementazioni del progetto con più unità di ricerca (es. Wiki). 2. Should have: tecniche di georeferenziazione; codifica di luoghi, date e persone menzionati nei testi; strumenti più raffinati per l'analisi linguistica dei testi (PoS, Burrows Delta, frequenze in base a sillabe) anche in considerazione del problema specifico che pongono i documenti multilingua; analisi semantica dei testi; topic modeling; un progetto più complesso di backend per i collaboratori, con la possibilità di modificare i documenti attraverso un editor di testo e di gestire il flusso di lavoro. 3. Could have: un progetto più specifico per alcune tipologie di documenti, come ad esempio le lettere, numerose all'interno del corpus; la possibilità di creazione, da parte dell'utente, di un corpus personalizzato con analisi testuali comparate attivando processi in tempo reale; integrazione della codifica TEI con modelli semantici (Ciotti et al., 2016; Ciotti, 2018); Named Entity Recognition per luoghi, persone e date (Erdmann et al., 2016; Simon et al., 2017); 4. Would have: inclusione delle digitalizzazioni dei documenti (Rosselli Del Turco, 2015; 2019), che talvolta contengono disegni, mappe e altre rappresentazione grafiche di particolare rilevanza.  Figure 1. Il prototipo della piattaforma.  Il prototipo della piattaforma è stato realizzato con più componenti: per l'interfaccia è stato utilizzato il CMS Wordpress, scelto soprattutto per una facile gestione delle pagine informative e per la pubblicazione della documentazione del progetto; la Digital Library è sviluppata in Java EE, con una UI basata su tecnologie web e realizzata utilizzando Javascript; si basa sul motore di ricerca Elastic Search con possibilità di effettuare ricerche full text anche con l'utilizzo della sintassi Lucene. Al suo interno è stato aggiunto un tool dedicato alle analisi linguistiche, attualmente alla sua versione 1.0: si tratta di un strumento realizzato in Python e basato su CLTK (Burns, 2019) e NLTK (Bird et al., 2015), chiamato ELA-tool, che restituisce in formato JSON i risultati delle analisi linguistiche previste dai requisiti primari del progetto (Must Have). Nel momento in cui avviene l'upload del documento, ELA-tool processa il testo. I risultati vengono poi memorizzati e utilizzati sia per la visualizzazione delle analisi linguistiche, sia da Elastic Search per raffinare le possibilità di ricerca. Nell'ottica di poter effettuare continue migliorie del tool procedendo con il rilascio  63  \fdi nuove versioni perfezionate, è stato previsto che nel backend per l'upload e la gestione dei documenti sia presente una funzione di refresh che esegue i processi di ELA-tool sui testi già caricati in precedenza su esplicita richiesta di un utente amministratore della piattaforma.  Figure 2. Visualizzazione dei risultati processati dal tool di analisi linguistica.  I testi sono codificati in XML seguendo lo schema TEI P5 (Tei Consortium 2015), con un TEI header modellato grazie al censimento realizzato con OpenRefine; si pone una particolare attenzione alla codifica di luoghi, date, persone (Wikidata, VIAF e, per i luoghi, primariamente Pleiades Gazetteer) e l'eventuale presenza di inserti in lingue diverse dal latino (è il caso, ad esempio, di Sapientia Sinica di Costa e Intorcetta).  4  Bilanci, prospettive  Alla conclusione della fase di start-up, ELA ospiterà i primi cento testi tratti dal censimento, scelti per importanza e per varietà nelle caratteristiche, per permettere una verifica sul campo del modello di codifica scelto e intervenire con correzioni in corso di implementazione. La piattaforma sarà ospitata presso il centro di calcolo dell'Università di Siena, con un progetto di business continuity e disaster recovery, con un piano programmato di snapshot e backup. Rispetto agli obiettivi del progetto, i 'must have' risultano oggi completamente raggiunti, così come quasi tutti i 'should have'. In questo contesto, pare utile una revisione e valutazione del lavoro sulla base di indagini qualitative e quantitative. Tra queste, appare prioritaria un'analisi dei risultati ottenuti dal sistema di lemmatizzazione di CLTK, attraverso una comparazione con altri lemmatizzatori, sulla scorta di Mambrini e Passarotti (2019) e Eger et al. (2015, 2016). La revisione del lavoro potrà includere anche una valutazione sulla user experience della piattaforma, con un questionario per gli utenti che utilizzano il prototipo. Se la fase di start-up è prossima alla sua conclusione, quella di consolidamento e implementazione di Eurasian Latin Archive va ora pianificata nell'ottica della sostenibilità sul medio e lungo periodo per il raggiungimento di risultati più ambiziosi: l'avvio a regime della piattaforma e la programmazione del suo incremento in termini di numero di documenti trattati, con un piano redazionale che comprenda anche la pubblicazione e il mantenimento di tutte le sezioni della piattaforma; l'ampliamento dell'utenza prevista: allo stato attuale ELA è pensato soprattutto per un pubblico specializzato, ma non si esclude, in futuro, la possibilità di rivolgersi a una comunità più ampia di utenti, anche attraverso nuovi percorsi di disseminazione e riutilizzo dei materiali; infine l'integrazione con altri progetti: ELA è stato primariamente pensato per essere interoperabile con ALIM, tuttavia si ritengono necessari, anche per la sostenibilità del  64  \fprogetto stesso, la condivisione dei dati e degli strumenti realizzati (che saranno messi a disposizione su GitHub) e un dialogo costante per operare a fianco di altri progetti in corso (Passarotti et al., 2019). "
	},
	{
		"id": 12,
		"title": "Digitized and Digitalized Humanities: Words and Identity",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Claire Clivaz"
		],
		"body": "1 Introduction: Words and Identity in Digital Humanities As the 2020 AIUCD conference topic underlines, the identity and definition of the Humanities that has met the computing world, is in constant reshaping (Ciotti, 2019)1. The English language has acknowledged the important turn from humanities computing to digital humanities at the beginning of the 21st century (Kirschenbaum, 2010), whereas French-speaking scholarship is wrestling between humanit\u00E9s num\u00E9riques (Berra, 2012; Doueihi, 2014) and humanit\u00E9s digitales (LeDeuff, 2016; Cormerais\u2013Gilbert, 2016; Clivaz, 2019). Moreover, new words are often tested to express the intensity of what is at stake: if Jones has chosen the term 'eversion' for describing the present state of the digital turn (Jones, 2016), the French thinker Bernard Stiegler focuses on 'disruption' (Stiegler, 2016). German and Hebrew link digital humanities naming with the vocabulary of spirit\/mind, whereas the outmoded word humanit\u00E9s has come back in French through the naming of the humanit\u00E9s num\u00E9riques, recalling the presence of the body (Clivaz, 2017). Inscribed in this linguistic effervescence, a phenomenon has so far not drawn the attention of the humanist scholarship: the difference between digitization and digitalization, or between digitized and digitalized Humanities. The present paper will explore, as far as possible, the emergence of this dualistic vocabulary, inside and outside of digital humanities scholarship, looking for its meanings and implications. It represents only a first overview about the scare definitions and occasional uses of 'digitalization', even if the debate between digitization and digitalization can sometimes inform implicitly the discourse, as we will see in Section 4 (Smithies, 2017). Section 2 will first comment similarity and difference between both words, looking for 'digitalization' definitions, and its uses. Section 3 discusses in detail the only definition article we have so far debating these two concepts. Section 4 considers more broadly the digitalization perspective and presents the author's point of view on the issue, including its articulation to the AIUCD 2020 topic.  2 Looking for 'digitalization' definition and uses English native speakers would surely ask first if there is really a difference between 'digitization' and 'digitalization.' 'Digitalization' does not benefit from its own entry in Wikipedia or in the Collins Dictionary  1  Many thanks are due to the reviewers for their remarks, to Andrea Stevens for her English proof-reading, and to Elena Giglia for her translation of the Italian abstract. 67  \fonline.2 However, the Oxford English Dictionary (OED) dates the first use of digitalization as equivalent to digitization in 1959, 3 whereas the medical sense appeared in 1876. 4 OED presents also digitalization as meaning 'the adoption or increase in use of digital or computer technology by an organization, industry, country, etc.'5 In the Wikipedia entry 'digital transformation', a similar definition is given for 'digitalization': 'unlike digitization, digitalization is the \u2018organizational process' or \u2018business process' of the technologicallyinduced change within industries, organizations, markets and branches.'6 A most decisive shift in the sense of a difference between the two words can be seen in the International Encyclopedia of Communication Theory and Philosophy, which published an entry on 'Digitalization' by J. Scott Brennen and Daniel Kreiss in 2016. They argue in favour of a distinction from 'digitization' (Brennen\u2013Kreiss, 2016). This publication is in itself a quite clear signal, according to our cultural and scholarly habits, that 'digitalization' exists with its own meanings, since it has been defined in an encyclopedia. As far as I have been able to determine, it is the only article trying to define both concepts and is discussed in detail in Section 3. As we see, references to digitalization's definition are quite scare. So far, there it is not even possible to do a systematic overview of its theoretical background based in the scholarly literature because it is not discussed, with the exception of the Brennen\u2013Kreiss article. But if we look at its uses, some aspects clearly emerge. 'Digitalization' is mainly used in the business and economical world, and very infrequently in digital humanities. For example, according to Jari Collin in a 2015 Finnish volume of collected essays, digitalization refers to the understanding of 'the dualistic role of IT in order to make right strategic decisions on IT priorities and on the budget for the coming years. IT should not be seen only as a cost center function anymore!' (Collin, 2015, 30). Digitalization seems to be 'one of the major trends changing society and business. Digitalization causes changes for companies due to the adoption of digital technologies in the organization or in the operation environment' (Parvianien et al., 2017, 63). According to M\u00E4enp\u00E4\u00E4 and Korhonen, 'from the retail business point of view, the \u2018digitalization of the consumer' is of essence. People are increasingly able to use digital services and are even beginning to expect them. To a certain extent, this is a generational issue. The younger generations, such as Millennials, are growing up with digitalization and are eagerly in the forefront of adopting new technology and its affordances' (M\u00E4enp\u00E4\u00E4\u2013Korhonen, 2015, 90). In 2018, Toni Ryyn\u00E4en and Torsti Hyyryl\u00E4inen, members of the Helsinki Institute of Sustainability Science at the Faculty of Agriculture and Forestry, published an article seeking to fill the gap between the digitalization process and digital humanities, by focusing on the concern for 'new forms of e-commerce, changing consumer roles and the digital virtual consumption' (Ryyn\u00E4en \u2013 Hyyryl\u00E4inen, 2018, 1). In this process, the role of digital humanities is described in a way that is quite hard to recognize for DHers, at least for those not involved in digital social sciences: 'A challenge for digital humanities research is how to outline the most interesting phenomena from the endless pool of consumption activities and practices. Another challenge is how to define a combination of accessible datasets needed for solving the chosen research tasks' (Ryyn\u00E4en \u2013 Hyyryl\u00E4inen, 2018, 1). In light of such clear descriptions of what 'digitalization' means for business and economy, digital humanities scholarship demonstrates a deafening silence about this notion. The 2004 and 2016 editions of the reference work Companion to Digital Humanities do not mention the word. In the established series Debates in the Digital Humanities, one finds one occurrence in the five volumes, under the pen of Domenico Fiormonte (2016). As a third example, the collected essays Text and Genre in Reconstruction: Effects of Digitalization on Ideas, Behaviours, Products and Institutions, edited by Willard McCarty (2010), can only surprise the reader: indeed, 'digitalization' stands in the title, but the word is then totally absent from the volume. When questioned about this discrepancy, McCarty answered that the publisher had requested to have this word in the title. This request has led to a damaging side effect in terms of Google searches: if one searches for 'digitalization' and 'digital humanities', one gets several book titles that do not contain no mention of this word other than a reference to Text and Genre's title. It is also the case in my 2019 book Ecritures digitales.  Entry 'digitization' in Wikipedia: https:\/\/en.wikipedia.org\/wiki\/Digitization; entry 'digitalize' in the Collins Dictionary online: https:\/\/www.collinsdictionary.com\/dictionary\/english\/digitalize. All hyperlinks have been last checked on 30\/11\/19. 2  Entry 'digitalization n.2', OED, https:\/\/www.oed.com\/view\/Entry\/242061 Entry 'digitalization n.1', OED, https:\/\/www.oed.com\/view\/Entry\/52616: 'the administration of digitalis or any of its constituent cardiac glycosides to a person or animal, esp. in such a way as to achieve and maintain optimum blood levels of the drug. Also: the physiological condition resulting from this'.  3 4  Entry 'digitalization n.2' in the Oxford English Dictionary online: https:\/\/www.oed.com\/view\/Entry\/24206 Entry 'digital transformation' in Wikipedia: https:\/\/en.wikipedia.org\/wiki\/Digital_transformation#Digitization_(of_information) 5  6  68  \fDigital writing, digital Scriptures: the unique occurrence of 'digitalization' occurs in my reference to McCarty's collected essays (Clivaz, 2019). One can sometimes meet infrequent uses of digitalization in digital humanities, such as a 2013 article by Amelia Sanz. She uses the word to describe Google Books and the Hathi Trust's effect on Spanish literature: 'Digital Libraries as Google Books or Hathi Trust include numerous works belonging to our study period among its digitalized collections in US universities, because most of these forgotten authors make part of the Spanish diaspora after the Civil War (1936-39) and during the subsequent dictatorship (1939-1975). In fact, European copyright legislation has made Google digitalize only works prior to 1870 in Spain, and, unfortunately for Spanish researchers, those works appear to be in \u2018limited access' due to the existing diffusion\/circulation rights, but available in \u2018full text' mode for researchers located in the US' (Sanz, 2013, n.p.). The two italicized words are the unique occurrences of digitalization vocabulary in an article focused on the effects of digitization. When asked about her use of these two words, Sanz answered that it was probably a misuse of language, since she is not a native English speaker. Usually in digital humanities scholarship, one speaks about 'Humanities digitized' (Shaw, 2012)7, and the mutation to the digital sphere is seen as a pre-step before the processes of interpretation.8 Uses of digitalization and cognate terms remain rare, like Domenico Fiormonte, who is also a non-native English speaker and the only one to use digitalization in the series Debates in Digital Humanities: 'In the last ten years, the extended colonization, both material and symbolical, of digital technologies has completely overwhelmed the research and educational world. Digitalization has become not only a vogue or an imperative, but a normality. In this sort of \u2018gold rush', the digital humanities perhaps have been losing their original openness and revolutionary potential' (Fiormonte, 2016, n.p.). Fiormonte compares digitalization to a colonization process: if there is some consciousness of the digitalization vocabulary in humanities, it can be indeed found in research about cultural diversity and colonialism, such as in a 2007 article by Maja van der Velden, 'Invisibility and the Ethics of the Digitalization: Designing so as not to Hurt Others.' Van der Velden studies 'the designs of Indymedia, an Internet-based alternative media network, and TAMI, an Aboriginal database, [...] informed by the confrontations over different ways of knowing' (2007, 81). She points to the fact that, 'if we understand knowledge not as a commodity but as a process of knowing, something produced socially, we must ask about the nature of digitalization itself. As the Aboriginal elders say, \u2018Things are not real without their story'' (2007, 82). She documents in this way two examples of nonWestern digital projects, in which the diversity of the source codes and standards has led to recurrent negotiations: 'the confrontations over issues of privacy and control resulted in different ways of organizing access and information management' (2007, 89). Van der Velden's article allows one to understand, from a humanist point of view, what is at stake in the concept of digitalization, a perspective that the next section develops. But it should be underlined that, even in this article pointing to cultural and digital control issues, digitalization is not discussed as such. The apparent lack of awareness about this binomial vocabulary and its implication for DH scholarly literature appears to be a real blind spot that section 4 considers.  3 Claiming a Critical Use of Digitalization in Humanities In their overview article, Brennen and Kreiss give a general definition of 'digitalization' similar to the one presented in Section 2: 'We [...] define digitization as the material process of converting analog streams of information into digital bits. In contrast, we refer to digitalization as the way many domains of social life are restructured around digital communication and media infrastructures' (Brennen\u2013Kreiss, 2016, 1). They usefully remind us that 'digitization is a process that has both symbolic and material dimensions' (2016, 2), and that 'analog and digital media, [...] all forms of mediation necessarily interpret the world' (2016, 3). The authors also consider that 'the first contemporary use of the term \u2018digitalization' in conjunction with computerization appeared in a 1971 essay first published in the North American Review. In it, Robert Wachal discusses the social implications of the \u2018digitalization of society' in the context of considering objections to, and the potential for, computer-assisted humanities research. From this beginning, writing about digitalization has grown into a massive literature' (2016, 5). The reference to Wachal's article is a very interesting one, and it deserves more attention than the co-authors devote to it. Moreover, they omit any reference to Maja van der Velden's article or to similar approaches in Brennen and Kreiss's article. The 'winners' of their digitalization 7 One can also see uses of digitalization in the humanities in archaeology, notably in conjunction with 3D discussion (Ercek \u2013Viviers \u2013Warz\u00E9e, 2009). 8 See Earhart \u2013 Taylor (2016): 'Our White Violence, Black Resistance project merges foundational digital humanities approaches with issues of social justice by engaging students and the community in digitizing and interpreting historical moments of racial conflicts.'  69  \fdefinition are scholars from the vein of Manuel Castells, who argues that 'technology is society, and society cannot be understood or represented without its technological tools' (Brennen\u2013Kreiss, 2016, 5). To get a deeper understanding of the critical potential of digitalization, it is worth reading Wachal's 1971 article. He uses digitalization in just one sentence: 'The humanist's fears are not entirely without foundation, and in any case, as a humane man he naturally fears the digitalization of the society. He doesn't like to be computed. He doesn't want to be randomly fingered by a credit card company computer' (1971, 30). The entire article is an ironic confrontation between the habits of a humanist scholar and what a programmer and a computer could do for humanities. As a computer programmer teacher himself, Wachal remembers the term coined by Theodor Nelson, 'cybercrud': 'putting things over on people [by] saying using computers. When you consider that this includes everything from intimidation (\u2018Because we are using automatic computers, it is necessary to assign common expiration dates to all subscriptions') to mal implementation (\u2018You're going to have to shorten your name - it doesn't fit in to the computer'), it may be that cybercrud is one of the most important activities of the computer field' (1971, 30). In other words, computer scholars have a clear awareness about their world, as Nelson and Wachal after him demonstrate. After this captatio benevolentiae, Wachal raises what is for him the main issue with the humanist point of view on computing: 'Dare we hope that the day has come when humanists will begin asking some new questions?' (1971, 33), referring also to artificial intelligence (1971, 31). His 'personal view', as announced in the article title, is an open call that is still worth humanist scholars' attention. The complex elements of the discussion of the digitization\/digitized vs digitalization\/digitalized divide indicates that it is surely time for DHers to pay attention to this binomial expression, so successfully deployed in business or economy that a publisher can get it in a title of collected essays that does not contain the word digitalization at all. It is time to form an understanding of digitalization that still denounces 'cybercrud' when needed, or helps us to pay attention to 'the confrontations over issues of privacy and control resulted in different ways of organizing access and information management' (van der Velden, 2007, 89). To express it in an electronic vocabulary, Brennen and Kreiss present a 'path of least resistance' to the definition of digitalization, according to the path describing the third potential state of an electronic circuit (open, closed, or not working), because electricity follows the 'path of least resistance.' 9 But it is a core skill of the humanities to renounce the paths of least resistance and to wrestle with words, concepts, and realities. In that perspective, the last Section will develop some tracks to further the debate.  4 The effect of the 'digitalization' perspective The binomial expression 'digitization' versus 'digitalization' enters in the international debate through the English language. Such a distinction does not exist in French, Italian, or German, for example. But the inquiry of this article demonstrates that it this concept is worthy of exploration in an effort to grasp what is at stake in an explicit way in the English language. It represents surely one further argument in favor of a multilingual approach to digital epistemology, like the one developed in Digital writing, digital Scriptures (Clivaz, 2019). I firstly underline how striking it is that even in the few occurrences where humanist scholars consciously use the term 'digitalization' (van der Velden, Fiormonte), it is not discussed per se: a blind point exists in the scholarly discussion apart of Brennen and Kreiss's article. After all, the first use of 'digitalization' in relation to the computer sphere was by a programmer (Wachal, 1971), but nowadays its use in critical discussion is mainly found under the pen of scholars outside of humanities who make claims about the 'essence' of 'the \u2018digitalization of the consumer'' (M\u00E4enp\u00E4\u00E4\u2013Korhonen, 2015, 90; quoted in Section 2). In light of this consumerist perspective, DH scholars are generally confident in the traditional critical impact of their methodologies and knowledge. Alan Liu, for example, writes that 'the digital humanities serve as a shadow play for a future form of the humanities that wishes to include what contemporary society values about the digital without losing its soul to other domains of knowledge work that have gone digital to stake their claim to that society' (2013, 410). In the same line, the HERA 2017 call hopes that the humanities, when digitized, will be able 'to deepen the theoretical and empirical cultural understanding of public spaces in a European context.'10 But it could secondly be argued that the blind point of the absent discussion about digitization\/digitalization demonstrates an overconfidence of the digital humanities in its capacity to not lose the soul of the humanities in digital networks. Other voices are indeed more sensitive to the limitations imposed on humanities research See 'Path of Leaf Resistance', Wikipedia, https:\/\/en.m.wikipedia.org\/wiki\/Path_of_least_resistance See 'HERA Public Spaces', 31.08.17, http:\/\/heranet.info\/2017\/08\/31\/hera-launches-its-fourthjoint-research-programme-public-spaces\/ 9  10  70  \fby digital constraints, as we have seen with Maja van der Velden: even if she uses the word 'digitalization' without discussing it, her article clearly points to digital control issues in the practice of building a database or a virtual research environment. From a more general and theoretical point of view, James Smithies strongly underlines in his book The Digital Humanities and the Digital Modern the same issues, even if the word digitalization is totally absent in it. He suggests that 'our digital infrastructure [\u2026] has grown opaque and has extended into areas well outside scholarly or even governmental control' (2017, 11). His discourse becomes overtly political when he affirms the existence of a 'point of entanglement between the humanities and neoliberalism, implicating digital humanists and their critics in equal measure' (2017, 218). We are probably reaching here the main root of the silence about the digitization\/digitalization challenge in DH debates: this binomial expression points to the political dimension of the digital revolution in humanities, to its economic and institutional implications, something that we prefer to let aside, consciously or unconsciously. This fear is also described by Wachal: 'The humanist's fears are not entirely without foundation, and in any case, as a humane man he naturally fears the digitalization of the society' (1971, 30; quoted in Section 3). Listening to Wachal, and almost fifty years later to Smithies, can begin to lead us beyond the 'path of leaf resistance' of Brennen and Kreiss. We should consider digitalization rather as the top of a mountain: it can be reached only through the via ferrata of the debates about cultural and multilingual diversity, about multiple source codes and standards, a multiplicity that preserves, at the end, diversity in humancomputing knowledge productions. Moreover, we are probably reaching right now the start of the DH awareness of this linguistic debate. As I end this article, I have opened the debate in the list Humanist Discussion Group and Simon Tanner has signaled his interest in the point, referring to Brennen and Kreiss' definition: 'I have found the difference to be significant enough to seek to define it for my current book and in the past it has been a source of confusion or conflation that has not been helpful. I make it very clear to our students in the Masters of Digital Humanities or the MA Digital Asset and Media Management that they should not use the interchangeably' (Tanner, 2019). Third, since the binomial expression digitization\/digitalization is a vehicle for its own impact and meaning within the DH epistemology, is it possible to tie these concepts to the general challenge raised by the AIUCD 2020 call for papers? Notably, this discussion raises the following questions: 'is it still necessary to talk about (and make) a distinction between \u2018traditional' humanists and \u2018digital' humanists? Is the term \u2018Digital Humanities' still appropriate or should it be replaced with \u2018Computational Humanities' or \u2018Humanities Computing'? Is the computational dimension of the research projects typically presented at AIUCD conferences that methodologically distinctive?'11 At the root of these problems stands of course an important debate in Italian speaking DH, present in the name itself of the national DH organization, the AIUCD. This name mentions 'Humanities Computing' (informatica umanistica) and 'digital culture' (cultura digitale): AIUCD - Associazione per l'Informatica Umanistica e la Cultura Digitale.12 But beyond this specific Italian perspective, the importance of collaboration between DHers and other humanist scholars concerns all of us. The dialectic between Humanities Computing and Digital Humanities will in all cases remain in the historical memory of the DH development. But I am personally not convinced that a 'step back' in the form of a return to Humanities Computing, motivated by a desire to keep all the humanists together under the banner of the informatica umanistica, is viable. Why? When the Harvard Magazine published in 2012 one of its first articles about the digital humanities, it was entitled 'Humanities Digitized' (Shaw, 2012). It has always been meaningful for me to think in that direction. As I have argued elsewhere in detail, we could 'begin to speak about the digitized humanities, or simply about humanities again, instead of digital humanities. Such an evolution might occur, if one looks at the evolution of the expression \u2018digital computer' which was in common usage during the fifties, but it has been now replaced by the single latter word \u2018computer' (Williams, 1984, 310; Dennhardt, 2016). When humanities finally become almost entirely digitized, perhaps it is safe to bet that we will once again speak simply about humanities in English or about humanit\u00E9s in French, thus making this outmoded word again meaningful through the process of cultural digitization' (Clivaz, 2019, 85\u201386). According to this perspective, the debate between 'humanities digitized' or 'humanities digitalized', with all its cultural, economic, material, institutional and political dimensions, could signal a third step after Humanities Computing and Digital Humanities. This third step would stand at the crossroads where all humanists could meet up again, in an academic world definitively digitized, but hopefully not totally digitalized. It is up to all of us to decide if, in the third millennium, Humanities will be digitized or digitalized. 11  See 'Convegno annuale dell'Associazione per l'Informatica Umanistica e la Cultura Digitale. Call for papers', https:\/\/aiucd2020.unicatt.it\/aiucd-call-for-papers-1683. 12 See AIUCD, www.aiucd.it. 71"
	},
	{
		"id": 13,
		"title": "La geolinguistica digitale e le sfide lessicografiche nell’era delle digital humanities: l’esempio di VerbaAlpina",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Beatrice Colcuc"
		],
		"body": " Trattare i dati (lessicografici) nell'era delle digital humanities  La ricerca dell'era pre-digitale è stata contraddistinta da modalità di concezione progettuale fortemente individuali: la raccolta, l'analisi e l'illustrazione dei dati venivano effettuate da persone (o gruppi di persone) che operavano singolarmente. La comunicazione scientifica si compiva attraverso le pubblicazioni dei libri cartacei, i quali venivano conservati in luoghi circoscritti come ad esempio le biblioteche, e ciascuno studio rappresentava un progetto concluso in sé (cfr. Krefeld, 2016). Inoltre, molti dati rimanevano nelle mani degli stessi ricercatori che li avevano raccolti e, in generale, la comunità scientifica non compiva molti sforzi per mettere a disposizione del grande pubblico i dati provenienti dai diversi progetti scientifici. Ormai, l'avvento delle nuove tecnologie non è più un fenomeno di recente datazione. Il passaggio dal cartaceo al digitale è avvenuto in maniera sempre più repentina, contribuendo al mutamento dell'approccio scientifico nei confronti delle modalità di ricerca. La rivoluzione digitale ha rinsaldato l'idea della condivisione e, allo stesso tempo, provveduto a fornire gli strumenti necessari per favorire e facilitare l'interscambio di dati come pure, più in generale, l'interazione tra diversi progetti. Ciononostante, la creazione di una rete di progetti e la condivisione dei relativi dati non sono fornite da una mera digitalizzazione degli strumenti di ricerca. La collaborazione rappresenta l'essenza primaria del fare scienza, poiché è sulla base delle conoscenze già presenti che si costruisce il progresso. Per operare in maniera collettiva nell'era digitale è però necessario che i dati di ricerca soddisfino alcuni requisiti fondamentali. In primo luogo i dati devono essere strutturati, descritti  74  \fed eventualmente etichettati in maniera tale da prestarsi a essere maneggiati in sedi esterne al loro progetto originario e devono poter continuare a essere accessibili anche in un momento successivo all'eventuale chiusura del progetto. In questo contesto si inserisce l'idea di Web Semantico (e successivamente dei Linked Open Data), nata con l'obiettivo di rimodellare l'ambiente virtuale di internet. Numerosi sono i progetti che si collocano all'interno di questo pensiero: essi formano la cosiddetta Linked Open Data Cloud (cfr. Cyganiak e Jentzsch, 2007 -) e costituiscono, al contempo, una comunità interconnessa attraverso relazioni basate sui loro insiemi di dati (cfr. Bizer et al. 2009, 154). Le esigenze di condivisione e connessione dei dati sul web sono inoltre state formulate in maniera esplicita nel 2016, quando un grande numero di ricercatori provenienti da diversi Paesi ha pubblicato le linee guida per la gestione moderna dei dati di ricerca (cfr. Wilkinson, Dumontier et al., 2016). Tali raccomandazioni sono state racchiuse nell'acronimo FAIR, una sigla che raccoglie i quattro princìpi fondamentali sui quali dovrebbero essere basate la comunicazione e la cooperazione scientifiche nell'era digitale. Secondo tali princìpi, i dati della ricerca dovrebbero essere rintracciabili (findable), accessibili (accessible), interoperabili (interoperable) e riutilizzabili (reusable). Per essere rintracciabili, i progetti ai quali i dati appartengono, devono essere reperibili attraverso portali centrali quali, ad esempio, i cataloghi delle biblioteche. I dati di ricerca che non sono soggetti ad alcuna restrizione giuridica (come potrebbero essere ad esempio i dati strettamente personali) devono essere messi a disposizione del grande pubblico e rinunciare, di conseguenza, al diritto d'autore. Al fine di poter essere interoperabili, inoltre, i dati devono essere innanzitutto scissi, successivamente strutturati ed essere descritti in maniera precisa. Il riutilizzo dei dati si rende infine possibile attraverso una corretta applicazione dei tre princìpi precedenti: si tratta di princìpi estensibili che non si lasciano mettere in contrapposizione l'un l'altro (cfr. Force11, 2011-2017; GoFair, 2011-2017; L\u00FCcke, 2018). Le possibilità offerte dalla digitalizzazione in termini tecnici hanno altresì consentito di valutare da un nuovo punto di vista una delle questioni storiche relative al trattamento dei dati lessicografici. In linea di massima, le opere lessicografiche possono essere strutturate in maniera semasiologica (le parole di un dato idioma sono elencate seguite dal loro significato) oppure onomasiologica (si descrivono i significati e vi si collegano le diverse parole che ad essi conducono). Nell'era analogica tali opere erano strutturate secondo uno o secondo l'altro modo: in ambito romanzo, si ricordino, tra gli altri, l'atlante linguistico ed etnografico dell'Italia e della Svizzera meridionale (AIS), di stampo onomasiologico, oppure, il Dicziunari Rumantsch Grischun (DRG), strutturato, invece, in maniera semasiologica. Una concezione in entrambi i sensi non era possibile a causa di limiti pratici imposti dalle modalità di pubblicazione del passato, mentre oggigiorno, le possibilità fornite dalla digitalizzazione offrono nuovi strumenti e la concezione di un'opera lessicografica che vada in due direzioni è realizzabile. Ciononostante, benché l'aspetto tecnico non rappresenti più alcun problema alla messa in pratica di tale approccio bidirezionale, le sfide si aprono soprattutto dal punto di vista contenutistico, come si evincerà dai capitoli che seguiranno. La compilazione digitale di opere lessicografiche quali i dizionari sembra essere oggi un'attività relativamente consolidata. Non sono rari i dizionari che offrono la possibilità di essere consultati in rete, anche se, alcuni di essi, non si sono mai realmente distanziati da una concezione cartacea. Tali opere, presentano ancora un grande margine di sviluppo e ampliamento per potersi definire digitalizzate in modo interoperabile secondo i gradi definiti in L\u00FCcke (2016). A titolo illustrativo, ma non esaustivo, si pensi alla versione online del Romanisches Etymologisches W\u00F6rterbuch (Meyer-L\u00FCbke, 1935): il contenuto dell'opera è messo a disposizione online in formato PDF, ma, ai sensi della digitalizzazione, in essa potrebbero essere implementate diverse funzioni, tra cui, ad esempio, la ricerca di singoli lemmi. Lo scopo di una lessicografia basata sul web non si limita alla mera presentazione virtuale di una determinata opera. Lo sviluppo digitale è bensì rappresentato da un più ampio tentativo di costituzione di reti lessicali e semantiche messe in relazione tra di loro. Progetti volti a fornire tali interconnessioni sono stati iniziati già verso la metà degli anni Ottanta. A titolo esemplificativo si pensi a WordNet, il database lessicale per la lingua inglese, ma anche a EuroWordNet nato negli anni Novanta come rete semantica per le lingue europee (cfr. Fellbaum 2006, 665, 669). Tali progetti costituiscono un primo tentativo di strutturare il materiale in maniera semantica e non solo lessicale come si è soliti fare nei dizionari cartacei e, soprattutto, si inseriscono nel panorama dei lavori relativi all'elaborazione del linguaggio naturale multilingue. In tempi più recenti si colloca la concezione di BabelNet (https:\/\/babelnet.org\/), una rete semantica multilingue, automatizzata e di ampia copertura, ovvero un dizionario enciclopedico costituito unendo il contenuto lessicale di WordNet al sapere enciclopedico di Wikipedia attraverso processi automatizzati di integrazione dei contenuti di ambedue i database (cfr. Navigli e Ponzetto, 2010, 216).  75  \f2 VerbaAlpina: geolinguistica e lessicografia digitali Mentre la digitalizzazione dei primi dizionari e corpora lessicali si colloca tra gli anni Ottanta e gli anni Novanta (cfr. Chiari 2012, 98), più recente risulta invece essere il passaggio dal cartaceo al digitale per quanto riguarda gli atlanti linguistici. Relativamente all'area alpino-romanza, diversi atlanti sono oggi disponibili in rete in formati PDF o JPG ma non hanno percorso tutte le tappe del passaggio dal cartaceo al digitale (cfr. L\u00FCcke, 2016; Knapp, 2017). \u00C8, a titolo esemplificativo, il caso di NavigAIS (Tisato, 2009-2018) all'interno del quale, nonostante la sua presenza su internet sia lodevole, potrebbero essere implementate diverse funzionalità, tra le quali ad esempio la rintracciabilità di forme attestate oppure una visualizzazione quantificata dei dati, la possibilità di consultare singoli gruppi di dati linguistici in prospettiva onomasiologica o semasiologica, come pure l'esportazione dei dati. Alle esigenze della ricerca lessicografica e atlantistica in chiave moderna, cerca di dare una risposta il progetto VerbaAlpina dell'Università Ludwig-Maximilian di Monaco di Baviera, nato nel 2014 con l'intento di indagare lo spazio linguistico delle Alpi nella sua storica unità linguistico-culturale (cfr. Krefeld e L\u00FCcke, 2014 -). Fin dalla sua concezione, completamente digitale e pensata non solo sul web, ma per il web, il progetto VerbaAlpina ha operato nel pieno rispetto dei princìpi FAIR (che sarebbero stati formulati solamente due anni dopo) e promosso un'idea innovativa di lessicografia e atlantistica linguistica (cfr. Krefeld, 2018). Oltre all'aspetto linguistico, una parte consistente del progetto è specificatamente dedicata alla creazione di strumenti per la gestione dei dati di ricerca nei progetti digitali e pensati per il web. 2.1 Concezione e presentazione del progetto Nucleo centrale dell'attività di VerbaAlpina è la raccolta strutturata di una precisa cornice semasiologica e onomasiologica, costituita dagli ambiti terminologici alpini, alla quale è possibile accedere attraverso una cartina interattiva. La ricerca prende in esame il lessico dialettale degli idiomi alpini, in modo particolare le parole relative agli ambiti della natura (flora, fauna, formazioni paesaggistiche), della cultura alpina storica (lavorazione del latte) e di quella corrente (turismo). I dati raccolti e analizzati da VerbaAlpina sono puramente dialettali, mentre i termini relativi alle lingue standard non sono presi in considerazione. Diversi sono gli scopi perseguiti da VerbaAlpina: in primo luogo il progetto intende documentare e analizzare in prospettiva linguistica e storico-etimologica la regione alpina, uno spazio fortemente frammentato per quanto riguarda le lingue e i dialetti ivi parlati. I confini dell'area di ricerca sono definiti dalla Convenzione delle Alpi (http:\/\/www.alpconv.org\/), un trattato tra i Paesi del territorio alpino atto a promuovere e sviluppare questa area montana in diversi ambiti (cfr. L\u00FCcke 2018a). I dati sono forniti dagli atlanti linguistici e dai dizionari relativi all'area di ricerca, analogici o digitali, pubblicati nel corso del tempo. In un primo momento, il materiale linguistico georeferenziato proveniente dalle fonti affronta un percorso di digitalizzazione attraverso un sistema di trascrizione basato esclusivamente sui caratteri ASCII (cfr. Krefeld e L\u00FCcke, 2016). In un secondo momento, il materiale trascritto viene sottoposto a una tokenizzazione, un processo che separa in singoli token (parole) il materiale trascritto in un momento precedente. L'interesse principale del progetto si esplica nella presentazione dei punti di coesione tra i diversi idiomi e le diverse famiglie linguistiche presenti sul territorio alpino soprattutto in prospettiva lessicologica. Per l'adempimento di tale scopo, il materiale linguistico viene raggruppato in tipi di base, ossia secondo la radice lessicale comune a diverse attestazioni che possono appartenere anche a diverse famiglie linguistiche1 e in tipi morfolessicali, vale a dire in forme di un solo tipo di base, appartenenti a un'unica famiglia linguistica che presentano caratteristiche grammaticali comuni quali la parte del discorso, il genere e gli elementi di formazione delle parole (cfr. Krefeld e L\u00FCcke, 2016a). Ad esempio, il tipo di base latino *CASEU(M) \u2018formaggio' è presente sia in area linguistica germanica, sia in area romanza nelle forme deu. K\u00E4se e ita. cacio, i quali, a loro volta, rappresentano due tipi morfolessicali differenti. I dati linguistici storici rilevati dagli atlanti e dai dizionari sono completati attraverso una piattaforma di crowdsourcing sviluppata all'interno del progetto (https:\/\/www.verbaalpina.gwi.uni-muenchen.de\/en\/?page_id=1741&db=191). La piattaforma si rivolge direttamente ai parlanti dei dialetti delle Alpi al fine di raccogliere materiale linguistico attuale e poter osservare lo spazio alpino anche in prospettiva diacronica. Una volta aperta la pagina, viene chiesto agli utenti di scegliere una lingua di navigazione tra quelle proposte (francese, italiano, sloveno, tedesco). Successivamente, vengono mostrate le istruzioni per l'utilizzo 1  In molti casi non è dato sapere se la radice lessicale comune a diverse parole sia da ricollegare allo stesso sostrato linguistico oppure a un contatto linguistico più recente. Per questo motivo VerbaAlpina utilizza il termine 'tipo di base', in quanto 'etimo' si riferisce generalmente allo strato linguistico immediatamente precedente (cfr. Krefeld e L\u00FCcke 2016a). 76  \fdella piattaforma e gli utenti sono invitati a inserire l'idioma alpino di cui essi sono i parlanti. Nel caso in cui un idioma non sia presente nella lista, gli utenti hanno la possibilità di segnalarlo direttamente alla redazione che provvederà a inserirlo. Innanzitutto, gli utenti sono chiamati a inserire il nome del comune di cui padroneggiano l'idioma. Cliccando sull'apposito campo 'concetto', appare una lista con tutti i concetti esistenti nella banca dati di VerbaAlpina. Da qui, gli utenti possono scegliere per quali concetti inviare parole. I dati raccolti attraverso il crowdsourcing vengono trattati alla pari dei dati provenienti dai dizionari e dagli atlanti, con l'unica differenza che non sono sottoposti al processo di trascrizione. Per questi dati, la tokenizzazione avviene solamente qualora si tratti di un sintagma costituito da più elementi. La tipizzazione di queste parole avviene alla stregua dei dati raccolti dai dizionari e dagli atlanti linguistici. A livello di database, le singole attestazioni provenienti dal crowdsourcing ricevono un identificatore e sono collegate ai concetti di cui rappresentano le diverse forme dialettali. Successivamente al trattamento strutturato, i dati analizzati da VerbaAlpina possono essere visualizzati sulla cartina interattiva (https:\/\/www.verbaalpina.gwi.uni-muenchen.de\/it\/?page_id=27&db=191). Tramite l'utilizzo di filtri appropriati, i dati sono accessibili in prospettiva onomasiologica (si rappresentano tutte le attestazioni linguistiche collegate a un determinato concetto) oppure semasiologica (si rappresentano i concetti legati a un preciso tipo morfolessicale). Inoltre, la visualizzazione può essere impostata in modalità qualitativa, attraverso la quale è possibile evincere la distribuzione generale delle attestazioni linguistiche, oppure quantitativa, ossia indicante il numero di dati all'interno di una certa area. I dati possono essere visualizzati in prospettiva geografico-fisica oppure astratta: la prima mostra i dati distribuiti su una cartina fisica, attraverso la seconda, invece, i dati sono presentati su una mappa a nido d'ape. Parallelamente all'attività linguistica, il progetto ha profuso un grande impegno nella gestione dei dati digitali con l'obiettivo ultimo di promuovere la sostenibilità e la durabilità del progetto anche dopo la sua chiusura definitiva. La descrizione di una parte delle attività che sono state intraprese in questo senso avviene nel corso del presente contributo. VerbaAlpina si impegna a utilizzare strumenti tecnologici adatti al web e applicabili al pensiero open source, come ad esempio Wordpress per la piattaforma centrale, Leaflet per la cartina interattiva e le banche dati relazionali MySQL. Dato che VerbaAlpina intende altresì fungere da creatore di nessi tra istituzioni e progetti già esistenti al fine di interscambiare, integrare e completare i dati linguistici riguardanti l'area alpina, per i diversi partner sono messe a disposizione banche dati all'interno delle quali i progetti cooperanti possono inserire i loro dati e collegarli così a quelli di VerbaAlpina. 2.2 Status quo L'area di ricerca di VerbaAlpina prende in considerazione tutti gli idiomi parlati nell'arco alpino. Si tratta di una superficie di 190.000 km2 comprendente alcune regioni di sei Paesi diversi (Austria, Francia, Germania, Italia, Slovenia e Svizzera) e due interi stati (Liechtenstein e Montecarlo) per un totale di quasi 6000 comuni, i quali rappresentano per VerbaAlpina le unità di georeferenziazione. Considerato che in linguistica l'unanimità di opinioni su una definizione unitaria di dialetto rappresenta ancora una visione remota (e, a dire il vero, di scarso interesse per la disciplina stessa), non è possibile indicare un numero, nemmeno approssimativo, di varietà alpine locali parlate dalla Francia alla Slovenia. Pur concedendo grande importanza all'aspetto locale delle varietà, l'analisi linguistica di VerbaAlpina si eleva al livello delle tre famiglie linguistiche che occupano il territorio alpino (romanza, germanica e slava). Per questo motivo, attraverso il processo di tipizzazione di cui sopra, il variegato materiale linguistico locale è raggruppato in tipi morfolessicali etichettati rispettivamente con le sigle ISO 639-5 relative alle famiglie linguistiche: roa. per romanzo, gem. per germanico e sla. per slavo (cfr. ISO 639-5). La base di conoscenza di VerbaAlpina racchiude ad oggi (novembre 2019) 55.407 stimoli (si tratta solitamente dei titoli delle carte degli atlanti di riferimento) ai quali sono collegate 165.521 attestazioni linguistiche, distribuite tra 3.989 concetti e riassunte in 9.556 tipi morfolessicali. Per quanto riguarda i dati provenienti dal crowdsourcing, si contano 1.065 informanti diversi e 15.249 parole totali inviate dagli utenti.  3  La lessicografia tradizionale e le sfide per il futuro nell'era delle digital humanities  Come è stato già accennato, solitamente, i dizionari classici sono strutturati in maniera semasiologica, ovvero il lessico ivi contenuto viene elencato partendo dall'unità lessicale (parola) alla quale sono collegati i diversi significati. La fortuna di questo modello di dizionario è da ricondurre essenzialmente a due ragioni: da un lato tali opere lessicografiche hanno il compito di raccogliere e illustrare il lessico appartenente a un dato idioma (e in questo senso fungono da ausili per la documentazione di una lingua); dall'altro lato, concepire un'opera 77  \flessicografica in prospettiva semasiologica risulta essere un'operazione di più facile realizzazione. Per riprodurre una serie ordinata di segni che compongono un'unità lessicale si può contare su un sistema codificato e standardizzato di caratteri: la scrittura stessa. La difficoltà di creare un'opera lessicografica partendo dal contenuto concettuale (prospettiva onomasiologica) è invece molto più estesa. Il contenuto semantico dell'unità lessicale non può essere delineato così facilmente, né tantomeno può essere standardizzato. L'utilizzo di una determinata espressione, non predice nulla sulle caratteristiche intrinseche del concetto al quale è collegata la parola che si è cercato di riprodurre. Fondamentalmente, sia per quanto riguarda la riproduzione di una singola parola, sia per quanto riguarda la descrizione di un significato, si fa appello alla scrittura, ovvero, alla lingua. Tuttavia, tale ricorso alla lingua è problematico giacché è possibile utilizzare solamente un determinato idioma alla volta, mentre invece, alla luce di quanto detto poc'anzi e nell'ottica di una scienza interconnessa, sarebbe opportuno potersi riferire ai concetti indipendentemente dalla singola lingua. Il mero ricorso ai codici linguistici ostacola inoltre la condivisione dei dati e la loro connessione ad altri database, limitando in parte una più ampia collaborazione tra progetti scientifici.  4  L'approccio al problema sull'esempio di VerbaAlpina  Per l'accorpamento dei contenuti provenienti dai vari atlanti linguistici e dai dizionari, anche il progetto VerbaAlpina si è dovuto misurare con la suddetta questione. All'interno della banca dati relazionale che funge da base del progetto, è stata creata una tabella che racchiude i concetti (si tratta prevalentemente dei contenuti tematici delle mappe linguistiche). Le singole mappe degli atlanti sono quindi collegate al concetto appropriato corrispondente. Per quanto riguarda il contenuto semantico di un concetto, il minimo comune denominatore è rappresentato dall'insieme delle informazioni che compongono il dato concetto. Dal momento che VerbaAlpina tratta dati provenienti da diverse fonti esterne al progetto, la gestione e l'uniformizzazione degli stessi può essere concepita solamente attraverso una descrizione accurata dei dati che, nell'insieme, formano un singolo concetto. Tuttavia, il metodo di gestione appena descritto consente solo la comparabilità all'interno del progetto, mentre per collegare tra di loro diversi gruppi di dati, sarebbe auspicabile e necessaria una soluzione globale e indipendente dalla lingua. La sfida della standardizzazione è stata intrapresa da lungo tempo all'interno delle biblioteche, dove l'esigenza dell'uniformità mira a creare uno standard ad esempio per la gestione dei dati relativi agli autori delle diverse pubblicazioni o per la realizzazione di differenti indicizzazioni tematiche. \u00C8 da questa esigenza dell'ambito bibliotecario che nasce l'idea del cosiddetto authority control, ossia un sistema normato per la costituzione di un archivio (authority file) che possa contenere dati organizzati secondo uno stesso modello. In Germania, a partire dagli anni Ottanta del secolo scorso sono stati creati diversi sistemi per la standardizzazione dei dati relativi a persone (PND: Personennamendatei), enti (GKD: Gemeinsame K\u00F6rperschaftsdatei) e voci (SWD: Schlagwortdatei). Inoltre, tra il 2009 e il 2013, la Biblioteca Nazionale Tedesca (Deutsche Nationalbibliothek) e altre associazioni bibliotecarie di lingua tedesca hanno intrapreso un'iniziativa volta a creare il cosiddetto GND (Gemeinsame Normdatei), un sistema di controllo di autorità che riassume tutti gli elenchi sopraccitati in un unico file. Oltre alle tradizionali entità quali organismi o persone, il GND raccoglie anche concetti. Anche il database enciclopedico Wikidata, creato allo scopo di supportare Wikipedia, funziona attraverso il controllo di autorità (cfr. Wikidata a). In Wikidata, ogni concetto è registrato tramite un numero identificatore (ID) e descritto nel dettaglio attraverso relazioni gerarchiche. La concezione partecipativa di Wikidata ha permesso l'inserimento nella piattaforma di un numero considerevole di entità referenziabili. A partire dal 2018, tutti i concetti di VerbaAlpina sono stati connessi ai Q-ID (o Q-item) di Wikidata. Tale connessione attraverso gli identificatori permette di collegare con altre banche dati esterne le informazioni altrimenti gestite solo all'interno del progetto. In questo senso, le risorse interne, come ad esempio il database multimediale, possono essere collegate in maniera decentralizzata ed essere messe a disposizione di diversi progetti. Questa concezione permette ai dati di essere costantemente arricchiti di informazioni aggiuntive. Allo stesso modo, è possibile pensare anche alla presentazione dei contenuti in diverse lingue, in quanto Wikidata mette a disposizione le traduzioni delle denominazioni relative ai diversi concetti (o ai relativi Q-ID). La collaborazione tra Wikidata e VerbaAlpina prevede non solo una connessione attraverso l'applicazione dell'identificatore, ma anche una partecipazione attiva di quest'ultimo al database enciclopedico. VerbaAlpina dispone infatti di un account proprio sulla pagina di Wikidata al fine di mappare eventuali concetti ivi mancanti. Al momento (novembre 2019), 1000 concetti di VerbaAlpina sono muniti di un corrispettivo Q-ID. Una parte consistente di concetti contenuti nella banca dati di VerbaAlpina deve essere ancora elaborata e ogni entrata  78  \fcorredata del rispettivo Q-ID, un'attività che è in costante aggiornamento. Al momento, è stata data priorità all'applicazione degli identificatori ai concetti di VerbaAlpina, in un secondo momento avverrà anche la mappatura di concetti mancanti su Wikidata. L'inserimento dei Q-ID di Wikidata nella banca dati di VerbaAlpina avviene in maniera del tutto manuale. Le proposte sopraccitate si riferiscono a un tentativo di standardizzazione del contenuto semantico di un'ampia quantità di concetti. Dal momento che VerbaAlpina non si occupa solamente del trattamento di materiale semantico, ma anche di tipi morfolessicali, la creazione di un controllo di autorità applicabile anche a tali forme linguistiche sarebbe auspicabile per l'identificazione univoca del contenuto lessicale. Per un'etichettatura di questo genere, la situazione si presenta in maniera diversa: il GND non fornisce ancora un sistema mirato per la gestione dei dati in questo senso, mentre Wikidata offre la possibilità di creare attestazioni lessicali contenenti le informazioni legate al lemma stesso, alla lingua e alla categoria lessicale. Ogni attestazione lessicale è correlata a un identificatore L (L-ID) che viene generato automaticamente (cfr. Wikidata b). Le singole voci possono anche essere completate con informazioni quali genere e significato. Per varietà linguistiche di estensione meno ampia quali ad esempio il ladino dolomitico oppure il friulano, Wikidata richiede non solo l'inserimento del lemma vero e proprio, ma anche di indicare la cosiddetta spelling variant, ossia l'ortografia utilizzata per rappresentare un determinato lemma indicata mediante un codice linguistico. Ad esempio, se si desidera inserire il lemma paur\u00F9ns (forma ladina per il concetto \u2018siero di latte dopo la prima separazione della materia solida'), viene richiesto di indicare secondo quale ortografia è stato inserito il lemma stesso (cfr. Wikidata c). Inoltre, è possibile anche aggiungere le informazioni relative al numero e al caso linguistico e collegarle al rispettivo concetto. Quest'ultimo sistema di referenziazione consente di collegare ai singoli lemmi anche le informazioni riguardanti le derivazioni o l'etimologia, una pratica che faciliterebbe, di conseguenza, il lavoro lessicografico. Tale modalità di etichettatura del materiale linguistico è stata implementata da VerbaAlpina solo recentemente, ma sarà portata avanti in maniera progressiva con le stesse modalità applicate per i concetti. La connessione tra i concetti di VerbaAlpina e quelli di Wikidata attraverso l'applicazione di un identificatore non è fine a se stessa, ma si inserisce in un'ottica di condivisione più ampia in grado trovare punti di aggancio anche con il progetto GeRDI (Generic Research Data Infrastructure). Quest'ultimo nasce nel 2016 come aggregatore di dati allo scopo di offrire a tutti i progetti di ricerca in Germania la possibilità di archiviare, condividere e riutilizzare dati. GeRDI impiega Wikidata come base di conoscenza, realizzando così un sistema di consultazione di dati interdisciplinare e multilingue (cfr. Mutter, 2018). Wikidata rappresenta agli occhi di VerbaAlpina una piattaforma centrale attraverso la quale quest'ultimo può mettersi in relazione con altri progetti collegati analogamente alla stessa base di conoscenza. Un esempio potrebbe essere il dizionario enciclopedico BabelNet, anch'esso connesso a Wikidata. La connessione diretta tra VerbaAlpina e Babelnet sarebbe interessante per quanto riguarda l'identificazione dei lemmi, ma, quest'ultimo non dispone ancora di numeri univoci per questo tipo di materiale lessicale, né raccoglie dati dialettali, centrali invece per l'attività di VerbaAlpina. Ad ogni modo, benché non in maniera diretta, VerbaAlpina e Babelnet dispongono entrambe di Wikidata come progetto di collaborazione comune. 5  Prospettive e attività future  Facendo di nuovo riferimento ai princìpi FAIR menzionati all'inizio, organizzare e gestire i dati linguistici nel quadro dei sistemi di identificazione descritti poc'anzi, rappresenta un passo importante verso il rispetto di questi princìpi. I dati strutturati si presentano non solo più accessibili da parte di altri progetti e più facilmente reperibili grazie al collegamento in rete, ma la standardizzazione dei riferimenti esatti ne favorisce anche il trattamento dal punto di vista dell'interoperabilità. VerbaAlpina non è leale a questi princìpi solamente per quanto riguarda l'interoperabilità dei dati, ma anche relativamente alla loro rintracciabilità (f) attraverso l'inserimento del progetto nei cataloghi della biblioteca universitaria dell'università di Monaco di Baviera. VerbaAlpina sposa in toto l'idea di libero accesso alla conoscenza come bene comune e utilizza solamente licenze Creative-Commons (CC) rinunciando, di conseguenza, al diritto d'autore e permettendo l'utilizzo dei dati con la sola restrizione dell'obbligo di citazione (cfr. L\u00FCcke, 2016a); il riutilizzo dei dati è reso possibile attraverso la loro esportazione tramite un'interfaccia di programmazione di un'applicazione (eng. Application Programming Interface; API), la quale permette l'accesso all'intero dataset di VerbaAlpina. Una documentazione e spiegazione dettagliata è consultabile al seguente indirizzo: https:\/\/ www.verba-alpina.gwi.uni-muenchen.de\/?page_id=8844&db=191. Attraverso tale API i dati di VerbaAlpina sono accessibili meccanicamente (machine readable) e possono essere scaricati, modificati ed elaborati ulteriormente. La connessione dei dati con altri dataset è garantita attraverso lo schema di metadati di DataCite 79  \f(https:\/\/datacite.org), la quale si trova ancora in fase di sviluppo. Nonostante l'accesso  meccanico ai dati dall'esterno sia già possibile mediante l'API e la connessione dei dati con altri dataset attraverso i metadati, le procedure per inserire i dati nella nuvola dei Linguistic Linked Open Data (https:\/\/linguistic-lod.org\/) sono in fase di avvio allo scopo di creare un'ulteriore connessione tra progetti e contribuire in questo senso all'idea di web strutturato. Operare nell'era delle digital humanities significa creare conoscenza interconnessa, condivisibile, accessibile, una conoscenza più ampia e coesa. Equivale a creare strumenti e a metterli a disposizione non solo della comunità scientifica, ma anche del grande pubblico. Si tratta di un'amplificazione dell'originale pensiero umanista: creare sapere, renderlo accessibile e diffonderlo affinché l'umanità possa accrescere le proprie conoscenze. Trattare i dati (lessicografici) nell'era delle digital humanities  La ricerca dell'era pre-digitale è stata contraddistinta da modalità di concezione progettuale fortemente individuali: la raccolta, l'analisi e l'illustrazione dei dati venivano effettuate da persone (o gruppi di persone) che operavano singolarmente. La comunicazione scientifica si compiva attraverso le pubblicazioni dei libri cartacei, i quali venivano conservati in luoghi circoscritti come ad esempio le biblioteche, e ciascuno studio rappresentava un progetto concluso in sé (cfr. Krefeld, 2016). Inoltre, molti dati rimanevano nelle mani degli stessi ricercatori che li avevano raccolti e, in generale, la comunità scientifica non compiva molti sforzi per mettere a disposizione del grande pubblico i dati provenienti dai diversi progetti scientifici. Ormai, l'avvento delle nuove tecnologie non è più un fenomeno di recente datazione. Il passaggio dal cartaceo al digitale è avvenuto in maniera sempre più repentina, contribuendo al mutamento dell'approccio scientifico nei confronti delle modalità di ricerca. La rivoluzione digitale ha rinsaldato l'idea della condivisione e, allo stesso tempo, provveduto a fornire gli strumenti necessari per favorire e facilitare l'interscambio di dati come pure, più in generale, l'interazione tra diversi progetti. Ciononostante, la creazione di una rete di progetti e la condivisione dei relativi dati non sono fornite da una mera digitalizzazione degli strumenti di ricerca. La collaborazione rappresenta l'essenza primaria del fare scienza, poiché è sulla base delle conoscenze già presenti che si costruisce il progresso. Per operare in maniera collettiva nell'era digitale è però necessario che i dati di ricerca soddisfino alcuni requisiti fondamentali. In primo luogo i dati devono essere strutturati, descritti  74  \fed eventualmente etichettati in maniera tale da prestarsi a essere maneggiati in sedi esterne al loro progetto originario e devono poter continuare a essere accessibili anche in un momento successivo all'eventuale chiusura del progetto. In questo contesto si inserisce l'idea di Web Semantico (e successivamente dei Linked Open Data), nata con l'obiettivo di rimodellare l'ambiente virtuale di internet. Numerosi sono i progetti che si collocano all'interno di questo pensiero: essi formano la cosiddetta Linked Open Data Cloud (cfr. Cyganiak e Jentzsch, 2007 -) e costituiscono, al contempo, una comunità interconnessa attraverso relazioni basate sui loro insiemi di dati (cfr. Bizer et al. 2009, 154). Le esigenze di condivisione e connessione dei dati sul web sono inoltre state formulate in maniera esplicita nel 2016, quando un grande numero di ricercatori provenienti da diversi Paesi ha pubblicato le linee guida per la gestione moderna dei dati di ricerca (cfr. Wilkinson, Dumontier et al., 2016). Tali raccomandazioni sono state racchiuse nell'acronimo FAIR, una sigla che raccoglie i quattro princìpi fondamentali sui quali dovrebbero essere basate la comunicazione e la cooperazione scientifiche nell'era digitale. Secondo tali princìpi, i dati della ricerca dovrebbero essere rintracciabili (findable), accessibili (accessible), interoperabili (interoperable) e riutilizzabili (reusable). Per essere rintracciabili, i progetti ai quali i dati appartengono, devono essere reperibili attraverso portali centrali quali, ad esempio, i cataloghi delle biblioteche. I dati di ricerca che non sono soggetti ad alcuna restrizione giuridica (come potrebbero essere ad esempio i dati strettamente personali) devono essere messi a disposizione del grande pubblico e rinunciare, di conseguenza, al diritto d'autore. Al fine di poter essere interoperabili, inoltre, i dati devono essere innanzitutto scissi, successivamente strutturati ed essere descritti in maniera precisa. Il riutilizzo dei dati si rende infine possibile attraverso una corretta applicazione dei tre princìpi precedenti: si tratta di princìpi estensibili che non si lasciano mettere in contrapposizione l'un l'altro (cfr. Force11, 2011-2017; GoFair, 2011-2017; L\u00FCcke, 2018). Le possibilità offerte dalla digitalizzazione in termini tecnici hanno altresì consentito di valutare da un nuovo punto di vista una delle questioni storiche relative al trattamento dei dati lessicografici. In linea di massima, le opere lessicografiche possono essere strutturate in maniera semasiologica (le parole di un dato idioma sono elencate seguite dal loro significato) oppure onomasiologica (si descrivono i significati e vi si collegano le diverse parole che ad essi conducono). Nell'era analogica tali opere erano strutturate secondo uno o secondo l'altro modo: in ambito romanzo, si ricordino, tra gli altri, l'atlante linguistico ed etnografico dell'Italia e della Svizzera meridionale (AIS), di stampo onomasiologico, oppure, il Dicziunari Rumantsch Grischun (DRG), strutturato, invece, in maniera semasiologica. Una concezione in entrambi i sensi non era possibile a causa di limiti pratici imposti dalle modalità di pubblicazione del passato, mentre oggigiorno, le possibilità fornite dalla digitalizzazione offrono nuovi strumenti e la concezione di un'opera lessicografica che vada in due direzioni è realizzabile. Ciononostante, benché l'aspetto tecnico non rappresenti più alcun problema alla messa in pratica di tale approccio bidirezionale, le sfide si aprono soprattutto dal punto di vista contenutistico, come si evincerà dai capitoli che seguiranno. La compilazione digitale di opere lessicografiche quali i dizionari sembra essere oggi un'attività relativamente consolidata. Non sono rari i dizionari che offrono la possibilità di essere consultati in rete, anche se, alcuni di essi, non si sono mai realmente distanziati da una concezione cartacea. Tali opere, presentano ancora un grande margine di sviluppo e ampliamento per potersi definire digitalizzate in modo interoperabile secondo i gradi definiti in L\u00FCcke (2016). A titolo illustrativo, ma non esaustivo, si pensi alla versione online del Romanisches Etymologisches W\u00F6rterbuch (Meyer-L\u00FCbke, 1935): il contenuto dell'opera è messo a disposizione online in formato PDF, ma, ai sensi della digitalizzazione, in essa potrebbero essere implementate diverse funzioni, tra cui, ad esempio, la ricerca di singoli lemmi. Lo scopo di una lessicografia basata sul web non si limita alla mera presentazione virtuale di una determinata opera. Lo sviluppo digitale è bensì rappresentato da un più ampio tentativo di costituzione di reti lessicali e semantiche messe in relazione tra di loro. Progetti volti a fornire tali interconnessioni sono stati iniziati già verso la metà degli anni Ottanta. A titolo esemplificativo si pensi a WordNet, il database lessicale per la lingua inglese, ma anche a EuroWordNet nato negli anni Novanta come rete semantica per le lingue europee (cfr. Fellbaum 2006, 665, 669). Tali progetti costituiscono un primo tentativo di strutturare il materiale in maniera semantica e non solo lessicale come si è soliti fare nei dizionari cartacei e, soprattutto, si inseriscono nel panorama dei lavori relativi all'elaborazione del linguaggio naturale multilingue. In tempi più recenti si colloca la concezione di BabelNet (https:\/\/babelnet.org\/), una rete semantica multilingue, automatizzata e di ampia copertura, ovvero un dizionario enciclopedico costituito unendo il contenuto lessicale di WordNet al sapere enciclopedico di Wikipedia attraverso processi automatizzati di integrazione dei contenuti di ambedue i database (cfr. Navigli e Ponzetto, 2010, 216).  75  \f2 VerbaAlpina: geolinguistica e lessicografia digitali Mentre la digitalizzazione dei primi dizionari e corpora lessicali si colloca tra gli anni Ottanta e gli anni Novanta (cfr. Chiari 2012, 98), più recente risulta invece essere il passaggio dal cartaceo al digitale per quanto riguarda gli atlanti linguistici. Relativamente all'area alpino-romanza, diversi atlanti sono oggi disponibili in rete in formati PDF o JPG ma non hanno percorso tutte le tappe del passaggio dal cartaceo al digitale (cfr. L\u00FCcke, 2016; Knapp, 2017). \u00C8, a titolo esemplificativo, il caso di NavigAIS (Tisato, 2009-2018) all'interno del quale, nonostante la sua presenza su internet sia lodevole, potrebbero essere implementate diverse funzionalità, tra le quali ad esempio la rintracciabilità di forme attestate oppure una visualizzazione quantificata dei dati, la possibilità di consultare singoli gruppi di dati linguistici in prospettiva onomasiologica o semasiologica, come pure l'esportazione dei dati. Alle esigenze della ricerca lessicografica e atlantistica in chiave moderna, cerca di dare una risposta il progetto VerbaAlpina dell'Università Ludwig-Maximilian di Monaco di Baviera, nato nel 2014 con l'intento di indagare lo spazio linguistico delle Alpi nella sua storica unità linguistico-culturale (cfr. Krefeld e L\u00FCcke, 2014 -). Fin dalla sua concezione, completamente digitale e pensata non solo sul web, ma per il web, il progetto VerbaAlpina ha operato nel pieno rispetto dei princìpi FAIR (che sarebbero stati formulati solamente due anni dopo) e promosso un'idea innovativa di lessicografia e atlantistica linguistica (cfr. Krefeld, 2018). Oltre all'aspetto linguistico, una parte consistente del progetto è specificatamente dedicata alla creazione di strumenti per la gestione dei dati di ricerca nei progetti digitali e pensati per il web. 2.1 Concezione e presentazione del progetto Nucleo centrale dell'attività di VerbaAlpina è la raccolta strutturata di una precisa cornice semasiologica e onomasiologica, costituita dagli ambiti terminologici alpini, alla quale è possibile accedere attraverso una cartina interattiva. La ricerca prende in esame il lessico dialettale degli idiomi alpini, in modo particolare le parole relative agli ambiti della natura (flora, fauna, formazioni paesaggistiche), della cultura alpina storica (lavorazione del latte) e di quella corrente (turismo). I dati raccolti e analizzati da VerbaAlpina sono puramente dialettali, mentre i termini relativi alle lingue standard non sono presi in considerazione. Diversi sono gli scopi perseguiti da VerbaAlpina: in primo luogo il progetto intende documentare e analizzare in prospettiva linguistica e storico-etimologica la regione alpina, uno spazio fortemente frammentato per quanto riguarda le lingue e i dialetti ivi parlati. I confini dell'area di ricerca sono definiti dalla Convenzione delle Alpi (http:\/\/www.alpconv.org\/), un trattato tra i Paesi del territorio alpino atto a promuovere e sviluppare questa area montana in diversi ambiti (cfr. L\u00FCcke 2018a). I dati sono forniti dagli atlanti linguistici e dai dizionari relativi all'area di ricerca, analogici o digitali, pubblicati nel corso del tempo. In un primo momento, il materiale linguistico georeferenziato proveniente dalle fonti affronta un percorso di digitalizzazione attraverso un sistema di trascrizione basato esclusivamente sui caratteri ASCII (cfr. Krefeld e L\u00FCcke, 2016). In un secondo momento, il materiale trascritto viene sottoposto a una tokenizzazione, un processo che separa in singoli token (parole) il materiale trascritto in un momento precedente. L'interesse principale del progetto si esplica nella presentazione dei punti di coesione tra i diversi idiomi e le diverse famiglie linguistiche presenti sul territorio alpino soprattutto in prospettiva lessicologica. Per l'adempimento di tale scopo, il materiale linguistico viene raggruppato in tipi di base, ossia secondo la radice lessicale comune a diverse attestazioni che possono appartenere anche a diverse famiglie linguistiche1 e in tipi morfolessicali, vale a dire in forme di un solo tipo di base, appartenenti a un'unica famiglia linguistica che presentano caratteristiche grammaticali comuni quali la parte del discorso, il genere e gli elementi di formazione delle parole (cfr. Krefeld e L\u00FCcke, 2016a). Ad esempio, il tipo di base latino *CASEU(M) \u2018formaggio' è presente sia in area linguistica germanica, sia in area romanza nelle forme deu. K\u00E4se e ita. cacio, i quali, a loro volta, rappresentano due tipi morfolessicali differenti. I dati linguistici storici rilevati dagli atlanti e dai dizionari sono completati attraverso una piattaforma di crowdsourcing sviluppata all'interno del progetto (https:\/\/www.verbaalpina.gwi.uni-muenchen.de\/en\/?page_id=1741&db=191). La piattaforma si rivolge direttamente ai parlanti dei dialetti delle Alpi al fine di raccogliere materiale linguistico attuale e poter osservare lo spazio alpino anche in prospettiva diacronica. Una volta aperta la pagina, viene chiesto agli utenti di scegliere una lingua di navigazione tra quelle proposte (francese, italiano, sloveno, tedesco). Successivamente, vengono mostrate le istruzioni per l'utilizzo 1  In molti casi non è dato sapere se la radice lessicale comune a diverse parole sia da ricollegare allo stesso sostrato linguistico oppure a un contatto linguistico più recente. Per questo motivo VerbaAlpina utilizza il termine 'tipo di base', in quanto 'etimo' si riferisce generalmente allo strato linguistico immediatamente precedente (cfr. Krefeld e L\u00FCcke 2016a). 76  \fdella piattaforma e gli utenti sono invitati a inserire l'idioma alpino di cui essi sono i parlanti. Nel caso in cui un idioma non sia presente nella lista, gli utenti hanno la possibilità di segnalarlo direttamente alla redazione che provvederà a inserirlo. Innanzitutto, gli utenti sono chiamati a inserire il nome del comune di cui padroneggiano l'idioma. Cliccando sull'apposito campo 'concetto', appare una lista con tutti i concetti esistenti nella banca dati di VerbaAlpina. Da qui, gli utenti possono scegliere per quali concetti inviare parole. I dati raccolti attraverso il crowdsourcing vengono trattati alla pari dei dati provenienti dai dizionari e dagli atlanti, con l'unica differenza che non sono sottoposti al processo di trascrizione. Per questi dati, la tokenizzazione avviene solamente qualora si tratti di un sintagma costituito da più elementi. La tipizzazione di queste parole avviene alla stregua dei dati raccolti dai dizionari e dagli atlanti linguistici. A livello di database, le singole attestazioni provenienti dal crowdsourcing ricevono un identificatore e sono collegate ai concetti di cui rappresentano le diverse forme dialettali. Successivamente al trattamento strutturato, i dati analizzati da VerbaAlpina possono essere visualizzati sulla cartina interattiva (https:\/\/www.verbaalpina.gwi.uni-muenchen.de\/it\/?page_id=27&db=191). Tramite l'utilizzo di filtri appropriati, i dati sono accessibili in prospettiva onomasiologica (si rappresentano tutte le attestazioni linguistiche collegate a un determinato concetto) oppure semasiologica (si rappresentano i concetti legati a un preciso tipo morfolessicale). Inoltre, la visualizzazione può essere impostata in modalità qualitativa, attraverso la quale è possibile evincere la distribuzione generale delle attestazioni linguistiche, oppure quantitativa, ossia indicante il numero di dati all'interno di una certa area. I dati possono essere visualizzati in prospettiva geografico-fisica oppure astratta: la prima mostra i dati distribuiti su una cartina fisica, attraverso la seconda, invece, i dati sono presentati su una mappa a nido d'ape. Parallelamente all'attività linguistica, il progetto ha profuso un grande impegno nella gestione dei dati digitali con l'obiettivo ultimo di promuovere la sostenibilità e la durabilità del progetto anche dopo la sua chiusura definitiva. La descrizione di una parte delle attività che sono state intraprese in questo senso avviene nel corso del presente contributo. VerbaAlpina si impegna a utilizzare strumenti tecnologici adatti al web e applicabili al pensiero open source, come ad esempio Wordpress per la piattaforma centrale, Leaflet per la cartina interattiva e le banche dati relazionali MySQL. Dato che VerbaAlpina intende altresì fungere da creatore di nessi tra istituzioni e progetti già esistenti al fine di interscambiare, integrare e completare i dati linguistici riguardanti l'area alpina, per i diversi partner sono messe a disposizione banche dati all'interno delle quali i progetti cooperanti possono inserire i loro dati e collegarli così a quelli di VerbaAlpina. 2.2 Status quo L'area di ricerca di VerbaAlpina prende in considerazione tutti gli idiomi parlati nell'arco alpino. Si tratta di una superficie di 190.000 km2 comprendente alcune regioni di sei Paesi diversi (Austria, Francia, Germania, Italia, Slovenia e Svizzera) e due interi stati (Liechtenstein e Montecarlo) per un totale di quasi 6000 comuni, i quali rappresentano per VerbaAlpina le unità di georeferenziazione. Considerato che in linguistica l'unanimità di opinioni su una definizione unitaria di dialetto rappresenta ancora una visione remota (e, a dire il vero, di scarso interesse per la disciplina stessa), non è possibile indicare un numero, nemmeno approssimativo, di varietà alpine locali parlate dalla Francia alla Slovenia. Pur concedendo grande importanza all'aspetto locale delle varietà, l'analisi linguistica di VerbaAlpina si eleva al livello delle tre famiglie linguistiche che occupano il territorio alpino (romanza, germanica e slava). Per questo motivo, attraverso il processo di tipizzazione di cui sopra, il variegato materiale linguistico locale è raggruppato in tipi morfolessicali etichettati rispettivamente con le sigle ISO 639-5 relative alle famiglie linguistiche: roa. per romanzo, gem. per germanico e sla. per slavo (cfr. ISO 639-5). La base di conoscenza di VerbaAlpina racchiude ad oggi (novembre 2019) 55.407 stimoli (si tratta solitamente dei titoli delle carte degli atlanti di riferimento) ai quali sono collegate 165.521 attestazioni linguistiche, distribuite tra 3.989 concetti e riassunte in 9.556 tipi morfolessicali. Per quanto riguarda i dati provenienti dal crowdsourcing, si contano 1.065 informanti diversi e 15.249 parole totali inviate dagli utenti.  3  La lessicografia tradizionale e le sfide per il futuro nell'era delle digital humanities  Come è stato già accennato, solitamente, i dizionari classici sono strutturati in maniera semasiologica, ovvero il lessico ivi contenuto viene elencato partendo dall'unità lessicale (parola) alla quale sono collegati i diversi significati. La fortuna di questo modello di dizionario è da ricondurre essenzialmente a due ragioni: da un lato tali opere lessicografiche hanno il compito di raccogliere e illustrare il lessico appartenente a un dato idioma (e in questo senso fungono da ausili per la documentazione di una lingua); dall'altro lato, concepire un'opera 77  \flessicografica in prospettiva semasiologica risulta essere un'operazione di più facile realizzazione. Per riprodurre una serie ordinata di segni che compongono un'unità lessicale si può contare su un sistema codificato e standardizzato di caratteri: la scrittura stessa. La difficoltà di creare un'opera lessicografica partendo dal contenuto concettuale (prospettiva onomasiologica) è invece molto più estesa. Il contenuto semantico dell'unità lessicale non può essere delineato così facilmente, né tantomeno può essere standardizzato. L'utilizzo di una determinata espressione, non predice nulla sulle caratteristiche intrinseche del concetto al quale è collegata la parola che si è cercato di riprodurre. Fondamentalmente, sia per quanto riguarda la riproduzione di una singola parola, sia per quanto riguarda la descrizione di un significato, si fa appello alla scrittura, ovvero, alla lingua. Tuttavia, tale ricorso alla lingua è problematico giacché è possibile utilizzare solamente un determinato idioma alla volta, mentre invece, alla luce di quanto detto poc'anzi e nell'ottica di una scienza interconnessa, sarebbe opportuno potersi riferire ai concetti indipendentemente dalla singola lingua. Il mero ricorso ai codici linguistici ostacola inoltre la condivisione dei dati e la loro connessione ad altri database, limitando in parte una più ampia collaborazione tra progetti scientifici.  4  L'approccio al problema sull'esempio di VerbaAlpina  Per l'accorpamento dei contenuti provenienti dai vari atlanti linguistici e dai dizionari, anche il progetto VerbaAlpina si è dovuto misurare con la suddetta questione. All'interno della banca dati relazionale che funge da base del progetto, è stata creata una tabella che racchiude i concetti (si tratta prevalentemente dei contenuti tematici delle mappe linguistiche). Le singole mappe degli atlanti sono quindi collegate al concetto appropriato corrispondente. Per quanto riguarda il contenuto semantico di un concetto, il minimo comune denominatore è rappresentato dall'insieme delle informazioni che compongono il dato concetto. Dal momento che VerbaAlpina tratta dati provenienti da diverse fonti esterne al progetto, la gestione e l'uniformizzazione degli stessi può essere concepita solamente attraverso una descrizione accurata dei dati che, nell'insieme, formano un singolo concetto. Tuttavia, il metodo di gestione appena descritto consente solo la comparabilità all'interno del progetto, mentre per collegare tra di loro diversi gruppi di dati, sarebbe auspicabile e necessaria una soluzione globale e indipendente dalla lingua. La sfida della standardizzazione è stata intrapresa da lungo tempo all'interno delle biblioteche, dove l'esigenza dell'uniformità mira a creare uno standard ad esempio per la gestione dei dati relativi agli autori delle diverse pubblicazioni o per la realizzazione di differenti indicizzazioni tematiche. \u00C8 da questa esigenza dell'ambito bibliotecario che nasce l'idea del cosiddetto authority control, ossia un sistema normato per la costituzione di un archivio (authority file) che possa contenere dati organizzati secondo uno stesso modello. In Germania, a partire dagli anni Ottanta del secolo scorso sono stati creati diversi sistemi per la standardizzazione dei dati relativi a persone (PND: Personennamendatei), enti (GKD: Gemeinsame K\u00F6rperschaftsdatei) e voci (SWD: Schlagwortdatei). Inoltre, tra il 2009 e il 2013, la Biblioteca Nazionale Tedesca (Deutsche Nationalbibliothek) e altre associazioni bibliotecarie di lingua tedesca hanno intrapreso un'iniziativa volta a creare il cosiddetto GND (Gemeinsame Normdatei), un sistema di controllo di autorità che riassume tutti gli elenchi sopraccitati in un unico file. Oltre alle tradizionali entità quali organismi o persone, il GND raccoglie anche concetti. Anche il database enciclopedico Wikidata, creato allo scopo di supportare Wikipedia, funziona attraverso il controllo di autorità (cfr. Wikidata a). In Wikidata, ogni concetto è registrato tramite un numero identificatore (ID) e descritto nel dettaglio attraverso relazioni gerarchiche. La concezione partecipativa di Wikidata ha permesso l'inserimento nella piattaforma di un numero considerevole di entità referenziabili. A partire dal 2018, tutti i concetti di VerbaAlpina sono stati connessi ai Q-ID (o Q-item) di Wikidata. Tale connessione attraverso gli identificatori permette di collegare con altre banche dati esterne le informazioni altrimenti gestite solo all'interno del progetto. In questo senso, le risorse interne, come ad esempio il database multimediale, possono essere collegate in maniera decentralizzata ed essere messe a disposizione di diversi progetti. Questa concezione permette ai dati di essere costantemente arricchiti di informazioni aggiuntive. Allo stesso modo, è possibile pensare anche alla presentazione dei contenuti in diverse lingue, in quanto Wikidata mette a disposizione le traduzioni delle denominazioni relative ai diversi concetti (o ai relativi Q-ID). La collaborazione tra Wikidata e VerbaAlpina prevede non solo una connessione attraverso l'applicazione dell'identificatore, ma anche una partecipazione attiva di quest'ultimo al database enciclopedico. VerbaAlpina dispone infatti di un account proprio sulla pagina di Wikidata al fine di mappare eventuali concetti ivi mancanti. Al momento (novembre 2019), 1000 concetti di VerbaAlpina sono muniti di un corrispettivo Q-ID. Una parte consistente di concetti contenuti nella banca dati di VerbaAlpina deve essere ancora elaborata e ogni entrata  78  \fcorredata del rispettivo Q-ID, un'attività che è in costante aggiornamento. Al momento, è stata data priorità all'applicazione degli identificatori ai concetti di VerbaAlpina, in un secondo momento avverrà anche la mappatura di concetti mancanti su Wikidata. L'inserimento dei Q-ID di Wikidata nella banca dati di VerbaAlpina avviene in maniera del tutto manuale. Le proposte sopraccitate si riferiscono a un tentativo di standardizzazione del contenuto semantico di un'ampia quantità di concetti. Dal momento che VerbaAlpina non si occupa solamente del trattamento di materiale semantico, ma anche di tipi morfolessicali, la creazione di un controllo di autorità applicabile anche a tali forme linguistiche sarebbe auspicabile per l'identificazione univoca del contenuto lessicale. Per un'etichettatura di questo genere, la situazione si presenta in maniera diversa: il GND non fornisce ancora un sistema mirato per la gestione dei dati in questo senso, mentre Wikidata offre la possibilità di creare attestazioni lessicali contenenti le informazioni legate al lemma stesso, alla lingua e alla categoria lessicale. Ogni attestazione lessicale è correlata a un identificatore L (L-ID) che viene generato automaticamente (cfr. Wikidata b). Le singole voci possono anche essere completate con informazioni quali genere e significato. Per varietà linguistiche di estensione meno ampia quali ad esempio il ladino dolomitico oppure il friulano, Wikidata richiede non solo l'inserimento del lemma vero e proprio, ma anche di indicare la cosiddetta spelling variant, ossia l'ortografia utilizzata per rappresentare un determinato lemma indicata mediante un codice linguistico. Ad esempio, se si desidera inserire il lemma paur\u00F9ns (forma ladina per il concetto \u2018siero di latte dopo la prima separazione della materia solida'), viene richiesto di indicare secondo quale ortografia è stato inserito il lemma stesso (cfr. Wikidata c). Inoltre, è possibile anche aggiungere le informazioni relative al numero e al caso linguistico e collegarle al rispettivo concetto. Quest'ultimo sistema di referenziazione consente di collegare ai singoli lemmi anche le informazioni riguardanti le derivazioni o l'etimologia, una pratica che faciliterebbe, di conseguenza, il lavoro lessicografico. Tale modalità di etichettatura del materiale linguistico è stata implementata da VerbaAlpina solo recentemente, ma sarà portata avanti in maniera progressiva con le stesse modalità applicate per i concetti. La connessione tra i concetti di VerbaAlpina e quelli di Wikidata attraverso l'applicazione di un identificatore non è fine a se stessa, ma si inserisce in un'ottica di condivisione più ampia in grado trovare punti di aggancio anche con il progetto GeRDI (Generic Research Data Infrastructure). Quest'ultimo nasce nel 2016 come aggregatore di dati allo scopo di offrire a tutti i progetti di ricerca in Germania la possibilità di archiviare, condividere e riutilizzare dati. GeRDI impiega Wikidata come base di conoscenza, realizzando così un sistema di consultazione di dati interdisciplinare e multilingue (cfr. Mutter, 2018). Wikidata rappresenta agli occhi di VerbaAlpina una piattaforma centrale attraverso la quale quest'ultimo può mettersi in relazione con altri progetti collegati analogamente alla stessa base di conoscenza. Un esempio potrebbe essere il dizionario enciclopedico BabelNet, anch'esso connesso a Wikidata. La connessione diretta tra VerbaAlpina e Babelnet sarebbe interessante per quanto riguarda l'identificazione dei lemmi, ma, quest'ultimo non dispone ancora di numeri univoci per questo tipo di materiale lessicale, né raccoglie dati dialettali, centrali invece per l'attività di VerbaAlpina. Ad ogni modo, benché non in maniera diretta, VerbaAlpina e Babelnet dispongono entrambe di Wikidata come progetto di collaborazione comune. 5  Prospettive e attività future  Facendo di nuovo riferimento ai princìpi FAIR menzionati all'inizio, organizzare e gestire i dati linguistici nel quadro dei sistemi di identificazione descritti poc'anzi, rappresenta un passo importante verso il rispetto di questi princìpi. I dati strutturati si presentano non solo più accessibili da parte di altri progetti e più facilmente reperibili grazie al collegamento in rete, ma la standardizzazione dei riferimenti esatti ne favorisce anche il trattamento dal punto di vista dell'interoperabilità. VerbaAlpina non è leale a questi princìpi solamente per quanto riguarda l'interoperabilità dei dati, ma anche relativamente alla loro rintracciabilità (f) attraverso l'inserimento del progetto nei cataloghi della biblioteca universitaria dell'università di Monaco di Baviera. VerbaAlpina sposa in toto l'idea di libero accesso alla conoscenza come bene comune e utilizza solamente licenze Creative-Commons (CC) rinunciando, di conseguenza, al diritto d'autore e permettendo l'utilizzo dei dati con la sola restrizione dell'obbligo di citazione (cfr. L\u00FCcke, 2016a); il riutilizzo dei dati è reso possibile attraverso la loro esportazione tramite un'interfaccia di programmazione di un'applicazione (eng. Application Programming Interface; API), la quale permette l'accesso all'intero dataset di VerbaAlpina. Una documentazione e spiegazione dettagliata è consultabile al seguente indirizzo: https:\/\/ www.verba-alpina.gwi.uni-muenchen.de\/?page_id=8844&db=191. Attraverso tale API i dati di VerbaAlpina sono accessibili meccanicamente (machine readable) e possono essere scaricati, modificati ed elaborati ulteriormente. La connessione dei dati con altri dataset è garantita attraverso lo schema di metadati di DataCite 79  \f(https:\/\/datacite.org), la quale si trova ancora in fase di sviluppo. Nonostante l'accesso  meccanico ai dati dall'esterno sia già possibile mediante l'API e la connessione dei dati con altri dataset attraverso i metadati, le procedure per inserire i dati nella nuvola dei Linguistic Linked Open Data (https:\/\/linguistic-lod.org\/) sono in fase di avvio allo scopo di creare un'ulteriore connessione tra progetti e contribuire in questo senso all'idea di web strutturato. Operare nell'era delle digital humanities significa creare conoscenza interconnessa, condivisibile, accessibile, una conoscenza più ampia e coesa. Equivale a creare strumenti e a metterli a disposizione non solo della comunità scientifica, ma anche del grande pubblico. Si tratta di un'amplificazione dell'originale pensiero umanista: creare sapere, renderlo accessibile e diffonderlo affinché l'umanità possa accrescere le proprie conoscenze."
	},
	{
		"id": 14,
		"title": "Digital projects for music research and education from the Center for Music Research and Documentation (CIDoM), Associated Unit of the Spanish National Research Council",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Juan José Pastor Comín",
			"Francisco Manuel López Gómez"
		],
		"body": "The Center for Music Research and Documentation of Spain  The Centro de Investigaci\u00F3n y Documentaci\u00F3n Musical is an Associated Unit of Centro Superior de Investigaciones Cient\u00EDficas (CSIC, Spanish National Research Council). Founded in 2012 and formed by an interdisciplinary team of PhDs in Musicology, History, History of Art and Hispanic Philology coordinated by Professors Paulino Capdep\u00F3n and Juan José Pastor, CIDoM has among its objectives to replace and restore the musical heritage in one of the most important regions of Spain, Castilla-La Mancha, a large area of 80.000 Km2, that hosts civil and religious centres of a great and historical musical activity. Centres as the music chapel of the Toledo and Cuenca Cathedrals attracted a large number of composers. This musical legacy has remained, unfortunately, mostly unknown. In the last years CIDoM has developed several national I+D+i Research Projects (Research + Development + innovation) focused in the Musical Heritage of Castilla-La Mancha and its Critical Analysis, Reception and Digital Edition.  1  This work is inscribed within the context of the projects HAR2017-86039-C2-2-P. \"El patrimonio musical de la Espa\u00F1a moderna (siglos XVII-XVIII): recuperaci\u00F3n, digitalizaci\u00F3n, an\u00E1lisis, recepci\u00F3n y estructuras ret\u00F3ricas de los discursos musicales\" [The Musical Heritage of the Modern Spain (17th and 18th centuries): recovery, digitalisation, an\u00E1lisis, reception and rhetorical structures of musical discourses] and PTA2016-13106-I, Catalogaci\u00F3n y Digitalizaci\u00F3n del Patrimonio Musical de Castilla-La Mancha [Cataloguing and Digitalisation of the Musical Heritage of Castilla-La Mancha].  82  \f2  The digital database for Musical Heritage of Castilla-La Mancha  The digital database gathers the inventory of the musical sources to be consulted by the scientific community by means of a page web (beta.cidom.es) in order to analyse a little-known musical heritage: the Castilla-La Mancha's musical legacy during the Renaissance and Baroque period. The main aim is to cover the enormous gap existing in the region as regards the lack of an institution responsible for research and musical documentation. In this sense CIDoM is presented as a qualified proposal capable of fulfilling its responsibilities in the directions proposed by the International Council on Archives (ICA)2, the International Association of Music Libraries, Archives and Documentation Centers (IAML)3, the Spanish Association of Musical Documentation (AEDOM)4, the Spanish Society of Musicology (SEdeM) and the National Institute of Performing Arts and Music (INAEM)5, following the standards of quality and criteria of documentation and research sanctioned by these institutions to which the members of the Center belong. This essential task must allow, on the one hand, the analysis and study of the works of the fundamental composers of our region \u2212recognized by international musicology\u2212 belonging to different periods, such as Diego Ortiz, Sebasti\u00E1n de Covarrubias, Alonso Xu\u00E1rez, Torrej\u00F3n and Velasco. On the other hand, it will allow to order, classify and catalogue the sources of information and musical work ignored and contained in the cathedral and administrative archives of the region, Toledo, Sig\u00FCenza, Talavera de la Reina, Cuenca, Guadalajara, Pastrana, etc., with the aim to provide the professional community with the opportunity to publish and record this musical heritage. The digital catalogue is managed by two independent databases with which information relating to composers is recorded (including basic data of interest for the user's search, biography, registered works, bibliography and discography), on the one hand, and musical works, on the other. The process of cataloguing the latter has been developed from the standards ISAD(G) (1999), ALA (2004), and the guidelines by Gonz\u00E1lez-Valle (1996), Miliano (1999), Schultz and Shaw (2003), so that the following fields are considered: composer, title, type of document (manuscript or printed), genre, year of composition (or time bracket, when unknown), date of the source (or time bracket, when unknown), author of the text, literary source, premiere date, file in which it is kept, signature, coded musical incipit (allows works with the same musical beginning to be located, even if they are transported to another mode or tonality, so that it can automatically detect borrowings between different works)6, literary incipit, vocal and instrumental template, facsimile (allows the user to download directly a scanned copy of the source in PDF format), transcription (if available), data on the edition (if applicable), and observations (including data such as the diplomatic title of the work, information on the copy and the physical state of the medium, its measurements, the number of folios or pages, the tone or mode of the work and the time signature)7.  3  The digital database for Cervantes and Music  Every musical adaptation of a literary work has to be considered as an exercise of perception and interpretation that provides additional information on the hermeneutics of a writer's works. These adaptations allow us to understand and explain how the literary work has been recreated and transformed in each epoch. Once a play or a novel has been put in music, every musical version offers the audience a sort of critical and at the same time musical thought that reflects a new conception \u2212even misconception\u2212 of the work. It cannot be denied that Cervantes' works have provided composers excellent material for their musical compositions and this fact has to be taken into account to describe the process of his musical 2  They have developed the ISAD(G), General International Standard Archival Description, a guidance for cataloguing activities that has been considered for CIDoM's projects. 3 That includes the MARC 21 standard for the representation and exchange of bibliographic information data. 4 In particular, taking into account the orientations offered in Eudom (2010). 5 Under the conclusions of the symposium La gestion del patrimonio musical (Management of the Musical Heritage), expressed in \u00C1lvarez Ca\u00F1ibano et. al. (2014). 6 The coding adopted consists in recording the intervallic distances between the musical notes of the incipit. To this purpose, the quality of each interval (ascending or descending) in number of semitones is recoded. For example: C-E flat-D-G would be noted as 3a-1d-5a. 7 You can access the catalogue of the historical musical heritage in Castilla-La Mancha, made by the CIDoM, in the following link: http:\/\/beta.cidom.es\/patrimonio-musical\/patrimonio-musical-historico\/bases-de-datos-de-compositores-y-obras-de-castillala-mancha  83  \freception: how the characters and the episodes of his works have been selected by composers and perceived by the audience and what kind of musical treatment \u2212genres, musical patterns, etc.\u2212 each composer provides (Pastor, 2007; Pastor, 2009). At the same time, there cannot be any doubt that Cervantes' works reflect faithfully the Spanish musical world of 16th and 17th centuries: musical instruments, dances and bailes, romances and songs are often cited and performed in his pages in order to depict not only a special and picturesque environment in which his characters evolve such as a gypsy's world in La gitanilla or Muslim's traditions in La gran sultana or Los ba\u00F1os de Argel, but in addition assign a particular semantic value to each musical element adding a supplementary meaning to the work's understanding (Pastor, 2005; Pastor, 2006). Our digital project distinguishes between three different aspects considered as a powerful educative instrument: 3.1. Musical instruments This first point of the project will provide a catalogue of the musical instruments cited by Cervantes in his works, explaining their social functions in the texts and offering, from an educative point of view, different sound files, image files and text files in order to familiarize the users with the musical world around Don Quixote's author8. Let's consider some examples. In the First Part of Don Quixote (I, XXVI), the mad knight says to his squire: [\u2026] for know, Sancho, that all or most of the knights-errant of times past were great poets and great musicians; these two accomplishments, or rather graces, being annexed to lovers-errant. True it is, that the couplets of former knights have more of passion than elegance in them. (Don Quixote, I, XXVI)  In the Second Part, in the adventure in Duke's Palace, Don Quixote requests a lute to console Altisidora: 'Do me the favour, se\u00F1ora, to let a lute be placed in my chamber to-night; and I will comfort this poor maiden to the best of my power; for in the early stages of love a prompt disillusion is an approved remedy;' and with this he retired, so as not to be remarked by any who might see him there. He had scarcely withdrawn when Altisidora, recovering from her swoon, said to her companion, 'The lute must be left, for no doubt Don Quixote intends to give us some music; and being his it will not be bad.' They went at once to inform the duchess of what was going on, and of the lute Don Quixote asked for, and she, delighted beyond measure, plotted with the duke and her two damsels to play him a trick that should be amusing but harmless (Don Quixote, II, XLVI)  There cannot be any doubt that Cervantes's works faithfully reflect the Spanish musical world of the 16th and 17th centuries: musical instruments, dances and bailes, romances and songs are often mentioned and performed in his books depicting not only the environment in which his characters evolve but they also add a particular semantic value to each musical element. Participants in this galaxy of musical performance are representatives of all walks of life, from the highest noble to the lowliest peasant, and the number of instruments one encounters in Cervantes's writings is truly extensive. Cervantes groups them in pastoral, military, popular and aristocratic and there are fifty different instruments cited in his works. Let's go to see some examples, but I would caution previously that the English translations consulted don't respect exactly the nature of musical instruments. We see in Cervantes that harps and lutes are playing together. We have several texts in Cervantes that describe the performance of harps and lutes together: But the instant the car was opposite the duke and duchess and Don Quixote the music of the clarions ceased, and then that of the lutes and harps on the car, and the figure in the robe rose up, and flinging it apart and removing the veil from its face, disclosed to their eyes the shape of Death itself, fleshless and hideous, at which sight Don Quixote felt 8  To access the catalogue of musical instruments in Cervantes, use the following link: http:\/\/beta.cidom.es\/musica-yliteratura\/cervantes-y-la-musica\/instrumentos-musicales-en-cervantes.  84  \funeasy, Sancho frightened, and the duke and duchess displayed a certain trepidation. Having risen to its feet, this living death, in a sleepy voice and with a tongue hardly awake, held forth as follows: I am that Merlin who the legends say The devil had for father, and the lie Hath gathered credence with the lapse of time. (Don Quixote, II, XXXV)  The harp is used too as an aristocratic instrument for ladies: Calliope With so much peculiarity, with so much sweetness, with such harmony, she touched the harp of the graceful muse. She, having sounded the strings awhile, with a voice sonorous past conception, then gave utterance to these stanzas: Song of Calliope To the sweet sound of my attempered lyre Oh shepherds listen with attentive ear (La Galatea, V) Lucinda I passed in such employments as are not only allowable but necessary for young girls, those that the needle, embroidery cushion, and spinning wheel usually afford, and if to refresh my mind I quitted them for a while, I found recreation in reading some devotional book or playing the harp, for experience taught me that music soothes the troubled mind and relieves weariness of spirit. (Don Quixote, I, XXVIII) Altisidora He trembled lest he should fall, and made an inward resolution not to yield; and commending himself with all his might and soul to his lady Dulcinea he made up his mind to listen to the music; and to let them know he was there he gave a pretended sneeze, at which the damsels were not a little delighted, for all they wanted was that Don Quixote should hear them. So having tuned the harp, Altisidora, running her hand across the strings, began this ballad: O thou that art above in bed, Between the holland sheets, A-lying there from night till morn, With outstretched legs asleep; (Don Quixote,II, XLIX)  All these elements studied and analyzed can be consulted on the digital http:\/\/beta.cidom.es\/musica-y-literatura\/cervantes-y-la-musica\/instrumentos-musicales-encervantes\/instrumentos\/1\/arpa.html  platform:  3.2. Songs, romances, dances and bailes This second point deals with the accomplishment of the digital edition of the scores related with Cervantes' texts (Pastor, 2017). For example, some chapters of the First Part of Don Quixote begin with the first verse of a sung poem. This interactive frame will be accompanied by sound files, facsimile editions, bibliographical information about composers, and different articles explaining the significance of the relationship between music and poetry in Cervantes' works9. I would like to underline that some chapters of the First Part of Don Quixote begin with the first verse of a sung poem. Many chapters of both parts begin with one, two or several 'accidental verse-lines' \u00B4\u2013prose lines that may be read and, consequently, sung\u2013 as endecasyllables, octosyllables, heptasyllables: there are also so many indeed that we must assume they are not there by chance but deliberately. It shouldn't be overlooked that chapter one of the First Part of Don Quixote also begins with a ballad-line to identify the place where Don Quixote lived: 'En un lugar de la Mancha' [In a place in La Mancha]. Although in this last case we haven't got any evidence or proof of its musical performance, it's easy to imagine that Cervantes might have conceived the beginning of his novel like an epic poem composed to be sung (Pastor, 2005).  Use the following link to see the database of songs, romances, dances and bailes in Cervantes' texts: http:\/\/beta.cidom.es\/musica-y-literatura\/cervantes-y-la-musica\/danzas-y-bailes-en-cervantes. 9  85  \fThe same thing happens with another romance, 'Mira Nero de Tarpeya' ('Nero fiddled while Rome burned'), that relates the history of an indolent Nero playing the harp from Tarpeian hill while Rome was burning. This romance was very famous in Iberian Peninsula and was put in music by Bermudo (Declaraci\u00F3n de instrumentos musicales, 1555) and Venegas de Henestrosa (Libro de cifra nueva para tecla, harpa y vihuela, 1557). Cervantes introduces and intersperses in several episodes of Don Quixote this musical reference as an echo of the musical romance emphasizing the semantic value of the madness. First time it appears, is after Desperate song of Gris\u00F3stomo: Or comest thou to triumph in the cruel exploits of thy inhuman disposition, or to behold from that eminence, like another pitiless Nero, the flames of burning Rome; or insolently to trample on this unhappy corpse, as did the impious daughter on that of her father Tarquin? (Don Quixote, I, XIV)  Second occurrence, it appears as parody, when Sancho gets stuffed in Camacho's Wedding: Sancho beheld all this, and was nothing grieved thereat; but rather, in compliance with the proverb he very well knew, When you are at Rome, do as they do at Rome, he demanded of Ricote the bottle, and took his aim, as the others had done, and not with less relish. (Don Quixote, II, LIV)  The last occurrence of the romance is part of the fun of Altisidora, who makes mock of Don Quixote, integrated in another long romance she sings: Manchegan Nero, look not down From thy Tarpeian Rock Upon this burning heart, nor add The fuel of thy wrath. (Don Quixote, II, XLIV)  All these elements studied and analysed can be consulted too on the digital platform: http:\/\/beta.cidom.es\/musica-y-literatura\/cervantes-y-la-musica\/canciones-y-topicos-musicales-encervantes\/canciones\/8\/mira-nero-de-tarpeya.html 3.3. Musical reception of Cervantes' works Finally, the development of this project will provide a complete catalogue of musical compositions based in Cervantes' texts. Information included will be articulated by genres, countries, and musical periodization and it will be the first step to seriously study how the Cervantes' literary genius has encouraged the composers' creative imagination10. Some composers who have put the work of Cervantes into music can be consulted on our digital platform: http:\/\/beta.cidom.es\/musica-y-literatura\/cervantes-yla-musica\/la-recepcion-musical-cervantina\/recepcionmusical\/3\/millan-de-las-heras-manuel-1971--.html  4  Conclusions  For the CIDoM, the main objective is the cataloguing and digitalisation of the musical heritage of the region of Castilla-La Mancha, as well as facilitating the researcher's search and relation between data, and providing access to primary sources. In addition, it is crucial to project the results of our research on the area of Music Education and to disseminate this information to the educational community, in order to create and to implement educational tools \u2212demanded by music teachers\u2212 concerned with music heritage, thus increasing the quality and the cross-sectional relations of the musical education in the different educational levels. In this sense, the project about musical reception of Cervantes' works has a high pedagogical project for us (Pastor, 2016). For this reason, the interdisciplinary vocation with which the digital projects presented here are born seeks in the educational field the adequate space to project university research on the reality of other academic levels.  You can access to the database about musical reception of Cervantes' works in the following link: http:\/\/beta.cidom.es\/musicay-literatura\/cervantes-y-la-musica\/la-recepcion-musical-cervantina. 10 "
	},
	{
		"id": 15,
		"title": "Una proposta di ontologia basata su RDA per il patrimonio culturale di Vincenzo Bellini",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Salvatore Cristofaro",
			"Daria Spampinato"
		],
		"body": "Introduzione  Il patrimonio culturale conservato al Museo Civico Belliniano di Catania comprende collezioni di oggetti (o risorse) di natura molto variegata riconducibile ai settori museale, bibliografico e archivistico con la specificità del domino musicale. Allo stato attuale, le risorse identificate consistono di (circa): \u2013 250 oggetti tra dipinti, foto, pianoforti, spille, orologi, mobili, poster, medaglie, tessuti, ecc.; \u2013 4.500 fogli manoscritti di documenti e lettere; \u2013 9.300 fogli di spartiti manoscritti; \u2013 1.900 partiture a stampa; \u2013 50 opuscoli musicali a stampa; \u2013 280 libri della biblioteca del museo;  88  \f\u2013 60 dischi in vinile di varie composizioni musicali. Negli ultimi anni, questo ricco patrimonio culturale è stato promosso in particolare dal progetto BellinInRete (Del Grosso et al., 2018). Il progetto BellinInRete nasce dalla collaborazione tra il Comune di Catania, l'Istituto di Scienze e Tecnologie della Cognizione del CNR e il Dipartimento di Scienze Umanistiche dell'Università degli Studi di Catania. Esso mira a rinnovare e creare un cambiamento duraturo nella valorizzazione del Museo Civico Belliniano di Catania. Al fine di rendere il patrimonio culturale del Museo Belliniano interoperabile e riutilizzabile da studiosi, operatori culturali ed utenti generici si propone l'organizzazione semantica di questo patrimonio in un unico contenitore omogeneo, l'ontologia OntoBellini, progettata e sviluppata secondo i paradigmi del Linked Open Data e del Semantic Web. La grande varietà delle risorse museali coinvolte, non ancora completamente digitalizzate e catalogate, ha condotto all'idea di sperimentare lo standard di metadatazione RDA (Resource Description and Access).1 RDA è un package di concetti e istruzioni per la creazione di metadati di risorse eterogenee di biblioteche, archivi e beni culturali (Bianchini and Guerrini, 2016). In questo articolo viene descritto il lavoro in fase di sviluppo per la realizzazione dell'ontologia OntoBellini.2 L'articolo è organizzato come segue. Nella Sezione 2 vengono esaminati brevemente alcuni lavori correlati e nella Sezione 3 viene descritto il lavoro svolto e in corso di realizzazione relativo all'analisi e alla rappresentazione delle risorse del Museo Belliniano, motivando l'esplorazione e lo sfruttamento di RDA per la costruzione dell'ontologia OntoBellini. La Sezione 4 presenta, a titolo di esempio, una descrizione tassonomica ad alto livello della parte dell'ontologia OntoBellini che si intende sviluppare relativa ad un corpus di lettere di Vincenzo Bellini e, infine, nella Sezione 5 si traggono le conclusioni e si discutono suggerimenti per lavori futuri.  2 Lavori correlati Negli corso degli anni sono state presentate varie proposte riguardanti l'organizzazione semantica del patrimonio culturale dei musei. Molte di esse si basano sul CIDOC Conceptual Reference Model (CIDOCCRM),3 che rappresenta lo standard internazionale per lo scambio controllato di informazioni riguardanti i beni culturali dal 2006. CIDOC-CRM fornisce un'ontologia di base generale che può essere adottata in contesti concernenti il patrimonio culturale per sviluppare sistemi informativi semantici basati sul web e per migliorare la condivisione delle informazioni. Basandosi su CIDOC-CRM, sono stati sviluppati vari modelli di organizzazione della conoscenza volti a migliorare l'espressività semantica nel dominio del patrimonio culturale e per affrontare questioni specifiche non completamente contemplate da altri modelli esistenti. Questo è il caso, ad esempio, delle ontologie entry OA e entry F presentate in (Daquino et al., 2017), che arricchiscono le capacità descrittive di CIDOC-CRM attraverso la definizione di svariate possibili relazioni tra opere d'arte (entry OA) e fotografia (entry F), seguendo gli standard italiani promossi dall'ICCD 4 Scheda OA e Scheda F, rispettivamente. In (Moraitou et al., 2019) è possibile trovare un ampio elenco di altri progetti e proposte basate su CIDOC-CRM nel settore dei beni culturali. Nel contesto della promozione del patrimonio culturale è emerso di recente lo standard RDA. Gli obiettivi principali di RDA sono l'identificazione e la messa in relazione di entità a livello astratto. Inizialmente, RDA implementava il modello di dati Functional Requirements for Bibliographic Records (FRBR), classificando le risorse informative in termini di una gerarchia di entità a quattro livelli chiamata WEMI (Work, Expression, Manifestation, Item).5 Successivamente, dal novembre 2016, il comitato direttivo di RDA ha concordato l'adozione dell'IFLA Library Reference Model (LRM)6 come modello 1http:\/\/www.rda-rsc.org\/. Tutti gli url citati in questo contributo sono stati visitati il 27 novembre 2019. 2Il presente articolo costituisce una versione aggiornata del contributo ad opera degli stessi autori dal titolo OntoBellini: towards an RDA based ontology for Vincenzo Bellini's cultural heritage presentato al convegno JOWO 2019, 23 settembre 2019. 3http:\/\/www.cidoc-crm.org\/ 4ICCD (Istituto Centrale per il Catalogo e la Documentazione) - http:\/\/www.iccd.beniculturali.it\/ 5https:\/\/www.ifla.org\/best-practice-for-national-bibliographic-agencies-in-adigital-age\/node\/8915 6https:\/\/www.ifla.org\/publications\/node\/11412  89  \fconcettuale per lo sviluppo di RDA,7 sostituendo FRBR. RDA aspira a fornire uno standard universale per il data-recording, un codice univoco per rappresentare risorse eterogenee che si possono trovare in: (A) biblioteche (manoscritti, libri, musica e film); (B) archivi (documenti istituzionali, documenti personali e familiari e documentazione commerciale); (C) musei (opere d'arte, costumi, oggetti e foto). Si evidenzia che, nel contesto italiano, le risorse relative a biblioteche, archivi e musei sono gestite, attraverso l'utilizzo di norme solide e riconosciute, dalle rispettive istituzioni ICCU 8, ICAR 9 e ICCD. Mentre l'Associazione Italiana MAB 10 esplora le prospettive di convergenza tra i professionisti e le competenze in materia di musei, archivi e biblioteche. Negli ultimi anni RDA ha attratto l'interesse di diverse istituzioni culturali pubbliche sia europee che d'oltre oceano che lo hanno adottato e implementato, sperimentandone applicazioni alla catalogazione e condivisione di risorse bibliotecarie (si vedano, ad esempio, (Ducheva and Pennington, 2017) e (Panchyshyn et al., 2019)).  3  (Ri)organizzazione dei dati museali  Nell'ambito del progetto BellinInRete, il patrimonio del Museo Civico Belliniano è stato parzialmente studiato e analizzato da esperti musicologi e da specialisti con competenze museali, archivistiche e bibliotecarie, con l'obiettivo di recuperare informazioni sulle risorse del museo. Queste sono state quindi rappresentate formalmente come record di dati che comprendono diversi campi di informazione (si veda sotto). La collezione di questi record costituisce la base su cui si fonda la presente proposta di organizzazione semantica del patrimonio belliniano.11 I record delle risorse museali sono stati creati seguendo gli standard italiani ICCD e ICCU per la catalogazione e la documentazione (Scheda OA, Scheda F e schede SBN). Il numero dei campi di ciascuno record e il loro significato dipende dal tipo di risorsa rappresentata dal record stesso. Sono stati identificati 14 diversi tipi base di risorse museali, ossia: Manoscritti  Testi a stampa  Musica manoscritta  Musica a stampa  Materiale grafico  Arredi  Dipinti  Documenti  Foto  Medaglie  Statue  Strumenti musicali  Tessuti  Oggetti generici  All'interno di ciascun tipo base, le risorse sono suddivise, a loro volta, in sottotipi più specializzati. Ad esempio, i Manoscritti comprendono: lettere (originali), bozze e minute di lettere, copie di lettere, certificati di battesimo, certificati di morte, certificati di matrimonio, note di spesa, bollettini medici, ecc. Il Materiale grafico comprende i poster, mentre gli spartiti rientrano nella Musica manoscritta. Gli Oggetti generici comprendono oggetti personali di Vincenzo Bellini, come orologi e spille, e altri oggetti di vita quotidiana come cucchiai, coltelli, tazze, ecc. In Figura 1 si riporta una selezione di campi di record in forma tabellare. Si noti che il blocco di informazioni memorizzato in alcuni campi di record presenta un basso livello di granularità che potrebbe essere ulteriormente raffinato suddividendo il blocco tra campi aggiuntivi di dati atomici. Questo è il caso, ad esempio, del campo formato (si veda nella Figura 1 la penultima colonna della tabella più in alto) che viene utilizzato per descrivere alcune caratteristiche fisiche di un manoscritto, come dimensioni, numero di pagine, foliazione, direzione della scrittura, ecc. Si osservi anche che alcuni campi di record sono specifici per il particolare tipo di risorsa 7http:\/\/www.rda-rsc.org\/ImplementationLRMinRDA 8ICCU (Istituto Centrale per il Catalogo Unico) - https:\/\/www.iccu.sbn.it\/it\/ 9ICAR (Istituto Centrale per gli Archivi) - www.icar.beniculturali.it\/ 10MAB (Musei Archivi Biblioteche) http:\/\/www.mab-italia.org\/ 11Si noti che, attualmente, il numero dei record creati corrisponde a circa il 70% del numero totale delle risorse stimate del museo. Il coinvolgimento delle rimanenti risorse è programmato per il prossimo futuro.  90  \fFigura 1: Alcuni record corrispondenti alle risorse del Museo Belliniano: le righe verdi contengono i nomi dei campi dei record; le righe blu indicano i tipi base delle risorse. Si noti che la tabella più in alto coinvolge solo risorse cartacee. museale descritta da questi campi. Ad esempio, il campo lingua (cfr. Figura 1) è stato specificamente utilizzato per rappresentare la(e) lingua(e) delle risorse scritte e non può certo essere applicato agli oggetti; così come non ha senso parlare (ad esempio) della foliazione di un tavolo o di una sedia (infatti la foliazione è una informazione specifica del campo formato relativo ai manoscritti). Il patrimonio del Museo Belliniano coinvolge anche alcuni oggetti fisici composti (come contenitori per medaglie e cornici fotografiche) che richiedono una struttura gerarchica di record per essere ragionevolmente descritti.12 Inoltre, il Belliniano conserva anche alcuni libretti musicali che non sono stati ancora catalogati. Si sottolinea ulteriormente che diversi documenti d'archivio (come i vari certificati), hanno ricevuto ad oggi un'analisi solo approssimativa: all'interno del progetto BellinInRete si prevede di creare metadati dettagliati per essi seguendo gli standard adottati dal Sistema Archivistico Nazionale Italiano13 gestito dall'ICAR. Come emerge dalle considerazioni precedenti, le rappresentazioni delle risorse del Museo Belliniano create presentano, allo stato attuale, un carattere eterogeneo con un basso livello di granularità che rende difficile tradurle in una base di conoscenza ontologica espressiva ed efficace.14 (Si noti che ciò deriva in 12Allo stato attuale, tali oggetti composti non sono ancora stati disassemblati per motivi di conservazione, e quindi, al momento, è stato possibile recuperare poche informazioni descrittive per essi. 13http:\/\/san.beniculturali.it\/SAN 14Si osservi comunque che recentemente una parte (ristretta) delle risorse museali del Belliniano, consistente in un corpus di lettere di Vincenzo Bellini, è stata oggetto di studi sistematici approfonditi che hanno evidenziato diversi aspetti semanticamente interessanti facilmente formalizzabili in un'ontologia. (Ciò verrà discusso nella Sezione 4.)  91  \flettera nomina  haSupporto  entità  pagina  haDimensione  dimensione  haDestinatario Subclass of Subclass of haAutore  riferimento  Subclass of  haOpposto Subclass of  Subclass of Subclass of Subclass of originale  Subclass of  termine  Subclass of Subclass of  opera copia  preliminareDi luogo minuta  organizzazione bozza  persona  Figura 2: Schema ontologico del corpus epistolare belliniano. parte dai particolari criteri di rappresentazione adottati per la creazione dei record di dati corrispondenti alle risorse del museo.) Al fine di migliorare tali rappresentazioni sarebbe innanzitutto utile pulire e raffinare i record dei dati acquisiti, in modo da ottenere una collezione più uniforme. Quindi, le istruzioni RDA potrebbero poi essere proficuamente sfruttate per ottenere una (ri)organizzazione più efficace dei dati. Difatti se si dovesse rappresentare soltanto la collezione museale (composta da oggetti unici) si potrebbe utilizzare CIDOCCRM che è stato progettato principalmente per questa tipologia di risorse. Ma volendo utilizzare un unico modello di rappresentazione dei dati per l'intero patrimonio belliniano, RDA, attraverso il meccanismo di classificazione WEMI (ereditato da FRBR) si presta meglio alla descrizione delle risorse.15 In termini molto generali, le principali attività coinvolte nello sviluppo dell'ontologia OntoBellini, possono quindi essere schematizzate come segue. Dopo una prima fase di ristrutturazione dei dati, con l'obiettivo di creare collezioni di record più omogenee e più dettagliate (come descritto sopra), si prevede di identificare i concetti e le proprietà alla base dell'ontologia OntoBellini in conformità con il framework entità-relazioni di RDA, e quindi sviluppare l'ontologia stessa rendendola accessibile via web.  4 Il caso delle lettere Belliniane Una parte peculiare del progetto BellinInRete riguarda la rappresentazione, l'organizzazione e la codifica secondo lo standard TEI-XML (Text Encoding Initiative),16 di un corpus di lettere della corrispondenza di Vincenzo Bellini (corpus epistolare) che forniscono informazioni interessanti per diversi aspetti legati alla vita sociale e all'attività artistica in ambito musicale del compositore catanese (si veda (Del Grosso et al., 2018)). In questa sezione viene fornita, a titolo esemplificativo, una descrizione ad alto livello della tassonomia di base della parte dell'ontologia OntoBellini che si intende sviluppare relativa al solo corpus epistolare, presentandone il corrispondente schema ontologico (cfr. Figura 2). Questo corpus epistolare, infatti, è stato recentemente studiato ed analizzato in maniera più dettagliata ed approfondita rispetto alle altre risorse museali e, a differenza di queste ultime, le informazioni disponibili ad esso relative risultano, allo stato attuale, più complete e strutturate e permettono di delineare un quadro più concreto e definito degli item di conoscenza da rappresentare e dedurre attraverso l'ontologia. Più specificatamente, l'analisi del corpus epistolare belliniano ha condotto alle seguenti considerazioni. Il corpus epistolare è 15Si menziona che, basandosi sui modelli IFLA FR, è stata sviluppata un'estensione di CIDOC-CRM, ossia FRBRoo (http: \/\/www.cidoc-crm.org\/frbroo-0), che intende rappresentare la semantica delle informazioni bibliografiche e facilitare l'integrazione e lo scambio di risorse bibliografiche e museali. Tuttavia IFLA LRM, come modello concettuale sottostante di RDA, consente un livello di generalità maggiore rispetto a FRBRoo, poiché include meno dettagli di quest'ultimo. 16https:\/\/www.tei-c.org\/  92  \fcostituito essenzialmente da 4 tipologie di documenti epistolari (o lettere) ossia: bozze (di lettere), minute (di lettere), copie (di lettere) e originali (di lettere).17 Ogni documento epistolare ha uno o più autori, è indirizzato ad uno o più destinatari ed è fisicamente contenuto (scritto) su un supporto costituito da una o più facciate di fogli di carta (pagine) aventi differenti dimensioni: uno stesso documento può essere infatti frammentato su diverse pagine e (parti di) documenti diversi possono trovarsi su una stessa pagina o su pagine opposte (fronte-retro) di uno stesso foglio di carta. Inoltre, in ogni documento epistolare vengono nominate (in maniera esplicita o implicita) diverse entità interessanti quali persone, organizzazioni, luoghi, opere (musicali), termini (musicali e non) e riferimenti (bibliografici); in particolare, il complesso delle persone e delle organizzazioni include gli autori e i destinatari delle lettere menzionati sopra. Le precedenti considerazioni si traducono nello schema ontologico riportato in Figura 2 che rappresenta la tassonomia di base della parte dell'ontologia OntoBellini relativa al corpus epistolare del Belliniano.18 Si osservi che l'organizzazione semantica proposta e rappresentata in Figura 2 per il corpus epistolare è molto generale e di alto livello. Ai fini dell'espressività essa può (e deve) essere specializzata imponendo degli opportuni vincoli semantici attraverso l'introduzione di appositi assiomi di classi e proprietà. Ad esempio, in riferimento alla rappresentazione in Figura 2, sarebbe ragionevole assumere che le classi bozza, minuta, copia e originale siano disgiunte (si veda la nota n. 17) e che inoltre le proprietà haDimensione e haOpposto siano entrambe funzionali e con la seconda ulteriormente simmetrica e con inversa funzionale.19 In aggiunta si potrebbe postulare anche la validità della proprietà che gli autori e i destinatari delle bozze, delle copie e delle minute coincidano con quelli delle rispettive versioni originali, e così via.  5  Conclusioni e lavori futuri  In questo lavoro è stata proposta l'organizzazione semantica del patrimonio culturale conservato nel Museo Civico Belliniano di Catania attraverso un'ontologia condivisa \u2013l'ontologia OntoBellini\u2013, basandosi sulla grande quantità di dati attualmente acquisiti per le risorse del museo. Il basso livello di granularità e il carattere eterogeneo di questi dati richiede tuttavia una riorganizzazione preliminare degli stessi al fine di renderli più omogenei e facilmente codificabili nell'ontologia. A tal fine si prevede di sfruttare le indicazioni RDA per la creazione di metadati di risorse di biblioteche e beni culturali. A titolo di esempio è stata brevemente descritta una proposta di organizzazione semantica ad alto livello relativa ad un corpus di lettere di Vincenzo Bellini custodite nel museo Belliniano, presentandone il corrispondente schema ontologico."
	},
	{
		"id": 16,
		"title": "Biblioteche di conservazione e libera fruizione dei manoscritti digitalizzati: la Veneranda Biblioteca Ambrosiana e la svolta inevitabile grazie a IIIF",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Fabio Cusimano"
		],
		"body": "Le funzioni della biblioteca di conservazione e la digitalizzazione  Alla prova della tradizione storica, la mission che da sempre caratterizza le biblioteche consiste nel raccogliere, organizzare, ampliare e diffondere la conoscenza tramite l'accesso alle risorse in esse custodite (Bottasso 1999; Gorman 2004; Kempf 2013; Fabian 2015, 55-70), alla costante ricerca di un delicato equilibrio tra fruizione e conservazione. A tal proposito, come affermano Montecchi e Venuda, \u00ABLe due dimensioni dell'attività bibliotecaria, quella orizzontale dell'uso dei libri da parte dei nostri contemporanei e quella verticale della loro conservazione per i posteri, costituiscono i due poli attorno ai quali si strutturano i servizi di ogni biblioteca: al prevalere dell'uno o dell'altro avremo 'biblioteche di conservazione' o 'biblioteche d'uso', anche se è ben difficile incontrare biblioteche finalizzate unicamente ed esclusivamente all'una o all'altro. Non esiste, infatti, neppure in sede teorica, una netta opposizione tra questi due parametri, essendo la conservazione finalizzata all'uso sia presente che futuro del libro e, sull'altro versante, non potendo l'uso dei libri in biblioteca prescindere da forme di tutela e di conservazione che assicurino loro lunga vita tra gli uomini\u00BB (Montecchi e Venuda 2006, 79). Sebbene ogni epoca abbia vissuto fondamentali momenti di evoluzione e progresso in ogni campo del sapere e della tecnica \u2012 spesso inavvertiti (Montecchi e Venuda 2006, 23; Eisenstein 1986; Eisenstein 2004; Barbier 2005; Roncaglia 2010; Febvre 2011; McLuhan 2011; Bertolo 2016; Cursi 2016) dai contemporanei \u2012 la nostra società appare dotata di tali e tanti strumenti tecnologici potenzialmente utili a diffondere la conoscenza e la cultura che viene spontaneo chiedersi come sia possibile che tutto questo non abbia coinvolto nativamente il mondo delle biblioteche! Al giorno d'oggi, infatti, non siamo ancora riusciti ad affrancarci da quello che ormai sembra essere divenuto un vero luogo comune, ovvero il rapporto antitetico tra biblioteca e tecnologia: in un simile contesto, la biblioteca di conservazione viene ancora percepita come il luogo refrattario per eccellenza alla tecnologia e all'innovazione, destinato per definizione alla sola tesaurizzazione del proprio prezioso patrimonio. 95  \f2  Il XVII secolo e l'innovatività della Veneranda Biblioteca Ambrosiana di Milano  La Veneranda Biblioteca Ambrosiana (Rodella 1992, 121-147; Panizza 2012) di Milano viene tradizionalmente considerata \u2013 sin dalla sua solenne inaugurazione avvenuta l'8 dicembre 1609 \u2013 come uno dei primi e principali esempi di biblioteca pubblica (IFLA\/UNESCO Public Library Manifesto 1994) nell'accezione di un'istituzione creata con il chiaro intento di fornire accesso ai libri (Natale 1995, 1-2) a una comunità di lettori (Galluzzi 2011) quanto più ampia possibile (Serrai 2005, 7-9; Rovelstad 2000, 540-556). Si ritiene utile approfondire alcuni tratti caratteristici della fondazione della Biblioteca Ambrosiana che il suo ideatore e fondatore \u2013 il cardinale Federico Borromeo (Prodi 1971, 33-42; Ravasi 1992, 1-19; Buzzi e Ferro 2005) \u2013 fortemente volle aperta a tutti. Per farlo sarà opportuno approcciarsi al modello di biblioteca tipico del tempo (Burke 1992, 391-416; Ghilli 2015, 365-376; DeSeta 2016), soprattutto attraverso la testimonianza di Gabriel Naudé (Rovelstad 2000, 549), autore del celebre Advis pour dresser une biblioth\u00E8que (Naudé 1627). Egli riserva al IX e ultimo capitolo del suo Advis, dal titolo Quel doit estre le but principal de cette Bibliotheque, l'aspetto più importante legato alla trattazione teorica sull'allestimento di una biblioteca: quale debba essere lo scopo principale di una biblioteca ben allestita. In questo modo il Naudé loda senza riserve i particolari servizi che fanno dell'Ambrosiana una vera biblioteca aperta al pubblico, unica nel suo genere: Car pour ne parler que de l'Ambrosienne de Milan, & monstrer par mesme moyen comme elle surpasse tante en grandeur & magnificence que en obligeant le public beaucoup de celles d'entre les Romains, n'est-ce pas une chose du tout extraordinaire qu'un chacun y puisse entrer à toute heure presque que bon luy semble, y demeurer tant qu'il luy plaist, voir, lire, extraire tel Autheur qu'il aura agreable, avoir tous les moyens & commoditez de ce faire, soit en public ou en particulier, & ce sans autre peine que de s'y transporter \u00E9s iours & heures ordinaires, se placer dans des chaires destinees pour cet effect, & demander les livres qu'il voudra fueillerer au Bibliothecaire ou à trois de ses serviteurs, qui sont fort bien stipendiez & entretenus, tant pour servir à la Bibliotheque qu'à tous ceux qui viennent tous les iours estudier en icelle (Naudé 1627, 155-156). Proprio in relazione al precedente passo, il Serrai puntualizza che \u00ABper l'Ambrosiana il riconoscimento di Naudé, evidentemente frutto di esperienza diretta, va ancora oltre per sfociare in un'autentica stupefatta ammirazione\u00BB (Serrai 2005, 8). La descrizione del Naudé, infine, si avvia alla conclusione con altri interessanti spunti che vedono ancora l'Ambrosiana assunta a termine di paragone: [\u2026] il faudroit premierement observer que toutes les Bibliotheques ne pouvant tousiours estre ouvertes comme l'Ambrosienne, il fust au moins permis à tous ceux qui y auroient affaire d'aborder librement le Bibliothecaire pour y estre introduits par iceluy sans aucune dilation ny difficulté: secondement que ceux qui seroient totalement incognus, & tous autres qui n'auroient affaire que de quelques passages, peussent veoir chercher & extraire de toutes sortes de livres imprimez ce dont ls auroient besoin: tiercement que l'on permist aux personnes de merite & de cognoissance d'emporter à leurs logis les livres communs & de peu de volumes; [\u2026] (Naudé 1627, 161-162). Proprio riguardo agli spunti di cui il Naudé fa esplicita menzione, non si può non rimanere stupiti per quanto essi richiamino concetti e servizi di cui oggi si fa un gran parlare, quali, ad esempio, il servizio di reference e il prestito dei volumi: tutto questo può far sovvenire un collegamento tra la prassi descritta dal Naudé \u2013 che egli stesso auspica possa diffondersi quale strumento di base per l'utenza presso ogni biblioteca \u2013 e la digitalizzazione delle risorse catalografiche\/librarie. Quale migliore risposta agli ideali del cardinale Federico Borromeo e del Naudé stesso, di una biblioteca le cui risorse possano essere sempre liberamente accessibili, ricercabili e consultabili proprio attraverso specifici servizi online quali, appunto, una nuova biblioteca digitale ad accesso libero?  3  Dalla Bibliotheca alla Digital Library: cura delle collezioni e Data Curation  Un altro documento, stavolta strettamente collegato alla vita della Veneranda Biblioteca Ambrosiana, è di fondamentale importanza a proposito del tratteggio della figura del bibliotecario: si tratta delle Constitutiones Collegii ac Bibliothecae Ambrosianae (Bentivoglio 1835; Marcora 1986, 155-164; Annoni 1992, 149-184). 96  \fLe Consitutiones ambrosiane dedicano un intero capitolo alla figura del bibliotecario, alle sue mansioni e alle tipologie dei cataloghi: si tratta del Caput X, De Bibliothecario et Bibliotheca (Bentivoglio 1835, 32-39). Presso la Biblioteca Ambrosiana il Bibliothecarius è stato affiancato dal Custos catalogi, il custode del catalogo (Rodella 2013, 35-36): tale espressione risulta essere etimologicamente molto interessante e, come vedremo, gioca anche un ruolo importante nell'apertura verso funzioni e attività caratteristiche dell'era digitale, quali il Data Curator e il derivato Data Curation.1 Il Data Curator si ispira ai medesimi principi che guidavano il Custos catalogi del XVII secolo e opera per prendersi cura dei cataloghi (oggi prevalentemente OPAC), delle informazioni catalografiche (oggi prevalentemente codificate in formati standard come l'ISO2709), dei metadati (descrittivi, amministrativi, gestionali, tecnici, tutti accomunati dai tag e dai metalinguaggi adottati per la loro compilazione, quali, ad esempio, DublinCore e XML), degli oggetti digitali (così come dei diversi formati, specialmente per quanto concerne le immagini digitali), delle svariate procedure tecniche da attivare di volta in volta per avviare la produzione di nuovi oggetti digitali tramite l'utilizzo di differenti apparecchiature (macchine fotografiche digitali, scanner, ecc.), come anche per garantire la conservazione (storage) e il perdurare dell'informazione digitale. Altro fondamentale aspetto è quello della progettazione globale degli interventi di digitalizzazione e della messa a punto del necessario flusso di lavoro (workflow) ad essi collegati. 3.1.  La nuova biblioteca digitale della Veneranda Biblioteca Ambrosiana  Nel percorso d'attuazione della nuova fase di digitalizzazione presso la Veneranda Biblioteca Ambrosiana si è cercato di tenere in debito conto quanto già sperimentato presso altre realtà a livello internazionale, avendo cura di porre le basi per la realizzazione di un progetto necessariamente scalabile e aperto a proficue collaborazioni e condivisioni, nazionali e internazionali, sia a livello tecnico che scientifico. Il nodo del data reuse, per esempio, si è subito imposto in maniera molto concreta: con la precedente attività di digitalizzazione, infatti, è stata prodotta un'ingentissima mole di dati (oltre 1.800.000 immagini in formato .tif non compresso, colori, 24 bit) che rappresentano ancora oggi un prezioso nucleo composto da oltre 2.700 manoscritti integralmente digitalizzati su cui basare l'avvio di una nuova fase di digitalizzazione. Tale ingente quantità di dati, pari a circa 31 Tb di spazio-disco, necessita di cure costanti e va ad aggiungersi alla quotidiana produzione di nuove copie digitali di manoscritti, il tutto con il preciso obiettivo di rendere progressivamente disponibili online, gratuitamente e pubblicamente, le riproduzioni digitali integrali di parte del patrimonio manoscritto ambrosiano.  Figura 1: rappresentazione schematica dell'infrastruttura di digitalizzazione della Biblioteca Ambrosiana; in basso a destra: i loghi di IIIF-International Image Interoperability Framework e del visualizzatore Mirador.  1  Proprio nel merito delle funzioni del Data Curator emerge evidente il collegamento etimologico al Custos catalogi cui ho fatto riferimento in precedenza: l'etimologia del termine inglese curator è direttamente derivata dal latino, rispettivamente dal verbo curo e dal sostantivo curator, ed è proprio per questo motivo che è possibile mettere in relazione tra loro le due figure. 97  \fTale meritorio obiettivo chiama in causa un aspetto fondamentale al giorno d'oggi per la realizzazione di una nuova biblioteca digitale: l'utilizzo di IIIF-International Image Interoperability Framework (IIIF 2018) per la visualizzazione di contenuti digitali di qualità via Internet (Snydman 2015, 16-21; Brantl 2016, 10-13; Salarelli 2017, 50-66; Magnuson 2018; Cusimano 2019; Lit, Lambertus Willem Cornelis, van 2020, 160167). Viviamo in un'epoca in cui le tecnologie web based, la connettività, la diffusione di dispositivi mobili sempre più performanti rendono possibile ciò che solo due lustri fa non era nemmeno immaginabile: ogni biblioteca digitale di nuova generazione dovrebbe pertanto essere predisposta cercando di approfittare di tali condizioni tecnicamente favorevoli, avendo ben chiaro che essa sarà soggetta a diversi livelli di lettura che riguardano l'istituzione-biblioteca che la predispone e gli utenti che ne fruiranno. L'ecosistema IIIF, dunque, si configura come la 'scelta inevitabile' (cui non a caso faccio riferimento nel titolo del presente contributo) poiché consente \u2013 dal punto di vista degli utenti \u2013 una semplice ed efficace fruizione online dei contenuti digitalizzati; e, dal punto di vista dell'istituzione culturale promotrice (nel caso specifico, l'Ambrosiana), garantisce interoperabilità, scalabilità, personalizzazione e condivisione.2 La nuova biblioteca digitale dell'Ambrosiana3 implementa l'utilizzo delle APIs <IIIF Image API 2.1.1> (https:\/\/iiif.io\/api\/image\/2.1\/) e <IIIF Presentation API 2.1.1> (https:\/\/ iiif.io\/api\/presentation\/2.1\/); il visualizzatore Mirador e l'image server Cantaloupe; il tutto è interconnesso con l'OPAC dell'Ambrosiana al fine di garantire il collegamento diretto tra il record catalografico\/descrittivo del manoscritto ricercato e la relativa risorsa digitale: dalla scheda bibliografica presente nell'OPAC, infatti, tramite l'apposito link <Visualizza la copia digitale>, si attiva direttamente all'interno del browser il visualizzatore web Mirador (Mirador 2018) che consente all'utente online un'ottima esperienza di visualizzazione. Il Manifest .json relativo a ogni risorsa digitalizzata è pubblicamente reperibile all'interno della scheda informativa contrassegnata dalla 'i' posta in alto a destra nell'interfaccia del visualizzatore Mirador.  Figura 2: le risorse digitalizzate sono rese pubblicamente e gratuitamente fruibili grazie all'utilizzo del visualizzatore IIIF compliant Mirador, e il punto di partenza della fruizione digitale è proprio il catalogo (OPAC) della biblioteca.  La biblioteca digitale può essere consultata attraverso la landing page predisposta (in inglese) all'interno del sito web ufficiale della Veneranda Biblioteca Ambrosiana, attraverso due differenti percorsi tematici: <Scopri> (italiano: www.ambrosiana.it\/scopri\/biblioteca-digitale\/; inglese: www.ambrosiana.it\/en\/discover\/the-digital-library\/) e (italiano: https:\/\/www.ambrosiana.it\/studia\/biblioteca-digitale\/; https:\/\/www.ambrosiana.it\/en\/study\/the-digital-library\/). 2  italiano e in raggiungibile https:\/\/ https:\/\/ <Studia> inglese:  \u00AB[\u2026] One of the nicest things about the IIIF approach to shared content is that it lowers the barriers to building light-weight demonstrations like this for teaching and research purposes. The institutions that host the images are on the hook for long-term access and preservation, so it's not necessary to host your own copies of the images. [\u2026] There are thousands of manuscripts available now from interoperable repositories that can be used, and \u2013 with more institutions joining IIIF each year \u2013 thousands more in the offing. As the tools get easier to use and configure, it will be fascinating to see what becomes possible for medieval studies\u00BB. (https:\/\/tinyurl.com\/vnhpn3s). 3 La Veneranda Biblioteca Ambrosiana è stata ufficialmente inserita all'interno della lista delle istituzioni che utilizzano IIIF (https:\/\/iiif.io\/community\/#participating-institutions) quale unica istituzione culturale italiana. 98  \fFigura 3: indicazione dei percorsi tematici <Scopri> e <Studia> per la consultazione della biblioteca digitale all'interno del sito web ufficiale della Veneranda Biblioteca Ambrosiana (https:\/\/www.ambrosiana.it).  Figura 4: dettaglio del percorso tematico <Scopri> all'interno del sito web ufficiale della Veneranda Biblioteca Ambrosiana (https:\/\/www.ambrosiana.it).  Figura 5: dettaglio del percorso tematico <Studia> all'interno del sito web ufficiale della Veneranda Biblioteca Ambrosiana (https:\/\/www.ambrosiana.it).  La biblioteca digitale dell'Ambrosiana si apre al pubblico attraverso la sezione ad essa dedicata all'interno del proprio OPAC: https:\/\/ambrosiana.comperio.it\/bibliotecadigitale\/. Da qui ogni utente può accedere alla consultazione pubblica e gratuita delle copie digitali seguendo due vie principali: \u2022  \u2022  attraverso la consultazione diretta della scheda catalografica del manoscritto di proprio interesse a partire dalla segnatura dello stesso: in questo modo l'utente, utilizzando il catalogo per cercare tramite la segnatura il manoscritto cui è interessato, potrà accedere alla visualizzazione pubblica e gratuita della copia digitale seguendo il link <Visualizza la copia digitale> appositamente inserito all'interno della pagina di dettaglio di ciascun record catalografico; attraverso la consultazione della suddetta pagina riepilogativa (https:\/\/ambrosiana.comperio.it\/biblioteca-digitale\/), sfogliando idealmente la collezione digitale della Veneranda Biblioteca Ambrosiana tramite la lista dei 99  \fmanoscritti digitalizzati, peraltro riconoscibili grazie all'icona IIIF.  Figura 6: la pagina principale dell'OPAC dell'Ambrosiana con il nuovo riquadro di ricerca dedicato ai manoscritti digitalizzati (https:\/\/ambrosiana.comperio.it).  Figura 7: la pagina dell'OPAC dell'Ambrosiana dedicata alla digitale (https:\/\/ambrosiana.comperio.it\/biblioteca-digitale).  nuova  biblioteca  Si è anche proceduto a testare le potenzialità di Mirador collegando una porzione di testo trascritto in formato TEI \u2013 tratto da un manoscritto della Biblioteca Ambrosiana \u2013 alla corrispondente immagine digitale in IIIF dello stesso manoscritto tramite manifest .json, il tutto sfruttando le potenzialità dell'Annotation Tool integrato nel visualizzatore Mirador (Monella e Cusimano, 2019).  Figura 8: visualizzazione della trascrizione in formato TEI di una porzione del manoscritto digitalizzato grazie all'Annotation Tool di Mirador: ms. Ambr. D 23 sup., f. 13v \u00A9 Veneranda Biblioteca Ambrosiana.  100 "
	},
	{
		"id": 17,
		"title": "Repertori terminologici plurilingui fra normatività e uso nella comunicazione digitale istituzionale e professionale",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Klara Dankova",
			"Silvia Calvi"
		],
		"body": "1 Introduzione Il presente articolo intende investigare il rapporto tra terminologia e Digital Humanities rispetto all'utilizzo di strumenti computazionali per la conservazione e la valorizzazione del patrimonio culturale, veicolato dai termini della comunicazione professionale. La disciplina della terminologia è fin dalle sue origini profondamente legata al trattamento informatico: W\u00FCster, ingegnere elettronico e fondatore di questa disciplina (1931, 1979) considerava infatti la terminologia in un'ottica interdisciplinare ai confini tra linguistica, logica, ontologia e informatica. La presenza di riflessioni di natura informatica negli studi terminologici si intensificò al fine di introdurre nuove metodologie di lavoro che consentissero per la prima volta l'accesso a una enorme quantità di dati da analizzare, raccolti e categorizzati all'interno di risorse digitali. Il trattamento informatico in studi terminologici si arricchì via via nel corso degli anni 1970 e 1980, sia nella fase dell'estrazione di termini, sia in quella della costituzione di repertori terminologici, nella maggior parte dei casi basati sul concetto di synset (set of synonyms) (Zanola, 2018: 32). A partire dagli anni 1990, la terminologia iniziò ad essere associata all'ontologia, studiando i termini a partire dalla loro dimensione epistemologica e concettuale. Questo percorso di natura onomasiologica diede le basi teoriche per lo sviluppo dell'innovativo approccio introdotto da Christophe Roche2, basato sul concetto di ontoterminologia, ovvero una terminologia il cui sistema concettuale è un'ontologia formale (Roche, 2012: 2626). In questo studio si è scelto di adottare questo approccio, ritenuto il più appropriato per la rappresentazione dei concetti e la schedatura dei termini che li designano nell'ottica della divulgazione delle terminologie spontanee e non istituzionalizzate, create dai professionisti sul campo o in uso nelle pratiche di gruppi o di precise comunità professionali.  1  Klara Dankova ha redatto il \u00A7 4.1. Silvia Calvi ha redatto i \u00A7 1 e 2. I \u00A7 3, 4 e 5 sono frutto di una collaborazione delle due autrici. Le autrici ringraziano il professore Christophe Roche dall'Université Savoie Mont-Blanc per la disponibilità ad aver fornito l'accesso al programma Tedi, utilizzato ai fini del presente studio.  2  104  \f2 Obiettivi di ricerca Nel presente articolo si osserveranno i limiti dei repertori terminologici multilingui istituzionali rispetto alla descrizione di terminologie non ufficializzate ma in concreto uso in diversi domini. In un'ottica socioterminologica, si presterà particolare attenzione a come uno stesso concetto possa essere designato da una ricca varietà di termini in funzione del contesto e dell'utente di riferimento. Alla presentazione delle principali banche dati esistenti, repertori di terminologia ufficializzata \u2013 ovvero la terminologia ufficialmente riconosciuta in fonti primarie \u2013 seguirà la presentazione di alcuni esempi di terminologie non ufficializzate dei domini delle fibre tessili e dell'arrampicata sportiva. Questi esempi permetteranno di illustrare la varietà terminologica relativa a diversi contesti di utilizzo spesso non menzionati in repertori terminologici istituzionalizzati, lacuna che con la presente proposta si vuole cercare di colmare. Infine, illustrando un caso tratto dal dominio delle fibre tessili, si proporrà una metodologia per la progettazione di una banca dati, costituita da schede terminologiche multilingui. Obiettivi del presente studio sono quindi: - dimostrare che la terminologia spontanea deve essere conservata e ufficializzata, in quanto portatrice di un significativo patrimonio culturale, che consente di avvicinarsi anche al mestiere di riferimento per il quale la terminologia diviene custode del relativo saper-fare; - riflettere sulla rappresentazione e sulla divulgazione dei termini individuati, prestando particolare attenzione alla loro dimensione sociale e interculturale; - proporre un modello di realizzazione di schede terminologiche multilingui attraverso l'utilizzo di Tedi, ontoTerminology EDItor, software realizzato per progettare ontoterminologie multilingui (Roche, 2007).  3 Banche dati terminologiche e la comunicazione istituzionale e professionale Le banche dati terminologiche sono strumenti digitali che raccolgono le informazioni sui termini in una o più lingue, appartenenti a più settori e le presentano sotto forma di schede terminologiche redatte in modo standardizzato al fine di permettere la maggior condivisione dei dati raccolti. Diverse sono le informazioni fornite in una scheda terminologica, per esempio il termine, la marcatura morfologica, la definizione, eventuali sinonimi, il contesto di utilizzo e le note enciclopediche. Il beneficio maggiore dell'utilizzo dei termini definiti in modo univoco è la possibilità di comunicare in modo chiaro e preciso, indipendentemente dai soggetti coinvolti (Zanola, 2018: 64). Il bisogno di costruire una banca dati terminologica nasce prima di tutto nei contesti multilingui per rispondere alle necessità della pubblica amministrazione. Un modello efficiente di catalogazione dei dati terminologici è fornito dalle banche dati canadesi, dal Grand dictionnaire terminologique (GDT) dell'Office qu\u00E9b\u00E9cois de la langue fran\u00E7aise e da Termium Plus gestito dal Bureau de la traduction del governo canadese. L'urgenza di standardizzare l'uso dei termini si manifesta anche nel contesto europeo, in particolare nelle istituzioni dell'Unione Europea: la progettazione di IATE (InterActive Terminology for Europe) ha portato alla costituzione di una banca dati contenente soprattutto i termini usati nei testi legislativi e amministrativi, pubblicati dalle varie istituzioni dell'Unione Europea. Inoltre, ci sono anche delle banche dati terminologiche che operano esclusivamente a livello nazionale, come per esempio Termdat, la raccolta terminologica della Confederazione svizzera (Zanola, 2018: 64-67). Va sottolineato che queste banche dati sono state costruite pensando a un gruppo di utenti ben preciso (i cittadini quebecchesi, dell'Unione Europea, svizzeri ecc.). I termini sono stati individuati all'interno di un contesto specifico, quale il contesto giuridico e amministrativo dell'UE o del Canada, e, di conseguenza, non possono essere sempre utilizzati nei testi relativi ad altre realtà socio-culturali. Tuttavia, accanto ai termini recensiti e definiti in un contesto istituzionale esistono anche terminologie spontanee utilizzate nella comunicazione professionale, che trovano solo un parziale riscontro nelle banche dati ufficiali, le quali spesso trascurano le variazioni diastratiche. Consideriamo, per esempio, la varietà di termini in francese usati da vari gruppi di persone con riferimento alla fibra acrilica: gli ingegneri chimici useranno probabilmente una denominazione chimica che rivela la composizione della fibra (polyacrylonitrile), nella comunicazione tra professionisti in una fiera verrà invece più facilmente utilizzato il codice (PAN), mentre nell'ambito della moda sarà più frequente un termine più generico (acrylique o fibre acrylique). Nell'attuale contesto, il bisogno di disporre di raccolte multilingui di termini a uso dei professionisti di vari settori diventa sempre più forte, rendendo necessario lo sviluppo di un nuovo modello di catalogazione.  105  \f3.1. La terminologia non istituzionalizzata: il caso delle fibre tessili e dell'arrampicata sportiva I termini che designano le fibre tessili sono stati estratti manualmente da un corpus di testi in lingua francese, contenente quattro tipi di fonti: dei cataloghi delle fiere (Premi\u00E8re Vision Yarns, 12.02.-14.02 2019, Premi\u00E8re Vision Fabrics, 12.02.-14.02 2019), un documento istituzionale (DGE\/UBIFRANCE, 2006), un'opera di divulgazione (Fauque e Bramel, 1999) e un manuale tecnico (Weidmann, 2010). Il corpus risultante è costituito da 245 termini, di cui 60 sono nomi generici e 185 nomi di marca. Si è osservato che la terminologia delle fibre tessili è molto complessa sia per le differenze culturali sia per le sue variazioni diastratiche. Quanto alla dimensione culturale, si possono riscontrare delle differenze tra i termini usati in contesti diversi. Infatti, esistono alcuni casi, in cui i termini usati per designare un determinato concetto differiscono da un paese all'altro, anche all'interno di una stessa lingua. Si considerino i termini in francese utilizzati per designare la fibra di elastan: mentre nei paesi dell'UE si usa \u00E9lasthanne, il termine corrispondente negli Stati Uniti è spandex, in Giappone polyur\u00E9thane e in Cina sono in uso i termini \u00E9lasthanne o spandex (ISO 2076: 2013). Per quanto riguarda le differenze tra terminologia istituzionale e professionale, si può notare che alcuni termini esclusi dall'uso nella comunicazione istituzionale di un paese possono continuare ad essere usati tra gli esperti del settore. Per esempio, i termini fibranne e rayonne, che designano in francese rispettivamente le fibre discontinue e i filamenti continui di viscosa, sono stati sostituiti nel 1976 nella comunicazione istituzionale francese dal termine viscosa (Browaeys, 2014: 18; Baum e Boyeldieu, 2018: 256). Questo non impedisce però che vengano occasionalmente utilizzati dagli esperti del settore, per mettere in evidenza la differenza tra le due forme della fibra di viscosa3. A proposito dei nomi di marca bisogna mettere in evidenza che, anche se designano lo stesso tipo di fibra (la poliammide 6.6), la loro composizione è diversa: mentre Nylon è una fibra di poliammide 6.6 convenzionale, Ultron presenta delle caratteristiche antistatiche e Sylkharesse è un materiale prodotto in forma di microfibra. Infine, nella terminologia delle fibre tessili si riscontra una varietà di termini che designano lo stesso concetto, ma comunque non possono spesso essere usati nello stesso contesto. Nel caso di Nylon possiamo individuare le seguenti tipologie di termini: 1) denominazione chimica (poliesametilenadipamide), 2) nome generico (poliammide 6.6), 3) nome generico istituzionale (nei paesi dell'UE poliammide o nylon), 4) codice indicato sull'etichetta di composizione (PA), 5) codice usato tra i professionisti (PA 6.6), 6) nome di laboratorio (Fibre 6.6), 7) nome di marca (Nylon). Differenze culturali e variazioni diastratiche possono essere osservate anche nella terminologia dell'arrampicata sportiva, sport antico che tuttavia ha solo recentemente ottenuto un riconoscimento ufficiale, quale l'introduzione tra i nuovi sport olimpici di Tokyo 2020, e che giunge a fissare definitivamente per questa ragione i propri usi terminologici. Nello studio condotto per l'arrampicata sportiva sono stati estratti manualmente 96 termini a partire da un corpus eterogeno in lingua italiana composto da manuali di arrampicata (Commissione Nazionale Scuole di Alpinismo e Arrampicata Libera della Commissione Centrale per le pubblicazioni, 2009; Bressa, Denicu, Capretta, 2010; Ponta, 2016), documenti pubblicati da enti ufficiali quali C.A.I (Club Alpino Italiano) e F.A.S.I. (Federazione Arrampicata Sportiva Italiana), articoli di riviste specializzate come Montagna 360\u00B0, la rivista ufficiale del C.A.I. Trattandosi di una realtà internazionale in cui il confronto tra esperti in occasione di gare e manifestazioni è all'ordine del giorno, è interessante osservare come le differenze culturali tra i termini individuati siano poche e prevalentemente legate alle scale utilizzate per misurare i gradi di difficoltà e ai prodotti che possono essere messi in commercio in forme e dimensioni diverse da paese a paese, per esempio mentre in Italia la magnesite può essere acquistata in forma granulosa, non è stato trovato un equivalente nelle fonti canadesi, in cui tale prodotto sembra essere acquistato prevalentemente in formati differenti. Quanto alla variazione diastratica si può osservare come il termine arrampicata su massi non venga spesso utilizzato nelle fonti ufficiali che prediligono invece mantenere il termine internazionale boulder per agevolare la comunicazione tra professionisti provenienti da realtà culturali differenti. Inoltre, si può constatare che per questa terminologia la rappresentazione visiva degli oggetti e delle tecniche di arrampicata è di grande importanza per la comprensione dei concetti e deve perciò essere presa in considerazione in fase di stesura delle rispettive schede terminologiche.  4  La progettazione di ontologie in prodotti terminologici  La rappresentazione dei concetti e la schedatura dei termini richiede la comprensione dell'organizzazione concettuale del dominio oggetto di studio, attraverso la progettazione di ontologie formali. Diversi sono i 3  Si veda per es. Daniel Weidmann. 2010. Aide-m\u00E9moire textiles techniques. Dunod, Paris, p. 64. 106  \fprogrammi attualmente disponibili che permettono questa operazione, tra cui il Lexicon Model for Ontologies (Lemon) modello sviluppato dalla Ontology Lexicon Community il cui principale obiettivo è la presentazione di informazioni di natura linguistica all'interno di ontologie (McCrae et al. 2017); Prot\u00E9gé programma realizzato dallo Stanford Center for Biomedical Informatics Research per supportare il OWL 2 Web Ontology Language (Tudorache et al., 2013); Tedi programma proposto dal Condillac Research Group in Knowledge Engineering per la progettazione di ontoterminologie multilingui4. Ai fini del presente studio si è scelto di utilizzare il programma che meglio rispecchia la natura della disciplina terminologica, intesa come ontoterminologia: ovvero Tedi, programma il cui punto di partenza non è la dimensione linguistica del termine, come avviene per il Lexicon Model for Ontologies quanto la sua dimensione nozionale e concettuale. La decisione di prediligere Tedi rispetto a Prot\u00E9gé è invece giustificata dal fatto che nel primo la distinzione termine-concetto è più immediata in particolare in ottica di una rappresentazione terminologica multilingue. Un esempio tratto dall'ambito delle fibre tessili consentirà di illustrare una proposta di metodologia di lavoro da adottare per la realizzazione di schede terminologiche basate su un'ontologia formale. 4. 1. Tedi, una proposta per la realizzazione di un'ontologia con schede terminologiche multilingui. La terminologia delle fibre tessili: il caso del termine polyamide 6 L'editore di ontoterminologie Tedi si basa sulla distinzione della terminologia in due dimensioni: 1) la dimensione concettuale extralinguistica, condivisa dalle diverse comunità linguistiche 2) la dimensione linguistica, composta da diversi sistemi lessicali. Le due dimensioni sono strettamente legate, poiché i termini rappresentano i nomi dei concetti in lingua naturale (Roche, 2019: 5). La distinzione delle due dimensioni, permettendo una migliore comprensione del dominio, consente anche di effettuare delle ricerche non soltanto in base alle relazioni linguistiche tra i termini (iperonimia, sinonimia), ma anche in base alle relazioni logiche tra i concetti (concetto generico, concetto specifico) (Roche et al., 2014: 2). 4.2 Tedi e la dimensione concettuale dell'ontoterminologia Per ricostruire il sistema concettuale del dominio, Tedi mette a disposizione dell'utente il concept editor. Questo editor permette di definire i concetti in un linguaggio formale, che consiste nell'indicazione delle caratteristiche essenziali del concetto, dette anche 'differenze', in quanto rappresentano una delle possibilità di realizzazione di una certa caratteristica, predefinita secondo l'asse dell'analisi corrispondente. A titolo di esempio, l'asse dell'analisi 'origine della fibra' fornisce due caratteristiche essenziali 'naturale' e 'chimica'. Nel caso del concetto <poliammide 6> (Fig. 1), l'utente definisce l'origine della fibra scegliendo la caratteristica essenziale 'chimica'.  4  Si veda: http:\/\/new.condillac.org\/projects\/tedi 107  \fFigura 1: Il concetto <poliammide 6> definito nel concept editor di Tedi Una volta identificate le caratteristiche essenziali del concetto, il sistema genera in funzione di esse il nome del concetto, che permette di comprendere la natura degli oggetti che rientrano sotto il concetto stesso (es. il concetto <poliammide 6> viene denominato < Fibre textile chimique synth\u00E9tique traditionnelle liaisons amides r\u00E9currentes caprolactame>). In seguito, l'utente inserisce il concetto nella rete di relazioni tra i concetti del sistema, indicando il suo concetto generico (il concetto <poliammide>: <Fibre textile chimique synth\u00E9tique traditionnelle liaisons amides r\u00E9currentes>) e, eventualmente, i suoi concetti specifici (es. il concetto <Lilion>: <Fibre textile synth\u00E9tique caprolactame traditionnelle chimique liaisons amides r\u00E9currentes soci\u00E9té Snia Viscosa>). In questo modo, si ricostruisce l'organizzazione concettuale del dominio, che viene visualizzata nel concept editor di Tedi nell'angolo in alto a sinistra (vedi Fig. 1). Nel caso di alcune terminologie, quali quella dell'arrampicata sportiva, un ruolo importante nella comprensione del concetto è svolto dalla sua rappresentazione visiva. Per venire incontro a questa esigenza, Tedi è dotato della funzione link-illustration che consente di associare al concetto non solo immagini, ma anche video e collegamenti ipertestuali. 4.3 Creazione di una scheda terminologica in Tedi: la dimensione linguistica I termini vengono definiti nel term editor che propone degli editor indipendenti per una serie di lingue (es. francese, italiano, inglese). La definizione del termine si inserisce nella lingua naturale, con la possibilità di utilizzare un modello di definizione, elaborato da Tedi in base alla definizione formale del concetto. Oltre alla definizione del termine, Tedi consente di inserire altri dati relativi al termine, quali la fonte della definizione, l'informazione morfologica, lo status del termine ('preferenziale', 'alternativo', 'obsoleto'), il suo contesto di utilizzo, le note enciclopediche, le varianti ortografiche e le forme flesse. La sezione note enciclopediche è di particolare interesse per la terminologia non istituzionalizzata in quanto in questa sezione è possibile presentare sia delle note di carattere culturale sia delle indicazioni circa la variazione diastratica. Nel caso delle fibre tessili per esempio si può indicare se il termine oggetto di studio è connotato culturalmente e se esso si riferisca alla denominazione chimica, al nome di laboratorio o al suo codice. Inoltre, nel term editor appaiono in modo automatico anche gli equivalenti, gli iperonimi, gli iponimi e i sinonimi del termine, recensiti nella banca dati e associati a un concetto definito nel linguaggio formale a sua volta in relazione con il concetto designato dal termine in questione (Fig. 2).  108  \fFigura 2: Il termine polyamide 6 definito nel term editor di lingua francese di Tedi I termini equivalenti in varie lingue (per esempio polyamide 6 (fr), poliammide 6 (it), polyamide 6 (en)) sono associati a un unico concetto, definito in un linguaggio formale e quindi extralinguistico, il che permette di creare una banca dati terminologica multilingue, contenente una rappresentazione delle relazioni tra i concetti, che può essere visualizzata esportando i dati presenti nel concept editor in altri programmi, quali Cmap Tools, Prot\u00E9gé. Inoltre, l'esportazione delle informazioni sui termini in una lingua (nel nostro caso in francese) in formato HTML consente la progettazione di un dizionario elettronico, composto dalle schede terminologiche che forniscono diverse indicazioni tra cui la definizione del termine in lingua naturale, gli equivalenti in altre lingue e l'elenco delle caratteristiche essenziali del concetto (Fig. 3).  109  \fFigura 3: Il dizionario elettronico: la scheda terminologica di polyamide 6 (fr)  5  Conclusioni  Disponendo attualmente delle banche dati terminologiche di carattere istituzionale o nazionale, ci si trova di fronte a un nuovo bisogno, ossia quello di progettare un ricco repertorio terminologico digitale, facilmente utilizzabile nella comunicazione professionale multilingue. \u00C8 necessario tenere in considerazione il fatto che i bisogni dei professionisti di un settore differiscono spesso da quelli dei principali destinatari delle banche dati fornite dalle istituzioni (per esempio traduttori, legislatori, giuristi, redattori di testi). Come dimostrato dalla proposta illustrata nel presente articolo, l'utilizzo del programma Tedi, basato su un approccio ontoterminologico, permetterà quindi di progettare delle innovative banche dati terminologiche, attente alla dimensione socio-culturale della terminologia spontanea e in uso nella comunicazione professionale. Questo percorso, applicabile a più domini, permetterà quindi di conservare e ufficializzare un ricco patrimonio terminologico che, finora, non ha goduto dello stesso trattamento della terminologia istituzionalizzata."
	},
	{
		"id": 18,
		"title": "The digital Lexicon Translaticium Latinum: Theoretical and methodological issues",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Chiara Fedriani",
			"Irene De Felice",
			"William Michael Short"
		],
		"body": "Introduction  The Lexicon Translaticium Graecum et Latinum is a collaborative international project aimed at developing an on-line, extensible, open-access lexicon of metaphors in the ancient languages \u2013 beginning, in reverse chronological order, with Latin. Unlike existing electronic dictionaries for Latin, which simply re-create their printed counterparts in machine-readable form, the Lexicon Translaticium incorporates  112  \finsights from up-to-date theories of meaning and, in particular, the view developed in cognitive linguistics of metaphor as a key structuring device of language and thought. In capturing deeply entrenched and highly conventionalized metaphoric and metonymic patterns that organize meanings pervasively throughout this language and at different orders of linguistic encoding, the Lexicon Translaticium is meant as a psychologically realistic model of the conceptual system underpinning Latin. Built on top of the ontology provided by the Latin WordNet, the Lexicon will be interoperable with existing electronic corpora and thus capable of delivering rich figurative data for integration into natural language processing applications. The project, directed by William Short and Chiara Fedriani and staffed by an international five-member team, is currently on-going. However, its intellectual rationalization is well established and its technical design and implementation have progressed to the point where preliminary \u2018test' data is already publicly available, with about 20 metaphors presently annotated. We aim to launch the Lexicon officially in Spring of 2020 with a fuller dataset consisting of about 100 conceptual metaphors.  2  Theoretical background  Recognizing the all-pervasive character of certain metaphorical patterns in language, George Lakoff and Mark Johnson (1980) argued that the frequent clustering of metaphorical linguistic expressions around abstract, intellectual or otherwise intangible concepts in fact reflects the inherently metaphorical workings of cognition itself. People talk about most abstract concepts metaphorically, that is, because \u2013 it is claimed \u2013 they actually conceive of them metaphorically in terms of other (usually more concrete) concepts. On this view, metaphors are the projections of conceptual structure and content from one domain to another that occur as a way of mentally representing and reasoning about experiences not directly grounded in the physico-spatial world. Cognitive linguists argue, moreover, that it is the systematic nature of metaphors \u2013 in other words, that metaphors characterize regular mappings between organized domains of knowledge \u2013 that allows people to think and reason (and therefore also to speak) meaningfully about experiences that may be difficult to comprehend in and of themselves. In line with this theory, the \u2018entries' of our metaphor dictionary \u2013 unlike those of a traditional lexicon \u2013 will therefore consist of large-scale patterns of metaphorical understanding that link together concepts, rather than the semantic structures of words per se and so structure the meanings of words across the lexicon and at different levels of linguistic encoding. The kinds of metaphors that constitute the data of our Lexicon are those that are so conventionalized and so entrenched in the shared linguistic and cognitive habits of Latin speakers that they seem not to have been perceived as figurative at all \u2013 and indeed deliver Latin speakers' entirely regular, \u2018everyday' ways of conceptualizing certain experiences. Of course, as \u2018imaginative' or \u2018creative' (or more narrowly \u2018literary') metaphors most often derive in some way from more conventionalized metaphors, these kinds will also be represented. In this way, the Lexicon Translaticium Latinum will form a comprehensive catalogue of the range of metaphorical themes that structure meaning in Latin.  3  Technical implementation  Technically, the Lexicon will be realized as a computerized relational database, whose data model combines aspects of the architecture of the MetaNet Project of the International Computer Science Institute in Berkeley, California, with the WordNet framework. The Berkeley MetaNet is an electronic repository, viewable interactively on the Internet as a Wiki, that contains records for hundreds of attested conventional and imaginative metaphors in English, including time metaphors, mind metaphors, and emotion metaphors, as well as metaphors relating to government, disease, and violence. Most importantly for our purposes, the MetaNet provides a set of high-level ontologies for annotating and organizing figurative language data under the theory of conceptual metaphor in cognitive linguistics. In particular, the MetaNet provides a theoretically-grounded formal specification for encoding kinds of conceptual metaphors as well as kinds of relations between metaphors. For example, the \u2018type' of a metaphor can be tagged with values such as \u2018primary', \u2018composed', or \u2018entailed', which correspond to well defined theoretical categories. A primary metaphor is one that emerges directly from correlations in experience, as in more is up or purposes are destinations, while complex metaphors are those built up out of at  113  \fleast two more basic primary ones. Entailed metaphors are specialized submappings that can be inferred through experiential knowledge from a primary or complex metaphor, and which often form the basis of coherence between metaphors. Likewise, metaphors can be organized into hierarchies through simple relations of super- or subordination, or into more intricate systems according to different kinds of (again theoretically grounded) relationships, such as \u2018extension' (where one mapping takes advantage of conceptual material left unused by another), \u2018elaboration' (where one mapping embellishes another with additional conceptual material), \u2018combination', or \u2018questioning'. \u2018Reciprocality' is another common feature of metaphor systems and is available to capture \u2018orientational' metaphors that involve body-based experiential polarities such up vs down, left vs right, center vs periphery, in vs out, and so on. Whereas the MetaNet specification provides the foundation for encoding metaphors (as mappings between concepts) and their relations, the ontologies and data structures of the WordNet deliver the core repertoire of concepts that participate in these relations. As a semantic database, the WordNet represents lexical meaning in terms of synsets, which are uniquely identifiable \u2018definitions' for hypothetically all the senses capable of being expressed in a given language (thus organizing the lexicon into discrete \u2018synonym sets'). In other words, a WordNet synset \u2013 which pairs a unique identifier, consisting of a part-of-speech tag and a string of between six and eight integers, with a descriptive gloss and possibly higher-order \u2018domain'-level tags \u2013 should be seen as representing a distinct concept that may constitute the meaning of a word or words in the language under scrutiny. A WordNet for Latin was developed by Stefano Minozzi for the Fondazione Bruno Kessler's MultiWordNet Project (see Minozzi, 2008), consisting of about 9,000 lemmas tagged with synsets drawn from English and Italian. This is now being expanded through an international collaboration directed by the University of Exeter, to include over 70,000 words covering the archaic through classical periods of this language, as well as language-specific synsets defining meanings that are peculiar to Latin and not represented among the 100,000 or so synsets originally defined for English.  4  Innovations of design  Because the Latin WordNet (and indeed the WordNet specification generally) does not presently distinguish between literal and figurative sense attributions, it is being re-architected to accommodate the encoding of metonymic and metaphoric as well as literal senses of words. Annotation at the level of the lemma of specific sense (synset) assignments as being either literal, metonymic, or metaphorical is in fact one of the major new \u2018layers' at which figurative information is represented within the Lexicon. Consider, for example, the database entry for the word baculum, which can be accessible and marked-up by project participants through our bespoke on-line curation and annotation interface. In classical Latin, this word meant \u2018walking stick' and thus has been tagged with synset n#03585559, \u2018a stick carried in the hand for support in walking' as one of its literal senses (and indeed also its prototypical sense). Over time, however, and particularly in the early Christian period, the word came to be used more abstractly in the sense of any \u2018support' and in ecclesiastical texts regularly exhibits this meaning. This chronologically circumscribed figurative meaning of the word (n#04399253, \u2018something providing immaterial support or assistance to a person or cause or interest') is therefore annotated as a metaphorical sense. Differentiating between literal, metonymic, and metaphorical signification introduces an entirely new dimension of semantic structure into the WordNet framework, validated by modern linguistic theory. Along with annotations at the level of lexical semantic structure distinguishing between a word's literal, metonymic, and metaphorical senses (represented by synsets), conceptual metaphors themselves will be coded as a relationship between synsets, understood as discrete concepts. For example, the fear is a weapon metaphor, known in Latin in expressions such as the one in (1), is represented as a mapping between the synset that means \u2018fear' (n#05590260) and the one that means \u2018weapon' (n#03601056). In turn, the anxiety is a substance metaphor, again illustrated by the passage in (1), is structured as a mapping between the synsets meaning \u2018anxiety' (n#04491326) and \u2018substance' (n#00010572), respectively. 1. ipsius regis non tam subito pavore perculit pectus, quam anxiis inplevit curis (LIV. 1, 56) \u2018As for the  114  \fking himself, his heart was not so much struck with sudden terror as filled with anxious forebodings' Accordingly, any lemma annotated with one of these synsets as a literal, metonymic, or metaphorical sense is automatically linked (and accessible) via the metaphor by virtue of those sense attributions. In other words, as the theory posits, the metaphor operates as a supralexical structuring device of meaning in Latin: it helps determine, and motivate, the specific semantic developments of words and explains why the vocabulary of \u2018weapons' (not only the word corresponding to weapon but the whole conceptual domain relating to weapons and their use) can be used to talk about fear. Without the conceptual metaphor, there is no way to explain why weapon concepts are so regularly used to represent fear concepts and these would have to remain isolated, and \u2013 worse \u2013 arbitrary \u2013 facts of Latin's semantics. Crucially, moreover, the layer of more global conceptual-metaphorical information is tightly integrated with the more local layer of lexical-semantic information. In other words, the two layers of annotation \u2013 1) the conceptual metaphor itself, as a mapping between synsets (concepts) and 2) the attribution of synsets to lemmas as specifically metaphorical senses \u2013 work hand in hand. When a lemma is tagged as \u2018having' a synset as one of its literal, metonymic, or metaphorical sense, the annotator is also able to indicate the specific metaphor that underpins the given sense. This is to recognize within the relational structure of the database \u2013 and thus of the organization of Latin's semantic system \u2013 the theoretical claim that metaphors operate supra-lexically and provide motivating conceptual frameworks for the figurative extension of word meaning. In other words, rather than belonging to the semantic structure of any particular word (or determining, wholesale, the possible figurative meaning of a word), metaphors provide the specific pathways of figurative development that specific word senses may undergo in the course of a language's history. For instance, baculum's metaphorical sense of \u2018something providing immaterial support or aid', would be tagged with the metaphor an emotional support is a physical support (or even more generally, the emotional is the physical). This metaphor operates independently of this word's semantic structure \u2013 it very likely also determines the metaphorical usage of, e.g., fulcio \u2013 literally, \u2018to prop up' \u2013 in the sense of \u2018to uphold (emotionally)', as in CIC. Rab. 16, 43, veterem amicum suum (. . . ) labentem excepit, fulsit et sustinuit re, fortuna, fide (\u2018he supported his old friend \u2013 who was slipping downward \u2013 with his goods, his fortune and his confidence') \u2013 and so provides a powerful mechanism of bringing together otherwise disparate aspects of Latin's semantic system and discovering relationships that otherwise might remain hidden, obscured by outmoded principles of lexicographic organization. Finally, the ability to organize metaphors into highly articulated networks or groupings via different kinds of mapping relations recognizes that, at a higher level of conceptual structure, metaphors participate in systems. Besides the relations mentioned above, another \u2018organizing' mechanism of metaphors is that of the image schema. In conceptual metaphor theory, an image schema is 'a recurring dynamic pattern of our perceptual interactions and motor programs that gives coherence and structure to our experience' (Johnson 1987: xiv). Metaphorical mappings are usually encoded at a quite specific level of semantic granularity, and can be seen as detailed instantiations of more superordinate metaphors relying on general image schemas (e.g., force, container, object). In turn, mappings can give rise to further subordinate figurative patterns, with more semantic details filled in. These hierarchical relationships are all annotated within each metaphor record and give rise to a dense network of interconnected figurative meanings.  5  Annotation procedures and tagging scheme  Annotators first identify (a set of) documented metaphor(s) used by Latin writers to express an abstract concept, corresponding to a given synset, by analysing all occurrences of a relevant (set of) lemma(s) included in the synset within a selected corpus of literary texts. Encoding of metaphors, conceived as mappings between two synsets, is manually conducted through an annotation layer which has been designed expressly for this purpose. Very specifically, a metaphor is annotated according to its status (conventional, literary, or imaginative), type (primary, complex, orientational, ontological, one-shot image) and period of documentation. Moreover, it is labelled with a shorthand expression (e.g. \u2018ideas are food') and an adjectival descriptor (e.g. \u2018alimentary') following conventions in cognitive linguistics. The mapping itself is represented as a unidirectional relationship between two synsets, identified as the  115  \fsource and target. Additional information includes relationships between two or more metaphors at higher or lower levels of semantic specificity, namely through superordinate and subordinate mappings. Annotators can also catalogue relations between mappings (e.g. extension, elaboration, reciprocity, derivation, combination, and entailment) that may characterize complex metaphor systems. To exemplify this methodology, we present a case study of metaphor annotation pertaining to the semantic field of fear. A preliminary step identified synset n#05590260, \u2018an emotion experienced in anticipation of some specific pain or danger' (which pertains to five lemmas pointing to the concept of fear in Latin: formido, metus, pavor, terror, timor), as the primary target domain of the mapping. We scrutinized all occurrences of these lemmas (4,995) in the \u2018Antiquitas' section of the Bibliotheca Teubneriana Latina (3 BCE to 4 CE), distinguishing between literal (ex. 2) and figurative (ex. 3) usages. We counted but discarded literal usages, and further subclassified figurative usages into more fine-grained metaphorical subschemas. 2. prae metu ubi sim nescio (PLAUT. Cas. 413) \u2018I don't know where I am for fear' 3. huic aliquem in pectus iniciam metum (PLAUT. Cas. 589) \u2018I'll inject some fear into his heart' Through careful analysis of the literal wording of the contexts in which these words appear, we identified 23 metaphorical mappings which instantiate three main superordinate image schemas, namely force, container, and object. An example of a metaphor actualizing the force schema is fear is a military force (ex. 4); whereas fear is a substance that fills the experiencer (ex. 5) exemplifies the object schema. 4. tum vero ingens metus nostros invadit (SALL. Iug. 106, \u00A7 6) \u2018at last a great fear assailed the Romans' 5. vidi hominem XIIII Kal. Febr. plenum formidinis (CIC. Att. 9, 10) \u2018I saw him on January 17, thoroughly cowed [lit. filled up with]' Once the catalogue of subschemas appeared to cover all possible metaphorical expressions involving the relevant lexical field, a generalized annotation template was used to record details about each mapping. For example, the annotation record for fear is a military force is as follows: status <conventional> type <ontological> period Naev.+ <Pun. fr. 57, magnae metus tumultus pectora possidit> shorthand expression <fear is a military force> adjectival descriptor \u2018military' source <n#06088783 | \u2018an opposing military force'> target <n#05590260 | \u2018an emotion experienced in anticipation of some specific pain or danger'> derives from <fear is a hostile force>  And it is annotated as follows in the Lexicon interface (Figure 1):  Figure 1. The annotation layer of the fear is a military force metaphor.  116  \fFinally, the metaphor entry is enriched with illustrative examples drawn from literature (ex. 6). 6. olim iam adversus hunc metum emunivit animum (SEN. Con. 3, 17, 10) \u2018but he has long since fortified his mind against fear of that' According to this annotation procedure, users will be able to search the database using a variety of query types. For example, it will be possible to search for a single lemma (like amor), for a specific figurative source (like \u2018fire') or target domain (\u2018love'), for an image schema (counterforce), and thus to view all the metaphorical concepts built up from any of these elements. This will make it straightforward to discover certain features of figurative structuration within Latin's semantic system, such as the set of source domains that characterize the understanding of a given concept (what cognitive linguists called the \u2018range of the target') or, conversely, the set of target domains that are structured by a concept (the \u2018scope of the source'). It could also help shed light on the ways in which presumably human-universal aspects of cognition (sensorimotor gestalts) provide the scaffolding for culture-specific conceptualizations. What is more, because the metaphorical information of the Lexicon Translaticium Latinum piggybacks on the ontology provided by the WordNet, users will automatically be able to take advantage of the rich lexical and semantic knowledge already present in this database, enabling highly complex figuratively-aware queries. The Lexicon therefore portends to have significant implications for corpus search, text-processing and other natural language understanding applications.  6  Conclusions  The theoretical and methodological underpinnings of this project, along with the practical annotation procedure it has implemented, suggest that the Lexicon Translaticium Latinum could contribute significantly not only to cognitive and semantic approaches and to metaphor theory, but also to linguistic, literary, and cultural research in Classical Studies, especially as part of this field's wider ecosystem of natural language understanding applications. Indeed, we hope to position the Lexicon not merely as a repository of figurative usages in Latin, but as an interface to the system of knowledge itself that Latin speakers relied upon in thinking and speaking in diverse contexts of symbolic expression, and thus as a resource for better understanding how members of Roman society \u2018made sense' in, and of, their world. "
	},
	{
		"id": 19,
		"title": "Selling autograph manuscripts in 19th c. Paris: Digitising the Revue des Autographes",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Simon Gabay",
			"Lucie Rondeau du Noyer",
			"Mohamed Khemakhem"
		],
		"body": "Introduction  The number of projects dealing with French related objects circulating on the private market is increasing: research is currently being carried out in art history (Saint-Raymond, 2018), book history (Montoya, 2018), medieval manuscripts (Wijsman, 2017). . . Following recent trends, the latter project, that is the most relevant to our own work, is now sharing its data with other teams (Burrows et al., 2019) in the USA1 and in the UK 2 to trace the history of manuscripts over time and places, across national and linguistic borders. Unfortunately, no similar survey has been conducted yet on modern French autographs. If Renaissance manuscripts and their history are better known thanks to the Biblissima project (Turcan-Verkerk and Bertrand, 2014), no systematic work has been carried out on 17th, 18th and 19th c. materials. However, sale catalogues are recognised as being useful, since they are, for instance, regularly used as sources for critical editions (S\u00E9vigné, 1978, p. 158) (Voltaire, 1960, p. 18) (Lamartine, 2001, p. 348). Such a hole in our knowledge is due to the fact that it remains extremely tedious to extract information, because this task is either performed manually or with imperfect digital solutions (Cuadra and Michels, 2013; Barman, 2019). In the present paper, we therefore want to propose an (almost) automated workflow for the retroconversion of catalogues to transform images into structured information and create a database of sold items.  2  The corpus  2.1  The manuscript market  Since the beginning 19th c., rich collectors have been selling manuscripts on the private market (Bodin, 2000). Archives and libraries still keep sales catalogues that have been published on a regular basis (fixed price catalogues) or for special sales (auction catalogues) by dealers. Most of the manuscripts sold in these catalogues are modern and contemporary autograph manuscripts. The information contained in these catalogues is crucial for at least four different reasons. 1https:\/\/sdbm.library.upenn.edu 2http:\/\/mappingmanuscriptmigrations.org  119  \f\u2022 It helps assessing the authenticity of autographs: it is unlikely that a document sold repeatedly on the private market, and therefore authenticated each time by an expert, is a forgery. \u2022 It documents the reception of authors, via the history of collections (i.e. who collected what?) and prices (i.e. who costs how much?). \u2022 It informs us on the distance between what has been sold and what is available in libraries (i.e. are there autographs we do not know about?). \u2022 It provides us with images of documents which are still in private hands, because catalogues sometimes offer either facsimiles or pictures of autographs sold. For a first test phase, we have concentrated our efforts on the Revue des Autographes, a journal published since 1860's in Paris by Gabriel Charavay. 2.2  The RDA collection  In the second half of the 19th c., the autograph market is mature and the first generation of dealers begins to retire. In 1865, Gabriel Charavay (1818-1879) cease the opportunity of Auguste Laverdet's (1809-1867) retirement to take over his business, following the example of his elder brother Jacques (1809-1867), who opened his own shop in 1830. At that moment, Gabriel abandons his role of editor for L'Amateur d'autographes, a journal about the autograph market in Paris created in 1862, which keeps being published by his brother Jacques. Realising the importance of a publication attached to his activities, Gabriel creates another journal one year after his installation, in 1866: the Revue des autographes, des curiosit\u00E9s de l'histoire et de la biographie (RDA). Two journals for such a small market is however too much: in December 1868, after eight months of interruption, the price of the publication is divided by two and part of the content consists now of a list based on the autographs for sale in Gabriel's stock. Over time, the proportion of articles keeps diminishing and the RDA becomes first a hybrid publication mixing news and items to be sold, and eventually a fixed-price catalogue with the name of a journal published (almost) monthly until 1936. In the meantime, Gabriel's shop is taken over by Gabriel's son Eug\u00E8ne (1879-1892), and then by Eug\u00E8ne's widow (1892-1918) and by Eug\u00E8ne's daughter (1918-1936). The transformation of an hybrid journal into a disguised fixed-price catalogue under a journal's name is confirmed by a modification of the format: Eug\u00E8ne Charavay opts for a two-columns layout and a smaller font (cf. figure 1), harder to read but easier to browse for readers, who are now buyers, looking for the autograph of their dreams.  Figure 1: One-column layout (1873) vs two-columns layout (1893).  120  \f3 3.1  Encoding Entries  It is the images of these catalogues that we want to transform into minable data. Each of them generally contains a minimum of c. 200 entries, all of them being extremely dense in information and always following the same structure:  Figure 2: RDA, n\u00B067 (March 1881), lot N\u00B055. We can clearly see the lot number (in brown), the name of the author (in red), a short biography (in orange), the material description of the autograph (in blue), the price (in pruple) and an additional description (in green). To render the structure of the document, we propose the following encoding in XML-TEI: Such an encoding allows a simple disambiguation of the entry and increases the accuracy of the search. In our example, we have the names of three major 17th c. French writers: the novelist Madeleine de Scud\u00E9ry (1607-1701), the bishop Daniel Huet (1630-1721), and the satirist and poet Nicolas BoileauDespr\u00E9aux (1636-1711). The three names are enclosed in three different tags (name, desc and note) reflecting their status in the document (author, addressee, mention): we can therefore easily narrow down our query to a name depending on its role. 3.2  Workflow  The presented encoding can be compiled semi-automatically through a simple three steps workflow:  Figure 3: Workflow. The scan is OCRised with Transkribus (Kahle et al., 2017), for which a substantive model of 125,000 words has been created (CER of 0.59%). The pdf with a text layer is then processed with GROBIDDictionaries (Khemakhem et al., 2018b), a tool relying on text and layout features (cf. Figure 4) to perform a supervised classification of the parsed text and generate a TEI compliant encoding where the various segmentation levels are associated with an appropriate XML tessellation (Khemakhem et al., 2017). Preliminary designed for the retroconversion of dictionaries (Bohbot et al., 2018), it is also used to parse large bibliographical collections (Lindemann et al., 2018) or address directories (Khemakhem et al., 2018a).  121  \fFigure 4: Features. GROBID-dictionaries provides an answer to important issues left open by previous attempts, that do not produce standardised data (e.g. in XML-TEI), are not open source (Cuadra and Michels, 2013) or do not offer fine-grained encoding (Barman, 2019). On top of this, GROBID is a free, language agnostic, easily trainable solution compatible with other sub-projects of the GROBID galaxy, which leaves the door open to further analysis with complementary tools, such as GROBID-NERD (Named-Entity Recognition and Disambiguation). After preliminary tests ensuring the compatibility of GROBID-Dictionaries with sale catalogues (Khemakhem et al., 2018c) models have been created for the RDA and many other catalogues (Rondeau du Noyer et al., 2019) with a new bigram template for the GROBID-Dictionaries models (Rondeau Du Noyer et al., 2019) to reinforce the parsing of the structure of each entry. With GROBID-Dictionaries, the document is annotated using a cascading approach: several Conditional Random Fields (CRF) models are applied one after the other, each of them corresponding to a granularity level in the final XML hierarchy: Levels  Tag(s)  Task  1 2 3 4 5  body entry num, form and sense name and desc subsense and note  separates the content from running titles, page numbers, . . . separates the entries in the <body> separates the lot n\u00B0, information on the author and the MS in <entry> separates the name of the author and its biography in <form> separates the MS description and the additional note in <sense>  Table 1: GROBID-Dictionaries Segmentation levels The GROBID-Dictionaries output for our example is therefore the following: GROBID-Dictionaries being developed for lexicographic purposes, its results are encoded in a TEI compliant output, but with tags reflecting the content of dictionaries rather than catalogues. Therefore, we automatically convert the output into a second TEI document whose tags are dedicated for the described catalogue elements. The consistency of the transformation output is controlled with a specific schema, prior to its final publication via an XML database.  4  Future work  As for future work, four tasks will be undertaken. First, we will move towards a fully open source workflow and therefore abandon Transkribus for Kraken (Kiessling, 2019). Second, we will increase the size of our database by retroconverting the entire RDA collection (c. 500 catalogues), but also another important series of catalogues: the Lettres autographes et documents published by the other branch of the Charavay family up to the First World War (c. 500 catalogues). Third, we will improve our modeling and increase the granularity of our data collection to capture more informations regarding the document (size, format, length) and named entities (people, places). Fourth, we will use this additional information to reconcile entries and share our work with similar projects via an RDF export.  122  \fIdeally, we should eventually be able to go from the TEI digital edition of sales catalogues to a semantic dataset, described using controlled vocabularies where authors, places and manuscripts would be referred to using unique identifiers (ISNI, ISMI. . . ). It would allow federated search with other databases of sold manuscripts, but also with catalogues of libraries in France and abroad. "
	},
	{
		"id": 20,
		"title": "Enriching a Multilingual Terminology Exploiting Parallel Texts: an Experiment on the Italian Translation of the Babylonian Talmud",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Angelo Mario Del Grosso",
			"Emiliano Giovannetti",
			"Simone Marchi"
		],
		"body": "Introduction  Translation is the only way of making a text accessible to people that do not understand the language the original text is written in. Translation, in other words, allows to build bridges between peoples and cultures. It is no coincidence that it has been through a translation, contained in the well-known Rosetta Stone, that Egyptian hieroglyphs could be deciphered. The work we here describe is based upon a similar principle: how to exploit the translation in a \"known\" language of a text written in an \"unknown\" language to derive some linguistic information from the latter. In our case, the \"known\" language is a language for which tools and resources are available to automatically extract information from a text written in that language. Viceversa, the \"unknown\" language is the one that poses analytical problems, as it typically happens in projects involving ancient texts and languages. In particular, as detailed in the following section, we wanted to experiment a way of supporting the construction of a multilingual terminology by exploiting an existing translation. The use of parallel texts in support to lexicon construction is a field known as bilingual lexicon extraction, and it has a wide scientific literature (see for example (Fung, 1998), (Tufi\u015F et al., 2004), (Gutierrez-Vasques, 2015)). From a more applicative point of view, tools and software libraries have been implemented to assist developers in implementing the word-by-word text alignment necessary to process parallel texts. Giza++1 and the Berkeley aligner2, for example, have been largely adopted for these tasks. More in general, and in the context of Digital Humanities, the idea of exploiting parallel texts has been adopted in a number of initiatives, among which we point out the Perseus project, where the project team, together with the Von Humboldt professorship G. Crane within the Global Philology 1 http:\/\/www.statmt.org\/moses\/giza\/GIZA++.html 2 https:\/\/code.google.com\/archive\/p\/berkeleyaligner\/  125  \fproject, analyzed and implemented a collection of technologies and tools to envisage the \"complexities of working with a historical record that contains far more languages than any individual could study, much less master\" (Crane, Gregory et al., 2019).  2  Objectives and motivation  The experiment we here illustrate, still in progress, was conducted on the Babylonian Talmud Italian translation, in the context of the homonymous project3. The project, in addition to the development of the software Traduco used to support in the translation of the Talmud (Giovannetti et al., 2016), envisages the construction of a multilingual (Hebrew-Aramaic-Italian) terminological resource to support a number of activities, such as boosting the Translation Memory System with terminological information and creating an ontology of the talmudic domain. As described in Section 3, the Italian portion of the resource was built with the aid of a terminology extractor exploiting linguistic analysis tools for Italian. However, no tool or linguistic resource was available to automatically process the three main ancient languages appearing in the Talmud, namely, mishnaic Hebrew, biblical Hebrew and babylonian Aramaic. To obviate to this issue, and to the di\uFB03culty of automatically detecting the source terms through standard extraction processes, we chose to exploit the data produced in the last seven years of project activities, i.e. the available translated tractates of the Talmud. The results of the experiment suggested more ways of exploiting the obtained list of term-pairs in addition to the enrichment of the terminological resource, for example, as it will be discussed in the final version of the paper, to help in the lemmatization of semitic languages.  3  Methodology  Basically, the proposed approach makes use of a word-by-word alignment technique applied to a text in translation. The overall extraction process, leading to the enrichment of the terminological resource, followed a four step approach: i) encoding of the parallel text in TEI, ii) extraction of the Italian terms using a customized term extractor, iii) application of a word-by-word alignment technique to the parallel textual segments of the Talmud, iv) manual revision of the obtained alignment for the detection of the Hebrew\/Aramaic terms corresponding to the Italian ones. 3.1 TEI encoding of the parallel text We have first modeled and encoded the available parallel text (i.e. the Talmud and its Italian translation) by means of the best practices dictated by the Text Encoding Initiative (TEI), whose schema is currently the de facto standard to encode text-bearing objects (TBO) within the most authoritative scholarly projects involving literary inquiries. Actually, the choice to adhere to TEI environment provides benefits both to scholars, o\uFB00ering a standard model for the digital representation of critical texts, and to technicians, concerning modularity, data management, and, in particular, independence related to specific development choices. We have adopted the hierarchical text-group technique in order to encode the basic textual segments in three di\uFB00erent modalities: 1) the original talmudic text; 2) the Italian translated text; 3) the literal Italian translated text. Moreover, the linkage among the di\uFB00erent textual fragments has been conducted by means of the linkgroup technique 4. Section 3.3 will illustrate the word-by-word alignment task that has been developed. 3.2 Extraction of the target terms As mentioned before, given the lack of NLP tools and resources for Ancient Hebrew and Aramaic we could carry out the automatic extraction of the terminology only on the italian translation of the Talmud. For this purpose we used T2K2 (Dell'Orletta et al., 2014), a platform for linguistic analysis available at the Institute of Computational Linguistics (ILC) of the Italian National Research Council (CNR). T2K2 includes a stochastic module for terminology extraction which appeared adequate for our experimental purposes. We applied the extractor to four of the already translated and revised tractates of the Talmud, 3https:\/\/www.talmud.it 4Module number 16 of the TEI guidelines - Groups of Links. https:\/\/www.tei-c.org\/release\/doc\/teip5-doc\/en\/html\/SA.html#SAPTLGen\/html\/SA.html#SAPTLG  126  \fnamely: Berakh\u00F2t, Rosh haShanà, Ta'anìt and Qiddushìn. The corpus made of textual (plain-text UTF-8) documents was analyzed with T2K2 and the obtained output was furtherly processed in order to remove erroneous terms deriving from Part-Of-Speech tagging errors, and to sort the extracted terms by means of the TF-IDF (Term Frequency-Inverse Document Frequency) statistical measure. An important outcome of the TF-IDF is to permit to measure the relevance of each term for each tractate in which it appears: a high value of TF-IDF represents a high degree of relevance in the context of a specific tractate. The Table 1 shows some examples of term relevance by tractate. Berakh\u00F2t Terms Birk\u00E0t haMaz\u00F2n emissione di seme Shemà sogno gabinetto frutto della terra benedizione sul vino tipi di cibi pane dalla terra bisogni  Qiddushin tfidf  freq  Terms  0.0239 0.0209 0.0205 0.0166 0.0076 0.0072 0.0062 0.0060 0.0056 0.0047  120 105 206 167 38 36 31 30 28 47  documento perutà qiddushìn schiava terra di Israele divorzio rapporto sessuale padrone schiava ebrea trovatello  tfidf  freq  0.0159 0.0155 0.0142 0.0119 0.0107 0.0104 0.0101 0.0100 0.0088 0.0080  123 120 55 46 83 40 78 186 34 31  Table 1: The first ten Italian terms extracted from two of the four analyzed tractates and ordered by tf-idf. 3.3 Extraction of the source terms via alignment Word-by-word text alignment is a very useful technique to help understanding cross-lingual properties of parallel texts while processing only one half of the whole resource (Tiedemann, 2011). In order to add the Hebrew and Aramaic terms to the terminological resource we are building up from the Talmud, we set up the alignment process at token granularity. Specifically, we used an open source library realized by the Berkeley University (Liang et al., 2006) to develop a tool for the linking of Hebrew\/Aramaic textual segments with the corresponding Italian translations. Italian terms benedizione (2.1) Shemà (1.1) preghiera (2.2) pane (1.9) anno (2.14) mese (1.93) giorno (1.90) shof\u00E0r (0.87) obbligo (2.1) schiavo (0.82)  most likely Hebrew term \u202B( \u05B0\u05D1\u05BC \u05B8\u05E8\u05DB\u05B8 \u05D4\u202C0.41) \u202B( \u05B0\u05E7 \u05B4\u05E8\u05D9\u05D0\u05B7 \u05EA\u202C0.53) \u202B( \u05B0\u05EA\u05BC \u05B4\u05E4\u05DC\u05B8\u05BC \u05D4\u202C0.30) \u202B( \u05DC\u05B6 \u05D7\u05B6 \u05DD\u202C0.27) \u202B( \u05E9\u05B8\u05C1 \u05E0\u05B8\u05D4\u202C0.32) \u202B( \u05D7 \u05B6\u05B9\u05D3\u05E9\u05C1\u202C0.25) \u202B( \u05D9 \u05DD\u202C0.36) \u202B( \u05E9\u05C1 \u05E4\u05B8 \u05E8\u202C0.77) \u202B( \u05D9\u05B8\u05E6\u05B8 \u05D0\u202C0.34) \u202B( \u05E2\u05B6 \u05D1\u05B6 \u05D3\u202C0.80)  other candidates Hebrew terms \u202B( \u05B0\u05DE\u05D1\u05B8 \u05B5\u05E8\u202C0.29), \u202B( \u05B0\u05DE\u05D1\u05B8 \u05B0\u05E8 \u05B4\u05DB\u05D9\u05DF\u202C0.09) \u202B( \u05B0\u05E9\u05C1\u05DE\u05B7 \u05E2\u202C0.44) \u202B( \u05B0\u05EA\u05BC \u05B4\u05E4\u05DC\u05B8\u05BC \u05EA\u202C0.13), \u202B( \u05B0\u05EA\u05BC \u05B4\u05E4\u05DC\u05B8\u05BC \u05D4\u202C0.15), \u202B( \u05B0\u05EA\u05BC \u05B4\u05E4\u05DC\u05B7\u05BC \u05EA\u202C0.16) \u202B( \u05D4\u05B7 \u05E4\u05B7\u05BC \u05EA\u202C0.16), \u202B\u05D9\u05E4\u05EA\u05B8\u05BC \u05D0\u202C \u05B0 \u202B( \u05B4\u05E8\u202C0.16), \u202B( \u05E4\u05B7\u05BC \u05EA\u202C0.22) \u202B( \u05D4\u05B7 \u05E9\u05B8\u05BC\u05C1 \u05E0\u05B8\u05D4\u202C0.10), \u202B( \u05D4\u05B7 \u05E9\u05B8\u05BC\u05C1 \u05E0\u05B8\u05D4\u202C0.15), \u202B( \u05E9\u05B8\u05C1 \u05E0\u05B8\u05D4\u202C0.19) \u202B( \u05DC\u05B7 \u05D7 \u05B6\u05B9\u05D3\u05E9\u05C1\u202C0.21), \u202B( \u05D4\u05B7 \u05D7 \u05B6\u05B9\u05D3\u05E9\u05C1\u202C0.24) \u202B( \u05D1\u05B7\u05BC \u05D9\u05BC \u05DD\u202C0.09), \u202B( \u05D4\u05B7 \u05D9\u05BC \u05DD\u202C0.14), \u202B( \u05D9 \u05DE\u05B8 \u05D0\u202C0.19) \u202B( \u05B0\u05D1\u05BC\u05E9\u05C1 \u05E4\u05B8 \u05E8 \u05EA\u202C0.08) \u202B( \u05D7 \u05D1\u05B8 \u05D4\u202C0.09), \u202B( \u05D7 \u05D1\u05B8 \u05EA\u202C0.22) \u202B( \u05D5\u05B0 \u05E2\u05B6 \u05D1\u05B6 \u05D3\u202C0.09)  Table 2: Some examples of Italian-Hebrew\/Aramaic aligned terms. Italian terms with high entropy (such as \"preghiera\") have been aligned with multiple Hebrew\/Aramaic terms: the confidence that the term \"\u202B( \" \u05B0\u05EA\u05BC \u05B4\u05E4\u05DC\u05B8\u05BC \u05D4\u202Cthe one with the highest likelihood) is the actual translation of preghiera is low. To carry out the word-by-word alignment, the tool implements generative models that have been studied during the last decades by the IBM researchers and by the Machine Translation community  127  \f(Brown et al., 1993). In particular, it adopts the IBM Model-1 with the extension of the Hidden Markov Model paradigm (\u00D6stling and Tiedemann, 2016). The alignment task employed non-supervised machine learning algorithms adopting probabilistic models to calculate the likelihood estimation of aligning a term in a known language to a term in a foreign language. The expressiveness of these kinds of alignment models is particularly suitable in the literary domain, where translations tend to be more interpretative and less literal. Eventually, for each Italian term the computed probabilistic alignment model provided a list of Hebrew\/Aramaic candidate words. In Table 2, the numbers reported next to the Italian terms represents the entropy measure, which indicates the confidence of the translated word. The numbers next to the Hebrew\/Aramaic words indicate the likelihood that word is the translation of the corresponding Italian word. 3.4 Manual revision The aligner developed so far is based on statistical approaches which are, inherently, prone to errors. For this reason, the alignment environment requires a tool to validate and manually annotate the obtained outputs. We are thus developing a Web application able to manage and process the aligned text segments. As shown in 1, we have provided the proofreader with the possibility to annotate each word with a number of language and textual traits, namely lemma, Part-of-Speech, type of text, and language.  Figure 1: The annotation component of the proofreader.  The output of the aligner is formatted as a sequence of strings like 0-0 4-6 2-5 3-4 1-2 1-1 representing the word pairs that have been aligned. The order of the pairs is not significant, while the number within the pair represents the position of the word within the source-target strings; for example, in the two strings \"\u202B\u05DE\u05D5\u05BC\u05E8\u05D4 \u05D4\u05B8 \u05B4\u05E8\u05D0\u05E9\u05C1 \u05E0\u05B8\u05D4\u202C \u05B8 \u202B \"\u05E2\u05B7 \u05D3 \u05E1 \u05E3 \u05D4\u05B8 \u05D0\u05B7 \u05B0\u05E9\u05C1\u202Cand \"fino alla fine della prima veglia\" the pair 0-0 would indicate the word pair \"\u202B\u05E2\u05B7 \u05D3\u202C-fino\" (Hebrew is read from right to left). More details about the proofreader will be provided in the final version of the paper. Eventually, the revision process will allow to build a ground truth and\/or a gold training set and consequently put in place a complete validation process of the alignment results.  4  Preliminary results, discussion and next steps  As it was shown, a parallel text can be exploited fruitfully via text alignment techniques to help in the construction of a multilingual terminology. Our reference scenario was the Italian translation of the  128  \fBabylonian Talmud, carried out in the context of the homonymous project. At the current stage of the work, 219.000 tokens have been analyzed, distributed on 42.000 textual segments extracted from the four aforementioned tractates which have been translated so far. In addition to their use in populating the terminological resource, the obtained term-pairs may be also exploited in other ways. The first two applications we are going to investigate are: the boosting of text search, as recently experimented also in (Andonovski et al., 2019), and the support in the automatic processing of the source language. Concerning the next steps of this research, once a significant number of segments (and, thus, of the terms appearing in the segments) will have been revised by the expert of the Talmud, a formal evaluation of the accuracy of the approach will be carried out. Fig. 2 shows an example of revision of the alignment.  Figure 2: An example of use of the proofreader: the output of the automatic alignment (at the top) and the relative revision (at the bottom).  Besides, we intend to improve the performance of the approach by taking into account the variety of texts and languages that coexist inside the Talmud before the application of the aligner. As a matter of fact, the Babylonian Talmud is constituted by two (macro) texts, i.e. the Mishna and the Gemara, which, in turn, incorporate portions of other texts, such as, for example, quotes from the Tanakh (the Hebrew Bible). In the particular case of the Talmud, each text is written in a specific language: the Mishna in Mishnaic Hebrew, the Gemara in Babylonian Aramaic and the Tanakh in Biblical Hebrew. The idea is to automatically classify each segment of the Talmud on the basis of the text it belongs to and, after that,  129  \fto apply the aligner on each textual class composed of linguistically homogeneous segments. By doing this, we expect a better accuracy from the aligner and, ideally, no need from the revisor to indicate the language of each segment.  Acknowledgement This work was conducted in the context of the TALMUD project and the scientific cooperation between S.ca r.l. PTTB and ILC-CNR."
	},
	{
		"id": 21,
		"title": "Towards a lexical standard for the representation of etymological data",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Fahad Khan",
			"Jack Bowers"
		],
		"body": "Introduction  In this submission we will introduce a new standard, currently in an advanced stage of development1, for the modelling and publication of etymological data in computational lexical resources2. The standard in question, which we will refer to it as LMF-Ety, is the third part of a new multi-part revision of the Lexical Markup Framework (LMF), ISO 24613-4, originally published by the International Standards Organisation (ISO) as a single standard in 20083. We will motivate the need for LMF-Ety by describing some of the main challenges of modelling etymological data in computational lexical resources and showing how our new standard meets these challenges as well as how it differs from other previous models. Subsequently, we will describe the core concepts which we have so far established in our model and illustrate them through the use of an extended example taken from an etymological dictionary. Our intention is both to summarise the work we have carried out in the development of the LMF-Etymology standard as well as to showcase our broader approach to modelling etymologies. This approach entails the representation of etymologies as formal graphs describing simple narratives relating to a given lexicon phenomena; it is an approach that takes account of and consolidates previous attempts at modelling etymologies computationally but that also seeks to extend them in various different directions with a view to obtaining a more robust and expressive model. Additionally both authors are also involved in other initiatives for modelling etymologies in two other frameworks\/standards, the Text Encoding Initiative (TEI) (Bowers and Romary, 2016) and Linked Data (Khan, 2018). We will end the submission by 1At the time of writing, December 2019, the standard is being prepared for submission to an ISO ballot as a Draft International Standard (DIS). The classes and the approach which we present are now therefore fairly stable. 2The two authors are the joint project leaders of this standard. 3Note that the original version of LMF did not contain specific provision for modelling etymological\/diachronic information.  131  \fdescribing how LMF-Etymology may be rendered inter-operable with work being carried out in these two latter frameworks. This will also be relevant for understanding the practical details of how LMFEtymology can actually be used (SPOILER ALERT: Part 4 of the LMF standard is a serialisation of all the previous parts in TEI-XML).  2  Background  The importance of standards for the publication of scientific and scholarly datasets and resources for rendering them more findable, accessible, interoperable and reusable is by now well understood across the board. There have been a number of initiatives for promoting such standards and best practices in the field of language resources. Three of the most notable of these as applied to the case of lexical datasets are: the original version of the Lexical Markup Framework described below; the Dictionaries chapter of the Text Encoding Initiative (TEI) guidelines4; and finally the RDF-based Ontolex-Lemon guidelines (McCrae et al., 2017). These standards not only help to ensure a greater measure of interoperability between different computational lexicons, but they also facilitate the representation of lexical information in a way that makes it more amenable to advanced kinds of machine processing. Up until recently all three of these standards have dealt almost exclusively with synchronic lexical data5. This neglect of diachronic data is due, in part, to the awkwardness associated with the addition of extra temporal parameters to statements in data frameworks such as UML or RDF, and partly due to the (relatively) slow pace of development in the three standards overall \u2013 even if this would seem to constitute a missed opportunity, particularly in the case of etymological data since, at an abstract level, etymologies traditionally describe graph structures. They would therefore be ideally suited for representation in formalisms where this underlying structure can be rendered explicit, making such data easier to query and process. Moreover standards like LMF and especially the RDF-based Ontolex-Lemon would potentially make it easier to link together and query across different etymological datasets and to therefore create extended etymological networks. LMF-Ety is intended both for the creation of etymological datasets ex novo as well as for the conversion of legacy print resources as structured data. In this latter respect it should be noted that although our initial use cases have so far been largely concerned with the conversion of legacy dictionaries into structured resources, descriptions of etymological graph structures can be found in, and therefore potentially extracted from, numerous different kinds of texts. These include both scholarly works in linguistics, especially in the sub field of historical linguistics (articles, book chapters, monographs, etc), along with other genres of texts, literary, religious and philosophical6. It is clear then that the computational modelling of etymologies stands firmly at the intersection of computational linguistics, e-lexicography and the digital humanities. This inter-disciplinarity can also be appreciated in the fact that etymologies for languages with a sufficiently extensive written tradition will often contain attestations to coprora of historic texts; these texts can sometimes be reconstructions or have disputed interpretations as to particular word senses, (bringing to bear issues concerned with textual criticism and philology\/literary criticism more generally). 2.1  The Lexical Markup Framework  The original 2008 version of LMF, ISO 24613: 2008, was intended as a 'standardized framework for the construction of computational lexicons' (Francopoulo, 2013) an was conceived of as a common model both for lexicons for use in NLP applications as well as for computational versions of print or legacy dictionaries. Regarded as one of the most important standards in the field of lexical resources, LMF was enormously influential in the definition of the Ontolex-Lemon model and its predecessorlemon. A review of the Lexical Markup Framework undertaken by ISO in 2016 resulted in a decision to revise the standard, and to publish it as a multi-part standard (Romary et al., 2019) to render it more modular; the decision was also made to broaden the applicability of the new version of LMF to capture more kinds of lexical information. In consequence it was decided that one of the new parts of LMF should 4https:\/\/www.tei-c.org\/release\/doc\/tei-p5-doc\/en\/html\/DI.html 5The TEI guidelines do permit the representation of etymological data in a structured way but in a relatively shallow way. A recent proposal to allow for more salient kinds of etymological annotation can be found in (Bowers and Romary, 2016) 6See (Khan, 2018) which presents an example taken from Hobbes'Leviathan.  132  \fbe a specialised module dealing with diachronic lexical information. The different parts of the new LMF standard are: the Core Model (ISO 24613-1); the Machine Readable Dictionary Module (ISO 24613-2); the Diachrony-Etymology Module (ISO 24613-3); Serialisations in both TEI (ISO 24613-3) and LBX (ISO 24613-3). Two new modules dealing with syntax and semantics and morphology have also been proposed. 2.2  Related Work and Standards  A number of proposals have been made in the past to try and redress the lack of provision for encoding structured etymological data in lexical resources. These have included suggestions for extensions of both for TEI (Bowers and Romary, 2016) and LMF (Salmon-Alt, 2006), as well as proposals for new Ontolexlemon classes and properties(Chiarcos et al., 2016), (Khan, 2018). The standard described in the current work is influenced by the aforementioned works, and in particular it is informed by the approach in taken in (Khan, 2018) while abstracting away from specific details mentioned in that work and which pertain to the Resource Description Framework (RDF). Moreoever, it is the result of an attempt to converge towards a set of high level concepts, abstractions, that are sufficiently expressive to encode the main kinds of phenomenon and information which tend to be included in etymologies, as well as being simple enough to be usable by a wide community of potential users. The main concepts which we have determined upon are described in the next section. As we mentioned above the fourth part of the new standard is a TEI serialisation of all the other parts (due to be published at the same time as LMF-Etmyology). This should ensure that at a high level both TEI and LMF are interoperable and in particular the etymological apprpach taken by LMF-Etymology is compatible with TEI. It also means that LMF-Etymology should ultimately be accessible to digital humanists who are more used to working with TEI.  3  LMF Etymology  The definition of LMF Ety (ISO Standard 24613-3) is dependent on the two preceeding ISO standards in the new multi-part LMF standard, i.e., the Core Module, recently published by ISO, and the Machine Readable Dictionaries Module, due to be published in 2020 (Romary et al., 2019). Both of these standards contribute foundational concepts (for modelling lexicons) to LMF Ety such as Lexical Entry, Lemma, Form, and Sense: all of which keep their (fairly intuitive) meaning from the previous version of LMF and all of which share the meaning of similarly titled concepts in TEI and Ontolex-Lemon. On the basis of these foundational concepts then LMF Ety defines a number of additional classes which enable us to associate temporal\/historical information with lexical data encoded in LMF. The strategy we adopt is that suggested in (Khan et al., 2014) of modelling linguistic elements such as words, senses, forms, etc as perdurants, that is, as entities associated with a lifespan, which in the present case represents the interval of time in which they are considered to have been part of common usage within a given linguistic community7. This enables us to situate lexical entries etc in a temporal dimension and also to relate them together via diacrhonic linguistic processes. Our model then represents etymologies as simple narratives, or as rather simple narrative graphs, in which different linguistic phenomena (each of which can be potentially associated with a lifespan and situated on a timeline) are linked together using special etymological link elements, individuals of the class EtyLink, which can represent different kinds of historical linguistic processes such as inheritance or borrowing or semantic shift. The other new elements in the standard are the described below: \u2022 Etymon and Cognate: Two elements modelled as subtypes of Lexical Entry. What differentiates them from other lexical entries in a lexicon is their (specialised) role: they are used in describing the etymologies of other lexical entries: Etymons are lexical entries from which a given lexical entry is derived via some historical process; Cognates are lexical entries which share a common ancestor with a given a lexical entry; Additionally Cognate Set represents the reification of a set of cognates. \u2022 Etymology: An element that represents a single history of a lexical entry or other element. We associate Etymology individuals with an ordered series of EtyLink instances; this allows us to 7This approach makes it easier to represent such information in RDF.  133  \fFigure 1: Entry for forum.  Figure 2: Encoding of the entry for forum. define different etymologies featuring shared elements. In addition Etymology instances can be recursive, they can also be typed to define the changes undergone according to any number of linguistic processes. Although we have had to leave out a number of details in this brief summary, the classes which we have enumerated above are the fundamental ones for understanding and using the standard. We were able to establish these classes over the course of numerous iterative design cycles during which draft proposals were reviewed against a large and diverse number of use cases: evaluating them on the basis of their salience, expressivity, and understandability. In Figure 1 we present the entry for the Latin word forum from De Vaan's Etymological dictionary of Latin and the other Italic languages (De Vaan, 2018), and in Figure 2 a partial encoding of this entry using LMF. Here we have focused chiefly on the information in the written entry which concerns etymons and cognates8. Note the relationship between the lexical entry and its two etymons (both of which have been categorized as reconstructed lexemes). We have also added two Cognate Set elements which, although we haven't shown it in the diagram, can be linked to their associated etymons. Note that these elements are linked to the LMF lexical entry for forum via an Etymology element which is in reality a container for an ordered set of EtyLink elements. It is important also to note that not all of the information in an etymology can be easily represented in a graph like structure and this we can instead represent in additional textual elements. 8There exists provision in LMF-Ety for representing information concerning attestations, references to secondary literature and for adding textual information as notes attached to entries or etymologies although we haven't presented it here. We also haven't added explicit temporal important to this example either. Full details will be available in the final version of the standard.  134  \f4 Acknowledgements The first author was supported by the EU H2020 programme under grant agreements 731015 (ELEXIS European Lexical Infrastructure). "
	},
	{
		"id": 22,
		"title": "Workflows, digital data management and curation in the RETOPEA project",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Ilenia Eleonor Laudito"
		],
		"body": "Description and aim of the RETOPEA project  Funded by the European Commission under the program Horizon 2020, RETOPEA (REligious TOleration and PEAce) aims at creating a modern understanding of religious conflicts and peace-making history among youngsters and students throughout Europe. The intention is to teach in a comprehensible and appealing way, complex aspects of the past and present society. The project's target group are students between 12 and 18 years old, attending schools as well as non-academic institutions in European countries partnered with the project (which are Spain, UK, France, Belgium, Germany, Finland, Estonia and Macedonia). Characteristic to this project is its mixture of a historical corpus of peace treaties and agreements \u2013 spanning from settlements prior to the anno domini to the most recent Charter of Fundamental Rights of the European Union \u2013 and contemporary political discourses, popular culture among teenagers, new spiritual initiatives and heritage. The materials selected and processed by the historical research groups (called 'clippings'1) will be disclosed on the RETOPEA official website and will serve as primary resources and background information for the creation of short documentary films about the different aspects of tolerance and religious coexistence 1 A clipping is a piece of information with a length of ca. 200 \u2013 500 words, about a specific subject, possibly containing different media types and formats.  136  \fby the students participating to the project. These short films (called 'docutubes') will be published on the project's online platform and can be used in the future for further teachings. It is essential to the project's aim and purpose that all research data is correctly translated in the seven languages corresponding to the above-mentioned partnered European countries. This fundamental requirement assures that all students, independently from their knowledge of the topics presented and their level of understanding of the English language, are able to comprehend solidly the informations provided by the researchers.  2  Technical environment, workflow and data management  The Data Management Plan guarantees the storage sustainability and the long-term preservation of all relevant research data produced and processed by the historical research groups. The collected data will be publicly available via a Virtual Research Environment (VRE). Additionally, the designed workflow establishes a proper coordination between the historical research groups and the technical requirements of the project. 2.1  Technical enviroment  The technical specifications for the storage, preservation and visual presentation of the collected data consists of three main components: a collective access database (with an API to link material from other databases and platforms), a digital repository (TENEO) and a publishing tool (Omeka) that also serves as the project's official website. Crucial for the publication platform's selection was a user-friendly environment for students, teachers and researchers. Omeka is a web publication platform for sharing digital collections and creating media-rich online exhibits (Omeka S User Manual, 2019). This tool is mainly used by universities, archives, museums and galleries and fits the project's specific demands, due to the heterogeneity of its data. 2.2  Workflow  The workflow divides the clipping's production process in four main stages: creation of the source clippings in English, automatic translation of the clippings, review of the automatic translations and upload by the research data manager into the VRE.  Figure 1: The four stages of the workflow. All clippings require to be written in English to facilitate the translation of its content afterwards. The four stages of the is workflow. Additionally, the clippings need toFigure pass a2:readability test \u2013 which based on the English language \u2013 to match the student's reading and grade level. The selected material differs widely in subject matter, field and case study. According to the analysis made on the clippings reading ease and grade level, RETOPEA safeguards an appropriate level of understanding of the complex topics selected by the researchers. The pedagogical importance of this step underlines once again the project's priority concerning the didactical value of the data produced.  137  \fThe second stage concerns the automatic translation of the clippings, whereas the third stage concentrates on the review of the automatically translated content of the clippings. As explained, it is fundamental that all clippings are grammatically, linguistically and thematically comprehendible by youngsters. The current translation tools available are fallible, thus the need for a reviewing process by native language speakers. Nonetheless, the possibility to accelerate the workflow and lighten the workload of the translation process through automation is a major advantage. At the present state, the tools used for the automation were tested only on a small number of clippings, since the research groups must work on its production first. From the various web translators tested, the following tools will be used in the project: DeepL for Dutch, German, Spanish, French and Polish, Google Translate for Macedonian and Finnish and Tilde for Estonian. The most arduous languages to translate result to be Finnish and Macedonian, not only for the complexity of the language per se, but mostly for the lack of tools available. In the final step the research data manager will upload the complete dataset in the VRE and make small adjustments concerning the clippings' visual representation based on the resources and formats used (text, video and\/or audio), in concordance with the clipping's creators. Albeit this workflow is efficient and functional to RETOPEA's work organisation, it ought to be adapted to fit other requisites and necessities, if used in different projects. There are mainly two things that should be taken into consideration: first, the volume of the research data produced; second, the financial aspect of a reviewing process should not be underestimated and adequately evaluated. RETOPEA has a relatively small dataset of short text snippets (approximately 400 clippings) which can be uncomplicatedly handled and humanly reviewed. Due to this factor, the costs and extent of a reviewing process are limited. If a project has a broader dataset, then the human involvement and financial aspects should be carefully scrutinised, especially in the case of minor European languages. For example, as for the Finnish language review, it may be more convenient in terms of financial resources and time management to directly translate the data from the source language without going through an automatic translation. This notwithstanding, one major advantage of this workflow is that it can be easily managed, exploited and followed by researchers, independently from their computer skills and know-how. 2.3  Metadata structure  Every clipping necessarily includes a written part and may contain different media types in various formats beyond its written content (e.g. additional textual resources, images, video and\/or audio material, YouTube or other URLs, etc\u2026). To facilitate both the researchers' and the research data manager's work, the researchers will fill the clipping's metadata in two simple spreadsheets, containing the tags selected by the research data manager in the column header. The spreadsheets will be pasted and exported to CSV format and uploaded in the Omeka environment. Given the dataset's heterogeneity in terms of composition of resources, the development of a project-wide metadata scheme to normalise the metadata records was mandatory. The metadata standards and controlled vocabulary used in the project are the Dublin Core Metadata Standard (DC), the Bibliographic Ontology (BIBO), GeoNames (GN) and the Canadian Writing Research Collaboratory Ontology (CWRC). Omeka provides an automapping module that maps the metadata terms of the spreadsheet's header to the imported vocabularies (Omeka S User Manual, 2019).2 The module also allows to associate a media source (e.g. HTML, URL, YouTube link, etc\u2026) to a selected metadata tag. In RETOPEA's case, the clippings' contents are ingested as HTML-code through the Bibliographic Ontology Term <Content>. The tag will be hidden afterwards and will not be displayed as metadata, yet the content will be visible as media attachment.  2  For example, 'dcterms:title' is automatically mapped to the Dublin Core <Title> property. 138  \fFigure 2: Draft of a clipping as displayed on the RETOPEA website. Further, Omeka allows to aggregate the ingested data in user-made collections. The twelve collections used 3: The stages of the workflow.3 in this project aim at grouping theFigure clippings intofour abstract thematic classifications of project-relevant keynotes. The collections used in RETOPEA represent generic topical focuses (e.g. 'Religious practice', 'Gender and Sexuality', 'Propaganda and stereotyping', etc\u2026), to which clippings can belong independently of their subject. Subjects differ from the topical focuses, for the latter have a broader thematic range and may apply to an indefinite number of clippings, whereas the intent of the subject's list is to bundle a relative small number of clippings into strictly defined subjects (e.g. 'Peace of Augsburg', 'Edict of Nantes', 'YouTube channels', 'Political speeches', etc\u2026). The same clipping can appear in more than one collection, depending on how many relations the researcher associated to the clipping. This type of arrangement constructs an intricate entanglement between clippings in order to create vast links and relations between clippings that do not share the same subject matter. These relations and clusters belong to and are part of the metadata description, creating both a vertical and a horizontal hierarchical structure. The main purpose of this structure is to drive the website users and the teenagers using the provided clippings to produce their docutubes to discover as many clippings as possible, independently of the clipping's affiliation or subject matter.  139  \fBesides the clipping's title, description, contextual focus and content, the implemented BabelNet API will automatically extract and translate all other metadata tags and keywords through an HTTP interface that returns JSON (Navigli and Pozzetto, 2012). BabelNet not only functions as an online translator, but also recognises synonyms, word sense and (multilingual) semantic relatedness, shaping the possibility to generate linked data and semantic networks. The described metadata structure was designed specifically for RETOPEA and determined by the intensive  Figure 3: BabelNet translation of 'Good Friday'. collaboration, confrontation and discussion with the two historical research groups. Due to the project's distinctive didactical purpose and sundry data, it contains peculiar arrangements that are not always feasible or desirable in other DH-projects. This notwithstanding, this structure could be readily adopted and accordingly modified to fit other requirements. Considering RETOPEA's didactical and pedagogical purpose, the project's resources will be disclosed under the Creative Commons Licenses (i.e. CC BY-NC-SA). Most of the external materials used in the project can be likewise used and remixed by third parties. Additionally, future tasks will concern the implementation of external databases, like the IEGs 'Maps' and 'European History Online' (EGO) Databases, and the 'On site, in time' project. Further developments in RETOPEA will give a more precise evaluation about the translation tools used and the metadata structure. Moreover, the controlled vocabularies used leave open the possibility to connect, organise, retrieve and interlink the project's resources in Linked Open Data.  3  Innovation possibilities and DH importance  The methodology and the workflow described in this paper aims at giving a suggestion on how humanistic projects can organise and arrange the digital data produced and the immense possibilities that Natural Language Processing tools and approach may offer. In the last years the availability and growing amount of data extremely increased, also through the thriving of Digital Humanities related projects. The need for automatically translated documents, data and metadata will increase as more DH-projects arise worldwide. This need does not only apply to major and minor European languages, but also to Arabic and Asiatic languages as well as dialects.  140  \fAcknowledgements The research was supported by the Leibniz Institute for European History (IEG) and the Religious Toleration and Peace (RETOPEA) research project. This paper is based upon work supported and funded by the European Commission under the funding program Horizon 2020, Grant CULT-COOP-05-2017. Special thanks go to Marco B\u00FCchler, who provided insight and expertise and collaborated to the development of the workflow. Further gratitude goes to Bram De Ridder, who wrote the clipping shown in 'Figure 2', for granting and permitting the publication of his draft example."
	},
	{
		"id": 23,
		"title": "Il confronto con Wikipedia come occasione di valorizzazione professionale: il case study di Biblioteca digitale BEIC",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Lisa Longhi"
		],
		"body": "1 Cosmetica del nemico o delle diverse comunità L'attendibilità dei contenuti di Wikipedia continua ad essere oggetto di accesi dibattiti che vedono come protagonisti su un fronte i bibliotecari coinvolti nella redazione delle voci dell'enciclopedia, sull'altro i bibliotecari che ancora se ne tengono a distanza1. Esiste ancora un solco che si pensava colmato. Cosmetica del nemico è il titolo di un breve romanzo di Am\u00E9lie Nothomb, dove la parola \u2018cosmetica' va intesa nel suo significato semantico, ovvero \u00ABla morale suprema che determina il mondo\u00BB. Il problema è che ogni mondo ha la sua. La prima reazione, una decina di anni fa, all'idea di una collaborazione tra bibliotecari e wikipediani è stata infatti quella di collocare le due realtà agli antipodi: da un lato una professione con tradizioni secolari e normative condivise per il trattamento delle risorse bibliografiche; dall'altro una comunità giovane con pochi principi operativi ed etici - i cinque pilastri - che nascono dall'esigenza di un sapere libero2. Negli ultimi anni è apparso sempre più chiaro il fatto che questa impressione derivasse da un pregiudizio: sebbene infatti uno dei pilastri reciti che \u00ABWikipedia non ha regole fisse\u00BB, esistono molti punti di contatto fra le tradizioni bibliotecarie e le prassi di Wikipedia, laddove le intenzioni programmatiche di garantire l'accesso libero alla conoscenza si riflettono nell'utilizzo di strutture a schema fisso - come template e legami con URI - per organizzare le informazioni. Inutile sottolineare che si tratta di strumenti assai noti a chi si occupa di information literacy e di cataloghi bibliografici3.  2 Lo straniero o dei diversi bibliotecari Questo contributo prende tuttavia le mosse da un processo di crescita più lontano. Negli ultimi anni si è discusso a lungo sui percorsi formativi utili a diventare bibliotecari e sulla necessità di acquisire competenze che sviluppino capacità storico-umanistiche, ma anche una formazione tecnica e tecnologica. L'obiettivo è quello di aderire a un modello di bibliotecario ampio e complesso, per essere riconosciuti come bibliotecari in Italia e in Europa. A tal fine, si sta cercando di delineare un iter formativo unico e accreditato4.  1 Uno degli ultimi dibattiti è apparso a inizio 2019 sul forum di discussione Humanist, poi diffuso dalla lista AIB-CUR: i messaggi riguardanti Wikipedia sono raccolti nel thread the question on Wikipedia, <https:\/\/dhhumanist.org\/volume\/32\/>. 2  Cinque pilastri, <https:\/\/it.wikipedia.org\/wiki\/Wikipedia:Cinque_pilastri>. Esiste una nutrita bibliografia che attesta il dialogo e i risultati di una virtuosa collaborazione realizzata e possibile, dimostrando vicinanza sul piano operativo ma anche affinità metodologica. Questi argomenti sono raccolti per esempio in: L. Catalani-P. Feliciati, Wikipedia, le biblioteche e gli archivi \/ Wikipedia, Libraries and Archives, \u00ABJLIS.it\u00BB, 9, 3 (September 2018), p. I-III, <https:\/\/www.jlis.it\/issue\/view\/789>.  3  4  L'emanazione della l. 14\/1\/2013, n. 45 ha sancito in Italia l'obbligo di rintracciare dei criteri univoci, relativi alle conoscenze e alle competenze, che ogni professionista intellettuale. Per una disamina della legge e dei suoi effetti in ambito bibliotecario, si veda: R. De Magistris, Il riconoscimento delle professioni non regolate e la legge n. 4 del 14 gennaio 2013, \u00ABAIB studi\u00BB, 53, 3 (2013), p. 239-260, <http:\/\/aibstudi.aib.it\/article\/view\/9074>. 142  \fTuttavia, benché esistano pubblicazioni che descrivono quali siano le tappe di questo percorso5, benché l'Associazione Italiana Biblioteche si sia fatta carico di rilasciare l'attestato di qualità dei servizi presso il Ministero dello sviluppo economico6, benché l'ICCU abbia intensificato il piano di formazione e aggiornamento professionale7, ci troviamo ancora di fronte a numerosi singoli bibliotecari che mettono a frutto le loro diverse competenze, risultato di percorsi di studio eterogenei e non sempre pensati nell'ottica della professione bibliotecaria. A questo proposito, è significativo il caso dei \u2018bibliotecari-studiosi', vale a dire di quei, non pochi, bibliotecari provenienti dal campo della ricerca, che prestano le loro competenze di paleografi, musicologi, storici dell'arte, al lavoro di catalogatore, facendosi portavoce all'interno della biblioteca delle reali necessità degli utenti più specialisti. L'utente-studioso può trovare in questo tipo di bibliotecari delle alleanze: il catalogatore esperto infatti, seppur \u2018straniero di nascita', a differenza del protagonista di Camus, che non si integra nella sua stessa vita, sposa la missione del bibliotecario come facilitatore dell'apprendimento del lettore. Da ricercatore, conosce i manoscritti, i libri antichi o le partiture musicali e sa quali siano i pochi dati certi che non devono cambiare mai in un catalogo (titoli uniformi, voci di autorità normalizzate), ma è anche in grado di fornire quei dati sensibili che spesso si trovano fuori dai campi di ricerca, rispondendo così a un bisogno informativo specifico8.  3 I custodi del libro o della collaborazione con altre istituzioni I custodi del libro di Geraldine Brooks è un romanzo che racconta la storia di un libro - un codice ebraico del XV secolo - ma è anche la storia delle persone che hanno confezionato il libro nelle sue parti e soprattutto delle persone, i bibliotecari (cui il libro è dedicato), che l'hanno tutelato e tramandato nei secoli. E' una storia che invita alla collaborazione tra bibliotecari e tra biblioteche, come stimolo alla crescita e arricchimento, nella salvaguardia della cultura. Aprirsi al confronto in un panorama di cooperazione nazionale e internazionale è infatti una grande occasione per chi lavora nel settore dei beni librari, poiché reca giovamento non solo ai singoli bibliotecari, attraverso il confronto coi colleghi, ma anche alle istituzioni di appartenenza, che donano maggiore visibilità e fruibilità al loro patrimonio. Per le collezioni digitali poi il confronto e la cooperazione diventano necessari, poiché il rischio di confondersi nella rete e di rendere i dati poco reperibili diviene esponenziale. I progetti di cooperazione sono molteplici: possiamo citare Internet Culturale9, Europeana10, WorldCat11, il CERL12. Far parte di queste reti offre ai bibliotecari il vantaggio di favorire la circolazione delle informazioni, la visibilità dei dati e l'opportunità del confronto e della collaborazione con gli altri professionisti della rete. Tuttavia per favorire dialogo e cooperazione è necessario parlare la stessa lingua, ovvero adottare preventivamente standard aggiornati che permettano l'interoperabilità e il riuso di metadati affidabili in un'ottica internazionale.  4 Il filo del rasoio o del confronto con il mondo Wiki 'Camminare sul filo del rasoio è difficile', dice un passo delle Upanishad: in quel contesto - come nel romanzo di Somerset Maugham, che ne trae titolo e ispirazione - ci si riferisce allo stare in equilibrio tra senso pratico e spiritualità. Tra due piani diversi, tra due realtà. Se il bibliotecario-studioso già deve esercitare il proprio equilibrio, cimentandosi, da ricercatore, negli scenari più \u2018tradizionali' della creazione e dello scambio di 5 Associazione italiana biblioteche, Il portfolio delle competenze: un nuovo strumento per il professionista dell'informazione, a cura dell'Osservatorio Formazione (coordinatore Patrizia L\u00F9peri); contributi di Manuela De Noia, Matilde Fontanin, Patrizia L\u00F9peri, Roma, Associazione italiana biblioteche, 2017. 6 Ministero dello sviluppo economico, Associazioni professionali che rilasciano l'attestato di qualità dei servizi. 2016,<http:\/\/www.sviluppoeconomico.gov.it\/index.php\/it\/cittadino-e-consumatori\/ professioni-non-organizzate\/associazioni-che-rilasciano-attestato-di-qualita>. 7 Il piano di formazione e aggiornamento professionale ha previsto, per il 2019, ben due cicli di corsi di aggiornamento organizzati dall'ICCU indirizzati al personale bibliotecario specializzato, <https:\/\/www.iccu.sbn.it\/it\/attivita-servizi\/ formazione-e-didattica\/Corsi-di-formazione-attivi\/>. 8 Chiara Cauzzi [et al.], Conoscersi per riconoscersi: la partecipazione come specchio del bibliotecario, \u00ABAIB studi\u00BB, 56, 2 (maggio\/agosto 2016), p. 219-239, <http:\/\/aibstudi.aib.it\/article\/view\/11462>. 9 Internet Culturale, <http:\/\/www.internetculturale.it\/>. 10 Europeana Collection, <https:\/\/www.europeana.eu\/portal\/it>. 11 What is WorldCat?, <http:\/\/www.worldcat.org\/whatis\/default.jsp>. 12 Consortium of European research libraries, <https:\/\/www.cerl.org\/>. Il Consorzio promuove l'attività di ricerca attraverso gruppi di lavoro internazionali come Heritage of the Printed Book Database (<https:\/\/www.cerl.org\/ resources\/hpb\/main>), CERL Thesaurus, <https:\/\/data.cerl.org\/thesaurus\/_search>), Material evidence in incunabula (<http:\/\/data.cerl.org\/mei\/_search>).  143  \fmetadati bibliografici, la difficoltà aumenta quando il bibliotecario si trova a lavorare in ambienti apparentemente estranei. I progetti GLAM (acronimo per Galleries, Libraries, Archives, Museums, analogo internazionale dell'italiano MAB) sono nati formalmente nel 2010, proprio con l'intenzione di promuovere la collaborazione tra istituzioni culturali e Wikipedia, integrando bibliotecari, archivisti e curatori museali tra i propri contributori, ma alcuni tra questi professionisti del settore culturale avevano già contribuito - individualmente, come utenti - ai contenuti dell'enciclopedia libera fin dalla sua nascita nel 200113. Da quindi una decina di anni il mondo GLAM collabora con le comunità wikipediane di tutto il mondo per l'arricchimento dell'enciclopedia libera sui temi del patrimonio culturale: collaborazioni che vanno dalla semplice 'donazione' di dati (scansioni, fotografie d'archivio, dati bibliografici) a partnership più strutturate che prevedono il 'wikipediano in residenza', figura di mediazione che collabora a stretto contatto con l'istituzione culturale, formando il personale su come contribuire a Wikipedia e agli altri progetti Wiki14. Abbiamo detto che l'interazione tra le due comunità può essere utile alla crescita di entrambe, se la si sa osservare senza pregiudizi: Wikipedia è ormai una delle principali fonti di accesso all'informazione per molti utenti del web, ma l'enciclopedia ha bisogno di colmare le lacune nel settore dei beni culturali, proponendosi nel contempo di dare risalto al lavoro di archivisti e bibliotecari, altrimenti poco visibile in rete (con materiale digitale visibile solo dal sito dell'istituzione, ma non rintracciabile con ricerche sui motori di ricerca): in pratica sono i bibliotecari stessi, gli archivisti e gli operatori di musei a creare le voci relative ai propri patrimoni e ai relativi ambiti disciplinari, oltre a condividere le proprie immagini digitali con licenze libere e aperte (Creative Commons CC-BY e CC BY-SA) o nel pubblico dominio (CC0). La collaborazione è infatti finalizzata a condividere contenuti e risorse in un'ottica di promozione della conoscenza libera e le biblioteche digitali possono contribuire ulteriormente, fornendo ad esempio fonti bibliografiche verificabili alle voci dell'enciclopedia.  5 I doni della vita o dei beni comuni digitali I doni della vita è un romanzo di Ir\u00E8ne N\u00E9mirovsky e i doni cui fa riferimento l'autrice sono i solidi valori e le certezze di una famiglia che, in un momento di difficoltà materiale, si trasformano in forza. Fuori di metafora, occorre riconoscere l'opportunità di miglioramento offerta dalla contribuzione, bisogna avere la consapevolezza del fatto che aggiungere qualità alle voci create o arricchite, e contemporaneamente beneficiare di nuove conoscenze tecniche, è fonte di aggiornamento professionale e crescita personale. Una volta fugato il pregiudizio che porta ad arroccarsi nella contezza della propria professionalità e scegliere di non contribuire, può verificarsi il rischio, ugualmente insidioso, di contribuire, usando la rete come una cattedra. Essendo Wikipedia uno dei siti d'informazione più visitati al mondo ed essendo un progetto no profit aperto a tutti, ha inevitabilmente attirato a sé coloro che cercano visibilità sul web, trasformando la sua apertura in vulnerabilità. Per arginare il rischio dell'autopromozione da un lato i wikipediani si sono dotati di una burocrazia 'difensiva'15, dall'altro occorre che i professionisti dell'informazione - insegnanti, docenti universitari, bibliotecari, archivisti - si premurino di partecipare in un'ottica neutrale perché le informazioni di Wikipedia siano il più accurate possibile. \u00C8 auspicabile quindi una sempre maggiore contribuzione da parte degli addetti ai lavori: un bibliotecario, ancor più un un bibliotecario studioso, dovrebbe dare una mano a questo progetto di condivisione nella sua globalità, secondo il concetto di filiera del dato open, ben descritto da Andrea Zanni in un suo recente contributo16, in cui si dimostra quanto 'la partecipazione a un bene comune digitale diventa un mezzo per il bibliotecario di adempiere alla sua missione, andando a fornire le informazioni direttamente dove gli utenti della rete sono presenti, attraverso le competenze, le collezioni e i valori propri della professione bibliotecaria'. Del resto l'utilizzo di protocolli e formati aperti, di licenze e policy aperte garantisce l'interoperabilità necessaria fra macchine e macchine, ma anche fra umani e umani.  13  GLAM (cultura). In: Wikipedia: l'enciclopedia libera, <https:\/\/it.wikipedia.org\/wiki\/GLAM_(cultura)>; in particolare Progettto:GLAM\/biblioteche, <https:\/\/it.wikipedia.org\/wiki\/Progetto:GLAM\/Biblioteche>. 14 Progetto:GLAM\/Wikipediano in residenza, <https:\/\/it.wikipedia.org\/wiki\/Progetto:GLAM\/ Wikipediano_in_residenza>. 15 Wikipedia:Contenuti promozionali o celebrativi, https:\/\/it.wikipedia.org\/w\/index.php? title=Wikipedia:Contenuti_promozionali_o_celebrativi&oldid=88978752. 16 A. Zanni, Le biblioteche e la filiera dell'open, \u00ABJLIS.it\u00BB, 9, 3 (September 2018), p. 75-91, <https:\/\/www.jlis.it\/ 144 article\/view\/12486>.  \f6 Le affinità elettive o del metodo di lavoro Le affinità elettive di cui parliamo sono quelle tra Fondazione BEIC e Wikipedia. Anche Goethe aveva desunto il titolo del suo romanzo dall'affinità chimica, cioè la tendenza di alcuni elementi chimici a legarsi tra loro. Nel caso di BEIC e Wikipedia l'affinità nasce da una comunione di intenti: le due istituzioni hanno infatti l'obiettivo comune di promuovere e condividere contenuti e risorse. Dal 2008 la Fondazione BEIC17 sta realizzando una biblioteca digitale che raccoglie collezioni tematiche, selettive e multidisciplinari18. Per le digitalizzazioni si è affidata a studiosi e a istituzioni di prestigio che, attingendo a raccolte italiane e straniere, hanno selezionato gli esemplari in base a specifici criteri, legati alla rilevanza degli autori e delle opere e al carattere internazionale delle fonti. E' stata dedicata molta attenzione alla creazione di un catalogo amichevole che ponesse particolare cura nella presentazione dei dati e nella loro integrazione con altri cataloghi, così come nella rapida visualizzazione e nella facile manipolazione delle immagini19. In questo modo la Biblioteca Digitale BEIC si propone di rendere liberamente accessibile un vasto complesso di opere umanistiche e scientifiche, in un arco temporale che va dal medioevo all'età contemporanea20. Lo staff di Biblioteca Digitale BEIC, eterogeneo già al suo interno, ha una routine di lavoro che prevede il confronto quotidiano con cataloghi nazionali (SBN OPAC, Edit16) e internazionali (Karlsruhe, GW, ISTC), con database per il controllo delle voci di autorità (VIAF, CERL Thesaurus) e con strumenti enciclopedici (Dizionario biografico degli italiani, Wikipedia). Questa attività di consultazione ha portato alla consapevolezza di dover contribuire alle reti che si ritengono efficaci: la Biblioteca Digitale BEIC collabora quindi da tempo ad alcuni progetti, attraverso il riversamento - possibile grazie all'interoperabilità degli standard MARC - di registrazioni della biblioteca digitale in WorldCat, Europeana e ISTC. L'obiettivo è quello di valorizzare le proprie collezioni in un contesto internazionale e di permettere il massimo riutilizzo dei metadati, per questo rilasciati con licenza CC0 (pubblico dominio). Queste caratteristiche rendono la Biblioteca Digitale BEIC particolarmente adatta alla collaborazione con Wikimedia Italia. Dal 2014 è stato così avviato un progetto GLAM21, che, nell'ottica di una condivisione aperta dei contenuti e di diffusione della conoscenza libera, ha il fine di disseminare le risorse della Biblioteca Digitale all'interno di Wikipedia e dei progetti fratelli. Dall'inizio del GLAM a oggi, le attività svolte dalla BEIC insieme al Wikipediano in residenza - inizialmente Federico Leva, poi Marco Chemello - hanno spaziato dall'arricchimento delle voci dell'enciclopedia, al caricamento di immagini digitalizzate in Wikimedia Commons, alla collaborazione con i progetti di Wikisource22. Negli ultimi anni, tuttavia, è cresciuta la consapevolezza che uno in particolare fra i progetti sarebbe stato di grande interesse dal punto di vista bibliotecario: Wikidata23. Wikidata è un database libero che si basa su principi storici e fondamentali della teoria della catalogazione, come il controllo di autorità, e li affianca alla più recente ottica Linked Open Data (LOD). L'affinità di Wikidata con i modelli per la costruzione di registrazioni di autorità nei cataloghi è confermata dal fatto che le entità descritte in Wikidata sono arricchite dagli identificativi persistenti provenienti da dataset bibliografici autorevoli, come quello della Biblioth\u00E8que nationale de France o del CERL Thesaurus. Nel 2017, dunque, si è cominciato a procedere al riversamento in Wikidata dei metadati sottoposti a controllo di autorità della Biblioteca digitale BEIC, a partire dagli autori persona che ricorressero come intestazioni principali dei record bibliografici (circa 5000 nomi), seguiti poi dagli autori persona con 17 Biblioteca europea di informazione e cultura, Biblioteca digitale, <http:\/\/www.BEIC.it\/it\/ articoli\/biblioteca-digitale>. Per le tappe del progetto: La Biblioteca europea di Milano (BEIC): vicende e traguardi di un progetto, a cura di Antonio Padoa-Schioppa, Milano, Skira, 2014. 18 Biblioteca europea di informazione e cultura, Le collezioni, <http:\/\/www.BEIC.it\/it\/pagina\/ le-collezioni>. 19 Il protocollo catalografico BEIC ha come primo punto di riferimento le Regole italiane di catalogazione: REICAT, integrate con norme per il trattamento di risorse particolari (ad esempio, per la musica, ci si attiene a: Istituto centrale per il catalogo unico, Titolo uniforme musicale: norme per la redazione, Roma, ICCU, 2014, allo standard MARC21 e alle indicazioni di RDA. 20 Attualmente la Biblioteca Digitale BEIC conta circa 39.569 oggetti digitali per un totale di oltre 99.936 registrazioni, che spaziano per tipologia dagli incunaboli alle opere d'arte, dai libri antichi ai periodici, dai manoscritti alle risorse audio-video. 21 Progetto:GLAM\/BEIC, <https:\/\/it.wikipedia.org\/wiki\/Progetto:GLAM\/BEIC>. 22 Sono state caricate in Wikimedia Commons le immagini di migliaia di opere presenti nella Biblioteca Digitale BEIC in formato TIFF a 300 o 400 dpi; le immagini sono state inserite in migliaia di voci esistenti di Wikipedia in italiano e di altre 200 versioni linguistiche; sono stati aggiunti riferimenti bibliografici a oltre 1600 voci in italiano, aggiungendo i link agli oggetti digitali consultabili nella Biblioteca Digitale BEIC; sono state create oltre 650 nuove voci in italiano e inglese di autori; infine, sono stati creati in Wikisource gli indici di numerose opere in italiano, provenienti da Internet Archive o dalla stessa Biblioteca Digitale BEIC. Si veda: C. Consonni-F. Leva, Progetto GLAM\/BEIC, \u00ABBiblioteche Oggi\u00BB, 33 (marzo 2015), p. 47-50, <http:\/\/www.bibliotecheoggi.it\/rivista\/article\/view\/24\/265> ; F. Leva-M. Chemello, The effectiveness of a Wikimedian in permanent residence: the BEIC case study, \u00ABJLIS.it\u00BB, 9, 3 (September 2018), p. 141-147, <https:\/\/ www.jlis.it\/article\/view\/12481>. 23 <https:\/\/www.wikidata.org\/wiki\/Wikidata:Main_Page>  145  \fintestazione secondaria (quasi 15000 nomi)24. La fase preliminare al riversamento dei dati ha comportato la preparazione dei metadati di partenza: la Biblioteca Digitale BEIC utilizza lo standard MARC21, pertanto tutte le informazioni relative agli autori di tipo persona sono state rintracciate all'interno dei tag 100 (per le intestazioni principali) e 700 (per le intestazioni secondarie). Il fatto che il protocollo catalografico BEIC preveda in ogni caso l'inserimento delle date nel record di autorità (e non soltanto per disambiguare gli omonimi) ha permesso un'identificazione più certa nel corso del riversamento in Wikidata e ha assicurato la presenza di un dato stabile e di tipo enciclopedico in tutte le registrazioni. I metadati associati agli autori persona sono stati esportati dal catalogo, rielaborati e riversati in Mix'n'match25, un tool wiki che permette di confrontare i record del catalogo con gli elementi esistenti in Wikidata e abbinarli nel caso si riferiscano alle stesse entità. Gli elementi abbinati, cioè i casi in cui i nomi del catalogo BEIC corrispondevano a un elemento già esistente in Wikidata, sono stati arricchiti di una nuova proprietà che evidenziasse la corrispondenza con le singole entità del Catalogo BEIC (proprietà \u2018Descritto nella fonte'); per gli elementi unmatched, ovvero i nomi presenti nel catalogo BEIC che non avevano alcun abbinamento con entità di Wikidata (circa il 30% del totale), si è resa necessaria la creazione da zero di elementi Wikidata. In entrambi i casi, vista la mole di dati e la difficoltà di procedere manualmente alle modifiche e alle creazioni, si è deciso di utilizzare QuickStatements, un altro tool che permette di intervenire in Wikidata in maniera semiautomatica: partendo da un elenco di elementi Wikidata, questo strumento è in grado di inserire lo stesso tipo di informazione in tutti con un'unica operazione26. Le nuove entità sono state poi arricchite di nuove informazioni, come il genere o l'occupazione, attingendo le informazioni da fonti diverse. Nel caso dell'occupazione, il dato talvolta era già stato inserito nel record di autorità (per esempio, la qualifica di santo, papa, imperatore o elementi disambiguanti per le forme omonime)27, talaltra, il dato si è basato sull'analisi dei relator code associati ai tag 100 e 700 del record bibliografico28. Il risultato di questo lavoro vede per ora l'entità 'Biblioteca digitale BEIC' associata a quasi 15000 elementi Wikidata e fotografa lo stato attuale del progetto che, come si è detto, è ancora molto ampio. Una volta concluso il progetto sugli autori di tipo persona, l'obiettivo è quello di estendere la prassi descritta anche agli altri metadati sottoposti al controllo di autorità (autori ente, editori, luoghi e titoli): si sta già lavorando sulle forme normalizzate degli editori antichi e moderni \u2013 includendo anche quelli presenti nel catalogo dell'Archivio della Produzione Editoriale Lombarda, sempre allestito dalla BEIC \u2013 e sui titoli. Una volta raggiunto l'obiettivo di esportare i dati di autorità dal catalogo a Wikidata, la prospettiva sarà quella di integrare le informazioni presenti nelle entità di Wikidata all'interno dei record di autorità del catalogo, a partire dagli identificativi persistenti e dalle forme varianti del nome. Un'ulteriore prospettiva sarà quella di migliorare la struttura dei record di autorità del catalogo BEIC, creando per ciascuno di essi un identificativo persistente, che diventerà il riferimento stabile al di fuori del catalogo, in primo luogo negli elementi Wikidata, nel rispetto di un'ottica Linked Open Data. L'incontro (e talvolta scontro) tra catalogo BEIC e Wikidata ha dato risultati molto positivi: l'esposizione dei dati di autorità della Biblioteca digitale BEIC al di fuori del contesto del catalogo ha permesso il loro arricchimento, ha messo alla prova la loro struttura profonda in termini di interoperabilità e possibilità di riuso. In generale, la contribuzione ai contenuti di tutti i progetti Wikimedia ha portato notevole visibilità agli oggetti digitali, sia nei siti Wikimedia (con 23 milioni di visualizzazioni mese) sia nel sito di provenienza: a novembre 2019 il 75% delle visite al portale BEIC proveniva dai progetti Wikimedia. Quindi il progetto GLAM-wiki, come CERL, Europeana, ISTC, continua a essere per la Biblioteca Digitale BEIC un modo per guardare oltre i propri confini e per amplificare le competenze delle persone che lavorano al suo interno. 24  L'eterogeneità delle collezioni della Biblioteca Digitale BEIC si riflette sull'insieme di autori che le rappresentano: autori classici greci e latini, sconosciuti commentatori medievali, incisori e disegnatori Seicenteschi, fotografi del Novecento, direttori d'orchestra e musicisti contemporanei. 25 Creato da Magnus Manske, uno dei membri della comunità wiki internazionale. 26 I dati sono stati impostati secondo una struttura che accoppia la proprietà 'Descritto nella fonte' (P1343) al valore 'Biblioteca digitale BEIC' (Q51955019), a sua volta arricchito da due riferimenti: la forma del nome così come ricorre nel Catalogo BEIC e l'URL che lancia una ricerca corrispondente nel Discovery tool della biblioteca digitale. Nel secondo caso, gli elementi da creare non avrebbero dovuto contenere solo la coppia proprietà-valore relativa alla presenza nella Biblioteca digitale BEIC, ma anche alcune informazioni canoniche usate per descrivere le persone: nome, cognome, date di nascita e morte. Queste informazioni sono state desunte con facilità dai metadati bibliografici presenti nel catalogo e sono state successivamente normalizzate in una struttura adatta all'immissione in Wikidata. 27 Emblematico il caso di 'Francesco Rossi', nome presente nel Catalogo BEIC che si riferisce a quattro diverse persone, distinte proprio in base alla loro professione: chirurgo, egittologo, filologo, giurista. 28 Il relator code è un codice standard che permette di associare al nome dell'autore il ruolo che tale autore ha rivestito nell'ambito della risorsa descritta, ad esempio curatore, editore, illustratore, regista. 146  \f7 L'opera struggente di un formidabile genio o della crescita del bibliotecario In conclusione, ho scomodato uno dei titoli più evocativi della letteratura contemporanea, perché il protagonista cerca di capire le regole delle cose, di smontare i pezzi e di metterli in ordine, per mostrarli in un modo diverso e più semplice. E lo fa in una logica classificatoria, corredata di indici tematici, semplificati in un elenco di parole-chiave. E questo mi sembra un buon modo per parlare la lingua dei bibliotecari. Quello che mi stupisce della discussione apparsa in rete, e di cui abbiamo parlato in apertura, è la posizione di chi, tra i bibliotecari, vede a priori poca qualità in uno strumento di pubblica utilità, un'enciclopedia che può essere scritta da tutti, dimostrando una certa nescienza su cosa sia realmente Wikipedia. La parola enciclopedia viene dal greco \u1F10\u03B3\u03BA\u03CD\u03BA\u03BB\u03B9\u03BF\u03C2 \u03C0\u03B1\u03B9\u03B4\u03B5\u03AF\u03B1, istruzione circolare, insieme di dottrine che formano un'educazione completa. In questo senso Wikipedia ha lo stesso obiettivo che le biblioteche (soprattutto quelle pubbliche) si sono sempre poste: il servizio alla comunità, la circolarità della conoscenza. Nel caso di Wikipedia, una conoscenza a portata di tutti e perfezionabile da tutti. Circolante, quindi democratica. L'enciclopedia libera si basa sulle competenze che ciascun utente può mettere a disposizione e sul confronto che ne nasce per arrivare a una versione condivisa. Può sembrare una sfida, ma questo è l'atteggiamento che i bibliotecari dovrebbero tenere: non si contribuisce alla comunità semplicemente aggiungendo link ai propri fondi o a pagine dedicate alle istituzioni di appartenenza, occorre comprenderne la filosofia e adeguarsi alle regole. Il case study di Biblioteca Digitale BEIC dimostra quanto un approccio collaborativo, aperto al confronto e all'arricchimento, possa allargare le prospettive di tutta la comunità, in armonia - nel senso più olistico del termine - con la quinta legge di Ranganathan, laddove la biblioteca cresce insieme ai propri tempi, alla tecnologia, alle esigenze dei propri lettori."
	},
	{
		"id": 24,
		"title": "Making a Digital Edition: The Petrarchive Project",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Isabella Magni"
		],
		"body": "The Rerum vulgarium fragmenta is an icon of the Italian and Western literary tradition. Unlike other canonical works, we still possess authorial drafts of several poems and a partial holograph \u2013 MS Vaticano Latino 3195 \u2013 transcribed over roughly a decade (1366ca.-1374) in part by Giovanni Malpaghini1, a young copyist from Ravenna working under Petrarch's strict supervision (Dotti 1987). After completing the first four quaternions, part of the fifth, the seventh and part of the last quires, around 1368, for unknown reasons Malpaghini decided to leave Petrarch's employment and protection. After his departure, while transcribing the remaining poems, Petrarch began a long, difficult and often interrupted process of transcription and revision of the entire songbook. From its intended status as a fair copy, Vaticano Latino 3195 soon became a service copy in which the poet experimented his visual poetics as the basis for a potential but never realized final fair copy. The basis of any textual research on the Fragmenta is accepting that it is not a perfect work and that it was unbound and unfinished when Petrarch died in 1374. Copies of the Fragmenta often partial and unauthorized \u2013 at times even corrupted \u2013 were already circulating during the poet's lifetime. Petrarch himself often lamented in his letters that his youthful vernacular poems were disseminated without his consent among the 'multitude'2. Centuries of textual transmission and cultural mediation have progressively altered In a 2015 publication ('Malpaghini copista di Petrarca?' in Cultura neolatina LXXV: 2015 205-16) Monica Berté proposed to separate the historical figure of young Giovanni Malpaghini from that of Petrarch's scribe. 2 In a letter to Giovanni Boccaccio Petrarch writes: 'those brief and scattered vernacular works of my youth are no longer mine, as I have said, but have become the multitude's, I shall see to it that they do not butcher my major ones' (Sen. V 2, transl. Bernardo). And 1  148  \fthe way we visualize, read and ultimately interpret Petrarch's poems, and the way we reconstruct the history of the work, often more conjectural than factual3. The starting point of our work on the Petrarchive \u2013 and that of material and digital philology \u2013 is therefore to go back to the material sources and to re-examine original documents in order to dig below the surface of a 'modernized Petrarch' and to re-construct the forms and contexts in which the work was produced. The Petrarchive does not reproduce in OCR other editions of the Fragmenta but offers in XML-TEI and John Walsh's TEIBoilerplate, high-definition images of each charta of manuscript Vaticano Latino 3195, new diplomatic transcriptions and edited forms of the entire songbook, and its discoverable palimpsests. Through carefully structured text encoding, the site aims at re-constructing Petrarch's texts maintaining their most overlooked aspect: their visual and material forms4. The basic authorial principles that characterize Petrarch's 366 texts and his carefully and authorially constructed visual poetics in MS Vaticano Latino 3195 are: 1. 31-line per charta organized in two columns; 2. thematic and visual integrity of the charta, in which the poems are often not simply juxtaposed but carefully selected to form groupings of pomes deeply linked by meaning, thematic unity and contrast; 3. contrasting visual structures to distinguish the five different poetic genres of which the Fragmenta is composed: two-column horizontal reading strategy for sonnets (transcribed over 7 lines-two verses per line), madrigals, ballata and canzone, as opposed to the two-column vertical reading strategy of sestina; 4. use of space as organizational device, signaling, for example, the subdivision of the collection in two parts (cc.49-52v), or a new trajectory of the macro-text with the transcription on c.22v of the canzone Mai non vo' più cantar com'io soleva anticipated by blank space (eight transcriptional lines) at the bottom of c. 22r. The Petrarchive's first task is therefore to re-visualize these basic authorial principles while maintaining a simple interface and ease of use. The encoding of charta 1v, which presents four sonnets, serves as an example of how the digital code translates textual and prosodic features together with visual aspects of the fa\u00E7ade of the charta: <pb n='charta 1 verso' facs='..\/images\/vat-lat3195-f\/vat-lat3195-f-001v.jpg' \/> <lg xml :id='rvf005' type='sonnet' n='5'> <lg type='octave'> <lg type='dblvrs' corresp='#canvasline'> <l n='1'><hi rendition='#red #fs24pt'>Q<\/hi><hi rendition='#small-caps'>u<\/ hi>ando io <choice><orig>mouo<\/orig><reg>movo<\/reg><\/choice> i sospiri a chiamar <choice><orig>uoi<\/orig><reg>voi<\/reg><\/choice><supplied>,<\/supplied><\/l> <l n='2'><choice><orig>El<\/orig><reg>E 'l<\/reg><\/choice> nome che nel cor mi scrisse <choice><orig>amore<\/orig><reg>Amore<\/reg><\/choice>&v2c ;<\/l> <\/lg> [\u2026]5 Petrarch's visual poetics is maintained in the digital code: every pair of verses is translated in the strip of encoding as a <lg> (line group) of two verses (type='dblvrs') corresponding to one canvas line (corresp='#canvasline'). The result of the transformation of the encoding onto the web page is a new  in a letter to Pandolfo Malatesta, responding to the request by the signore of Rimini to receive a copy of his letters, Petrarch writes: 'Now they have all circulated among the multitude, and are being read more willingly than what I later wrote seriously for sounder minds. How could I deny you, as great a man and so kind to me and pressing for them with such eagerness, what the multitude has and mangles against my wishes?' (Seniles XIII, 11. Transl. Bernardo). 3 For a critique of the still widely accepted theory of the 'forms' of the canzoniere see, among others, Dario Del Puppo and H. Wayne Storey's 'Wilkins nella formazione del Rvf di Petrarca.' (2003); Teodolonda Barolini's 'Petrarch at the Crossroads of Hermeneutics and Philology. Editorial Lapses, Narrative Impositions, and Wilkins' Doctrine of the Nine Forms of the Rerum Vulgarium Fragmenta.' (2007); and Carlo Punsoni's 'Il metodo di lavoro di Wilkins e la tradizione manoscritta dei Rerum vulgarium fragmenta.' (2009). 4 See among others H. Wayne Storey's Transcription and Visual Poetics in the Early Italian Lyric (1993) and Furio Brugnolo's Libro d'autore e forma-canzoniere: Implicazioni grafico-visive nell'originale dei Rerum vulgarium fragmenta (1991). 5 The tag <pb> indicates a page break including the facsimile image (facs='..\/images\/vat-lat3195-f\/vat-lat3195-f-001v.jpg') of charta 1v (n='charta 1 verso') and is followed by the markup of the first line group (<lg>): sonnet Rvf 5 (type='sonnet' n='5'). Every tag of the alphanumeric strip of encoding refers to one specific textual, prosodic or visual component of the manuscript. This fourteenverses line group is then subdivided into two subsequent <lg>: octave (lg type='octave'), the first four verses organized over four canvas lines (lg type='dblvrs' correps='#canvasline'); and sestet (lg type='sestet'), the remaining six verses transcribed over two canvas lines (lg type='dblvrs' correps='#canvasline'). For more on the Petrarchive encoding see my essay I codici paralleli dei Fragmenta (2015). 149  \frepresentation of Petrarch's visual poetics and editorial principles for which he worked restlessly for over a decade:  Figure 1. The Petrarchive, c.1v: diplomatic transcription and manuscript image, displayed side by side. URL: http:\/\/dcl.slis.indiana.edu\/petrarchive\/content\/c001v.xml#c001v?facs=active Other than being a research tool6, text encoding also represents a close reading of the texts: it \u2018forces' encoders to \u2018break down' each poem and to \u2018label' its single components using specific tags. It can therefore also serve as an alternative and highly stimulating teaching tool: while being asked to encode Petrarch's texts, students necessarily need to re-think the very basic words, linguistic and prosodic structures of each poem to be able to digitally translate them into tags. Advanced uses of text encoding also offer new ways of representing and analyzing erasures, renumbering, palimpsests, while maintaining a clean and simple digital interface. The most notable example is the palimpsest Donna mi vene spesso ne la mente on c.26r, erased and overwritten by Or vedi amor che giovenetta donna (Rvf 121) by the poet himself:  Figure 2 and 3. The Petrarchive, c.26r: diplomatic transcription of Rvf.121 Or uedi amor (left); diplomatic transcription of palimpsest Donna mi uene. URL: http:\/\/petrarchive.org\/content\/ c026r.xml  For more on digital editing and TEI encoding see, among others, the MLA White paper Considering the scholarly edition in the digital age: a white paper of the modern language association's committee on scholarly editions (2015 and 2016); Elena Pierazzo. Digital scholarly editing: Theories, models and methods (2015); Elena Pierazzo and Driscoll (eds.). Digital Scholarly Editing: Theories and. Cambridge (2016); Kenneth Price, and Ray Siemens. Literary Studies in the Digital Age: An Evolving Anthology. Modern Language Association, 2013; and Siemens, Ray and Susan Schreibman, A Companion to Digital Literary Studies (2008). 6  150  \fThrough a combination of common Web design techniques and text encoding, by clicking on the manicula7 in the right margin, the user can easily move from one version of Rvf 121 to the other as diplomatic and edited versions of both poems are available. To encode the simultaneous presence of erased and overwritten poems, we employ the following TEI elements: <del> (deletion): to contain the erased 'Donna mi vene'; <add> (addition): to contain the added 'Or vedi amor'; <subst> (substitution): to wrap the related <del> and <add> and assert that the <add> is substituted for the <del>; and <ab type='blockSubst'> (anonymous block)8. Petrarch's visual poetics is so authorially designed and so deeply part of the collection that a reader can recognize the genre and sometimes function of poems even before reading them. The SGV images created to digitally reconstruct the fa\u00E7ade of the manuscript page are simple graphic representations of the Fragmenta's material structures. On charta 1 verso and charta 3 verso, for example, users can easily identify the most frequent four-sonnets \u2018canvas' (c.1v) and distinguish the shift in directionality between the sonnet, horizontal, and the sestina, vertical (c.3v):  Figure 4 and 5. A visualization of c.1v and c.3v created for the Petrarchive project. The description of Petrarch's visual poetics embedded in the text encoding is therefore also represented in the Petrarchive visual indexes9: the graphic image files are in fact XML files in the Scalable Vector Graphics format (SGV) which contain the information necessary for the Web browser to reproduce the image. The graphic information in the SVG files may also be derived from the codes embedded in the TEI\/XML file, proving the visual and representational capabilities of the encoded document. From a brief look at the visual index of the entire songbook, also developed for the Petrarchive, the nine pairings sonnet-sestina10 present in the collection are immediately recognizable: once again, even before accessing the textual contents of the poems, the Medieval reader \u2013 and now the digital user \u2013 can collect a series of information regarding the poetic genres, their material and visual treatments:  7 The manicula is not present in Vaticano Latino 3195: it is an interface element introduced by the Petrarchive, mimicking those Petrarch and other medieval and early modern readers used to draw attention to specific passages in manuscripts. 8 For more information about the encoding developed to digitally reconstruct the palimpsest see Isabella Magni and John A. Walsh's Digital Representations and the Pivotal Instability of Donna mi vene spesso ne la mente in the Study of the Fragmenta (2016). 9 In the examples in Figure and 5: arrows signaling the directionality of the text distribution (on c.3v the user can distinguish the shift in directionality between the sonnet, horizontal, and the sestina, vertical); paragraph markers in the sestina as internal visual indexicality indicating the vertical disposition of the text over the two columns; initials on the right of the text indentation signaling to the medieval reader, and now to the contemporary user, the beginning of new poems, marking the passage from fair- to work- and service copy (red and blue the rubricated fair-copy initials, in blank ink the remaining ones) and functioning as a textual index; blank space serving as additional punctuation device (visible in light blue color in the graphic SGV images). 10All of the Fragmenta's sestine \u2014 except for the double sestina of part II (Rvf 332) \u2014 are always presented in contrast to a sonnet on the same charta (see Petrarchive Glossary 'Sestina'. URL: http:\/\/dcl.slis.indiana.edu\/petrarchive\/ content\/glossary.xml#sestina).  151  \fFigure 6. The Petrarchive: visual index of MS Vaticano Latino 3195. URL: http:\/\/dcl.slis.indiana.edu\/petrarchive\/images\/ Petrarchive__Visual_Index_to_Vat__lat__3195.jpg  The representational values established by the Petrarchive visual indices also offer a unique insight into the preparation of the manuscript, still in the form of loose gathering at the time of Petrarch's death: from the original project revealed by the rubricated chartae transcribed by Malpaghini (in brown) and set aside in 136811, to Petrarch's addenda in his own hand12, and to the last service-copy transcriptions for the poet only13 (both in dark blue). A newly developed visual index arranged by fascicles, also allows users to navigate into Petrarch's material construction of his fascicles, including the two final binions (cc. 63-66 and 67-70) that the poet inserted last into an already existing binion (cc.61-62, 71-72):  Figure 7. The Petrarchive: visual index by fascicles. URL: http:\/\/dcl.slis.indiana.edu\/petrarchive\/ visindex_fascicles.php  Five quaternions (cc.1r-40v) in Part I and two fascicles (cc.53r-60v) (cc.61r-62v and 71r-72v) in Part II. Another quaternion (cc.41r-48) in Part I and four more chartae (c.59r-62v) in Part II. 13 This last section includes four chartae at the end of Part I (cc.49r-52v) and the last binion added towards the end (cc.69r-70v) with the transcription of canzone Quel' antiquo mio dolce empio signore. 11 12  152  \fDigital tools allow us to start from what we possess, the material evidence, and to dig below the surface to re-discover the original contexts in which the text was produced. Through carefully studied TEI encoding and the virtual representation on the web page of the different aspects of Petrarch's Fragmenta - its textual and graphic, temporal and spatial components \u2013 the Petrarchive aims at re-building the structural and visual principles implemented by the poet himself at the level of single charta, fascicles and macro-structures and therefore to re-propose Petrarch's editorial choices, diminishing the distances between the experience of contemporary users and that of manuscript readers in the medieval context and providing innovative ways of teaching and conducting philological and literary research."
	},
	{
		"id": 25,
		"title": "Extending the DSE: LOD support and TEI/IIIF integration in EVT",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Paolo Monella",
			"Roberto Rosselli Del Turco"
		],
		"body": "1 Introduction In the currently available DSEs there is a considerable lack of homogeneity since, besides a small number of (necessarily) common features, some are still rather traditional in the way of modelling the data, of conceiving the visualization of the edited text and in the kind of tools which they make available to the scholar, while others explore the enrichment of the edition's contents and the design of research tools in a highly innovative way. As W. Gibson (1990) said 'The future is already here \u2013 it's just not very evenly distributed'. As a matter of fact, the first, pioneering DSEs were quite conservative in their approach to User Interface (UI) layout. This is understandable, since the first electronic editions were considered an equivalent of traditional editions in a different medium: the 'printed page paradigm' (Sahle, 2016) inspired a mimetic design which would take into account very few advantages of the new publication framework besides its ubiquity and the apparently endless space it grants. The 'remediation' (Bolter & Grusin, 2000) of the scholarly edition in the new media had not yet taken place. When the methodology advances possible thanks to a Web-based digital edition started to be evident, a new research field was born: digital philology can be traced back to the desire to explore the potential of a truly Digital Scholarly Edition. This impulse has led to a lively but somewhat chaotic research activity, with an apparent paradox: \u25CF  only projects which can rely on generous resources may afford to explore new approaches to a DSE design, but usually they are focused on the specific task at hand, while the decision makers aren't interested in broader reflections on the general methodology;  1  For the purposes of the Italian academy, R. Rosselli Del Turco is responsible for sections 1-4 and P. Monella for sections 5 and 6. The initial abstract was jointly written by the two authors, who also planned and revised the article together. 154  \f\u25CF  interested scholars, on the other hand, may be hampered not only by the lack of resources to experiment, but also by the fact that the DH-related IT world is moving very fast, so fast that sometimes new technologies are introduced, enhanced, exploited and set aside within a few years.  This article aims at presenting some of the latest methods which can be applied to a DSE with the purpose of making it an even better (more flexible, modular, distributed, interconnected) research tool, and will also consider the software design and implementation issues that they imply.  2 No DSE is an island Of the many limits artificially imposed to the DSE by the printed page paradigm, the first to fall was that of the book as a monolithic product, isolated from other books and unalterable, if not with rather high costs, after its publication. A DSE, in fact, can be changed both occasionally, f.i. to correct errors, and systematically, to add new texts, commentaries, bibliographic items, etc., so that a publication date by no means implies the end of the editing process, rather just the beginning of a new phase. Furthermore, a DSE is a dynamic, not a static object, a research tool which assists the scholar in data interpretation and analysis; and it is not a 'closed box', but it can engage in dialogue and interaction with other Internet-based resources thanks to the global linking framework upon which the Web itself is built (Bodard & Garc\u00E9s, 2009; van Zundert & Boot, 2011). As a consequence, the greatest advantage of Web-based publication is not only that it makes scholarly content dissemination much easier and cheaper, but that the DSE can access other resources (and be accessed), and it can rely on external assets and services for specific functionalities. Examples include: \u25CF \u25CF \u25CF  taking advantage of semantic web technologies and Linked Open Data to enrich the edition content; text\/image linking, pointing to digital collections of images of manuscripts maintained by external repositories (IIIF framework); modelling intertextual relationships through canonical text services (CTS and DTS protocols).  While this deep interconnecting and sharing of resources may look like a 'quantitative' only advantage when compared to traditional scholarly editions (i.e., you can modify your edition when you want and integrate all the available content that you may see fit), there are important methodological consequences: \u25CF \u25CF \u25CF \u25CF \u25CF \u25CF \u25CF  the definitive rejection of the concept of an edition as an isolated and immutable entity; the acknowledgement of the fact that the use of external materials to integrate a DSE is highly desirable as it results in an enrichment of the DSE itself; an impulse to the collaboration between different edition projects, possibly adopting principles typical of the social edition (Siemens et al. 2012); as a consequence, an incentive to adopt a modular and distributed approach in the design of digital editions, making them more flexible; the possibility of virtually re-assemble dismembered manuscripts scattered across several preserving institutions (see f.i. the Fragmentarium project); a simplification of the problem of copyright management for digital reproductions of MS images by libraries, since images are functionally integrated in the DSE, but remain resident in their repository; the DSE itself may become a resource for other editions if the data on which it is based is published in such a way as to make it available to third parties.  This approach, however, poses a number of problems: \u25CF \u25CF \u25CF  greater technical complexity: so far only the editions defined as 'haute couture' by E. Pierazzo (2019) can afford to adopt this approach because its digital implementation is certainly more complex than a simple reproduction of static images and texts in a Web-based edition; qualitative homogeneity of the different components of a DSE: if part of the content is entrusted to external resources, it is of critical importance that these resources meet the same academic standards as the original materials; long term sustainability: the interdependence between the 'internal' and 'external' components of a 155  \fDSE makes it possible to modify the existing connections and to add new materials as soon as they become available, but it also implies a strict and continued control on the actual availability and compatibility of these materials in the long run. What is needed on the methodological level: \u25CF \u25CF \u25CF  open protocols, for data-sharing infrastructures, and open licenses, for resources to be shared; an open and ongoing scholarly discussion on the systematization of fundamental concepts to create a shared conceptual framework: we need shared terminologies and open ontologies as a necessary methodological condition for the creation of such shared resources; an integration of those methodological experiments in common editorial practice.  3 EVT 2: the linked DSE EVT 2 is the second version, currently based on the AngularJS framework, of EVT (Edition Visualization Technology), a software tool for publishing editions based on the TEI\/XML format, which has been developed in such a way as to go beyond the printed page paradigm \u2013 especially with regard to the User Interface design (see Di Pietro & Rosselli Del Turco 2018). The way in which it manages data, however, makes it mostly suitable for self-contained editions, created on the basis of local resources. Furthermore, at the present moment EVT is based on the client-only architecture, which presents many advantages (it is easy to install, little to no maintenance is required, it has indefinite durability), but also quite a few limits concerning important functionality (such as server functionality, f.i. for textual searches or to serve images). For these reasons, it is an important goal of the development team to add support for protocols such as LOD for semantic Web resources, IIIF for images and CTS\/DTS for intertextual relationships, so that scholars can count on a pr\u00EAt-à-porter tool for their work. In the long term this goal may be achieved by adding RESTFul services, to add server functionality without encumbering the software too much, but it is already possible for projects based on EVT 2 to include LOD and IIIF resources. This will also have the beneficial effect of allowing more widespread knowledge of these protocols.  4 EVT and LOD Although LOD resources are extremely diversified in terms of type of content, semantic-based operation, etc., it is undeniable that this is a very promising area which has seen constant growth in recent years. EVT is already able to access resources on the Web thanks to URIs specified in the TEI markup, in fact some of the existing EVT-based editions (e.g. the Codice Pelavicino Digitale) use resources such as external repositories to provide additional information about the named entities identified in the text. The management of named entities can be significantly improved through the use of LOD resources such as FOAF and DBpedia. In the Codice Pelavicino Digitale, person names are already linked to the Dizionario Biografico degli Italiani when possible, for instance: <!--Code snippet 1--> <note>Guido da Velate, arcivescovo di Milano dal 1045 al 1068, morto nel 1071 (si veda <ref target=\"http:\/\/www.treccani.it\/enciclopedia\/guido-da-velate_ %28Dizionario-Biografico%29\" type=\"biblio\">Guido da Velate nel Dizionario Biografico degli Italiani<\/ref>).<\/note>  This information can be supplemented or replaced by linking to a DBpedia entry: <!--Code snippet 2--> <ref target=\"http:\/\/dbpedia.org\/page\/Guido_da_Velate\">Guido da Velate<\/ref>  so that it is possible to take advantage of the wealth of connections and of the sophisticated ontologies that LOD resources allow. Furthermore, as hinted above the DSE itself can make (part of) its material available as LOD, so that other editions can build upon it. The concept of 'distributed edition', therefore, is coming closer to reality, in fact this is the goal of a new research project aiming at disseminating a DSE on sustainable and public resources such as Zenodo and GitHub (see O'Donnell et al., 2018; note that EVT already runs on GitHub).  156  \f5 EVT and IIIF 5.1 The rise of IIIF One notable example of an open protocol for online resource integration is IIIF \u2013 International Image Interoperability Framework. IIIF is rapidly emerging as a technology to exchange and integrate image-based resources in Web-based systems. One interesting use-case is a DSE in which portions of a TEI-encoded text based on a primary source such as a manuscript or a printed book are linked to the images of that source, stored and described via the IIIF framework. For instance, the transcription of a page of a manuscript can be linked to its facsimile, and the transcription of a line can be linked to the region of that facsimile corresponding to the line. 5.2 IIIF Image and Presentation APIs The IIIF protocol defines different APIs, two of which will be briefly discussed here: 1. The IIIF Image API 'specifies a web service that returns an image in response to a standard HTTP or HTTPS request'. Simply put, this API returns one image or a portion of it. The description on the image are stored in a JSON file named info.json. 2. The IIIF Presentation API instead 'describes how the structure and layout of a complex image-based object can be made available in a standard manner'. Such a complex object can be a collection of digital images of a manuscript, accompanied by the relevant metadata, stored in a JSON file named manifest.json. 5.3 TEI and IIIF: a marriage made in heaven? The TEI approach has always been text-centric, and only more recently the TEI editors have included a document-based approach in which the digital images of a textual source have equal dignity as its textual representation, via the <facsimile> \/ <surface> \/ <zone> encoding approach. On the other hand, IIIF is overtly and intentionally image-based. The TEI\/IIIF integration thus looks very promising and productive for DSEs aiming to combine textual representation and digital images. However, at this point this is very much an open field of experimentation. 5.4 Two directions for a TEI\/IIIF integration Two approaches are theoretically possible for this integration: 1. Linking from IIIF to TEI \u25CF How: according to the IIIF Presentation API, the node of a IIIF manifest identifying a specific (portion of an) image can point to an annotationList (a separate JSON file) including an annotation pointing to an external TEI XML file with the relevant textual representation (transcription). \u25CF Why this might not be a good idea: the institution curating the IIIF collection (for textual sources, most probably a library or an archive) should create, curate and update the annotations linking to the TEI XML files. If those files belong to external DSEs incorporating the IIIF images, the DSE URIs might change and require constant update from the library's side \u2013 which is clearly not sustainable. On the other side, the library could create those TEI XML files itself to store and expose transcriptions of its own manuscripts, but in the current division of labour, libraries focus on digital imaging and metadata rather than on full transcriptions and textual criticism. 2. Linking from TEI to IIIF \u25CF How: within a TEI XML file, f.i. in a <pb\/> (page beginning) or <lb\/> (line beginning) element, a @facs attribute points to a IIIF URI, either directly or indirectly. \u25CB 2A - Directly: @facs takes the relevant IIIF URI as value (identifying, f.i., a whole manuscript page or a rectangle of that page including a line); \u25CB 2B - Indirectly: @facs points to a <surface> or <zone> element within the <facsimile> section of the TEI file, and the <surface> \/ <zone> element points to the relevant IIIF URI. 157  \f\u25CF  The indirect strategy adds a layer of complexity, but also increases flexibility. Why this might be a good idea: this approach keeps resources separated (TEI XML files for textcentered digital philology and a IIIF infrastructure for digital image collections), with modularity and interoperability in mind. Many DSEs can link to the same IIIF image collections. Shortly said, philologists work on text with TEI, librarians work on document digitization with IIIF.  5.5 IIIF implementation in EVT EVT 2 now features IIIF integration thanks to OpenSeadragon, its embedded image viewer. It implements the second approach described in the previous paragraph ('Linking from TEI to IIIF') and uses the IIIF Image API. Digital philologists can thus integrate external images of a textual source, hosted by a third-party IIIF image server, in the DSE, with an arbitrary level of alignment granularity. IIIF-compliant servers include e-codices, the Veneranda Biblioteca Ambrosiana in Milan (Cusimano, 2019) or the Biblioteca Apostolica Vaticana in Rome. More precisely, EVT currently implements encoding strategy 2A described in paragraph 5.4 above: 1. The TEI XML source code has an element pointing to an image exposed by a IIIF server (typically a facsimile of a page) or to a portion of that image (typically a rectangle including a line or an other textual division). In the following code sample, we are pointing to a whole image (representing a manuscript page) from the IIIF server e-codices - Virtual Manuscript Library of Switzerland. The value of @facs is a URI following the IIIF Image API: <!--Code snippet 3--> <pb facs=\"https:\/\/www.e-codices.unifr.ch\/loris\/csg\/csg-0730\/csg-0730_002.jp2\/ full\/full\/0\/default\/jpg\"\/>  2. The image viewer integrated in EVT, OpenSeadragon, dereferences the URI from the @facs attribute, fetches the image from the external IIIF server and shows the whole image of the manuscript page alongside its TEI-based transcription. Please note that the version of OpenSeadragon currently embedded in EVT (2.4.1) also allows to align a <pb\/> element with a specific portion of an image, defined as a rectangle as per the IIIF Image API. Thus code snippet 3 above can be edited to: <!--Code snippet 4--> <pb facs=\"https:\/\/www.e-codices.unifr.ch\/loris\/csg\/csg-0730\/csg-0730_002.jp2\/ 1800,600,3000,5000\/full\/0\/default\/jpg\"\/>  to crop out the manuscript page margins. Coordinates '1800,600' (in pixels) define the top left corner of the rectangle, '3000' defines the rectangle's base, '5000' its height. The encoding strategies described so far fulfill the common need of editors to pair <pb\/> elements with a manuscript or book page (as well with the surface of an inscription, a tablet or any other support) and to display it aside the transcription. The following code sample, instead, exemplifies the TEI XML encoding strategy currently supported by EVT to link elements such as <lb\/>, <p> or <div> to smaller portions of the surface image: <!--Code snippet 5--> <surface> <zone lrx=\"1052\" lry=\"211\" rend=\"visible\" rendition=\"Line\" ulx=\"261\" uly=\"156\" xml:id=\"zone-line-2-1\"\/> <\/surface> [...] <pb facs=\"https:\/\/www.e-codices.unifr.ch\/loris\/csg\/csg-0730\/csg-0730_002.jp2\/ full\/full\/0\/default\/jpg\"\/> <lb facs=\"#zone-line-2-1\"\/>  In code snippet 5 (which incorporates snippet 3 above), the <pb\/> element references the whole manuscript page via a IIIF URI; <lb\/> points to a <zone> element in the <facsimile> section that defines a rectangle within the IIIF image through the internal TEI XML encoding strategy: @lrx and @lry define the coordinates of the lower right corner of the rectangle, @ulx and @uly those of the and upper left corner (see attribute class att.coordinated in the P5 TEI Guidelines). Please note that this is different than strategy 2B from paragraph 5.4 because it still is the <pb\/> element, not <zone>, that is retrieving the IIIF image.  158  \f5.6 Future development Support for encoding strategy 2B (<pb\/> or <lb\/>'s attribute @facs points to <surface> or <zone>, and the latter points to an image in an IIIF server) is not yet available in EVT, but the development team aims at including it in future releases. These are examples of TEI XML code that will be managed by EVT: <!--Code snippet 6--> <facsimile> <surface xml:id=\"image-p2\"> <graphic url=\"https:\/\/www.e-codices.unifr.ch\/loris\/csg\/csg-0730\/csg0730_002.jp2\/ full\/full\/0\/default\/jpg\"\/> <!--'zone' elements may be included here--> <\/surface> <\/facsimile> [...] <pb facs=\"#image-p2\"\/>  Or, with a more compact encoding: <!--Code snippet 7--> <facsimile> <surface xml:id=\"image-p2\" facs=\"https:\/\/www.e-codices.unifr.ch\/loris\/csg\/ csg-0730\/csg-0730_002.jp2\/full\/full\/0\/default\/jpg\"> <!--'zone' elements may be included here--> <\/surface> <\/facsimile> [...] <pb facs=\"#image-p2\"\/>  Another feature that may be implemented in the future, should any project collaborating with the EVT team express this need, is the definition of a rectangle within an image (e.g. for a manuscript line encoded with <lb\/>) not through the TEI XML strategy (attributes @lrx, @lry, @ulx and @uly in <zone>), but directly through the IIIF image API, such as in the following code: <!--Code snippet 8--> <lb facs=\"https:\/\/www.e-codices.unifr.ch\/loris\/csg\/csg-0730\/csg-0730_002.jp2\/ 1900,930,2580,140\/full\/0\/default\/jpg\">  Finally, an experimental version of EVT based on the Angular 8 framework, not yet available for download, already supports the IIIF Presentation API and loads the full manifest.json document description including metadata on all manuscript images, while the image API currently supported by EVT provides access to one manuscript page image at a time. To load a whole manifest.json file, it will be enough to specify its URL in the EVT config.json configuration file, e.g. <!--Code snippet 9--> { \"title\": \"My Digital Edition\", \"manifestURL\": \"https:\/\/www.e-codices.unifr.ch\/metadata\/iiif\/csg0730\/manifest.json\" }  Besides allowing a quick publication of all pages of a manuscript, this is a first step towards exploiting the full potential and flexibility of the IIIF framework.  6. Conclusions The EVT development team is committed to supporting the current trend towards the distributed DSE, integrating resources such as entity or relationship definitions (LOD) and images (IIIF), as well as text fragments (CTS\/DTS) in future versions. Internal and external objects alike can be better modelled by the new modular and object-oriented implementation adopted in EVT 2. Further development aims at supporting alternative encoding strategies for linking to external resources while keeping the whole edition (XML, other internal and external data, software) sustainable, durable and compliant with the FAIR principles, according to which 'all research objects should be Findable, Accessible, Interoperable and Reusable (FAIR) both for machines and for people' (Wilkinson et al., 2016)."
	},
	{
		"id": 26,
		"title": "Mapping as a contemporary instrument for orientation in conferences",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Chloe Ye-Eun Moon",
			"Dario Rodighiero"
		],
		"body": "Introduction  Maps are formidable instruments for abstracting territory and travels. Centuries of cartographic mapping marked the evolution of world history, and today's technological innovation has opened a new paradigm of mapmaking (Dodge et al., 2011; L\u00E9vy, 2016). Maps are no longer static; instead, they can change dynamically given external inputs. For instance, users can zoom into the map to obtain detailed information, and they can choose to filter selected information. Due to the big data technology, maps are now able to represent larger and larger datasets (Kitchin, 2014). Moreover, maps now have a new level of abstraction not only for the territory but also for individuals; unlike before, visualizing social relationships became a common practice since Jacob Moreno's work in social relationships (1934). This article presents a case study of human mapping, specifically of scholars and their scientific productions, focusing on the Digital Humanities Conference 2019 that took place in Utrecht, the Netherlands in July 2019. Such a case study develops the previous work of Rodighiero (Rodighiero, 2015, 2018; Rodighiero et al., 2018), demonstrating how a map can be a powerful instrument of reduction not only for territory but also for individuals. The cartography of DH20191 is intended to be a generic tool for mapping scientific communities, in the form of an open-source project2. The article will present the result of such cartography by discussing the following four sections: 1) Documentality as a way human activity is regulated through textual inscriptions 2) Lexical Analysis as the way documents can be automatically analyzed under human supervision 3) Graphic Design as a visual translation that makes a conference 1The cartography of DH2019 is accessible at https:\/\/rodighiero.github.io\/DH2019\/ 2The open-source project is available at https:\/\/github.com\/rodighiero\/DH2019\/  162  \fattended by a thousand authors wholly graspable 4) Reading as a form of user interaction through which the map reader acquires information from the visual media.  2  Documentality  A series of social rules govern humans behaviors (Kaplan, 2012), and scholars are no exception. Their activities follow specific rules when they attend a conference; submitting articles, waiting for reviews, attending the conference, and presenting their work are the steps they are required to complete in order to be a part of an academic field. Such behaviors are auto-regulated by the scientific community itself, which gives a structure to the research domain. Articles play a significant role in this process as they are an extension of their authors. They convey different types of information, such as collaboration, affiliation, scientific interests, and the writing proficiency of the authors. Therefore, articles are valuable as they portray the authors' values and social relevance. We hypothesize that documents embody textual information, which can be used to measure the proximity between scholars most effectively. As individuals express themselves through their language, authors can be described through their writing. Terms are not a barrier, nor they are private. Everyone can choose and use preferred words and speech styles according to their taste, and the choice is profoundly affected by social and cultural environments, such as their education level and location. Nonetheless, there is complete freedom in the selection of language, and this freedom goes beyond any collaboration or citation. Just like how Pierre Bourdieu used personal interviews to classify individuals (Blasius and Schmitz, 2014; Romele and Rodighiero, 2019), the goal here is to map scholars using scientific articles.  3  Lexical Analysis  Natural Language Processing (NLP) is a branch of artificial intelligence that aims to process and analyze large amounts of text (Manning and Schuetze, 1999). Since humans' natural language has no structured rules, computers can understand, NLP is especially challenging; therefore, the techniques in NLP are significant as they derive meaningful insights from texts written in such a language. In order to profile the authors who attended the Digital Humanities 2019 conference, we performed a lexical analysis to map the distance from one author to another. First, the XML data of DH2019 were cleaned for a more accurate analysis utilizing JavaScript programming language and the Cheerio library3. From each article are extracted authors, title, and text body. Since multiple authors can co-author a paper, the text body is grouped by authors, allowing multiple occurrences of the same publication if a paper is co-authored. Each text is then tokenized using a lexical analyzer provided by the Natural library4 for NLP. Successively, tokens are singularized and filtered by a list of stopwords in various languages, including Brasilian, English, French, German, Italian, and Portuguese. The arrays of tokens associated with authors are then computed via the Term Frequency - Inverse Document Frequency algorithm, also known as TF-IDF (Luhn, 1957; Sparck Jones, 1972). TF-IDF extracts the most relevant terms for each author by counting the frequency of each term with respect to the inverse frequency of the entire collection of words. The list of terms for each other is then shortened to the fifteen most relevant terms in order to simplify the visual computation. Table 1 shows a sample concerning the scholar Fr\u00E9d\u00E9ric Kaplan.  3Cheerio is an open-source library available at https:\/\/github.com\/cheeriojs\/cheerio\/ 4Natural is an open-source library available on GitHub at https:\/\/github.com\/NaturalNode\/  163  \fToken Parcel Amsterdam Street City Cadastre Rue Neighbourhood Urban Century Bottin Transcription Geometrical Extraction Geographical Geometry Wine Activity Dialect Cinema Time  Value 156.29240966203554 147.92161777735498 97.3716554233416 78.98538009996346 68.97376382900369 64.52753719852456 62.45323622544596 61.733372168535126 52.240930125777 51.62202975881964 51.58659792981266 44.74034261035416 43.895897928544414 41.91500209834842 39.821449253041536 38.71652231911473 37.66779857289015 37.53334200758336 34.18896461357028 33.67020614879688  Table 1: An excerpt from the JSON file that describes the profile of Prof. Fr\u00E9d\u00E9ric Kaplan. Among the metadata are his name, the number of articles, and a list of fifteen tokens weighted through the TF-IDF algorithm. As his research is mainly focused on the European Time Machine, which is focused on the computational analysis of ancient maps, the result can be considered adequate.  4  Graphic Design  Data analysis is followed by the creation of a network, in which each author forms a node, and the shared tokens are transformed into weighted edges. The resulting visual rendering does not recall a classic network visualization, such as Gephi's (Bastian et al., 2009), but rather a cartographic projection. It is a hybrid form that combines the characteristics of networks and maps. Authors are placed on the map using the Simulation function5 from the d3.js library (Bostock et al., 2011). Then, between each pair of authors, is displayed the most relevant token whose size corresponds to the TF-IDF value; a high TF-IDF value corresponds to a high degree of relevance in the whole collection. The elevation contours (Monmonier, 1991) displayed at the end of the simulation due to computation limits make the density of documents visible; as a result, an author who authored many articles is placed at a peak. The result is an elevation map that shows the most relevant tokens, such as languages, music, newspapers, and films. When zoomed in, the map can be enlarged to display the details, and the user will notice the tokens changing. The reason is that the tokens are selected according to the zoom level; when the map is utterly visible, the user can see the most relevant tokens with high-frequency values, and by zooming he will see the most generic ones. This choice makes the view more specific and less generic, while a zoom-in allows viewing the complete gradient of tokens (See Figures 1, 2, and 3).  5Simulation runs to place the nodes in a proper way, more information is available at https:\/\/github.com\/d3\/ d3-force\/  164  \f5  Reading  In cartography, the reader is the individual who interacts with the map (Dodge et al., 2011; L\u00E9vy, 2016). Readers interpret the cartographic visualization differently depending on their knowledge and culture. In front of the DH2019 visualization, the reader identifies a configuration of scholars who attended the conference with an article. When authors share the same text because they co-authored a paper, they appear to be close on the map, and the most relevant token between them appears (Figure 1). When there is a continuity of tokens, it indicates an area of particular interest (Figure 2). When the density of individuals is visible, there is a high chance that it represents a collective work (Figure 3). The map offers a novel point of view on the conference, which is a different approach from the proceedings or the website. It makes all the authors and their work graspable at the same time, while also allowing the readers to navigate through individual authors. When a reader starts to explore a specific part of the map by zooming in, the interaction becomes personal and unique. Therefore, readers play an active role while putting an interpretation on the map; their pathway influences what they see. Furthermore, if a reader who participated in the conference recognizes her identity in the map, the reading validates the reader's representation by evaluating the correctness of the map and the neighborhood where the reader is placed (Rodighiero and Cellard, 2019).  6  Conclusion  Language is not only a means to convey one's ideas, but also to express interest and background. If a conference forms a scientific community by attendance, the articles presented at the conference shape the specific language of such a community with a delicate balance of different voices (Von Glasersfeld, 1992). Lexical analysis of terminologies is an effective means to study the community. Thanks to the current technology and visualization techniques, we are now able to create a dynamic, interactive map of the community, which is dense and rich in information. From this new form of data visualization, readers can interpret the lexical proximity of all the authors at a glance, both the distance and placement. Now the question is, why don't we use the map as an instrument during the conference? It would undoubtedly be a much more contextually rich and visually intriguing way of understanding the conference, instead of merely using statistics to summarize the event.  Acknowledgement We want to thank Kurt Fendt for his constant support and supervision, and the MIT Literature section for hosting us, in particular Shankar Raman, Diana Henderson, and Alicia Mackin. Thanks also to Jeffrey Schnapp and the laboratory members of Harvard MetaLab. Acknowledgement also goes to Stephan Risi for developing the search function and Philippe Rivi\u00E8re for his priceless collaboration, which was fundamental for recent projects. A special mention to Daniele Guido, an inseparable friend and colleague whose design capacities are stimulus for doing better; the grapefruit color palette is his merit. This article is part of the grant Early Postdoc.Mobility P2ELP1_181930 Worldwide Map of Research funded by the Swiss National Science Foundation.  165  \fFigure 1: The cartography of DH2019. At this level of zoom, the most specific terms suggest some entry points to explore. For instance, in the middle, the terms 'news' and 'newspaper' invite to zoom in these areas. The reader is also free to explore empty spaces or search a scholar by using the search box at the top right of the interface.  166  \fFigure 2: This image illustrates a specific area of the map in which the term \u2018dariah' is recurrent. The term reassembles the people working within the Dariah community. By zooming, the terms change according to the scale; the more the reader zooms in, the more generic the terms are.  Figure 3: Co-authoring is an easily recognizable phenomenon, especially in areas with low density. At the center, it is visible a group of scholars that share the term \u2018interdisciplinarity.' Terms, like in this case, can be used to spot small communities within the conference."
	},
	{
		"id": 27,
		"title": "Argumentation mapping for the History of philosophical and scientific ideas: The TheSu annotation scheme and its application to Plutarch’s Aquane an ignis",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Daniele Morrone"
		],
		"body": "1 Introduction The field of 'computational history of philosophy' (Betti et al., 2019) is rather new but promising, as it can provide historians with powerful research tools to work with large amounts of data in an organized fashion, giving them the possibility of finding patterns, similarities and links. History of philosophy and History of science can be regarded as subfields of History of ideas \u2013 meant in the broadest possible sense \u2013 and although digital methods seem to have only recently been introduced in this latter (Betti and van den Berg, 2016)1, History of science has been benefiting from them for a long time already, under the influence of Computational linguistics (Dibattista, 2009). By presenting the novel XML annotation scheme TheSu, this paper aims to contribute to the general trend of digitalizing the research methods in these fields, focusing on 'ideas' in the sense of judgements about states of things and giving relevance to the way these judgements are presented and promoted by their authors. Plutarch's short conference (D'Ippolito and Nuzzo, 2012, pp. 180\u2013191) Aquane an ignis utilior sit (Aq.) \u2014 'Whether fire or water is more useful'\u2014 has been annotated according to the TheSu scheme to give some examples of this latter's possible applications and capabilities. The digital XML\/TEI edition (TEI Consortium, 2019) of the original Greek text chosen as a base for the annotation has been downloaded from PerseusDL\/canonical-greekLit (Cerrato et al., 2019), and corresponds to Bernardakis's critical edition of the work (1895, pp. 1\u201310). 2 TheSu and related work in Argumentation Mining  1  Betti and Van den Berg do not seem to consider the activity of the ILIESI (Istituto per il Lessico Intellettuale Europeo e Storia delle Idee) in Rome, which has long been working on History of ideas in the frame of Digital Humanities. See http:\/\/ www.iliesi.cnr.it\/. 169  \fThe aim of the TheSu (Thesis-Support) annotation scheme is to provide the possibility of easily navigating through enunciates (Theses) contained in written texts and all their linked explanations, justifications and refutations (Supports), each indexed as a node in an abstract network defined as 'Argumentative-Expository System' (AE System), which is stored in a database. Focusing on argumentative relations of whatever rhetorical nature, TheSu can be likened to the various annotation schemes that are being proposed in the field of Argumentation Mining (Lippi and Torroni, 2016; Stede and Schneider, 2019), even if it doesn't share their common objective of digitally automatizing argument extraction from texts. TheSu, although similar to these approaches, is different from them for two main reasons: (1) It builds its system on theses abstracted from the texts by human interpreters, which can then be linked to their possible textual supports (if there are any). Argumentation mining approaches influenced by Toulmin (2003 [1958]) and Walton (1998; Id. et al., 2008) tend to directly search the texts for premise-conclusion enunciative pairs to tag them under schemes such as Walton's 'argumentation schemes' (see e.g. Lauscher et al., 2018; Mochales Palau and Moens, 2009; Rocha et al., 2016; Green, 2018a); approaches based on Rhetorical Structure Theory (RST) instead (see Mann and Thompson, 1987; Taboada and Mann, 2006, secs. 2.4, A.2) select their elements through objective textual markers (see the definitions of EDUs \u2014Elementary Discourse Units\u2014 in e.g. Carlson et al., 2001; Marcu et al., 1999), and as a consequence segment the text into discrete \u2014albeit interconnected\u2014 non-overlapping units (on the undesirable aspects of these approaches see Green, 2018b; Peldszus and Stede, 2013, pp. 15\u201319). In contrast, TheSu focuses first on the indexing of individual theses, i.e. treating every single declarative sentence as a 'claim', and then on their connection with supportive spans of text: the latter can be contiguous to their targeted theses or very far away in the text, as well as in other works from the same author or from different authors too (as will become clearer below). (2) While Argumentation Mining methods are generally concerned with textual cohesion and natural argumentation patterns, TheSu is interested in the coherence and justification of an author's ideas in her thought, inasmuch as it is exhibited in her textual production. This also differentiates TheSu from annotation  Figure 1. Fragment of a concept visualization of a TheSu map: Argumentative-Expository contexts linked to the theses in Aquane an ignis utilior sit instantiating the proposition \u2018Fire is better than water'. Every THESIS and SUPPORT that doesn't receive justifications or explanations is highlighted by 'none \u2013targets\u2192': it's desirable to be able to notice them at a glance because it can be proof that their speaker considered them clear and non-controversial enough not to spend more (supportive) words on their presentation, thus being the ideological \u2018building-blocks' of the whole argumentative discourse. schemes in Argumentation Mining that seem to be more independent from Walton's and RST's influence (e.g. Peldszus and Stede, 2013). An intellectual historian, while researching on an author's thought, usually tries to reach a comprehensive view of it in order to identify trends and elements of cohesion, incompatibility, and evolution. When the historian extends the scope of her research to include texts from different authors, her 170  \faim is usually to be able to discover traces of historical influences or innovations based on independent reasoning. Sometimes she tries to elucidate the author's texts by putting them in relation to others pertaining to the same culture or current of thought: when certain ideas are presented synthetically and without explanation, she can always look at works from different authors \u2014culturally and philosophically close to the first\u2014 to find their plausible sense and justifications (on current research practices in History of ideas cf. e.g. van den Berg et al., 2014, sec. 3). TheSu is intended as a tool to help the historian reach these aims, by providing databases for generating maps of the networks of ideas conveyed by texts, and arrange and filter them according to her interests (see Figure 1). TheSu is thus distinguished from the other annotation schemes in a way that can be summarized as follows: although it always starts from a text containing natural argumentation, it only uses it as a proof for the existence of a scientific discourse that the text's author intends to convey. The 'discourse' is composed of both explicitly stated enunciates and their implicit assumptions and alluded consequences, as well as all the explicit and implicit argumentative links between them. These are only 'scientific' in the sense that they are to be \u2018taken seriously' by the interpreter, who must always start by assuming the hypothesis that the author has legitimate reasons to believe in and present all of them: to test her hypothesis, the interpreter must thus do her best to find in the text all the supports that might qualify the claims as well founded and adopted critically by the author, and so 'scientifically' legit (in the context of their existence). In so doing, the interpreter cannot but be guided by a strong principle of charity2, and in this way detach the scientific discourse from the text up above a certain degree of \u2018charitable' arbitrariness. The structure of the scientific discourse, then, can not always correspond to the structure of the text, and the latter is only used as grounding for the reconstruction of the former. TheSu annotations, in addition, can serve the purpose of gathering organized data as a basis for logical and epistemological evaluations of an author's style of reasoning. To make these further analyses possible, the interpreter must be as non-judgemental as possible in the annotation phase: weird and weak as they may seem, every extra-logical 'argumentation' practice deserves the same space as the actual 'demonstrations'\u2014 adopting Perleman and Olbrecht-Tyteca's distinction (2013)\u2014 in the network of ideas. This also distinguishes TheSu from more \u2018normative', logically rigid, approaches in Argumentation Mining (e.g. Green, 2018a), and from the CRMinf Argumentation Model, an extension to the CIDOC CRM that complements CRMsci, a model for the structuring of metadata about contents and practices of current empirical sciences (Stead et al., 2019). In CRMinf, the epistemological evaluation of the arguments is embedded in the annotation itself (e.g. its class 'I3 Inference Logic' can only include \u00ABanything that is scientifically or academically acceptable as a method for drawing conclusions\u00BB, ib. p. 11), and much of the discourses' rhetorical contexts is thus ignored. Plutarch's Aq. has been chosen as a case study because of its short length and its elaborate, though very clear, argumentative structure. It is a rhetorical exercise where both the superiority of water and the superiority of fire are argued for in persuasive speeches that are symmetrical in extension as well as in cogency, and wherein no final solution is provided to the controversy. It contains way more 'argumentation' than 'demonstration', and its interesting rhetorical features have already been analysed by Milazzo (1991), although with a different approach. In this paper, its theses will only be quoted by their annotated paraphrases in English, which is the standard language for the TheSu sheets: considering that all the theses have been extracted from the original Greek text, in this case every paraphrasis is also a translation, original to this annotation and sometimes diverging from the previous ones \u2014including Helmbold's in Cherniss and Helmbold (1957)\u2014 to improve on clarity and faithfulness. The original (pre-annotated) text will be quoted in translation as well. 3 Encoding the Argumentative-Expository Systems Every TheSu XML sheet corresponds to at least one work to be annotated. Considering the general need for historians to keep track of the textual locus of every passage that they analyse and quote, it's better for the annotator to work on already-existing XML\/TEI editions of the texts, if suitably provided with milestone elements with IDs corresponding to the desired reference system. This has been the case with the adopted digital edition of Aq. Often TheSu elements need to include non-contiguous spans of text. These, in turn, can often be interpreted as composing multiple theses or supports (explicit or implicit) cumulatively, sometimes leading to the problem of overlapping hierarchies. For these two reasons stand-off markup has been chosen as the annotation method for TheSu: each of its elements has to refer to a span of text in another document, linked through xLink and xPointer. 2  See Davidson's 'Principle of Coherence' (Davidson, 1991). 171  \fEvery TheSu sheet contains an Argumentative-Espository System (AE System), that is theoretically defined as a set containing theses, their argumentative and expository supports, and the functional relations between the two. As will be shown below, this also needs to include a few more elements in its digital implementation. A 'thesis' is an instantiation of a declarative proposition at a certain point of the text representing the stance of its speaker. It can be explicit in the form of an enunciate (e.g. \u2018Putrefaction is the decay of liquids in the flesh', Aq. 957e) or implicit, e.g. in the form of a rhetorical question (e.g. \u2018[ Water is more useful to humans than fire ]' in \u00ABhow, then, should water not be more useful\u2026 ?\u00BB, 957b). A 'support' is a segment of text that is presented by its speaker in function of a part of the scientific discourse conveyed by the same text. A 'support' can: [1] provide justifications for the acceptance or refusal of a thesis or of another support (argumentative support): e.g. \u00ABIn most cases, it's not possible to use water without fire: in fact, it's more useful when it's heated, otherwise it's harmful\u00BB, 958a. [2] explain more clearly, stylistically, or in depth the meaning of another segment of text containing theses and\/or supports (expository support): e.g. \u00ABIsn't it more helpful what we always and continuously stand in need of, like a tool and an instrument, \u2026?\u00BB, 955f; [3] expand on an information conveyed by a thesis, favouring a more complete knowledge and understanding of it (expansive support or excursus): \u00AB\u2026 and (don't you see) that every sense partakes of fire, as it fabricates the vital principle, and especially sight, which is the keenest of the bodily senses, being an ignition of fire\u2026 ?\u00BB, 958e; [4] contextualize the interpretation and reception of another segment of text containing theses and\/or supports (contextualizing support): \u00ABIn fact, (about) the saying that sometimes humans exist without fire: humans can't at all exist (without it)\u00BB, 958b. The reader here may notice that in TheSu's annotation scheme the 'support' elements, having four distinct functions, include rhetorical uses that do not correspond directly to argumentative and expository aims. One can still speak of 'Argumentative-Expository Systems', though, because careful consideration of both the expansive and contextualizing supports is needed for a complete understanding of the argumentative and expository roles of the theses surrounding them, and of their linked segments of text. 'Theses' and 'supports' are encoded as THESIS and SUPPORT XML elements, both children of an AEsystem, which is in turn child of a work. Aq.'s AE System, in its current version, contains 259 manually annotated THESIS elements (corresponding to 334 theses, 56 of which are implicit) and 216 SUPPORT elements (121 implicit). These numbers are striking if the very short nature of the text is considered (1627 words in total). It's clear that a high amount of information on an author's thought and on her cultural context can always be extracted from even relatively small bits of text: mapping it in detail can be crucial to avoiding misinterpretations and misattributions. Every THESIS and SUPPORT must have its own ID, so that each can be targeted by SUPPORT elements through xPointer. THESIS elements' IDs are also necessary for the most original feature of the TheSu annotation scheme. Absent, to the best of my knowledge, from current Argumentation Mining techniques is the possibility of linking together unrelated argumentative-expository chains when converging towards the same idea. It is a need for the historian, when studying the thought of a certain author, to have a clear view of how the same theses are presented and argued for in different contexts, even when unrelated. For example, if the author does not provide supports for a judgement in a certain work or paragraph, it does not necessarily mean that she does not argue for it, or better explains it, elsewhere. To have a map where all its occurrences in different loci, with all their corresponding argumentative-expository apparatuses, are linked together, would naturally be helpful to the researcher. This is made possible, in TheSu, through the creation of a 'propositions' sheet containing only PROPOSITION elements (a modified version of THESIS for the annotation of non-textual declarative sentences), and by linking to their IDs all the textual THESIS elements instantiating them. In Aq., the proposition e.g. \u2018{ Water is more useful than fire }' is repeatedly argued for in different manners, and implicitly conveyed by the words in [a] 955f-956a, [b] 956c and [c] 957b. The thesis at [a] is the target of 5 supports, the one at [b] of 5 more, and the one at [c] of only 2. It is undesirable to keep these 12 supports fragmented in their respective rhetorical chains, as they all converge towards the same idea. Indeed, it is interesting to see how this proposition is argued for in all of its enunciative occurrences. Accordingly, it is preferable to connect each of the textual theses to their common abstract proposition within the same network. The usefulness of such a connection becomes even clearer if one imagines its extension to the whole textual production of an author, as well as to works from different authors. What follows is a non-exhaustive presentation of some of the required or optional attributes and subelements of the [i] THESIS and [ii] SUPPORT elements. 172  \f[i] Every THESIS has an @id, a @value (affirmative or negative) and a @quantity. It can sometimes be @implicit (boolean), as has been explained above. Every non-propositional THESIS can have one or more child elements instanceOf, each with a @propRef pointing to the corresponding PROPOSITION. A required child element is the speakersGroup, containing at least one speaker, corresponding to the person, group or entity the thesis is interpreted to be \u2018pronounced' by, with a @ref pointing to its name in an authority sheet. The THESIS's child element assent is used to specify whether the thesis is shared, unaccepted or actively attacked by its speaker (sub-element assentSpeaker with its @assentValue), or by the author of the work (assentAuthor). The child element thesisType mainly serves indexing purposes, as it classifies the THESIS through its sub-elements: value (epistemic \u2014 to specify with @valueTag whether the thesis is offered as the speaker's real stance, as a hypothesis, or fictitiously), macroThemesGroup (to specify the \u2018macroscopic' theme(s) of the thesis, e.g. 'physical', 'historical', 'axiological'), microThemesGroup (for the \u2018microscopic' theme(s) of the thesis, e.g. 'physiology', 'cosmology', 'dialectic'), and keywordsGroup (to point through keywordRef elements to the textual or implicit keyword(s) corresponding to the object(s) of the thesis). Note that each keywordRef's @ref links to the ID of a keyword that is a child of AEsystem. Separating the keywords from the theses becomes necessary due to the possibility of different theses including the same keywords: in 957c (\u00ABbut, in general, water (\u03C4\u1F78 \u1F54\u03B4\u03C9\u03C1) is so far away from being self-sufficient for selfpreservation or the bringing-forth of other things that lack of fire, for it, is even destruction\u00BB) the theses \u2018not(Water is self-sufficient for self-preservation)', \u2018not(Water is self-sufficient for the bringing-forth of other things)' and \u2018Without fire, water is destroyed' all share the textual keyword \u03C4\u1F78 \u1F55\u03B4\u03C9\u03C1. Each keyword can point to a segment of the annotated text or be \u2018implicit', and must always be tagged semantically through an attribute @namely, pointing to a class in a vocabulary sheet (e.g. 'water'). Although the choice of the controlled vocabulary can be left to the interpreter, all new exhaustive TheSu annotations should consider the keyword classes already used in the previous ones, to facilitate the linking of the novel theses to all the corresponding previous propositions. It is better not to refer to an ontology of real-world entities, both to free the classification from the need of specifying vague or untranslatable terms, and to avoid projecting alien categories of thought to different cultural and scientific contexts. More freedom can be granted in the choice of the classes for the 'macro-' and 'micro-themes', as coherent keywords give sufficient help for the discovery and aggregation of (quasi-)equivalent theses. Each of the microTheme and keywordRef elements also has an attribute @focus to specify, by order of rank, their relative prominence in the thesis: the one just quoted, \u2018Without fire, water is destroyed', is about 'water' and 'fire' and includes both as its keywords, but it's more relevant to an understanding of Plutarch's ideas on water than those on fire. The keywordRef linked to it has thus been given @focus = 1, and the other @focus = 2. keywordRef can be used as grounding for visualizable analyses such as the one in Figure 2, where fire- and water-related keywords are assigned a score ('Epistemic relevance') based on the quantity of THESIS elements containing them at different points of the text, weighted on the basis of their @focus. One can learn from such a graph that a comparative style is maintained (almost) throughout the text, instead of it featuring two \u2018separate' speeches on the individual excellence of each element: such an analysis can lead to interesting findings if compared to similar analyses of other works of the same genre. Other child elements of THESIS are recap and text. The former contains a short paraphrase in English of the thesis as interpreted and annotated: no logical formalization is required, as the annotation process must remain accessible to interpreters untrained in logic. The same goes for the PROPOSITION elements' recap: avoiding a strict logical formalization of the propositions allows the interpreter to consider as their instances theses that are not quite logically equivalent, but that can count as synonymous enough for the History of ideas, as is the Use in SUPPORTS  Form of SUPPORT as premises  Employed in justifications  THESES  Employed in explanations Employed in jstf.\/expl.? Unused All THESES  Figure 2. Relevance of fire- and water-related keywords to the theses conveyed by different contiguous spans of Aquane an ignis's text.  as illustrations in other forms  as examples  Justified? yes no yes no yes no yes no yes no yes no yes no  THESIS quantity 36 38 65 75 9 41 9 21 0 1 19 64 208 126  Total  Justified \/ Total  74  49%  140  46%  50  18%  30  30%  1  0%  83  23%  334  62%  Table 1. Theses in Aquane an ignis in relation to supports: by how many and in which forms they are employed, and by how many they are targeted. 173  \fcase with the thesis in the bottom-right corner of Figure 1 (quoting \u2018Fire is first, in relation to water') in respect to \u2018Fire is better than water'. Finally, the text points through its sub-element textRef (containing at least one segment with @from and @to) to the textual proof of the existence of the thesis at a certain point of the discourse. [ii] SUPPORT elements share with THESIS the attributes @id and @implicit. The sub-elements speakersGroup, assent, recap and text are present here as well. The first unique child element of the SUPPORT is targetsGroup, containing at least one target pointing through @ref to the ID of a supported element. Very useful is empolyedTheses, including one or more thesisRef (with @ref) to link to the theses in the SUPPORT's textual span that are actually presented to support the targeted element(s), discriminating between them and other non-relevant theses possibly annotated in the same text, thus solving ambiguities. For mainly indexing purposes, as with thesisType, each SUPPORT element contains a supportType, also necessary for the analysis of the reasoning styles of the discourses they are part of. While their child element value is identical to the one in thesisType, they also include their own function and form. The function's sub-elements are justification, explanation, expansion and contextualization, each with a @rank (default = 4) representing their relative centrality to the support (most central = 1). The idea is that every support, as everything else in a cohesive discourse, is always at the same time justifying, expository, expansive and contextualizing of its surroundings to a certain degree (cf. Perelman, Olbrechts-Tyteca, 2013 [1958], p. 203), and that its speaker, in order to achieve different rhetorical effects, simply choses to make one or another of these functions more prominent than the others. The possibility of ranking the functions solves the problems that would come from having to choose only one of them even in cases where there is enough ambiguity to make it seem impossible. For the annotation of whether the support, when 'justifying', serves the purpose of arguing for or against its target(s), justification has been given the attribute @for ( = 'acceptance', 'refutation' or 'mix'). Finally, using the element form the interpreter can classify the support by its rhetorical type, referring through @formTag to any class in a typology contained in an authority sheet. The TheSu standard typology of supportive forms is meant to be very simple and intuitive for intellectual historians: among the 'justifying' forms, the 'logical premise' is a sentence from which the supported target can be inferred by deduction, the 'illustration' is a particular case from which the conclusion can be derived by induction, the 'authority' is an appeal to an authoritative figure that adheres to the targeted idea, etc. Table 1 illustrates a quantitative analysis strictly dependent on the elements SUPPORT, function and form: it is not surprising that in a rhetorical work such as Aquane an ignis a very high amount of theses are given argumentative support (62%), but it is not necessarily expected that 'illustrative' supports are twice the deductive 'premises' (140 to 74), characterizing the speech as scarcely 'logical' in tone and much more 'exemplary'. It is also interesting that theses employed in supports tend here to attract further argumentation, especially the 'premises' (49% justified) and 'illustrations' (46%), in contrast with the theses not used in supports (23%). This breakdown is only a small tile of the mosaic that is Plutarch's personal argumentation style, waiting for further analyses to be combined with and compared to. Conclusion The previous sections have described the essential features of the TheSu annotation scheme, its theoretical framework, and some of the potential uses of a TheSu sheet. This exposition has focused on the methodological usefulness of this kind of argumentation and exposition mapping for an historian working on a text, but TheSu can also be helpful for an optimal, transparent and reusable, exposition of the basis and results of her research: a historian's \u2018secondary' interpretation of a certain text \u2014e.g. its ideas' dependency from the ones in a contemporary philosophical current, or their ideological or popular nature\u2014 always depend on a \u2018primary' interpretation of the argumentative and expository chains it is composed of. Storing these primary interpretations in easily-accessible TheSu databases would help with the evaluation of the secondary interpretations proposed by the historian, and would facilitate the work of future researchers who wish to build upon her research and generate new interpretations from the argumentative-expository material. This is only possible thanks to digital interfaces and database interrogation techniques, and would otherwise be too difficult and\/or time consuming using traditional, non-digital methods. Acknowledgements This publication is part of the research project Alchemy in the Making: From Ancient Babylonia via GraecoRoman Egypt into the Byzantine, Syriac, and Arabic Traditions, acronym AlchemEast. The AlchemEast  174  \fproject has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (G.A. 724914)3."
	},
	{
		"id": 28,
		"title": "Leitwort Detection, Quantification and Discernment",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Racheli Moskowitz",
			"Moriyah Schick",
			"Joshua Waxman"
		],
		"body": "1 Introduction 1.1 The Leitwort in Literature In music, a Leitmotif is a recurring musical phrase (a 'motif') which is used to 'lead' or guide the listener to recall something or make a connection. In literature, the Leitwort ('leading word') plays a similar function. A certain word, or word root, is unusually repeated several times in a passage, in a way that jumps out at the reader, in order to establish a theme. Optionally, once established, it might then be echoed in a later passage to recall that theme. Here is one of the many Leitworte which Pinault (1986) identifies in his analysis of stylistic features in The Arabian Nights. In the story 'The City of Brass', over the course of a few consecutive pages, in poetry and prose, there is repetition of words with the Arabic root \u202B \uFEE3\uFEEE\u062A\u202C\/ mawt, meaning death. \u202B \u0623\uFE91\uFE8E\u062F\u06BE\uFEE2 \u0627\uFEDF\uFEE4\uFEEE\u062A\u202C\/ abadahum mawt, \"Death destroyed them\". \u202B \uFEDB\uFE84\u0633 \u0627\uFEDF\uFEE4\uFEEE\u062A\u202C\/ ka`s al-mawt, \"the cup of doom\". \u202B \u0625\u0646 \uFEDB\uFE8E\u0646 \uFEE3\uFEEE\uFE97\uFEF2 \uFEBB\uFEFC\u0629 \uFEB3\uFEA0\uFE8E\u0644\u202C\/ in kana mawti mahtuman cala cajal, \"when my death was decreed all at once\". \u202B \uFE91\uFE88\uFEB3\uFEE2 \u0627\uFEDF\uFEA4\uFEF2 \u0627\uFEDF\uFEAC\u064A \uFEFB \uFBFE\uFEE4\uFEEE\u062A\u202C\/ bi-ism al-hayy alladhi la yamut, \"in the name of the Living, who dies not\". \u202B \u064F\uFBFE\uFEB0\uFEA7\uFEAE\uFED3\uFBAD\uFE8E \u0627\uFEDF\uFEB8\uFBFF\uFEC4\uFE8E\u0646 \uFEDF\uFEFA\uFEE7\uFEB4\uFE8E\u0646 \u0625\uFEDF\uFEF0 \u0627\uFEDF\uFEE4\uFEE4\uFE8E\u062A\u202C\/ yuzakhrifuha al-shaytan lil-insan ila al-mawt, \"Satan adorns it for man to lead him to death\". (MacNaghten edition) 177  \fThis literary technique has been discussed by scholars as it is found in sacred texts such as the Hebrew Bible (Alter, 1981) and the Koran (Wansbrough, 1978). It is similarly employed in the works of Goethe, Nietzsche, Heidegger and others. Then, one task of those who analyze these texts is to discern the use of a Leitwort and explain its purpose. 1.2 Leitworte and the Hebrew Bible Various interpreters of the Hebrew Bible within the past century have taken note of the use of the Leitwort, though they differ as to its parameters and purpose. In Buber (1927), and in Buber and Rosenzweig (1936), the purpose of a thematic repetition in a passage is to reveal or clarify a meaning in the text, or to emphasize that meaning. They select Leitworte based on their subjective estimation of the word's significance, rarity, and the degree of repetition. These are all factors which contribute to a word capturing the attention of the reader. The repetitions can occur densely in a single passage, or can be distributed throughout the text. They draw connections between passages in which the same Leitwort occurs, as indicating an allusion or thematic echo. In one famous example from Buber, the seven scenes of revelation that compose the Abraham story arc are all tied together by the use of the term \u202B \u05E8\u05D0\u05D4\u202C\/ ra'ah \/ 'see' in each passage, and unlike other translations which obscure this connection, Buber and Rosenzweig's German translation preserves the Leitwort by translating it consistently throughout. At around the same time, Umberto Cassuto also took an interest in Leitworte in the Hebrew Bible. Cassuto was first the chief rabbi of Florence and subsequently a professor at University of Florence and at the University of Rome La Sapienza. Like Buber and Rosenzweig, Cassuto (1961, published posthumously) considered that Leitworte in the Bible served the purpose of establishing the theme of a passage or emphasizing a point. However, he was more selective in what types of repetitions he would consider to be bona fide Leitworte. In Cassuto's view, repetitions are only interpretable if they occur within a coherent passage and occur in a multiple of 3, 7, or 10. As illustrated below, he discusses the threefold repetition as a way of emphasizing a point within or between passages. The numbers seven and ten are chosen because they are particularly significant to an ancient Israelite author from a ritual and spiritual perspective. For instance, the number seven is echoed from the Creation narrative to a weekly cycle of seven days, a seven-year cycle before each Sabbatical year appears, and seven Sabbatical years leading up to the Jubilee. Similarly, the number ten brings the Ten Commandments immediately to mind. We present an extended example to illustrate the position of Leitworte in Cassuto's textual analysis. In Exodus 2:2-6, in the background of Egyptian governmental decrees to drown all firstborn Hebrew boys, Moses is born. This story, Passage A, contains the word-root \u202B \u05E8\u05D0\u05D4\u202C\/ ra'ah \/ 'saw', repeated three times: Moses' mother 'saw [\u202B \u05B7\u05D5\u05B5\u05EA\u05B6\u05BC\u05E8\u05D0\u202C\/ vateire`] him that he was a goodly child', hid him for as long as she could, and then placed him in an ark on the riverside. When Pharaoh's daughter visited the river, 'she saw [\u202B \u05B7\u05D5\u05B5\u05EA\u05B6\u05BC\u05E8\u05D0\u202C\/ vateire`] the ark among the flags, and sent her handmaid to fetch it. (6) And she opened it, and saw it [ \u202B\u05B7\u05D5\u05B4\u05EA\u05B0\u05BC\u05E8\u05B5\u05D0\u05D4\u05D5\u05BC\u202C \/ \u25CC\u05B7vatir`eihu], even the child; and behold a boy that wept. And she had compassion on him, and said: 'This is one of the Hebrews' children.' ' (Jewish Publication Society Translation) A few verses later (Exodus 2:11-15), Moses, after being raised as a prince in Pharaoh's house with his biological mother as his nursemaid, decides to check on his enslaved brethren, sees their suffering, and reacts. Once again, the thrice-repeated index root is ra'ah, though there are others marked. We label this Passage B. \u202B \u05B7\u05D5\u05B7\u05D9\u05B0\u05BC\u05E8\u05D0\u202C,\u202B \u05D9\u05D1 \u05B7\u05D5\u05D9\u05B4\u05B6\u05BC\u05E4\u05DF \u05DB\u05BC \u05B9\u05D4 \u05B8\u05D5\u05DB \u05B9\u05D4\u202C.\u202B\u05B4\u05E2\u05B0\u05D1\u05B4\u05E8\u05D9 \u05B5\u05DE\u05B6\u05D0\u05B8\u05D7\u05D9\u05D5\u202C-\u202B \u05B7\u05DE\u05B6\u05DB\u05BC\u05D4 \u05B4\u05D0\u05D9\u05E9\u05C1\u202C,\u202B \u05B0\u05D1\u05B4\u05BC\u05E1\u05B0\u05D1\u05B9\u05DC\u05B8\u05EA\u05DD; \u05B7\u05D5\u05B7\u05D9\u05B0\u05BC\u05E8\u05D0 \u05B4\u05D0\u05D9\u05E9\u05C1 \u05B4\u05DE\u05B0\u05E6\u05B4\u05E8\u05D9\u202C,\u202B \u05B7\u05D5\u05B7\u05D9\u05B0\u05BC\u05E8\u05D0\u202C,\u202B\u05B6\u05D0\u05B8\u05D7\u05D9\u05D5\u202C-\u202B\u05E9\u05C1\u05D4 \u05B7\u05D5\u05D9\u05B5\u05B5\u05BC\u05E6\u05D0 \u05B6\u05D0\u05DC\u202C \u05B6 \u05B9 \u202B \u05B7\u05D5\u05D9\u05B0\u05B4\u05BC\u05D2\u05B7\u05D3\u05BC\u05DC \u05DE\u202C,\u202B\u05D9\u05D0 \u05B7\u05D5\u05D9\u05B0\u05B4\u05D4\u05D9 \u05B7\u05D1\u05BC\u05D9\u05B4\u05B8\u05BC\u05DE\u05D9\u05DD \u05B8\u05D4\u05B5\u05D4\u05DD\u202C .\u202B \u05B5\u05E8\u05B6\u05E2\u05B8\u05DA\u202C,\u202B \u05B8\u05DC\u05B8\u05DE\u05BC\u05D4 \u05B7\u05EA\u05B6\u05DB\u05BC\u05D4\u202C,\u202B\u05E9\u05C1\u05E2\u202C \u05B8 \u202B \u05B8\u05DC\u05B8\u05E8\u202C,\u202B\u05E9\u05C1\u05D9\u05DD \u05B4\u05E2\u05B0\u05D1\u05B4\u05E8\u05D9\u05DD \u05B4\u05E0\u05B4\u05E6\u05BC\u05D9\u05DD; \u05B7\u05D5\u05D9\u05BC \u05B9\u05D0\u05B6\u05DE\u05E8\u202C \u05B4 \u202B\u05B2\u05D0\u05B8\u05E0\u202C-\u202B\u05E9\u05B5\u05C1\u05E0\u05D9\u202C \u05B0 \u202B \u05B0\u05D5\u05B4\u05D4\u05E0\u05B5\u05BC\u05D4\u202C,\u202B\u05E9\u05B4\u05BC\u05C1\u05E0\u05D9\u202C \u05B5 \u202B \u05D9\u05D2 \u05B7\u05D5\u05D9\u05B5\u05B5\u05BC\u05E6\u05D0 \u05B7\u05D1\u05BC\u05D9\u05BC\u05D5\u05B9\u05DD \u05B7\u05D4\u202C.\u202B \u05B7\u05D1\u05BC\u05D7\u05D5\u05B9\u05DC\u202C,\u202B \u05B7\u05D5\u05D9\u05B0\u05B4\u05BC\u05D8\u05B0\u05DE\u05E0\u05B5\u05D4\u05D5\u05BC\u202C,\u202B\u05B7\u05D4\u05B4\u05DE\u05B0\u05BC\u05E6\u05B4\u05E8\u05D9\u202C-\u202B \u05B6\u05D0\u05EA\u202C,\u202B\u05B4\u05DB\u05BC\u05D9 \u05B5\u05D0\u05D9\u05DF \u05B4\u05D0\u05D9\u05E9\u05C1; \u05B7\u05D5\u05B7\u05D9\u05B0\u05BC\u05DA\u202C \u202B \u05D8\u05D5\u202C.\u202B \u05D0\u05B5\u05B8\u05DB\u05DF \u05E0\u05D5\u05B7\u05B9\u05D3\u05E2 \u05B7\u05D4\u05B8\u05D3\u05B8\u05BC\u05D1\u05E8\u202C,\u202B\u05E9\u05C1\u05D4 \u05B7\u05D5\u05D9\u05BC \u05B9\u05D0\u05B7\u05DE\u05E8\u202C \u05B6 \u05B9 \u202B\u05B7\u05D4\u05B4\u05DE\u05B0\u05BC\u05E6\u05B4\u05E8\u05D9; \u05B7\u05D5\u05D9\u05B4\u05BC\u05D9\u05B8\u05E8\u05D0 \u05DE\u202C-\u202B\u05E9\u05C1\u05E8 \u05B8\u05D4\u05B7\u05E8\u05D2\u05B0\u05B8\u05EA\u05BC \u05B6\u05D0\u05EA\u202C \u05B6 \u202B \u05B7\u05DB\u05B2\u05BC\u05D0\u202C,\u202B\u05B7\u05D4\u05B0\u05DC\u05B8\u05D4\u05B0\u05E8\u05D2\u05B5\u05E0\u05B4\u05D9 \u05B7\u05D0\u05B8\u05EA\u05BC\u05D4 \u05D0 \u05B5\u05B9\u05DE\u05E8\u202C--\u202B \u05B8\u05E2\u05B5\u05DC\u05D9\u05E0\u05D5\u05BC\u202C,\u202B\u05E9\u05C2\u05E8 \u05B0\u05D5\u05E9\u05C1 \u05B5\u05B9\u05E4\u05D8\u202C \u05B7 \u202B\u05E9\u05B0\u05C2\u05DE\u05B8\u05DA \u05B0\u05DC\u05B4\u05D0\u05D9\u05E9\u05C1\u202C \u05B8 \u202B\u05D9\u05D3 \u05B7\u05D5\u05D9\u05BC \u05B9\u05D0\u05B6\u05DE\u05E8 \u05B4\u05DE\u05D9\u202C .\u202B\u05B7\u05D4\u05B0\u05D1\u05B5\u05BC\u05D0\u05E8\u202C-\u202B\u05E9\u05C1\u05D1 \u05B7\u05E2\u05DC\u202C \u05B6 \u05B5\u202B\u05B4\u05DE\u05B0\u05D3\u05D9\u05B8\u05DF \u05B7\u05D5\u05D9\u05BC\u202C-\u202B\u05E9\u05C1\u05D1 \u05B0\u05D1\u05B6\u05BC\u05D0\u05B6\u05E8\u05E5\u202C \u05B6 \u05B5\u202B \u05B7\u05D5\u05D9\u05BC\u202C,\u202B\u05E9\u05C1\u05D4 \u05B4\u05DE\u05B0\u05E4\u05B5\u05BC\u05E0\u05D9 \u05B7\u05E4\u05B0\u05E8\u05E2 \u05B9\u05D4\u202C \u05B6 \u05B9 \u202B\u05E9\u05C1\u05D4; \u05B7\u05D5\u05D9\u05B0\u05B4\u05BC\u05D1\u05B7\u05E8\u05D7 \u05DE\u202C \u05B6 \u05B9 \u202B\u05DE\u202C-\u202B \u05B7\u05D5\u05D9\u05B0\u05B7\u05D1\u05B5\u05E7\u05BC\u05E9\u05C1 \u05B7\u05DC\u05B2\u05D4\u05E8 \u05B9\u05D2 \u05B6\u05D0\u05EA\u202C,\u202B\u05B7\u05D4\u05B8\u05D3\u05B8\u05BC\u05D1\u05E8 \u05B7\u05D4\u05D6\u05B6\u05BC\u05D4\u202C-\u202B\u05E9\u05B7\u05C1\u05DE\u05E2 \u05B7\u05E4\u05B0\u05BC\u05E8\u05E2 \u05B9\u05D4 \u05B6\u05D0\u05EA\u202C \u05B0 \u05B4\u202B\u05B7\u05D5\u05D9\u05BC\u202C (11) And it came to pass in those days, when Moses was grown up, that he went out unto his brethren, and looked [vayar`] on their burdens; and he saw [vayar`] an Egyptian smiting [makeh] a Hebrew, one of his brethren. (12) And he looked this way and that way, and when he saw [vayar`] that there was no man, he smote [vayakh] the Egyptian, and hid him in the sand. (13) And he went out the second day, and, behold, two men of the Hebrews were striving together; and he said to him that did the wrong: 'Wherefore smitest [takeh] thou thy fellow?' (14) And he said: 'Who made thee a ruler and a judge over us? thinkest thou to kill me [haleharegeni], as thou didst kill [haragta] the Egyptian?' And Moses feared, and said: 'Surely the thing is known.' (15) Now when Pharaoh heard this thing, he sought to slay [laharog] Moses. But Moses fled from the face of Pharaoh, and dwelt in the land of Midian; and he sat down by a well. (Jewish Publication Society Translation) 178  \fBefore we turn to Cassuto's analysis, a brief discussion of Passage B can help us understand the general phenomenon of Leitwort. Note that while \u202B \u05E8\u05D0\u05D4\u202C\/ ra`ah, 'saw', is repeated three times, this is evident only when reading the passage in the original Hebrew, rather than in the English translation above. It appears twice in verse 11; the first time it is rendered as 'looked' and the second time as 'saw'. Meanwhile, in verse 12, the word 'looked' appears translating a different word-root, \u202B \u05E4\u05E0\u05D4\u202C\/ panah, while the word 'saw' translates \u202B \u05E8\u05D0\u05D4\u202C\/ ra'ah. These threefold repetitions seem deliberate. Firstly, an author can include or omit details while still advancing the narrative, so many times, a repeated word didn't truly need to appear. Secondly, Hebrew has synonyms. An author can select from an inventory of terms. For instance, in the same passage, the roots nakhah ('smite', 'strike') and harag ('kill') are used to refer to Moses' killing of the Egyptian taskmaster. In analyzing Passage B, Cassuto notes the threefold repetition of the root ra'ah. He says it is for emphasis. It matches the threefold repetition of the same root in passage A. The parallel is not accidental, but it is to stress that just as Moses' mother, and Pharaoh's daughter saw and had mercy on him, so did Moses take pity and have mercy on his brethren. Cassuto also notes the threefold repetitions of smiting and killing. 1.3 The Problem of Leitwort Operationalization Despite the value of Leitworte as a literary technique that unifies a text and enriches the experience of the reader, any attempt to accurately identify Leitworte is somewhat problematic, particularly in a large and varied corpus such as the Hebrew Bible. The problem, essentially, is that language is repetitive by nature. Zipf (1945) discusses the relative frequencies of all words in a corpus, repetitions within clusters, as well as intervals between clusters, and these phenomena are observed in the absence of deliberate stylistic repetition. Subject matter or narrative concerns can require a certain word to appear more than once in a story. For instance, the first chapter of Carlo Collodi's Le Avventure di Pinocchio contains repetitions of the Italian word legno, 'wood' and pezzo, 'piece'. This is because it is where we first encounter a talking, weeping, and laughing piece of wood. Even if Collodi had no intention of drawing the reader's attention to these words, the narrative would be senseless without them. Luhn (1958) demonstrates that this occurs in non-narrative texts as well, and establishes the content of a text based on the non-deliberate repetition of words or word roots as an author advances his arguments or elaborates on an aspect of a subject. Additionally, certain words, such as function words (e.g., the articles 'a' and 'the'), are extremely common in language because they assist communication. These words will necessarily occur numerous times throughout a text, and will be totally unrelated to any theme that the writer wishes to emphasize. In fact, Luhn suggests that words that occur above a certain frequency threshold can generally be considered insignificant. In the midst of a sea of non-significant word repetitions, identification of meaningful Leitworte for further study poses a challenge which can be addressed in one of two ways. The first solution is to maximize subjectivity, simply relying on a human interpreter to name which words should be considered Leitworte. This traditional approach has the benefit of embracing the nuances of human insight and the finely-honed skills of expert analysis, but can also be considered arbitrary and subjective by its very nature. An alternative solution, which we implement in this paper, is to introduce objective measurement tools in an attempt to quantify significance of repeated words in a given passage. Of course, our algorithm cannot truly appreciate a text, and standardized rules do not take intuitive understanding into account. However, our program has the advantage of working systematically to find every candidate Leitwort, in contrast to human experts who are subject to the limitations of their scanning and matching capability and who may be biased by selective interest in certain terms. This novel approach allows us to move towards greater objectivity in analysis, and is a great tool for scholars who want to consider every potential Leitwort in the text. In the next section, we detail our programmatic approach. Our goals were to devise a quantitative measure of repetition significance, and to compare output of our program to the list of Leitworte identified by Cassuto, a traditional expert with a relatively systematic approach. To that end, our operational definitions are modeled on Cassuto's definitions but strip out the subjective components of his expert analysis.  2 Our Approach For clear and consistent Leitwort identification, several practical questions must be answered. First, what constitutes a 'word'? Second, what is a qualifying number of repetitions? Third, how closely spaced must the repetitions be? Fourth, how is significance of candidate words defined? Here we explain how we addressed each of these questions in designing our program. 179  \f2.1 Reduction to Lexemes or Roots Since Semitic languages such as Hebrew are inflected, and the typical Leitwort is based on repetition of the word's root, we first reduce the words in the corpus to their lexemes. For the Hebrew Bible, we use the ETCBC dataset described in Roorda (2015), which contains manual marking of rich linguistic features by human experts. One such feature is the lexeme, which is a close approximation to the root. In Hebrew, most words contain a triliteral root which conveys a core meaning. For instance, \u202B \u05D0\u05D5\u05E8\u202C\/ `or has the meaning of 'light'. In the ETCB dataset, words with this root are divided into two separate lexemes, \u202B \u05D0\u05D5\u05E8\u202C\/ `or ('light') and \u202B \u05DE\u05D0\u05D5\u05E8\u202C\/ ma`or ('luminary', thing which gives light). The lexemes are stemmed versions of the full word, stripping out definiteness, gender, number and person. Thus, the full word \u202B \u05B4\u05DC\u05B0\u05DE\u05D0\u05D5\u05B9\u05E8 \u05B9\u05EA\u202C\/ lim'orot \/ 'as luminaries' in Genesis 1:15 is marked with the lexeme \u202B \u05DE\u05D0\u05D5\u05E8\u202C\/ ma`or while the word \u202B \u05B0\u05DC\u05B8\u05D4\u05B4\u05D0\u05D9\u05E8\u202C\/ leha`ir \/ 'to give light' in the same verse is marked with the lexeme \u202B \u05D0\u05D5\u05E8\u202C\/ `or. The ETCB dataset does not have a root feature. We differ here from Cassuto, who primarily considers repetitions of roots. However, it is noteworthy that many of these lexemes are also the simple root (such as the \u202B \u05D0\u05D5\u05E8\u202Cexample above). Of the 788 sevenfold lexemebased Leitwort candidates our algorithm discovered across the Pentateuch, 81% consisted of triliteral roots. Many of the non-root lexeme candidates are names of nations or places. 2.2 Counting Repetitions Following Cassuto, our candidate Leitworte must occur a multiple of 3, 7, or 10 times in a passage. We gravitate towards Cassuto's definition of Leitwort for a few reasons. Many modern Biblical interpreters (such as Elchanan Samet of Yeshivat Har Etzion) employ both Buber and Cassuto-type Leitworte in their analyses, but consider the more rigorously defined Cassuto-type Leitworte as especially significant (Grossman, 2011). Further, as discussed above, Cassuto shows that these are meaningful numbers for an ancient Israelite author, and he consistently demonstrates that thematic words are repeated this precise number of times, or a multiple thereof. Indeed, we are treating this threefold and sevenfold repetition as a mark of authorial deliberateness \u2013 that the author has set out to employ the Leitwort style. If a word were repeated by chance, simply because it is the topic of a passage (see the legno and pezzo examples above) or because it is a commonly occurring word (such as 'said'), then it mostly would not occur specifically as a sevenfold repetition. 2.3 Scanning for Repetitions We scan for repetitions in the text, in a moving window. For face validity, we require a certain minimum density of repetition. Buber did not require close proximity; the seven Abraham scenes that he connects with the root \u202B \u05E8\u05D0\u05D4\u202C\/ ra'ah \/ 'see' span 178 verses over 8 chapters. Cassuto only identified Leitwort occurring within self-contained passages, but personally determined section and paragraph boundaries based on his own close reading analysis. Neither of these approaches is appropriate to our method, as both rely on subjective expert judgement to decide whether a given set of repetitions occurs within an acceptable space. To define objective limits, we turned to the historical Jewish segmentation scheme of the sidra: the entire Pentateuch is chanted by a reader in synagogues over the course of a lunar year, one portion each week, on the Jewish Sabbath, though there are modifications due to holidays. The Pentateuch was divided into 54 such portions, or sidrot. While the calendar influenced the number of portions, scholars segmented the text at appropriate positions, such that there is often a consistency in the narrative or legal codes within the text. To make use of this narrative consistency, we only count repetitions within a sidra. Another Biblical segmentation scheme, of Christian origin, is the well-known series of chapter divisions (e.g. Genesis 1, Genesis 2), which breaks up the full text into chapters of about 30 verses each. We further require our repetitions to occur within a maximum window of 60 verses (approximately 2 chapters). Thus, if a word randomly occurs 4 times at the start of a sidra and much later has a sevenfold repetition, for a total of 11 occurrences, the sevenfold repetition will still remain a candidate. Once a qualifying repetition has been found, we continue scanning along two pathways: one in which the passage stops with the most recent verse (and can thus be far smaller than 60 verses in length), and one in which it continues and allows for higher multiples to be identified. Because our sections have flexible starting and ending points, a separate method must ensure that word appearances from other sections are kept distinct. In line with Cassuto's numerical definition of Leitworte, an eighth appearance of a candidate word in the same passage should disqualify the word. However, we would not wish to incorrectly disqualify a candidate merely because it occurs in an unrelated passage later in the 180  \fsidra. Using his idiosyncratic paragraph divisions, Cassuto would find a sevenfold repetition within a paragraph and ignore an unassociated occurrence one or two paragraphs earlier. Lacking such boundary lines, we create a buffer zone around each of our identified Leitwort passages. This zone is defined as \u00BC the number of sentences of the passage span, and we require a total absence of the candidate word within that zone. Thus, a word will qualify as a Leitwort candidate if it occurs 7 times within a 16-verse span but does not appear at all in the 4 preceding and 4 subsequent verses. By requiring the word to appear in this 'island,' we create de facto passage boundaries in a flexible way. 2.4 Filtering for Significance Finally, we rate the candidate words for significance. In making estimation of significance the last step in our process, we diverge from the traditional expert-reader model. Scholars such as Cassuto and Buber would start with an impression that a word was significant, in the sense of meaningful and important. To Buber, if such a word was relatively rare (an undefined term) and also repeated within a story, it was a Leitwort. Cassuto required a precise number of repetitions and did not restrict based on rarity, but only examined words that he deemed especially significant rather than identifying every threefold or sevenfold repetition. Indeed, it would be simplistic to say that all of them are significant; these numbers can occur by chance just like any other. Therefore, after systematically compiling a list of all sevenfold repetitions within our corpus, we employ a tf-idf measure to weed out the most clearly insignificant of them. The term frequency (tf) is the number of times a word appears in a given document, while the inverse document frequency (idf) is the log of the total number of documents N divided by the number of documents that contain the word. If a word is frequent in the current document and infrequent elsewhere, then the product of the tf and the idf will be high. The 'documents' we use for this computation are the 54 aforementioned sidrot, since the text in each such division will typically be of a consistent genre (e.g. genealogy, legal code, narrative) and topic (e.g. trials in the wilderness). We stress that the purpose of this tf-idf ranking is not to discover the emphatic and thematic words. The specific numerical repetition establishes that. Rather, our aim was to filter out common yet highly insignificant words, which will occur in sevenfold repetition (along with eightfold repetition, ninefold repletion, etc.) purely by chance. For this reason, we set our tf-idf threshold very low, at 0.07. After examining a small portion of unfiltered candidates, we chose this value because it could retain words that appeared thematically relevant, while excluding common words with high frequency throughout the corpus but no discernable relevance to the specific passage. We did not use a simple stoplist of frequent words since a common word might be extremely significant in a given context. For instance, in Genesis 1-2, in which God creates the Universe in a sequence of speech acts, the lexeme \u202B \u05D0\u05DE\u05E8\u202C\/ amar \/ 'said' occurs 28 times (a multiple of 7) and has a tf-idf score of 0.13, above our threshold of significance. It also occurs seven times in Deuteronomy 5-7 with a non-significant tf-idf score of 0.04.  3 Results In the five books of the Hebrew Bible, we discovered a total of 788 potential Leitwort candidates that appeared a multiple of seven times in an island of text. Of these, 332 (or 42%) exceeded our tf-idf threshold and were counted as significant. Passage span ranged from 5 to 60 verses; and candidate lexemes were repeated within these passages 7, 14, 21, 28, 35, 42, or 49 times. As would be expected, threefold repetitions had shorter passage spans on average, and many fewer of them were deemed significant. We compiled a comprehensive list of Cassuto's Leitworte and compared them against the output of the program. Cassuto wrote commentary on the first 13 chapters (out of 50) of Genesis and on the entire (40 chapter) book of Exodus, identifying 164 Leitworte, of which 142 were of simple word or root repetitions. Of these root repetitions, 59 represented a sevenfold recurrence. For the same group of chapters, we found 207 potential candidates appearing a multiple of seven times, of which 102 (49%) exceeded our tf-idf threshold. Table 1 cross-tabulates our results with Cassuto's. Twenty words were deemed significant by our program and also discussed by Cassuto, 82 are marked at Leitworte by our program only, 39 by Cassuto only, and 105 words that do not appear in Cassuto's work were originally flagged by our program but fell below our significance threshold.  181  \fAlgorithm  Cassuto  Yes  No  Yes  Total: 20 Genesis: 8 Exodus: 12  Total: 39 Genesis: 18 Exodus: 20  No  Total: 82 Genesis: 17 Exodus: 65  Total: 105 Genesis: 23 Exodus: 82  Table 1: Cross-tabulation of the results of Cassuto and our algorithm. 'Yes' means that it appears in the list (and, for the algorithm, deemed significant). Cells representing agreement of the two sources are italicized. A few facts are apparent from these results. We see that our algorithm identified many more potential Leitworte overall than Cassuto. Also, Cassuto and the algorithm agreed about 50% of the time, and were much more likely to agree that a word was non-significant than that it was significant. Cassuto discussed many Leitworte that were not accepted by the algorithm, and vice versa. Finally, it is noteworthy that the results differ substantially based on specific text. Cassuto described almost as many Leitworte in the first 13 chapters of Genesis as in the 40-chapter Exodus. Meanwhile, the algorithm flagged repetitions with similar density across the two books and consistently identified about half of them (52% in Genesis, 48% in Exodus) as potentially significant. Cassuto and the algorithm therefore find about the same number of Genesis Leitworte, with few of Cassuto's appearing in the computer-generated list, whereas the algorithm finds more than twice as many Exodus Leitworte as Cassuto does. If Cassuto's work is held up as the gold standard, one can say that the algorithm achieved 19.6% precision (32.0% in Genesis, 15.6% in Exodus) and 33.9% recall (30.8% in Genesis, 37.5% in Exodus). This suggests that it is able to catch about a third of the Leitworte discerned by an expert, and introduces a high number of spurious candidates, particularly in Exodus. Valid Leitworte can be missed by the program either because they are never identified or because they are rejected as insignificant. Close inspection of the data reveals that only 5 of Cassuto's Leitworte that were flagged by our algorithm fell below our tf-idf significance threshold. Most did not meet the algorithm's criteria for being a sevenfold repetition. This may be because we were restricted to using lexemes while Cassuto primarily used roots or was more flexible about linguistic features, or because we lacked his sharp boundaries of paragraph and story. Therefore, we may have inadvertently cut off our 'passages' before the end of a scene, or disqualified a true Leitwort because it re-appeared in an unrelated context within our buffer zone. Further work can address some of these issues. If, on the other hand, the objective algorithmic approach is considered the ideal, one can say that Cassuto obtained 33.9% precision (30.8% in Genesis, 37.5% in Exodus) and 19.6% recall (32.0% in Genesis, 15.6% in Exodus). This suggests he found about a fifth of possible Leitworte in his chosen text, and that about a third of his self-defined Leitworte are valid. Due to the limits of human attentional capacity, it would be practically impossible for a person to manually identify all existing Leitworte in such a complex text. Humans can be biased by their own interests to overlook many details, which can lead both to false positive and false negative detection errors. Notably, Cassuto's list for Genesis, the text in which he first perceived Leitworte and which evoked tremendous enthusiasm for the task, has the highest recall and lowest precision compared against the algorithm. The truth probably lies somewhere between these extremes. Dismissing 80% of the algorithm's suggestions as invalid merely because Cassuto did not talk about them ascribes omniscience to the human expert, which is absurd. Similarly, it is ridiculous to say that Cassuto's analysis is only meaningful if its tf-idf score falls above our program's cut-off. The low overlap between Cassuto's results and the algorithm's is evidence that the two methodologies bring different perspectives and different strengths. Only a human being can explain the meaning of a Leitwort in context and weave it into a consistent tapestry with other methods of literary analysis. However, the ability to systematically evaluate every instance, and to apply objective criteria undiluted by personal bias, are core benefits of computerized Leitwort detection. The best use of such digital tools will be to allow merging of these two approaches by using algorithms before or after the human eye. Modern scholars of Biblical literature might use our program to systematically generate a list of repetitions to consider in their analyses, or consult its quantitative information (e.g. tf-idf scores) to consider whether their initial impressions might be distorted and in need of further scrutiny. Thus, objective methodology can become a thread woven into the subjective tapestry. 182"
	},
	{
		"id": 29,
		"title": "From copies to an original: the contribution of statistical methods",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Amanda Murphy",
			"Raffaella Zardoni",
			"Felicita Mornata"
		],
		"body": "1 Introduction 1.1. The missing original veronica According to tradition, the veronica, the medieval relic conserved in St Peter's, was the sudarium St Veronica offered Christ, on which his face was imprinted. Despite vast documentation from 1200 to 1500,1 and the witness of pilgrims (such as Dante and Petrarch) visiting the relic in Rome, and countless extant copies, the exact aspect of the veronica is unknown.2 There is great variance among its copies: e.g. Christ's face can be transfigured, or show signs of suffering, be with\/without the crown of thorns, with open\/closed eyes. The first systematic study of copies of the relic in order to find the original was the work of Karl Pearson in 1887.3 He compared literary and liturgical texts, and lined up images in chronological order, seeking reasons for the lack of continuity between representations. Scholars from literature, history, history of art, and theology have continued this research and the year 2000 saw a growth in interest in the topic;4 there is, however, still no definitive answer to the question of what the original medieval veronica looked like. 1.2. Veronica Route project The Veronica Route project5 (VR) joined this field of research in 2010, with the creation of an open, expanding, interdisciplinary database of artistic and literary citations, ordering \u2018the infinite copies' of the image in an online catalogue. VR holds 4500+ tagged objects, particularly from the Middle Ages.6 The classifications were carried out manually, following a traditional methodological approach.7 The collected data was rendered 1  In 1289 the Veronica was declared the most important relic in St Peter's (Coll. Bull. SS. Eccl. Vat., I, 214). The image kept in St Peter's Basilica shows indistinct markings and has not yet been studied. We can only be certain of the size of the medieval veronica (40 x 37 cm) thanks to the 14th century frame kept in the Vatican (Sturgis, 2000:75). 3 It is a curious coincidence that Pearson worked out the statistical index of correlation, a starting point for several statistical methods. 4 Belting (1990); G. Morello (1997); Hamburger (1998); Kessler, Wolf (1998); D'Onofrio (1999); Frugoni (1999); Morello, Wolf (2000); Di Blasio (2000); Burgio (2001); Di Fruscia (2013). 5 Veronica Route was presented at the conference \u2018The European Fortune of the Roman Veronica', Magdalene College, University of Cambridge, April 2016. 6 The works are signalled and sent in by volunteers together with the information found in loco: the sources are considered trustworthy, unless an error of attribution or dating is easily demonstrable. Many works recorded in Veronica Route do not yet have captions as complete as those in museum catalogues. Although the VR database covers all centuries, the richest and most relevant period for the present purpose is pre-1600. 7 With time, it will be interesting to be able to adopt automatic face analysis tools, which are not yet appropriate for the recognition of iconographic characteristics, although they already work well in estimations of eye, nose and mouth positions, the degrees of different emotional expressions, etc. as shown in the Selfiecity project, classified as one of the most significant examples of 'distant viewing'. 2  184  \favailable through tags marking iconographic characteristics, the dimensions of time (dating) and space (geographical positioning), with a visualisation function of the results allowing maps of veronicas to be manipulated.8 Thus classified according to >50 features, the images were turned into information. Data mining with appropriate statistic methodologies on a statistically significant number of images has yielded the definition of significant models and new interpretative hypotheses\/research paths about the relic which \u2018for three centuries, exerted such a great influence on the literary and artistic artefacts of our ancestors'.9  2 The variants of the iconographic subject and their geographical spread To be able to identify the prototypes of the veronica, we used two different statistical tools, the index of transversality and multivariate analysis which juxtapose and aggregate the various tagged features. 2.1 Index of transversality The recurrent iconographic characteristics, the subjects (St. Veronica, angels, sudarium, etc.), the various transversal themes (Roman relic, Strozzi, St. Spirit, etc.) and the supports (painting, miniature, sculpture), available for each veronica in the Veronica Route database, are all dichotomic variables (0-1 presence\/absence). The frequencies of the dichotomic variables to be compared through the centuries have to be normalised in order to make the data homogeneous. For this reason we identified the index of transversality in time, which was calculated as follows: \uD835\uDC3C\"# =  %\uD835\uDC39\uD835\uDC5F\uD835\uDC52\uD835\uDC5E\"# \u2044\uD835\uDC39\uD835\uDC5F\uD835\uDC52\uD835\uDC5E\" \u2211 # , %\uD835\uDC39\uD835\uDC5F\uD835\uDC52\uD835\uDC5E# \u2044\uD835\uDC39\uD835\uDC5F\uD835\uDC52\uD835\uDC5E\u2211 # ,  \uD835\uDC3E = \uD835\uDC50\u210E\uD835\uDC4E\uD835\uDC5F\uD835\uDC4E\uD835\uDC50\uD835\uDC61\uD835\uDC52\uD835\uDC5F\uD835\uDC56\uD835\uDC60\uD835\uDC61\uD835\uDC56\uD835\uDC50,  \uD835\uDC46 = \uD835\uDC50\uD835\uDC52\uD835\uDC5B\uD835\uDC61\uD835\uDC62\uD835\uDC5F\uD835\uDC66  Figure 1 shows that the graphic representation of some indices clearly show the strong coupling of DOUBLEPOINTED BEARD-OPEN EYES and the late appearance of the feature CLOSED EYES. closed eyes  double-pointed beard  open eyes  350 300 250  i n d e x  200 150 100 50 0 1200  1300  1400  1500  1600  century  1700  1800  1900  2000  Figure 1. Index of transversality  8  Software developed by Giacomo Aletti, professor of Probability and Statistical Mathematics, Dipartimento di Scienze e Politiche Ambientali, Università degli Studi di Milano. 9 \u2018In this age when scholars attempt to trace the journey of a saga from India to Iceland, I hope that a description of the origin and development of a legend which has exercised such a huge influence on the literary and figurative works of our forebears over the centuries will not be considered superfluous.' (Karl Pearson, Die Fronica, p. 94, our translation) 185  \f2.2 Multivariate analysis: K-Means Cluster Analysis From 1300 on, the considerable number of veronicas makes multivariate analyses on the available data meaningful. Another fruitful approach to investigating the correlations between the features is to aggregate veronicas from the same time frame into homogeneous groups, using the methodology K-means Cluster.10 This algorithm considers each veronica like a point in a space of N dimensions (N = the available fields for each record). The value of each field is interpreted as distance from the origin along the corresponding spatial axis. In the K-means methods the original choice of a value for K determines the number of clusters that will be found. The analyst experiments with different values of K and each set of clusters is then evaluated: the one which shows the clearest interpretation of the data is chosen. It is an iterative process, which begins by identifying K points as seeds, and continues by aggregating all the other records, assigning each record to the closest centroid cluster. After this process the new cluster centroids are calculated, and, according to the proximity rule, the cluster to which each point belongs is recalculated. This iterative process ends when the cluster boundaries stabilize. Once the clusters have been defined, the results can be interpreted. In order to describe the elements shared by the veronicas belonging to the same cluster, the matrix of the \"final centroids\" must be analysed, which, being dichotomous variables, quantifies the influence of each variable within the cluster. Therefore, Final Cluster Center \u00E81 corresponds to the predominance of that variable in that particular cluster, and vice versa. 2.3 The fourteenth century  10  1  2  3  without crown  0,937  0,922  0,897  open eyes  0,984  0,882  0,759  transfigured  0,921  0,745  0,655  St.Veronica  0,000  0,725  0,000  cruciform halo  0,556  0,667  0,379  double-pointed beard  0,444  0,353  0,448  head of Christ  0,032  0,196  0,000  cut out  0,492  0,039  0,138  sudarium  0,508  0,020  0,000  dark face  0,048  0,020  0,000  fleury cross  0,032  0,020  0,034  arma Christi  0,032  0,020  0,000  suffering  0,000  0,020  0,069  transparent veil  0,000  0,020  0,000  ascent to Calvary\/Calvary  0,000  0,020  0,000  open mouth\/visible teeth  0,063  0,000  0,069  dark veil  0,048  0,000  0,000  Sts.Peter and Paul  0,048  0,000  0,000  imago pietatis  0,048  0,000  0,000  green crown  0,016  0,000  0,000  crown of thorns  0,000  0,000  0,069  blank veil  0,000  0,000  0,000  triple veil  0,000  0,000  0,000  folds  0,000  0,000  0,000  monochrome  0,000  0,000  0,000  angel\/s  0,000  0,000  0,862  Mass of Saint Gregory  0,000  0,000  0,000  Implemented in the software SPSS Statistics (Analyze \/ Classify \/ K-Means Cluster procedure). 186  \fway of the Cross Frequency of veronicas (143)  0,000 63  0,000 51  0,000 29  Figure 2. Clusters in the fourteenth century The works in the 1300s tagged with ROMAN VERONICA11 can be analysed in three clusters (excluding those with the tags BADGE and TEXT). In Figure 2, the predominant characteristics are in yellow, the absent ones in green. In this period all the veronicas are characterised, homogeneously, by the serene face of Christ, without a trace of suffering. Differences are found in the subjects showing the veronica: cluster 1 includes almost all the cases of the sudarium alone (with the tags CUT OUT and DARK FACE); cluster 2 aggregates the figure of St Veronica (particularly in Lombardy where we can find one of the first pictorial representations of the saint);12 cluster 3 aggregates angels holding up the sudarium, positioned throughout eastern and central Europe. 2.4 The Fifteenth century The 1400s are the most popular century for the image13 (with 1122 works compared to 264 in the 1300s); in the second half of the century, veronicas appear with Christ's face bearing signs of suffering and drops of blood, and with the iconographic subject of the ascent to Calvary14. The considerable number of works and their variations in this century determined the choice to analyse the data in four clusters, in the interest of the best interpretation of the dominant characteristics in the variants. The 709 works tagged with ROMAN VERONICA, (excluding those tagged BADGE and TEXT) were thus aggregated: in Cluster 1 (234 veronicas, coloured in light blue in Figure 3) we find the transfigured face of Christ; in Cluster 2 (139 veronicas, dark blue) St Veronica; in Cluster 3 (166 veronicas, red) the new features of the Passion; and Cluster 4 (170 veronicas, orange) the sudarium. Figure 3 displays the distribution of the works across Europe: we see a dominance of the figure of St Veronica with the transfigured face of Christ in France, where St Veronica is considered the evangeliser of the region,15 and in Flanders (where she is the patron saint of linen and cloth merchants); the first veronicas with signs of suffering appear in all European countries, and it becomes slightly prevalent as a characteristic in Germany, while in England there is a prevalence of the sudarium as the iconographic subject.  11  The tag ROMAN VERONICA is used when the veronica is the only subject or the main subject, and not when the veronica is part of another work, such as Arma Christi, Madonna of the Seven Sorrows, and such like. 12 Stefano Candiani, The iconography of Veronica in the Lombardy region, late XIII-early XV centuries, in A. Murphy et al. Ed., The European Fortune of the Roman Veronica in the Middle Ages, Convivium Supplementum 2017, Turnhout: Brepols. p. 264. 13 'From the 14th century, wherever the Roman Church went, the veronica would go with it' (Neil MacGregor and Erika Langmuir, 2000, p.92) 14 The moment in which the veil was imprinted was not initially linked to the ascent to Calvary, but to Jesus' public life. 15 Her tomb is still preserved at Saint Seurin (Bordeaux), on the pilgrims' way to Santiago di Compostela. 187  \fFigure 3. 1400s - distribution of the clusters 2.5 The Sixteenth century There are 1075 works from the 1500s in Veronica Route. In a 4-cluster analysis of the 948 works, we find the SUDARIUM in cluster 1 (165 veronicas, including those characterised by DARK FACE and CUT OUT); cluster 2, SAINT VERONICA (always dominant in France), 295 veronicas; cluster 3, 281, SIGNS OF SUFFERING (a feature which becomes dominant in Italy); and in cluster 4, with 207 veronicas, the ASCENT TO CALVARY and BLANK VEIL. This feature, meaning that the face of Christ imprinted on the cloth is no longer visible, seems to shift  attention away from the relic kept in Rome and onto the woman's pious gesture. In actual fact, the Protestant Reform and the Sack of Rome of 1527 interrupted \u2013 for various reasons \u2013 the history of the Roman relic.  3 Research on the Roman relic: validation The last investigation concerns the relic kept in St Peter's, of which there are no photographic reproductions and which has never been an object of study. In Veronica Route the tag ROMAN RELIC is assigned when the historical sources of the work refer directly to the relic. This feature is present only in 3.4% of the 1081 veronicas that are catalogued up to the end of the 1400s (excluding the veronicas with the tags TEXT and BADGE). These are decidedly small numbers for making a predictive analysis of the characteristics of the ROMAN RELIC. The investigation therefore proceeded by evaluating the concentrations of the tag ROMAN RELIC in the other features and ordering them according to decreasing values. In the graph, the average concentration of 2% - index 100 \u2013 is indicated by the blue line and the characteristics with a higher index are those that correspond most to the ROMAN RELIC; the clear emergence of the features CUT OUT and DARK FACE can be seen, whereas the features DOUBLE-POINTED BEARD - CROWN OF THORNS - SUFFERING, positioned underneath the blue line are clearly separate from the Roman relic.  188  \f500  <=1400  400  i n d e x  300 200 100  in g fe r su f  th or of n ow cr  do ub le -  po in te d  be a  ey op en  ns  rd  es  n ow cr ith ou t w  ns fig ur ed  rm uc ifo cr  tra  ha  lo  ce fa rk da  cu  to  ut  0  Figure 4. Concentration of the ROMAN RELIC CUT OUT and DARK FACE are the features characterising the Mandylion in the Vatican (Figure 5),16 a work likened to the medieval relic.17 Identifying the Vatican Mandylion with the medieval Veronica would not be contradicted by the data, but the proportion of works tagged as CUT OUT and DARK FACE compared to the total works in the database (5 in 1200, 3 in 1300, 53 in 1400) and the late spread of the iconography, suggest the need for further research on this.  Figure 5. Left, the Vatican Mandylion, Lipsanoteca of the Pontifical Palaces, Vatican, next to works tagged CUT OUT and DARK FACE: Veronica d'oro, 1368 ca. Prague, Cathedral Treasury; Santa Veronica col velo tra i SS. Pietro e Paolo, 1430, altar Santa Maria del Monastero, Manta.  Conclusions Firstly, the Veronica Route project intends to continue investigating the origins of the medieval relic's iconography. Secondly, we intend to break up the temporal arches (linked so far to centuries) so as to align them better with historical events, such as the Holy Years, in order to investigate the origins and development of the relic variations more precisely. Lastly, an exploration of the features which do not seem to derive from the Roman relic, but which have nevertheless become highly famous, would be an interesting new direction.  16  The Mandylion was once considered the most ancient reproduction of the Face of Christ, even though it is documented in Rome only from 1517. Until 1870 it was kept in the Church of St Sylvester the First (San Silvestro in Capite), and is now kept in the Vatican. 17 G. Morello (2012) p. 78. 189"
	},
	{
		"id": 30,
		"title": "FORMAL. Mapping Fountains over time and place. Mappare il movimento delle fontane monumentali nel tempo e nello spazio attraverso la geovisualizzazione",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Pamela Palomba",
			"Emanuele Garzia",
			"Roberto Montanari"
		],
		"body": "1 Introduzione L'esistenza umana si organizza e pensa se stessa anche in termini spaziali, è nel suo rapporto con lo spazio che l'individuo struttura la sua esperienza, in maniera situata (Merleau-Ponty, 1965), tanto che si può a buon diritto affermare che l'esistenza è spaziale e lo spazio è esistenziale. Lo spazio in tal modo, inteso in senso antropologico, cessa di essere solo un'astrazione geometrica e definisce il campo di studi oggetto delle spatial humanities. L'interesse crescente per lo spazio negli studi storiografici è uno degli aspetti della svolta culturalista (Torre, 2008) dell'ultimo quarto del XX sec., che, con l'abbandono del concetto di spazio assoluto del sistema cartesiano in favore di un concetto di spazio relativo, ha comportato una svolta che viene definita spatial turn ossia il passaggio a un sistema tolemaico che distingue tra spazio assoluto e relativo, tra geografia e corografia (Cosgrove, 2003). La prima, con le sue capacità descrittive basate sulla matematica e sul rilievo scientifico, e la seconda, imperniata su un'impostazione di tipo più visuale e letterario, ma in grado di connettere però la dimensione storica a quella geografica. In sostanza si tratta di una svolta verso un'interpretazione simbolica del paesaggio che tenga conto della sua doppia anima di spazio naturale e culturale: quella che si riferisce a processi naturali e sociali, e quella che corrisponde alle conseguenze delle azioni umane che lo trasformano. Lo spazio in questo senso da entità geometrica astratta si trasforma in luogo, ossia l'espressione peculiare di uno spazio geografico.  191  \fLa \u2018peculiare' natura del luogo lo definisce come l'ordine secondo il quale diversi elementi vengono distribuiti entro rapporti di coesistenza, gli uni affianco agli altri; un luogo è dunque una configurazione di posizioni, una indicazione di stabilità. Al contrario lo spazio è un incrocio di entità mobili, è in qualche modo il prodotto dell'insieme dei movimenti che si verificano al suo interno e lo animano, orientandolo, rendendolo contingente tanto da tramutarlo in unità polivalente di programmi conflittuali o di prossimità contrattuali (de Certeau, 2001). Lo spazio non è il semplice scenario dell'azione storica, ma un prodotto significativo e determinante di cambiamento. Ne deriva che un movimento produce uno spazio e ne traccia la storia. Ogni narrazione che si rispetti si struttura lungo una sequenza di eventi producendo quella che può essere chiamata \u2018la forma del tempo' e privilegiando così la componente cronologica, ma tutte le narrazioni implicano un mondo di estensione spaziale. Alcuni teorici infatti riconoscono un più che evidente collegamento tra spazio e tempo, come si evince dalla concezione di cronotopo di Mikhail Bakhtin (1981) intesa come \u00ABl'intrinseca connessione di relazioni spaziali e temporali\u00BB, con il tempo che fornisce la quarta dimensione dello spazio. In questa prospettiva la narrazione è immaginabile come \u00ABla rappresentazione del movimento all'interno delle coordinate di spazio e tempo\u00BB, con gli eventi marcati dall'intersezione di assi orizzontali e verticali in un intreccio dinamico tra superficie e profondità (Bakhtin, 1981). Sulla base di queste premesse, con il presente lavoro di ricerca si è inteso rilevare nella catalogazione e visualizzazione della distribuzione e del movimento delle fontane storiche di Napoli le condizioni narratologiche per produrre una narrazione spaziale attraverso la geo-visualizzazione, un ramo della Gis Science che sviluppa tecniche e strumenti disegnati per rendere visuali dei fenomeni spaziali (Craine and Aitken, 2009). L'operazione di traduzione dei concetti di spazio e luogo in quelli rispettivamente di mappa e itinerario, intesi come linguaggi simbolici e antropologici dello spazio e come i due poli dell'esperienza (de Certeau, 2001), ha condotto alla messa in scena della loro interazione, attraverso l'impiego di una piattaforma di database management system come nodegoat1, in grado di produrre uno scenario in cui i luoghi figurano come \u2018OGGETTI' della mappa, e gli itinerari come tracciati degli spostamenti delle fontane nel tempo e nello spazio. L'operazione effettuata consente così di arrivare a una più profonda comprensione dei contesti spaziali in cui i beni culturali sono inseriti come testimonianze vive e multilivello, capaci di raccontare una storia stratificata sia in senso cronologico che spaziale, in cui possano essere rilevabili quindi come parti in relazione complessa con i luoghi ai quali conferiscono valore di civiltà. Per comprendere e diffondere queste informazioni con contenuti multidisciplinari il data storytelling si rivela uno strumento efficace nella divulgazione di fenomeni complessi. Per trasformare questi dati in informazioni utili alla conoscenza è necessaria la visualizzazione così come il racconto attraverso una storia. Alcuni strumenti come penne, mappe e calcolatrici possono essere considerati artefatti cognitivi che migliorano la nostra conoscenza, amplificando i processi cognitivi coinvolti nell'interazione con le rappresentazioni tipiche del mondo esterno. Nell'Information Visualisation un ruolo fondamentale è dato dal data storytelling. Gli esseri umani hanno sempre utilizzato le storie per trasmettere informazioni, valori culturali ed esperienze attraverso mezzi tecnologici che si sono evoluti nel tempo (si pensi alla scrittura, alla stampa e oggi ai computer). Una buona narrazione trasmette una mole di informazioni in un formato facilmente assimilabile dal fruitore o dallo spettatore (Segel and Heer, 2010). La visualizzazione delle informazioni attraverso i dati, combinata con un adeguato storytelling, permette di produrre rappresentazioni visive molteplici. Ogni visualizzazione può essere utilizzata per raccontare una storia e le diverse modalità sono funzionali ai differenti tipi di storia. Proprio per queste loro caratteristiche, le tecniche di visualizzazione dimostrano di essere un valido strumento nelle mani dello studioso interessato a valorizzare il patrimonio informativo collegato ai rapporti fra la città, la sua evoluzione e i beni culturali.  1  Pim van Bree and Geert Kessels. 2013. nodegoat: a web-based data management, network analysis & visualisation  environment, http:\/\/nodegoat.net from LAB1100, http:\/\/lab1100.com 192  \f2 Caso d'uso: FORMAL, Mapping fountains over time and place. Il nome del progetto, Formal, oltre ad essere un acronimo delle parole che lo specificano (Mapping Fountains OveR tiMe And pLace), si ispira al termine \u2018formale' che indica uno dei segmenti di cui si compone la rete degli acquedotti ed è dunque anche usato per definire un canale principale di alimentazione delle fontane pubbliche. Testimoni di eccezione delle trasformazioni politiche, sociali e urbanistiche della città, le fontane pubbliche di Napoli hanno una storia complessa, fatta di traslazioni, mutilazioni, cambiamenti che si è tentato di restituire in forma semplessa grazie all'impiego di nodegoat, un DBMS web-based in grado di processare, analizzare e visualizzare dataset complessi in modalità relazionale, diacronica e spaziale. La prima fase del progetto è consistita nel reperimento di tutti i dati della ricerca, di tipo bibliografico, cartografico e iconografico, necessari alla compilazione del database. Successivamente si è passati alla costruzione del modello dei dati (data modelling) , dapprima a livello concettuale attraverso la creazione del dataset e sulla base delle esigenze della domanda di ricerca e, in seguito, a livello logico tramite l'inserimento dei dati nelle schede di struttura approntate. La piattaforma nodegoat ha consentito la creazione di un database completamente personalizzato da parte di un utente non esperto e perfettamente rispondente alle esigenze di ricerca. Il data modelling nell'ambito umanistico è largamente percepito come un processo epistemologico, piuttosto che come un processo ontologico. L'interfaccia dell'applicazione del database può far nascere infatti nuove opportunità o creare sfide ulteriori. La prima operazione necessaria è stata la definizione dei type principali di consultazione; il type principale consente di leggere nella scheda di ciascun oggetto (object) a esso riferito tutte le informazioni anagrafiche, geografiche e storiche (Figura 1). Sono state inserite 28 fontane storiche pubbliche, che coprono un arco cronologico che va dal XVI sec. al XXI sec., 14 di esse hanno subito almeno uno spostamento, con il caso eclatante della fontana del Nettuno che ha avuto ben otto trasferimenti (cfr. Figura 3). Figura 1. Scheda dell'object Fontana della Sirena  Gli obiettivi sono stati essenzialmente due: da un lato catalogare e ordinare il materiale di studio raccolto e sistematizzarlo attraverso la produzione di schede anagrafiche e multimediali che descrivessero le caratteristiche di ciascuna fontana (object); dall'altro ottenere per ciascun object la visualizzazione geografica della sua posizione nello spazio e nel tempo con il tracciamento dello spostamento da un luogo all'altro nelle  193  \fdiverse epoche storiche (sub-object). Quest'ultima operazione è consistita in particolare nell'approntare l'apparato di coordinate leggibili attraverso la \u2018Geographical Visualisation' (Figura 2).  Figura 2. Geographical visualisation degli spostamenti della Fontana Medina Per quanto riguarda la collocazione si è scelto di visualizzare sulla mappa, attraverso l'uso di punti e linee, la posizione e il percorso compiuto dalla fontana in caso di diverse collocazioni, usando il sistema di coordinate geografiche sia della collocazione originaria che delle collocazioni successive per georeferenziare il punto di interesse (point of interest). Nel caso di luogo non più esistente si è fatto ricorso alla cartografia storica per identificare il punto corrispondente da georeferenziare. Le coordinate geografiche del punto di interesse, così come le corrispondenti datazioni di collocazione e\/o spostamento, sono inserite usando la sezione sub-object della relativa scheda dell'object di ciascuna fontana (Figura 3).  Figura 3. Sezione sub-object con gli spostamenti della Fontana del Nettuno  Ciascun type è collegato in maniera incrociata (cross-referencing) agli altri type rilevanti, consentendo così l'esplorazione di una serie di relazioni (network analysis) tra i dati che ha rivelato la sua efficacia dal punto di vista della ricerca, come da quello del possibile impiego dei dati stessi per la costruzione di narrazioni. Nel primo caso infatti è stato possibile analizzare il dataset in base alle diverse domande di ricerca, ad esempio attraverso il filtro del personaggio storico (committente, artista), piuttosto che per cronologia, per elemento decorativo ricorrente o per toponimo. Relativamente invece alla potenzialità narratologica è stato previsto il type \u2018Story' che contiene una selezione di brani tratti dalla letteratura periegetica del XVII sec., nello specifico le Notitie del bello, dell'antico e del curioso della città di Napoli per i signori forastieri date dal canonico Carlo Celano napoletano, divise in dieci giornate di Carlo Celano, che ci informano sulla conformazione urbanistica della città nonché sulla descrizione delle fontane pubbliche della Napoli del 1692. Impiegando la network analysis per indagare il campo Story è possibile avere un quadro chiaro e immediato di tutte le fontane esaminate dal cronista nel 194  \fcorso della specifica giornata e trarne considerazioni spazialmente orientate in merito alla fonte impiegata. A mero titolo di esempio si può rilevare che grazie alla network analysis è stato possibile ragionare sugli spostamenti programmati dal cronista per la stesura del testo letterario della singola giornata: attraverso la visualizzazione dei dati collegati in maniera relazionale appare chiaro il percorso spaziale che struttura la narrazione e nello stesso tempo pianifica e suggerisce itinerari al destinatario (quante e quali fontane sono descritte da Celano nella Giornata V con schema delle relazioni topografiche e topologiche che le legano, cfr. Figura 4).  Figura 4. Network analysis del campo Story  Appare chiaro che un tale uso delle fonti spazialmente strutturato, relazionale e incrociato rende in questo caso le fontane un possibile espediente, dal punto di vista narrativo storicamente delineato, per costruire storie che si avvalgano dei documenti, digitali e non, per generare contesti narrativi scientificamente validi. Nell'ottica del riuso creativo delle fonti digitalizzate e open access si è fatto ricorso ad esempio all'edizione digitale curata dalla Fondazione Memofonte alla quale si rimanda, all'interno della singola scheda, in maniera contestuale al luogo del testo. Allo stesso modo si è fatto ricorso allo stradario ufficiale del Comune di Napoli integrando nel DBMS il dataset con licenza IODL (Italian Open Data License) per effettuare il collegamento georeferenziato con l'attuale mappa cittadina. Infine si è prestata particolare attenzione alla scelta delle opzioni di visualizzazione dello \u2018Scenario' di geovisualizzazione allo scopo di creare un data storytelling efficace dal punto di vista della fruizione per la futura pubblicazione su web. Sulla base della modellizzazione proposta da Segel e Heer (Segel and Heer, 2010) si è optato per un approccio che prevede una posizione di controllo dei contenuti erogati (Author-driven approach), ideale per lo storytelling e la comunicazione di contenuti educativi, temperandolo con uno di tipo più interattivo (Reader-driven approach) che consentirà all'utente di esplorare lo scenario, interrogandolo in base a diverse chiavi di ricerca con la possibilità di produrre forme più complesse e personalizzate di analisi.  3 Sviluppi futuri La fase successiva del progetto prevede l'implementazione di un'interfaccia pubblica per la fruizione web con la produzione di scenari dedicati in base alla fascia di utenza e l'approfondimento dell'indagine tramite la network analysis e il data storytelling per la scelta di una narrazione da visualizzare, valorizzare e divulgare. Sarà inoltre analizzato e geovisualizzato un secondo dataset relativo alle fontane scomparse e alla distribuzione delle acque affioranti cittadine, che in parte le alimentavano, rintracciate attraverso le fonti storiche.  4 Conclusioni Ciò che di interessante emerge dall'interazione tra la mappa e l'itinerario è una dimensione spaziale narrativa in cui la mappa giustifica l'itinerario e l'itinerario definisce la mappa come spazio geografico e culturale insieme, non dunque solo una carta geografica, ma anche un libro di storia. Abbiamo infatti da un lato la mappa, che ha una funzione topica ossia di definitore di luoghi e, dall'altro, un racconto fatto di 195  \fspostamenti ossia topologico, relativo alla deformazione delle figure. La metodologia usata aiuta a preservare e presentare le complessità che sono insite nelle fonti storiche e nel loro impiego incrociato attraverso il deep mapping. La visualizzazione e il data storytelling invece si configura come un'interfaccia interattiva utile per la ricerca nelle digital humanities, in particolare nel cultural heritage per la storia della città e delle sue trasformazioni, con risvolti interessanti anche per un impiego a servizio della fruizione dei beni culturali e delle imprese creative. Lo strumento scelto ci consente di esplorare contemporaneamente le due dimensioni, topica e topologica, in senso sincronico (es. analizzare e visualizzare quali e quante fontane nello stesso secolo) e diacronico (es. tappe del loro percorso all'interno della città nel corso del tempo). Tra queste due determinazioni vi sono dei passaggi che portano alla conclusione che i racconti spaziali effettuano un lavoro che trasforma i luoghi in spazi o gli spazi in luoghi, con un'azione creativa e performativa, delimitando con le loro attività su di essi una scena da narrare che ancora vive."
	},
	{
		"id": 31,
		"title": "Paul is dead? Differences and similarities before and after Paul McCartney’s supposed death. Stylometric analysis of transcribed interviews",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Antonio Pascucci",
			"Raffaele Manna",
			"Vincenzo Masucci",
			"Johanna Monti"
		],
		"body": "Introduction  Paul McCartney's supposed death (dated 9th November 1966 because of a car accident) represents a legend which does not belong to the music business only but embraces other worlds, both for Paul McCartney's fame given by Beatles' everlasting success, and because of many stories born around this episode. Paul is dead (PID) theory represents one of the most controversial legends in the history of music, enough to be still debated after more than half a century, during which numerous stories are born, some feeding and some other damping its truth. In this paper, we show the results of a stylometric analysis conducted on Paul McCartney's interview transcriptions using three different approaches in order to detect differences and similarities in his speeches before and after 9th November 1966. Our research is based on: \u2022 the Beatles Interviews Database (http:\/\/www.beatlesinterviews.org\/), a collection of one-hundred sixty-three interviews from 1962 to 1984; \u2022 YouTube subtitles, which we manually corrected, if necessary, listening to the audio. To the best of our knowledge, this research represents one of the very first stylometric analyses on interview transcriptions. In this paper, we also present the Let IT Corpus (Paul McCartney's Interview Transcriptions), composed of fifty-two documents concerning interviews before 9th November 1966 and fifty-two documents concerning interviews after 9th November 1966. Let IT Corpus is still in its embryonic stage: we foresee to expand it with further texts so that it can be used for more accurate analyses in the near future.  197  \fThe strongest supporters of PID theory claim that immediately after his death, Paul McCartney has been replaced by a lookalike. There are several theories that even today sustain the veracity of PID, many of which spread by the Beatles themselves, who sometimes enjoyed including subliminal messages in their songs. For example the celebrated John Lennon's whisper in the song I'm so tired, if listened backwards, seems to say Paul is dead, man: miss him! miss him! miss him!. The Abbey Road's album cover also shows at least ten references to Paul McCartney's death. On the other hand, some theories remove all doubts, claiming that Paul McCartney never died and all PID hypotheses are nothing but a business choice, which has contributed to add extra charm to Beatles' success. In October 1969, the Beatles' press office categorically denied PID rumours, labelling them as a load of old rubbish. PID theory has been investigated in literature (Cartocci, 2005) and in automatic-recognition (Holland et al., 2014). The present contribution is organized as follows: in Section 2 we show Related Work. The Let IT Corpus is described in Section 3, in Section 4 we describe three different approaches adopted in the analysis and their results. In Section 5 the stylistic differences and similarities detected by the linguistic analysis are thoroughly discussed. Conclusions are in Section 6. In Section 7 we introduce Future Work.  2  Related Work  CS is the statistical analysis of writing style (Zheng et al., 2006) and it is used to identify or profile the author of a text. The main assumption of Authorship Attribution (AA) is that each author operates choices which are influenced by sociological (age, gender and education level) and psychological (personality, mental health and being a native speaker or not) factors (Daelemans, 2013) which determine a unique writing style. In AA some studies are being conducted on speech transcriptions. In 2014 (Herz and Bellaachia, 2014) investigated the authorship of Barack Obama's speechwriters on a corpus composed by thirty-seven speech transcriptions. They based their research on the supposition that Barack Obama has four principal speechwriters and deal with the AA of Barack Obama's speeches with four different approaches, that reached different results, but still showing that CS can be used to differentiate authors who write in a similar style. (Airoldi et al., 2006) conducted a similar research on Ronald Regan's radio speeches. The corpus they used for their investigation is composed of a thousand thirty-two radio addresses delivered by Ronald Reagan between 1975 and 1979. The scholars focused the experiment on three-hundred twelve radio addresses for which no direct AA evidence is available, and they concluded that in 1975, Ronald Reagan drafted seventy-seven speeches and his collaborators drafted seventy-one, whereas over the years 1976-1979, Ronald Reagan drafted ninety speeches and his collaborator Peter Hannaford drafted seventy-four speeches. The study of (Herz and Bellaachia, 2014) and that of (Airoldi et al., 2006) share a problem: it is not possible to know the accuracy of the AA results of their study. CS is also useful in studying changes in the style of an author over time. As argued by (Rybicki, 2015) time is one of the most significant factors for the evolution of the literary lexicon. With this in mind, some researches are conducted on stylochronometry (for a survey, see (Stamou, 2007)), namely the study of the change of style correlated to the passing of time. (Forsyth, 1999) differentiates the style of the poet William Butler Yeats between younger Yeats and older Yeats, devising along the way a measurement he calls a youthful Yeatsian Index. (Van Hulle and Kestemont, 2016) use sylometry to periodize Samuel Beckett's works, finding stylistically innovative change in his late style. Lastly, the findings of (Evans, 2018) show that the dramatic style of Aphra Behn over the course of her 20-year career, can be divided in three different phases. Obviously we must keep in mind that our analysis is based on transcription of speeches, and therefore not on written texts. Until now, to the best of our knowledge, no stylistic research analysis has been carried out to detect differences and similarities in interview transcriptions before and after Paul McCartney's supposed death.  3  Let IT Corpus  For our research we investigated the Beatles Interviews Database1, a collection of one-hundred sixtythree transcription of Beatles' interviews from 1962 to 1984 created in 1997 by Jay Spangler and now managed by Jude Southerland Kessler and Suzie Duchateau. The website also contains a songwriting and 1http:\/\/www.beatlesinterviews.org\/  198  \frecording database, a collection of Beatles' movies, quotes and pictures. We also investigated thirty-five Beatles' interviews available on YouTube: in this case we analyzed the automatic captions generated by speech recognition, and we corrected texts if necessary. In each interview, we isolated Paul McCartney's speeches and we created a document for each interview. The Let IT Corpus is a very small balanced corpus composed of one-hundred four documents belonging to two different classes: I) before (composed by fifty-two documents concerning interviews before 9th November 1966) and II) after (composed by fifty-two documents concerning interviews after 9th November 1966). A few texts belonging to the after class found on YouTube date after 2000. The majority of texts of the Let IT Corpus are from the Beatles Interviews Database (32 before texts and 25 after texts, including a few chunks). The corpus contains also texts from the Beatles Interviews Database concerning interviews involving the whole Beatles group, from which we isolated Paul McCartney's speeches. The remaining part of Let IT Corpus consists of Beatles' interviews freely available on YouTube. Let IT Corpus is still in its embryonic stage, since it is composed of approximatively one-hundred texts and it represents the first step in this field. Further work will be carried out as soon as Let IT Corpus will be expanded.  4  Our three approaches to stylometric analysis  We investigated this AA issue with three different approaches, in order to compare the results. For all the experiments we removed punctuation and symbols, and we lowercased all characters. 4.1  Hybrid approach  In this section we describe the first approach to stylometric analysis, namely a hybrid approach based on CS, Linguistic Rules and Machine Learning (ML). Thanks to the analysis of approximatively five thousand English documents from a variety of sources (newspapers, social media and books) we identified several stylistic features that we used to write linguistic rules for English. Here we report a short list of stylistic features: sentence length (Argamon et al., 2003), vocabulary richness (De Vel et al., 2001), word length distributions (Zheng et al., 2006), punctuation (Baayen et al., 1996), use of a specific class of verbs or adjectives, use of first\/third person. The hybrid approach of CS, Linguistic Rules and ML consists in the following steps: I) Linguistic Definition of Stylometric Features: starting from the assumption that each author operates different grammatical choices when writing a text (Daelemans, 2013), we organized the grammatical characteristics of the case-study language (in this case, English) in a taxonomy. The work was carried out thanks to COGITO\u00AE by Expert System Corp., a semantic analysis software based on Artificial Intelligence algorithms. In each limb of the taxonomy it is possible to write linguistic rules concerning the language of the case study in order to recognize the grammatical characteristics of the analyzed texts (i.e. to detect modal verbs, we create the limb \"modal verbs\" and we associate to it linguistic rules that allow to find modal verbs in the texts); II) Semantic Engine Development: Expert System's semantic engine is trained in order to extract the aforementioned features from texts and is implemented thanks to COGITO\u00AE 's semantic network (called Sensigrafo); III) Features Extraction: texts are analyzed and all features (based on the grammatical characteristics of the texts) are extracted; IV) Supervised ML Process: the features extracted are used to train the model in order to detect the features in the untagged texts. For ML process we exploit WEKA (Hall et al., 2009), a software with ML tools and algorithms for data analysis. The hybrid approach is evaluated through the 10-folds Cross Validation method. We tested two different algorithms, Random Forest (RF) and Tree J48 (J48). During previous AA investigations RF resulted to be the most performing algorithm for a binary classification. The results we obtained for 10-folds Cross Validation test confirm this result and Table 1 presents the performances in terms of Precision, Recall and F-Measure for both algorithms (namely, RF and J48). In order to evaluate the performances of the classifier, after this process, we tested both RF and J48, as well as 10-folds Cross-Validation (Table 1). Compared to the results obtained for the 10-folds Cross Validation (see Tables 2 and 3), J48 performances (Table 3) are better than RF performances (Table 2).  199  \f10-folds Cross Validation (RF) 10-folds Cross Validation (J48)  Precision  Recall  F-Measure  0.815  0.824  0.808  Precision  Recall  F-Measure  0.779  0.784  0.781  Table 1: 10-folds Cross Validation on the whole corpus with RF and J48 Test Set 80-20 (RF)  Precision  Recall  F-Measure  0.781  0.750  0.764  Table 2: Performances in terms of Precision, Recall and F-Measure with 80% of the corpus as Training set and the remaining 20% as Test set randomly selected with the support of RF algorithm Test Set 80-20 (J48)  Precision  Recall  F-Measure  0.853  0.800  0.819  Table 3: Performances in terms of Precision, Recall and F-Measure with 80% of the corpus as Training set and the remaining 20% as Test set randomly selected with the support of J48 algorithm 4.2  Support Vector Machine (SVM)  For our second approach we exploited SVM with a Bag-of-Words (BoW) features set created using TF-IDF vectorization. As stated by (Diederich et al., 2003) SVM is capable to process thousands of inputs, which allows to use all the words of a text directly as features. SVM involves building a decision boundary to separate the data into classes (in our case, before and after), which may be non-linear if the kernel trick is used to transform our existing data into a higher dimensional space. As such, the right choice to take when fitting an SVM classifier is kernel in addition to others hyperparameters specific to that kernel. In applying SVM to AA, (Schwartz et al., 2013) used a linear kernel, while (Diederich et al., 2003) examined a range of different kernels. Since our AA is a binary classification problem we used the linear kernel for our model and considered C values in the set {1, 10, 100}. The optimal value of C was determined using GridSearchCV function with a default 3-fold Cross-Validation and accuracy used as the scoring metric. The optimal C value was determined to be C = 1. Results are in Tables 4 and 5. SVM-BoW  Precision  Recall  F-Measure  0.785  0.761  0.773  Table 4: 10-folds cross validation SVM - BoW features set. SVM-BoW  Precision  Recall  F-Measure  0.885  0.809  0.818  Table 5: Performances in terms of Precision, Recall and F-Measure with 80% of the corpus as Training set and the remaining 20% as Test set randomly selected. 4.3  Convolutional Neural Network (CNN)  To deal with the problem of AA of speech transcriptions, our third approach consists in a two-class text classification based on a deep CNN. We built a neural network that exploits the morpho-syntactic  200  \finformation to improve the classification and correctly identify the given samples. The input data are preprocessed and tagged with linguistic information using the Part-of-Speech (PoS) tagger provided by the free NLP open-source library Spacy. Given the importance of function words (Kestemont, 2014), conjunctions, prepositions, interjections, adverbs and auxiliary verbs were taqken into account for this analysis. In fact, as proved by (Mosteller and Wallace, 1963) and confirmed by (Koppel et al., 2006), function words are discriminators of authorship, since the usage variations of such words are a strong reflection of stylistic choices. Our proposed architecture receives a sequence of tagged texts as input and then is transformed into padded sequences of fixed length. The sequences are then processed by four modules: an embedding module, a convolutional module and two max pooling layers to consolidate the output of the convolutional layer. The output of the three modules are processed by one Dense layer and an output layer. Results are shown in Tables 6 and 7. CNN-PoS  Precision  Recall  F-Measure  0.681  0.734  0.706  Table 6: 10-folds cross validation CNN + PoS. CNN-PoS  Precision  Recall  F-Measure  0.692  0.818  0.750  Table 7: Performances in terms of Precision, Recall and F-Measure with 80% of the corpus as Training set and the remaining 20% as Test set randomly selected.  5  Differences and Similarities before and after 9th November 1966  Thanks to the linguistic analysis of the texts belonging to the two different classes, we detected stylistic differences and similarities in the speech transcriptions before and after 9th November 1966. We started by dividing Paul McCartney's interviews into separate sentences. A number of stylistic features are extracted from these sentences and then all features are used for K-Means clustering. Here we report a list of some features extracted: number of function words, number of verbs and a number of interjections. For clustering, the average of each feature is calculated. Further, a SVM classifier is trained on 70% of the interviews and tested on the remaining 30%. Performing this process means to see whether a link is present and consistent over time through Paul McCartney's style. Accuracy is shown in Table 8. Accuracy on test set 0.561 Table 8: SVM to test stylometric similarity Here we report some examples of interview transcriptions before and after 9th November 1966 and we highlight the most noticeable differences and similarities. It is very important to consider that all the interviews collected are from different sources (TV, radio, newspaper), that means that speeches can differ from source to source as well as according to the historical moment in which they were done. \u2022 You know like, number one records, Sunday Night At The Palladium, Ed Sullivan Show, go to America, you know. All kinds of ambitions like that. (Carnegie Hall - New York, 1964 February 12th); \u2022 The only thing is that we've gotta do a lot from London, 'cuz a lot of the TV shows are down in London, you know. And so, we're forced to do a lot down in London. I mean, it's like someone said the other day Why doesn't Harry Secombe go to Cardiff? You know, he never does. But no  201  \fone ever moans about Harry never going...You know what I mean? (BBC-TV by Gerald Harrison - Liverpool, 1964 July 10th); \u2022 Personal differences, business differences, musical differences, but most of all because I have a better time with my family. Temporary or permanent? I don't really know. (Break-up - 1970 April 10th); \u2022 It was like a gesture to Russia because normally records are released first in America and England in Europe and then Russia gets them last and because Gorbachev and Reagan were talking about glasnost and we're talking about arms reduction. I think a lot of us in Europe were very happy to hear this so I had the opportunity to release this record so I wrote a little note on the record saying this is the peace gesture the hand of friendship from the west to the east and I just felt it might just help a bit of glasnost it's my little bit of glasnost. (Flemish Public Television Interview - 1989) As we can see, slang expressions and fillers such as 'cuz, I mean, You know? and You know what I mean? completely disappear in interviews after 9th November 1966. The use of slang disappears also in other interviews after this date, in which we can find a different Paul McCartney, who seems to be more serious and not only because of an older age. Changes can be brought about by the different topics addressed in the interviews, but we also believe that speech preserves some characteristics (such as slang) in different contexts. In texts belonging to the after class, sentences are longer compared to those of the before class. We noticed also that in texts belonging to the after class style changes occur continously not allowing for the identification of a specific style. For these reasons we also report the date and the source. Our research highlights some similarities in before and after texts: the overuse of expressions such as We are gonna do and a lot\/a lot of is confirmed in both periods. These represent the most used expressions by Paul McCartney in his speeches. In the interviews in the Let IT Corpus we also noticed that Paul McCartney is inclined to rely on lists both in before and in after periods.  6  Conclusions  In this paper we have presented the Let IT Corpus, namely a corpus of one-hundred four transcriptions from speech to text of Paul McCartney's interviews collected from the Beatles Interviews Database and YouTube. The aim of this research is to detect possible differences and similarities in Paul McCartney's speeches before and after 9th November 1966 (date of his supposed death). For this reason texts have been organised in two classes: I) before and II) after. We investigated three different text classification approaches and we detected that all methods achieved high percentage of accuracy classifying texts in two different classes referring to two different periods. To reinforce these results and on the basis of the analysis of the stylistic features set out above, it is clear that the way of modulating the words of Paul McCartney is quite distinguishable between the two periods examined.  7  Future Work  The corpus is in its embryonic stage, since it is composed of approximatively a hundred texts. Future work therefore concerns the expansion of the Let IT Corpus, so to allow a more thorough investigation. To corroborate our hypothesis it might be interesting to see if the differences we detected between the two classes represent a pure coincidence. A possible experiment in this respect can be carried out considering a different temporal division of the texts.  Acknowledgements This research has been partly supported by the PON Ricerca e Innovazione 2014-20 and the POR Campania FSE 2014-2020 funds. Authorship contribution is as follows: Antonio Pascucci is author of Sections 1, 2, 3, 4.1 and 7 and Raffaele Manna is author of Sections 4.2 and 4.3. Section 5 is in common. This research has been developed in the framework of two Innovative Industrial PhD projects in Computational Stylometry (CS) by 'L'Orientale' University of Naples in cooperation with Expert System Corp. We are grateful to Vincenzo Masucci and Expert System Corp. for providing COGITO\u00AE for research and to Prof. Johanna Monti for supervising the research. "
	},
	{
		"id": 32,
		"title": "Prospects for Computational Hermeneutics",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Michael Piotrowski",
			"Markus Neuwirth"
		],
		"body": "Introduction: The Theoretical DH  The interpretation of human artifacts in order to understand their 'meaning' is the central concern of the humanities. They are therefore often characterized as being 'qualitative-hermeneutical,' in contrast to the natural sciences and to computer science, which are supposedly 'empirical,' 'quantitative,' and much less dependent on interpretation. However, as Piotrowski (2018) argues, disciplines are not defined by their methods alone but rather by a 'unique combination' of a research object and a research objective; research methods, he notes, are 'secondary in that they are contingent on the research object and the research objective.' In addition, technical and scientific progress not only enables methods to evolve, but also requires them to adapt, while research objects and objectives largely remain stable. Even though particular methods may be 'typical' for a particular discipline, all disciplines can, in principle, use any methods, including computational ones, as long as they fit their research objectives. As Orlandi puts it, 'un po' di aritmetica ha sempre fatto parte delle discipline umanistiche' (Orlandi, 1990, 114). Conversely, other fields also use methods commonly associated with the humanities. For example, Frodeman (1995) has argued that geology is really a 'historical and interpretive science,' rather than a 'derivative science, relying on the logical techniques exemplified by physics' (see also Comet, 1996). Similarly, artificial intelligence (e.g., Winograd, 1981) and computer science more generally (e.g., West, 1997) attempted to formalize and apply the key concept of hermeneutics, namely, understanding. The research carried out under the heading of 'Digital Humanities' (DH) currently tends to focus on quantitative analyses, which have long been difficult or even impossible in the humanities and which  204  \fyield important new insights complementing traditional ('manual') qualitative analyses. However, if we understand DH as the construction of formal models in the humanities (Piotrowski, 2018; McCarty, 2014; Orlandi, 1990), we must not neglect the qualitative-hermeneutic dimension. If the humanities want to succeed in answering their research questions\u2014which are primarily qualitative in nature\u2014they cannot rely on quantitative methods alone. Instead, a multilayered research process is required, one in which quantitative and qualitative analyses continuously alternate. One of the main challenges for the theoretical digital humanities (Piotrowski, 2018) remains to find ways to integrate hermeneutic methods and insights into formal models, rather than keeping interpretation detached, as a kind of afterthought to automatic analyses (or vice versa). In this regard, there are noteworthy initiatives to exploit the computer as a 'modeling machine' (McCarty, 2014, 256) while continuing the long philosophical tradition of hermeneutics (e.g., Dilthey, Heidegger, Gadamer, Ric\u0153ur, Iser, Jauss etc.). However, there does not seem to be a transfer back in the other direction. The goal of this paper is thus to outline the prospects for a novel approach that might be called computational hermeneutics and to stimulate a wide discussion on the possibility of a unified science bridging the gap between the humanities and the sciences.  2  Hermeneutics and Understanding  The main goal of any hermeneutic approach is to achieve what Dilthey called Verstehen, an 'understanding' of human artifacts in order to answer questions about the 'Why?' and 'How?' and to uncover underlying patterns (Bod, 2015). But what exactly is understanding, and the understanding of what? One may argue that hermeneutic interpretation aims at uncovering the meaning of a given text by reference to the author's intention (whether empirical or idealized), the envisaged reader, and the dense web of meanings invoked by the text. Understanding thus involves a reconstruction of (a) the reasons why a given author (or group of authors) produced a particular text (text understood in a broad sense), (b) the overt or hidden layers of meaning of a given text, (c) the type of recipient envisaged by the author and\/or the text, and (d) the potentially infinite number of contexts in which the reconstruction of meaning can take place (Ric\u0153ur's 'conflit des interpr\u00E9tations'). Whenever we interpret language, we need to rely on some pre-understanding that provides the basis on which to build an interpretation. It is thus a recursive process, in which (pre-)understanding is necessary for interpretation, which in turn produces understanding, and so forth\u2014hence the term hermeneutic circle (for an illuminating discussion see G\u00F6ttner (1973)). Since this process leads to a progressive approximation to an (ideally exhaustive) understanding of a given text, Bolten (1985) has proposed the more apt metaphor of a hermeneutic spiral. Despite the supposed 'death of the author' (as famously heralded by Roland Barthes), authorial intention remains an important component of pre-understanding: the interpretation of texts cannot be successful when one just relies on lexical meanings and sentence semantics alone, both of which may not even be available when we understand texts in a broad sense. For an interpretation to be sound, one has to make complex inferences that rely on vast knowledge about the world and on the attribution of mental states (especially intentions) to the author. Here Grice's conversational maxims play a particularly important role in guiding the inferential process. It is characteristic of the ways in which hermeneutics is commonly construed that these inferential processes about the other mind, however foreign, are rarely reflected in depth (see Winograd's model of the speaker as part of the reader's model of the world (Winograd, 1981)). Nonetheless, despite the importance of authorial intention, it is necessary to draw a distinction between the meaning of a text and the meaning as intended by the (empirical rather than ideal) author. In other words, the meaning of a text cannot be reduced to the intended meaning either.  3  Hermeneutics and Digital Humanities  There are essentially two 'native' strands of interpretation in DH, which both now have long traditions going back to the beginnings of computing in the humanities.  205  \fContext of interpretation  Work  Figure 1 \u2013 Hermeneutics considers a work in a particular context of interpretation, peculiar to a reader, here modeled as a network of concepts. Links between concepts can be of various types; they can be thought of as mental associations.  One strand is that of annotation, exemplified by the Text Encoding Initiative (TEI, 1987).1 It goes back to an even longer editorial tradition in philology, focusing primarily (though not exclusively) on a single text and the textual phenomena therein. It is thus a relatively 'weak' form of interpretation, in the sense that it makes only limited connections to the extra-textual (which for Ric\u0153ur (2017, 103) is a defining feature of interpretation)\u2014but intentionally so: editions are generally used as a basis for a later interpretation of the text. The other strand, which Rockwell and Sinclair (2016) call 'computer-assisted interpretation,' builds on an equally long-standing tradition in literary studies, in particular concordancing, stylometry, and other quantitative analyses, and belongs to the first applications of computers in the humanities (see, e.g., Kroeber, 1967). The modern evolution of this strand can be exemplified by Rockwell and Sinclair (2016) and their work on Voyant.2 This strand is oriented towards tools and automatic analyses informing human interpretation. Rockwell and Sinclair's notion of the 'hybrid essay, an interpretive work embedded with hermeneutical toys' (Rockwell and Sinclair, 2016, 17) illustrates well the idea of the computer providing scholars with new evidence. Both strands are not limited to philology and literary studies; they can also be found in other humanities disciplines, and images or other artifacts may replace texts as research objects. Outside of DH, computerassisted interpretation (in the above sense) remains controversial (see, e.g., the debates following the publication of Da, 2019); critics typically question the legitimacy of quantitative methods in general. However, both annotation and computer-assisted interpretation have an inherent limitation in common, which is rarely, if ever, discussed: human interpretation remains outside of the formal framework. In the case of annotation, the (formal) annotation is the result of a preceding human interpretation that motivates a particular annotation (say, that tagging of some text as 'deleted'), but only the result (in the form of a <del> tag) is formally documented, the reasoning for this choice generally remains inaccessible, at least to the computer. Furthermore, it is usually difficult, or even (practically) impossible, to record alternative interpretations.  4  Proposal  How, then, could we link hermeneutics to formal models, so that human interpretations can be taken into account as well and different types of methods can be combined to truly complement each other? The idea of mixed methods, which originated in the social sciences (Kuckartz, 2014), certainly cannot be transferred to the domain of the humanities without modification. It is important to stress that the goal cannot be to 'automate' interpretation; the bedrock of Verstehen is a shared understanding of the conditio humana. 1https:\/\/tei-c.org 2https:\/\/voyant-tools.org  206  \fThe goal must rather be to support the scholar by making it possible, for example, to process qualitative human interpretations alongside the results of automatic quantitative analyses. The basic idea of our proposal is to model the context of interpretation\u2014i.e., a reader's knowledge of cultural concepts and the associations between them\u2014as a semantic network or knowledge graph (see Fig. 1), and interpretation as the linking of features of the interpreted object to nodes of this network, i.e., the construction of a new network, as illustrated in Fig. 2. Understanding can thus be defined as the integration of the object's properties into a preexisting network. Computationally, this model can be represented using Semantic Web and Linked Data technologies, which has the advantage that existing tools and methods can be leveraged. In particular, we propose to use nanopublications, a knowledge representation approach originally developed in bioinformatics (Groth et al., 2010), although the conceptual model is neutral with respect to a particular implementation. Nanopublications were developed as a common framework for describing scientific statements together with contexts (e.g., original publication, authors, organisms involved) in a machine-readable fashion, so that scientific results are easier to discover, unambiguously referenced and connected to particular scholars, and can be automatically aggregated and analyzed. Context of interpretation Context of interpretation 2  Context of interpretation  P1 Work P2  P1  P4 Work P2  P3  P3  (a)  (b)  Figure 2 \u2013 (a) Interpretation links features of the work (here: passages P1\u20133) to concepts in the reader's context of interpretations. (b) The contexts of interpretation of different readers may partly overlap (and thus share associations) but may also have different relations and thus come to different interpretations of the same work.  5  Case Study  Is it possible to model the temporal (and geographical) dynamics of the horizon of expectations? Let us consider an example from music history to demonstrate our approach to computational hermeneutics. In his review of a symphony by Robert Volkmann (which is little known today), Selmar Bagge wrote: 'Volkmann's Dmoll-Symphonie ist eine durchaus pathetische Production' (AmZ 48, 1863, col. 806). Suppose this sentence originated from a present-day source. In this case, a translation such as the following would be perfectly possible: 'Volkmann's symphony in D minor is a quite emotive work'. However, since a model of understanding contains assumptions about the author and the time of his or her writing, such a translation would ignore that the German word pathetisch has undergone a significant semantic shift. Today, pathetisch has a rather negative connotation and would thus have to be translated as \u2018melodramatic' or \u2018pompous.' To reveal the (historical) meaning likely to be intended by Bagge, we need to explore and model the contexts in which pathetisch has been used. These contexts have to be distinguished according to their distance to the target object of interpretation. Generally, an interpretation is more likely if it is supported by sources that show proximity in terms of time and space. In other words, sources that have been written around the same time and in the geographical vicinity of the source under investigation are to be preferred over sources that show greater temporal and geographical distance. Both temporal and geographical distances can best be modeled using network approaches (see above).  207  \fWhen consulting one central source, the approximately contemporaneous Deutsches W\u00F6rterbuch by the Brothers Grimm, we find pathetisch glossed as \u2018powerful,' \u2018dignified,' or \u2018solemn.' In addition, the word is linked to both the passionate and Schiller's concept of the 'pathetic-sublime' (1793). Both of these usages are confirmed by much earlier sources: In Johann Georg Sulzer's Allgemeine Theorie der Sch\u00F6nen K\u00FCnste (1793), the 'pathetic' is considered a synonym of the 'passionate.' In Heinrich Christoph Koch's Musikalisches Lexikon from 1802, the reader interested in the meaning of 'patetico, pathetisch' is directed to the entry on the 'sublime' (Koch, 1802), thus suggesting that pathetisch and 'sublime' are synonyms. More distant 18th century sources even suggest an association of the sublime with the '(delightful) horror.' Given the historical distance, this connotation is less likely to be conveyed in Bagge's statement. Considering this complex semantic history, the modeling task consists in (1) linking related semantic concepts, (2) qualifying these links (e.g., as synonym, as super- and subcategory, or as semantic overlap), and (3) weighing links according to temporal proximity. The model reader that Bagge had in mind when making his statement about Volkmann's symphony as being pathetic is somebody who had a certain prior knowledge of that concept (as reconstructed from the sources just mentioned). In addition to the semantic history of words, further contexts that need to be considered concern a dense web of musical works. The prototypes of a pathetic work, as invoked by Bagge, are Beethoven's 5th and 9th symphonies. Readers of the time likely understood this to be the primary context of Volkmann's symphony without which a proper understanding could not be achieved. Further works featuring 'pathetic' in their titles are Beethoven's piano sonata op. 13 and, much later, Tchaikovsky's 6th symphony, the distance between these two works being roughly a hundred years. However, despite the lack of a title, many earlier symphonies (by other composers) from the late 18th century on have been referred to as invoking the 'sublime,' and hence are 'pathetic' in Koch's sense. The reason for Bagge's aesthetic judgment thus lies in the shared musical properties of all the works contained in the set of pathetic or sublime symphonies: the minor mode, the orchestral setting, a particular tempo, etc. As a result, a hermeneutic reconstruction must consider both the semantic tradition (and change) of the word 'pathetic' and the corresponding musical production.  6  Conclusion: Implications and Prospects  As outlined at the outset of our paper, the humanities and the sciences are widely assumed to be separated from each other by their respective methods, objects, and objectives. However, as suggested above, the humanities and the sciences face a common challenge: both have to address explicitly the issues of interpretation and decision-making under uncertainty. In particular, they need to formalize and model the contexts of interpretation and the inferential processes under uncertainty, seeking to exploit the rich potential of the computer as modeling machine (Piotrowski, 2019). The development of suitable probabilistic tools (Pearl, 2000) for modeling network-like relationships between objects is a crucial task for the whole scientific community, one that brings us closer to the ideal of a truly unified science. The use of formalization and modeling is often met with a certain hostility in the humanities. Many humanities scholars subscribe to the notion that interpretation can in principle never come to a conclusion, and indeed the fascination of hermeneutics seems to lie in its inherent incompleteness. In addition, it is assumed that multiple interpretations can exist alongside each other without the need (or even the possibility) to prefer one over the other; this is in keeping with the cherished notion of plurality and multiplicity of perspectives in the humanities. Yet exactly in this respect a computational approach may offer obvious advantages, as the possibilities of formally representing interpretations, their contexts, and the inference procedures allow scholars to better compare different interpretations and assign different probability values to them (for applying a Bayesian approach to historiography and the problems of assigning prior probabilities (see Tucker, 2004; Carrier, 2012). More generally, this approach can give rise to the idea of progress in the humanities (something that is notoriously rejected by many humanities scholars). Thus the essential challenge of the theoretical digital humanities is to come up with a convincing approach to a 'hermeneutic computer science' (West, 1997), whose tasks involves modeling interpretation contexts, inferential processes, and uncertainty."
	},
	{
		"id": 33,
		"title": "EModSar: A corpus of Early Modern Sardinian Texts",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Nicoletta Puddu",
			"Luigi Talamo"
		],
		"body": "1 Introduction In this paper1 we present the Early Modern Sardinian Corpus (EModSar: http:\/\/corpora.unica. it\/TEITOK\/emodsar)2, a historical corpus developed within a more general project whose aim is to describe the linguistic repertoire of Sardinia in the Modern Era3 (see section 2.1). Our main research question addresses the impact of language contact on Sardinian, and, in order to answer this question, we decided to build a pos tagged and lemmatized corpus covering texts from the 16th to the 17th century which also contains extralinguistic information about the chosen texts (see section 2.2). Moreover, given that our texts are written in Sardinian, but also contain sections written in Catalan and Latin, we wanted to both preserve multilingualism, and also ensure that our corpus tools focused on the linguistic analysis of Sardinian. As Pahta et al. 2018:10 point out, multilingual historical corpora are rarer than monolingual ones, and have not been used extensively in historical linguistics. However \"embracing a multilingual approach to language history leads the researcher to look beyond the main language of a text and consider what a holistic overview of all the languages in it reveals about the 'grammar' of non-monolingual writing on the one hand or individual identity or social practice on the other\" (Pahta et al. 2018:5). Consequently, we decided to adopt the TEI-P5 guidelines to code our documents in order to accomplish Lass 2004's three desiderata for a proper historical corpus (i.e. \"maximal information preservation', \"no irreversible editorial intervention', and \"maximal flexibility'). On the one hand, the use of TEI-P5 for our corpus allowed editorial choices to be preserved in the text at the philological level, while, all the relevant information could be inserted in the header. The use of a TEI-P5 encoding is not a common standard in historical corpora. As Jenset and McGillivray 2017:125 note, \"TEI is not very widely used for historical corpora, where there is a stronger emphasis on linguistic annotation rather than on paleographic and historical markup. However, in the case of historical texts, the information contained in these tags can 1For Italian academic purposes only, Nicoletta Puddu was responsible for Sections 1 and 2 and Luigi Talamo for Sections 3 and 4. 2The corpus is currently composed of nine manuscripts, for a total of 6495 tokens. 3EModSar has been developed under the project System for developing and annotating a corpus of ancient Sardinian texts, funded by the Regione Autonoma della Sardegna (Capitale Umano ad alta qualificazione, L.R. 7\/07, year 2015).  210  \fbe crucial to the interpretation of the text and should be considered by the language processing tools. [. . . ].' A convenient solution is the use of softwares such as TEITOK (Janssen 2016), a tool which can handle both textual mark-up and linguistic annotation. Since our texts have been annotated at three different levels (at the document-level, at the section-level and at the token-level (see section 3.2), queries in EModSar can combine different levels in order to connect linguistic information with extralinguistic information.  2 2.1  Language and texts Sardinian in the Modern Era  The linguistic repertoire of Sardinia in the Modern Era is largely understudied, but it is extremely interesting since it sees the presence of many different languages within the same period. From 1324 onwards, the kingdom of Aragon gradually took possession of the Island, and as a consequence Catalan became the official language. After the unification of the Kingdom of Aragon with the kingdom of Castile, Castilian began to spread, but Catalan actually remained in use for juridical and administrative purposes, while Castilian became the language of Universities and of the Church (Virdis 2017). Thus, between 1324 and 1720, when the Island was conceded to the House of Savoy and started its process of Italianization, Sardinia was under Iberian domination. However, Sardinian continued to be used in juridical documents of both a public and, especially, private nature particularly in the countryside. Both Catalan and Castilian deeply influenced Sardinian during the Iberian domination. The Sardinian language of this period is documented through two typologies of documents: literary sources and juridical sources. The Sardinian literati in the Modern Era usually wrote in the dominant languages (mainly Castilian). However, some of them (like Antonio Lo Frasso) inserted some Sardinian sections in their works or even wrote entire compositions in Sardinian (like Girolamo Araolla). What is clear, however, is that all the literati living in Sardinian were highly plurilingual (Marci 2006). As for juridical documents, Sardinian was used during trial courts, not only for testimonies, but also for other stages of the trial. In private documents Sardinian appears in notary deeds mainly containing sales, donations, debit notes, last wills and testaments (Cadeddu 2013). While we have critical editions of literary texts from the Modern Era, juridical documents are mainly kept in a number of archives in Sardinia. Only a small part of these documents have been published, mainly in historical studies: there are very few critical editions and no systematic linguistic studies. 2.2  The choice of texts  In our project, we wanted to study Sardinian of the Modern era, in the perspective of historical sociolinguistics in Romaine 1992's terms. In order to do so, we decided to create the Early Modern Sardinian Corpus by encoding and annotating juridical documents of the Modern Era, annotated by POS and lemma, and accompanied by contextual information. To date, we have encoded nine documents written in Sardinian dating from the 16th to the 17th century retrieved from the Archivio storico del Comune di Cagliari and the Archivio di Stato di Cagliari. Most of the retrieved documents come from villages in the Northern Sardinian area and from the towns of Sassari and Bosa. However, we know for certain that documents written in Sardinian datable to those centuries also exist in southern Sardinia. We do not expect to find any documents in Sardinian for the city of Cagliari where Catalan was widespread in all the written domains. Our documents have presented many problematic aspects typical of historical corpora which we will exemplify by discussing document Osp250 which contains the last will of Canonigu Montixi, the priest of the diocese of Arborea, who, in 1569, leaves a 'fellowship' to one of his relatives so that he can study grammar, philosophy and theology. First of all, our documents are characterized by a high level of orthographic variation, both between different documents and within the same document. For instance, in Osp250 the preposition 'in' can have different orthographic realizations (in, jn, en). Moreover, we have many cases of univerbation, such as insu 'in the', inpodere 'in power', etinsu \u2018and in the'.  211  \fSecondly, our documents are multilingual and we can have code-mixing both at the intersentential level and at the intrasentential level (on different levels of code-switching in historical texts see Kopaczyk 2018). Different codes often correlate with different sections of the document. If we adopt the traditional subdivision in the formulae which make up the document, we can see that the datatio and the dispositio (the core of the document) in Osp250 are written in Sardinian, while the roboratio testes and the completio are in Catalan. However, we also have intrasentential code-mixing. First of all, as could be expected in juridical documents, we have Latin expressions, such as ut supra, qui supra fidem facio. But, even more interestingly, we have Catalan and Sardinian code mixing. The datatio in Osp250 is in Sardinian, but we find the form en for the preposition \u2018in' , and the name of the month \u2018June' in the Catalan form junny. By contrast, in the completio, written in Catalan, the name of the month \u2018July' is in the Sardinian form treulas. Given the close affinity between the different languages present in the document, it is worth noting that, it is not always simple to identify the instances of code-switching, nor to distinguish code-mixing from borrowing. Finally, our documents are \u2018stratified', since they have come to us via several passages. Osp250 contains the last will of Canonigu Montixi, but the codicil was redacted by another scribe-priest, Antiogo Molarja. Moreover, the document we have was actually copied by the scribe Sebastià Polla in 1648 at the request of another citizen from Villanovafranca. The document finally arrived in the Archives of the Hospital of Sant'Antonio, since Canonigu Montixi had decided that, were the chain of heirs to die out, his house would have gone to the hospital.  3  Corpus building and annotations  3.1  Corpus building  Due to the mixed nature of our corpus, we needed a software that was able to combine philological aspects i.e., faithful rendering of the manuscripts, bibliographic and historical information with the standard tools used in corpus linguistics i.e., a powerful and flexible query engine. Our choice fell on TEITOK4 (Janssen 2016), a software developed by Marteen Janssen at the CELTA-ILTEC institute (University of Coimbra, Portugal); in a nutshell, TEITOK is organized in two main components: (i) a web-based application that renders XML files annotated according to the TEI-P5 guidelines and (ii) a suite of executable binaries that convert XML files into the Open Corpus WorkBench (CWB: Evert and Hardie 2011) file format. The first component of Teitok fits our philological needs, as we were able to reproduce our manuscripts with the original page and line breaks, ligatures and graphic variants of linguistic forms (words), while the second component allows us to search our corpus using the Corpus Query Processor (CQP), either from the standard command line facility or using the web application. Although Teitok is also a powerful XML editor, we employed external XML editors such as oXygen in order to deal with the TEI encoding and annotation processes. Once annotated according to the TEI-P5 guidelines5, TEI-XML files are uploaded to the web application where they are automatically split into tokens by the Teitok tokenizer. As for the linguistic annotations, Teitok contains some in-development pos-tagging and lemmatization facilities, which have been proven to perform well on historical varieties of languages (Janssen et al. 2017); however, the parts of speech tagging and lemmatization processes, as well as the difficult process of the annotation of graphic variants are all performed manually: at the moment the creation of annotation tools for Sardinian is work in progress (Puddu and Stein 2018) and no annotated corpus is available even for contemporary Sardinian. Summing up, our corpus building process can be summarized as follows: 1. creation of the XML files: encoding of manuscripts; 2. XML files become TEI-XML files: text annotation according to the TEI-P5 guidelines (TEI header and text elements); 3. automatic tokenization of the TEI-XML files, which are stored in the web application (Teitok); 4http:\/\/www.teitok.org 5The EModSar corpus complies with the latest version of the TEI-P5 guidelines, 3.6.0 released on 16\/07\/2019. Whenever relevant, we have indicated the URL for the online documentation in the footnotes.  212  \f4. manual pos-tagging, lemmatization and annotation of graphic variants. 3.2  Annotations  The annotations featured in EModSar can be conveniently divided into three types: (i) document-level annotation, (ii) section-level annotation and (iii) token-level annotation. The first type of annotation corresponds to the TEI element known as \u2018header' and contains bibliographic and, to a lesser extent, linguistic and sociolinguistic information; out of the five principal components described by the TEI-P5 guideline6, we have compiled the \u2018file description', the \u2018text profile' and the \u2018revision history' components. The \u2018file description' component7 contains bibliographic information such as the repository, collection and archival reference of the manuscript, a brief history of the manuscript tradition and the name(s) of the author and copyist. In the \u2018text profile' component8, we have gathered information about the place and redaction of the manuscript, the language(s) employed and a summary of the content. As we have pointed out in the previous section, this kind of information is of paramount importance for historical corpora. Finally, the \u2018revision history' component9, as the name suggests, works as a change log displaying the date when the TEI-XML file was last changed; the component is most useful during the process of corpus building, which is usually characterized by many versions of the same TEI-XML file, often shared between several collaborators. Annotations at the section-level are performed within the TEI element known as \u2018text', which in turn is divided into different sections, marked up by the <div> tag. Note that this text arrangement does not reproduce any formal elements of the original manuscript, but was carried out by the archivist during the encoding process. As mentioned earlier, we decided to mark this structure since it appears to be related to code switching. The <div> tag contains two attributes: the section attribute, describing one of the formulae in which a notary document is customarily arranged and the language attribute, giving the language used in the section. For instance, the following text snippet represents the section-level annotation of Osp250, whose formulae were mentioned in Sect. 2.2: ... <div <div <div <div <div <div <div <div <div <div <div ...  n=\"1\" type=\"datatio\" lang=\"srd\" id=\"div-1\"> ... <\/div> n=\"2\" type=\"dispositio\" lang=\"srd\" id=\"div-2\">...<\/div> n=\"3\" type=\"notitia testium\" lang=\"cat\" id=\"div-3\">...<\/div> n=\"4\" type=\"subscriptiones\" lang=\"cat\" id=\"div-4\">...<\/div> n=\"5\" type=\"completio\" lang=\"cat\" id=\"div-5\">...<\/div> n=\"6\" type=\"completio\" lang=\"cat\" id=\"div-6\">...<\/div> n=\"7\" type=\"dispositio\" lang=\"srd\" id=\"div-7\">...<\/div> n=\"8\" type=\"completio\" lang=\"cat\" id=\"div-8\">...<\/div> n=\"9\" type=\"dispositio\" lang=\"cat\" id=\"div-9\">...<\/div> n=\"10\" subtype=\"dorsale\" lang=\"ita\" id=\"div-10\">...<\/div> n=\"11\" subtype=\"dorsale\" lang=\"cat\" id=\"div-11\">...<\/div>  The third type of annotation takes place at the token level and, just like the previous section-level annotation, is implemented through the attributes of the <tok> tag; the tag is not described in the TEI-P5 guidelines and is added by Teitok during the automatic process of tokenization. Each token is annotated for graphic variants and for linguistic information, for a total of five different attributes; as for the graphic variants, we have distinguished between (i) \u2018written form', corresponding to the graphic variant as found in the manuscript, (ii) \u2018extended form', which is a written form with expanded abbreviations and (iii) \u2018normalized form', showing a tentative normalization of the graphic variant. For example, the annotation of the three different orthographic realizations of the preposition \u2018in', which we have discussed in Section 2.2 is given as follows: 6https:\/\/tei-c.org\/release\/doc\/tei-p5-doc\/en\/html\/HD.html#HD1 Last accessed on 23\/11\/2019. 7https:\/\/tei-c.org\/release\/doc\/tei-p5-doc\/en\/html\/HD.html#HD2 Last accessed on 23\/11\/2019. 8https:\/\/tei-c.org\/release\/doc\/tei-p5-doc\/en\/html\/HD.html#HD4 Last accessed on 23\/11\/2019. 9https:\/\/tei-c.org\/release\/doc\/tei-p5-doc\/en\/html\/HD.html#HD6 Last accessed on 23\/11\/2019.  213  \f<tok id=\"w-10\" form=\"en\" fform=\"en\" nform=\"in\" pos=\"PRE\" lemma=\"in\">en<\/tok> <tok id=\"w-100\" form=\"jn\" fform=\"jn\" nform=\"in\" pos=\"PRE\" lemma=\"in\">jn<\/tok> <tok id=\"w-513\" form=\"in\" fform=\"in\" nform=\"in\" pos=\"PRE\" lemma=\"in\">in<\/tok> As for the linguistic information, we provide annotations for (iv) parts of speech and (v) lemma; the parts-of-speech tagset is an adaptation of the tagset used in the Medieval Sardinian Corpus, which contains texts written in an earlier stage of Sardinian (Puddu 2015, Puddu and Stein 2018), and features 25 tags, some of which are specified for morpho-syntactic properties such as verbal mode and nominal definiteness. Finally, let us just briefly mention how we handled linguistic expressions - mostly, noun and prepositional phrases - written without spaces between words in the manuscripts. In order to faithfully reproduce the manuscripts, these linguistic expressions are encoded without spaces in the written form of EModSar, with a correspondence between a linguistic expression and a single token; at the same time and for the purpose of linguistic queries, the linguistic expression is split into tokens in the normalized form of our corpus by means of another non-standard TEI tag, <dtok>, which is introduced by Teitok (Janssen 2016:4038) and nested into the <tok> tag. Take for instance the prepositional phrase in podere, which was originally written as a single word in one of the manuscripts: <tok id=\"w-280\" form=\"inpodere\" fform=\"in podere\" nform=\"in podere\"> inpodere <dtok id=\"d-280-1\" form=\"in\" fform=\"in\" nform=\"in\" pos=\"PRE\" lemma=\"in\"\/> <dtok id=\"d-280-2\" form=\"podere\" fform=\"podere\" nform=\"podere\" pos=\"NOUN\" lemma=\"podere\"\/><\/tok>  4  Further developments  In building the Early modern Sardinian Corpus we have already achieved several objectives, summarized as follows: \u2022 we established an annotation schema for Early Modern Sardinian notary deeds which allows all the relevant external information to be preserved; \u2022 we have inserted our documents into Teitok which, not onlymakes it easy to use for different kinds of users, but also permits linguistic searches to be performed with standard corpus tools; \u2022 since the documents will be freely downloadable, they can be re-used for other searches (for instance, personalized queries through XPath, or through other platforms like TXM). The first studies on the languages used in the documents show the importance of being able to combine linguistic information and extralinguistic information and of considering texts in a multilingual perspective. For instance, we were able to confirm our idea that, some sections in our documents,such as the completio and the subscriptiones, are generally in Catalan while in others, like the datatio, Sardinian alternates with Latin. The use of Catalan and Latin thus seems to be correlated to more \"formal\" discourse moves and is used to add authority to the document. Moreover, since we also collected extralinguistic information, we were able to correlate linguistic phenomena with different levels of linguistic variation. For example, some of our documents show variants that mantain the original Latin consonant cluster -pl\/-bl- (as complimentu and obligare) while others have the innovative form in -pr-\/-br- (like comprimentu and obrigare). Our corpus allowed us to see that the forms in pr\/br tend to appear in documents which also show some other \"lower\" phenomena like the methathesis of -r- (as in frimadu for firmadu) and it can consequently be hypothesized that both correlate with diastratic variation. Future work will focus on two points: \u2022 at a more general level we need to develop the structural coding of more complex documents such as court trials, which arrived in the form of a summary report containing different documents such as letters, trial witness statements, and attestations relative to the delivery of convocations;  214  \f\u2022 some issues on normalization and lemmatization are still to be discussed, especially if we want to place our corpus in a diachronic and ambitious perspective as one of the steps for the construction of a diachronic corpus of Sardinian. It goes without saying that, only by increasing the size of our corpus, can we confirm the already noticed tendencies and give a more detailed picture of the multilingual practices in Modern Sardinia.  Acknowledgements The authors wish to thank Maarten Janssen for his wonderful support on Teitok."
	},
	{
		"id": 34,
		"title": "Shared emotions in reading Pirandello. An experiment with Sentiment Analysis",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Simone Rebora"
		],
		"body": "Introduction  Sentiment Analysis (SA) has recently grown in relevance in Digital Humanities. This computational technique, originally developed with the goal of analyzing \"people's opinions, sentiments, appraisals, attitudes, and emotions towards entities and their attributes\" (Liu, 2015), has found multiple applications in literary studies. From the widely discussed \"shapes of stories\" by Jockers (2014) and Reagan et al. (2016), to the study of fairy tales (Mohammad, 2012; Rotari, 2018), literary criticism (Rebora, 2017; Mellmann and Du, 2018), genre (Kim et al., 2017; Henny-Krahmer, 2018), and narrative structure (Zehe et al., 2016), SA has become one of the key methodologies in computational literary studies. For an extensive survey, see (Kim and Klinger, 2018). However, criticisms abound, both in theoretical (Ciotti, 2017) and practical (Sprugnoli et al., 2016) terms. With this paper, I will report on an experiment aimed at verifying the efficiency of the approach in the study of two related phenomena: the narratological structure of a story and its associated reader response.  2  The Experiment: Bringing Research and Didactics Together  The experiment was conducted during the Digital Humanities course (Informatica per gli studi umanistici) held at the University of Verona in the academic year 2018\/2019. Students were asked to read the short story (novella) \"Ci\u00E0ula scopre la luna\" (1907) by Luigi Pirandello and were provided with an XML file with the following structure: \u2022 the <novella> root tag; \u2022 the child <frase>, containing one paragraph from the short story;  216  \f\u2022 the child <sentiment>, which the students were asked to fill with a numeric evaluation of the sentiment of the paragraph, ranging between -5 and +5; \u2022 the child <commento>, where the students could write a free comment on the effects produced by reading the passage. Full text of the novella was downloaded from LiberLiber and based on the 1986 Mondadori edition (Pirandello, 1986). Sentences were automatically split using the SA software Syuzhet, that was adopted as a groundwork for the entire experiment1. At the end of the annotation process, a total of 51 students wrote at least one comment or sentiment evaluation, for a total of 1,884 comments (36.94 per student) and 1,401 sentiment evaluations (27.47 per student). The 51 XML documents were then anonymized and merged into a single file, available for consultation (together with the R scripts for its analysis) on Github. The experiment had the didactic purpose of letting students familiarize with the XML markup language and with SA computational techniques (both, in a very simplified form). In terms of research purposes, their annotations proved precious for a verification of the efficiency of SA approaches.  3  Analysis  3.1  Agreement on Sentiment Annotations  Figure 1 shows all sentiment annotations by the students. As evident, annotations are widely spread throughout the 111 paragraphs of \"Ci\u00E0ula\", with a dominance of the central levels of emotionality and a deviation towards the most positive levels only at the end of the novella.  Figure 1: Sentiment annotations on \"Ci\u00E0ula scopre la Luna\" (51 annotators: -5\/+5) For a more detailed understanding of the level of agreement, Krippendorff's Alpha (Krippendorff, 2018) was adopted. To maximize the possible agreement, annotations were reduced to a binary selection: \u2022 annotation value < 0: \"negative\" tag; \u2022 annotation value = 0: no tag; \u2022 annotation value > 0: \"positive\" tag. 1Note that Syuzhet was designed to work on a sentence level (in fact, repeated words do not count towards the total sentiment). This is why annotation was performed on a sentence (and not paragraph) level.  217  \fHowever, Krippendorff's Alpha was still substantially low (0.19), thus confirming the result of Sprugnoli et al. (2016), who showed how inter-annotator agreement advises against the application of SA to historical (or literary) texts. Given this acknowledgment, still, some interesting outcomes can be derived from the experiment.  Figure 2: Krippendorff's Alpha for sentiment annotation (51 annotators: POS\/NEG; 11-paragraph moving window) Figure 2 shows the evolution of inter-annotator agreement through a moving window procedure: Krippendorff's Alpha is calculated on just 10% of the text (corresponding to 11 paragraphs), moving from its beginning to its end. The most striking result is in the peak of inter-annotator agreement, that comes not at the very end of the novella (marked by a dominance of positive emotions), but a few paragraphs before. Precisely, it happens around paragraph 98 (\"Dapprima, quantunque gli paresse strano, pensò che fossero gli estremi barlumi del giorno\"), that signals the beginning of the emotional shift (the \"plot twist\") in the novella: Ci\u00E0ula, still fearing the blank darkness of the night, gradually discovers the presence of the moon. This result confirms how an actual sharing of emotions happens not at their climax but with their modification, and transformation\u2014as already noted by Oatley (2012)\u2014is the driving force of narratives. 3.2  Correlation in Sentiment Analysis  SA of text and comments was performed using the simplest method (wordcount) implemented by the Syuzhet package. Being Syuzhet designed for the analysis of English language and given the much more inflected nature of Italian language, analysis was performed on lemmatized texts, that were prepared through the UDpipe software. Two Italian sentiment dictionaries were prepared and uploaded in Syuzhet: \u2022 Sentix, where sentiment values were calculated as the product of polarity and intensity; \u2022 OpeNER (Russo et al., 2016), where sentiment values were calculated as the product of sentiment and confidence. To keep a direct connection between text and comments, for each paragraph of the novella: \u2022 a single sentiment value was calculated for the text; \u2022 the mean of all sentiment values was calculated for the comments. Figure 3 shows a comparison between the analyses of text and comments with the two sentiment dictionaries. A reference point (the black, dashed line) was set by calculating the means (per paragraph) of  218  \fFigure 3: Sentiment analysis of \"Ci\u00E0ula\": text (left) and comments (right) Focus  Sentix  OpeNER  paragraph comments  0.135 0.454*  0.266* 0.659*  Table 1: Pearson correlations between SA results and mean values of manual annotation. Asterisks indicate significant correlations (p-value < 0.05) the sentiment values annotated by the students. A mathematical evaluation of the similarity between the plots was provided by Pearson correlation tests. See Table 1 for an overview of the results. At least two phenomena call for attention. First, OpeNER seems to achieve better results than Sentix. Second, and most importantly, analyses of comments show much higher correlations than analyses of the commented text. This may be considered as a confirmation of the fact that SA is much more effective when studying reader response, than when analyzing narrative structure, as already shown by Rebora and Pianzola (2018). These results become even more striking when applying the \"rolling mean\" procedure, implemented in Syuzhet to harmonize plots (see Figure 4): here the similarity can be noticed with the naked eye.  Figure 4: Sentiment analysis of \"Ci\u00E0ula\": text (left) and comments (right), normalized with rolling mean  4  Conclusion  The small dimensions of the analyzed corpus call for caution when trying to generalize such results. However, they are in line with evidence already presented in previous studies and they call for new research on the topic. In particular, the high correlation in the SA of comments suggests how, notwithstanding the low agreement between readers when trying to evaluate the sentiment of a text, SA is still able to catch  219  \fgeneral trends in reader response. At this point, two main lines of enquiry seem advisable: one, that focuses on improving the methodologies further2; another, that tries to tighten the connection between computational methods and literary theory. In conclusion, while being still a very problematic and disputable technique, SA offers multiple stimuli for theoretical and methodological reflection, revealing how, through a direct confrontation with its limitations and imperfections, research in Digital Humanities can still progress towards unexplored grounds.  5 Acknowledgments I thank Tiziana Mancinelli for allowing me to perform this experiment with her students."
	},
	{
		"id": 35,
		"title": "DH as an ideal educational environment: the Ethnographic Museum of La Spezia",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Letizia Ricci",
			"Francesco Melighetti",
			"Federico Boschetti",
			"Angelo Mario Del Grosso",
			"Enrica Salvatori"
		],
		"body": "Introduction  The authors present the outcomes of an educational experimentation that took place in the academic year 2018-2019 at the degree course in Informatica Umanistica at the University of Pisa. The experimentation involved the courses of Digital Public History, Digital Text Encoding as well as Digital Philology, and at the beginning concerned the digitization of a corpus of postcards from the period of the First World War owned by the ethnographic Museum of La Spezia 'G. Podenzana'. The postcards have been historically contextualized, digitized, placed on a collaborative web platform and distributed to the students in order to be recorded, transcribed and encoded in XML-TEI. Students have been involved in the development of the web platform by tracking the usability issues as beta-testers. Students contributed also to the requirement analysis and the definition of the specifications necessary to extend the platform to a broader audience of users without specific skills in Digital Humanities. Indeed, the project aims were not only the historical study of the corpus, but also the organization of a public history project with the Museum, its targeted audience and the High School students of La Spezia. Arriving almost at the end of this educational experiment, we propose now to discuss the current achievements, based on the common educational statement of 'learning by doing' and announced months ago at AIUCD 2019.  222  \f2  Background  Within the previous annual conference of the Italian Digital Humanities Association held in Udine (AIUCD2019), a preliminary work towards a profitable collaboration between students and teachers of different DH classes at the University of Pisa was presented. The context of the project (Booth, 1996; Cati, 2006; Cole, 2016; Delle Cave, 2013) gave the actors the opportunity to collaborate with each other and with the 'G. Podenzana' museum of La Spezia outside the formal classroom constraints, involving also some activities carried out by non academic communities (Salvatori, 2017). In Salvatori et al. (2019), the authors discussed the objectives and the outcomes that have been achieved during the bootstrap phases of the project. In particular, within that paper, they pointed out the main problems and the added values of the initiative introduced above. To date, the collaboration has been getting wider and a few internships have been activated to improve the design and development skills of the interested students in order to enhance the tools already developed within the Euporia platform (Mugelli et al., 2016). These new activities facilitate students and general users in digital encoding historical documents, which have an inherently complex nature. This objective has been possible by adopting a formal but \"common\" rules for annotating and for processing textual data (Fowler, 2010). Moreover, as far as the actual TEI-XML encoding work (Burnard, 2014; Pierazzo, 2015) which concerns the digitization, the recording and transcription of the postcards provided by the involved cultural institutions, the main problems have been overtaken by putting in place a chain of document processing tasks. This process has been developed by using XSLT technology and by implementing a Web environment to publish the encoded documents (Del Turco and Di Pietro, 2016).  3  Methods  The project involves students, teachers and representatives of the Ethnographic Museum of La Spezia, which collaborate by sharing information, resources and tools. The Museum has made available two large corpora of postcards dating back to the Great War period, which have been digitized, uploaded onto the Euporia platform and encoded in XML-TEI by the students under the supervision of the teachers, according to a custom subset of tags declared in the CartolineXML schema. Furthermore, the Museum played the role of an interdisciplinary meeting center among High School students of La Spezia, students and teachers of the University of Pisa and the Museum managers, in order to share ideas about the project from different perspectives. Two students of the aforementioned courses and co-authors of this contribution, made an internship at the CNR-ILC to improve their skills in text encoding. They focused on the simplification of the annotation process, in order to involve High School students and volunteers that could actively collaborate in transcribing and annotating postcards, even if they have not specific skills in XML-TEI encoding. Therefore, they have defined a Domain Specific Language (DSL), CartolineDSL, with the same expressivity of its counterpart in XML-TEI but much less verbose. A DSL is a formal language with a simple, understandable and suitable syntax for the domain of interest we are dealing with and based on a limited and controlled vocabulary. A DSL must be defined by a Context-Free Grammar (CFG), that is a set of recursive rewriting rules used to generate string patterns. Therefore CartolineDSL is a language suited to the domain of postcards characterized by a series of \"*attribute: value\" fields that users can easily fill in.  223  \fFigure 1: Formal grammar code snippet for the postcard corpus within the Euporia digital environment  Fig. 1 illustrates some rewriting rules in the CFG of CartolineDSL, whereas Fig. 2 shows an example of data and metadata encoded in CartolineDSL.  Figure 2: CartolineDSL snippet  The conversion of CartolineDSL to XML-TEI is performed in two steps. In the first phase, the annotations encoded in CartolineDSL are parsed by the ANTLR compiler compiler (Parr, 2013) and converted in XML. The proprietary CartolineML schema allows the serialization in XML of the Abstract Syntactic Tree (AST) parsed by ANTLR. In the second phase, the proprietary XML document is converted to XML-TEI by an XSLT transformation. The XSLT style-sheet has been created by the students on the latest part of their internship at the ILC-CNR.  224  \fAs usual, other XSLT style-sheets are necessary to transform XML-TEI in HTML for visualization purposes. Fig. 3 shows the designed interface.  Figure 3: Mockup sketch of the ongoing web-app aimed at publishing the archive  4  Results  The main achievements of this didactic experimentation are listed below: 1. coordination of three courses, in order to work on the same materials from different perspectives (Public History for the historical contextualization of the project, Text Encoding for XML-TEI models and technologies, and Digital Philology for the treatment of uncertain readings and for the creation of an optimized human readable DSL); 2. engagement of students in the annotation process of a large sample of the postcards corpus (learning by doing); 3. transfer of knowledge and experience from students of the University of Pisa and High School students of La Spezia during meetings at the Museum; 4. involvement of students in the creation of Domain-Specific Languages meant to bridge the gap between the best practices of Digital Humanists and the simple practices of unskilled citizens that desire to participate in projects of Public History.  5  Conclusion and Future Work  We guess that the educational model that we experimented can be easily exported in other contexts, with a broader involvement of multidisciplinary communities of practice and applied to different textual and\/or iconographic (or multimedia) digital resources. In the next academic year we will release an updated version of Euporia, which currently is just a prototype, in order to allow High School students and volunteers to annotate further postcards in CartolineDSL."
	},
	{
		"id": 36,
		"title": "A digital review of criticals edition: A case study on Sophocles, Ajax 1-332",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Camilla Rossini"
		],
		"body": "In the next pages I will give an account of a still ongoing project conducted at the University of Leipzig under the supervision of professor G. Crane. The project aims at modelling a framework for publishing reviews of critical editions of classical works in a digital environment (Smart Reviews - SR), thus re-thinking the review genre at its roots. For a first experimental mock-up, the chosen case study is Sophocles, Ajax, 1-332. The sequential steps we designed are: 1) to select from two or more editions some noticeable readings; 2) to link them to the corresponding places in a text chosen as a base reference, with an unambiguous reference system; 3) to compare them with the aid of external tools, in order to explain the editorial choices behind them. After a paragraph on the purposes, the uses and the shortcomings of \u2018traditional' reviews on paper, I will proceed to show in further detail the advantages of a SR and the passages to realize it. Finally, I will list some future developments. Whenever a new critical edition of an ancient text is published, other scholars carefully read it, compare it to previous texts and finally publish reviews of it on academic journals. Besides overall judgments on the edition's quality, bibliographic suggestions and further comments on specific editor's remarks, a review of a critical edition usually provides an account of the most noticeable editorial choices on the text. Textual renditions of controversial readings, new conjectures or the recovery of old ones, and maybe the comparison with the latest edition(s) on some crucial passages, are what really defines the work of the editor on the text itself, and are thus the ultimate object of the reviewer's judgment. The reviews of critical editions, finally, play an irreplaceable role for the users as well. Not only are they often, at a practical level, the only way to access a new edition in the absence of it, while waiting, for example, for University libraries to purchase it; even more importantly, they provide a list of the differences between critical texts in different editions, thus saving the time for the reader to detect them by manually comparing two or more printed books. Nevertheless, such important tasks in this kind of reviews are, at least, hard to perform on a less-thanabstract level. An example will explain why: Finglass often succeeds in defending transmitted text: he agrees with OCT against Dawe's Teubner in about 22 cases (for example 446, 771, 782, 790, 988, 1027, 1059, 1282, etc.), the reverse occurring about 15 times (for example 114, 191, 420, 630, 1357, etc.)1.  This passage, from a review to Finglass' 2011 edition of Sophocles' Ajax, is just one of many similar ones. Finglass' work is compared with the two previous major editions (Lloyd-Jones \u2013 Wilson's and 1Catrambone, 2013, p.169 on Finglass, 2011.  227  \fDawe's2), but only some specimens of agreement or disagreement are quoted, and for each of them the mere verse number is provided. The job of finding out where and how the three editions are unanimous or less so, is for the reader to do. Of course, the limited space of a review requires conciseness, and an extensive - rather than intensive - approach. Such shortcomings are intrinsically linked to the printed (or printed-like, for the PDF distributed journals) format that the review articles have had so far. The main contents of a review, though, can be described as links between corresponding passages in different editions. In such a way, they could perfectly support a digital metamorphosis of the genre. Moreover, a fully digital distribution (what we could call a Smart Review, SR) could provide more effective comparisons between editions, and links to external resources could give the reader insights on the editors' choices. This way, not only the old, consolidated tasks of the \u2018traditional' reviews are performed better and in a more feasible way; but what is more, a SR could improve and widen the usefulness of the reviewing and comparison on multiple editions3. This shift in perspective is even more desirable if we think about the Scholarly Digital Editions (SDEs). More and more as we move on, new versions of the same ancient texts become available online: not only as scanned out-of-copyright editions, but also as new uploads in large online repositories for plain or annotated texts, like treebanks4. Even though the sense to assign to the expression \u2018SDE' is controversial, each of those new documents bears a specific version of a text that becomes available to a large public; moreover, both the digital-born plain texts and the linguistic annotated ones, often imply a critical revision by the digital editor. Unfortunately, the communication problems that have been acknowledged between printed editions, stand for digital publications as well. The artificial sense of fixedness of each of those \u2018base texts' is often reinforced by the absence of critical apparatuses, that flattens the editor's opinions and textual decisions in favour of a totally illusory objectiveness. It has been said that the technical possibility to publish all the witnesses and all the editions would lead to \u00ABa sort of \u2018B\u00E9dier effect'\u00BB5, where everyone publishes an edition or a witness without establishing a critical text. Although the study of single editions or single manuscripts can have great applicability in many fields, the differences among SDEs (broadly intended) is often underaddressed, and a great number of divergent passages remains unnoticed. This problem becomes even more visible when translations are involved. Not infrequently, the translations are made available online without their corresponding original text, making it difficult to address and explain the textual choices behind them6. To sum up, each digitally published text is liable of becoming an arbitrary base text. The idea behind a SR is the opposite. Its goal is to show the diverging readings in traditional or digital editions by juxtaposition, thus not necessarily stating a hierarchy between them, similarly to what happens in traditional reviews. It is true, though, that we can not do without a base text to anchor each reading to its proper position, because a section of the text where the two or more editions diverge doesn't have, by its definition, a lemma to unequivocally refer to. Thus, the first criticality to address is the need to provide an unambiguous anchoring of the noticeable readings. The most frequently implemented solution, the XML AppCrit module, is not suitable for our purpose. Firstly, it has a binary (and thus, hierarchical) distinction between lemma and reading. Secondly, a core need of a SR is to be flexible, updatable, reusable, and for those necessities a standoff markup seems like a better choice7. 2Lloyd-Jones and Wilson, 1994; Dawe, 1996. 3 Gabler, 2010. 4See Crane et al., 2014. On editorial interventions on treebanks see e.g. Bamman et al., 2009, 10: \u00ABA scholarly treebank [...] reflects an interpretation of a single scholar\u00BB. On textual variation and ambiguity in treebank annotation see also Bamman and Crane, 2010, p. 548; Beaulieu et al., 2012, p. 400. 5Bartoli, 2015. In 1928, J, B\u00E9dier suggested that, as the Lachmannian method was practically unreliable, a single witness (codex optimus) should be chosen and edited. See B\u00E9dier, 1928. 6 A basic example will show it. Accessing Soph., Aj. 35 on Perseus, one will find: \u03C3\u1FC7 \u03BA\u03C5\u03B2\u03B5\u03C1\u03BD\u1FF6\u03BC\u03B1\u03B9 \u03C7\u03B5\u03C1\u03AF (\u2018hand'). The corresponding English translation perfectly matches the text: \u00ABit is your hand that steers me\u00BB. Oppositely, if we take Romagnoli, 1926, whose Italian translation is freely available e.g. on Wikisource, we read: \u00ABil senno tuo per guida io prenderò\u00BB, whic translates as \u00ABI will always take your wisdom as a guidance\u00BB, and not \u00AByour hand\u00BB. Poetic license? No, only a varia lectio that is recorded in most editions. The tradition is divided between \u03C7\u03B5\u03C1\u03AF and \u03C6\u03C1\u03B5\u03BD\u03AF. Finglass, 2011, 80 chooses the former, Dawe, 1996, 3, the latter. 7See the fundamental benchmark of the database of latin texts by the Digital Latin Library (LDLT, 2019) that, in a much wider perspective, modified the XML TEI P5 module 12 for Critical Apparatus (Guidelines, 2019) for its own purposes (Cayless  228  \fFor these reasons, I tokenized and corrected an OCRed file of Pearson's 1922 out-of-copyright edition8. From this, I provided an automatically compiled list of references to each word, with unique identifiers (see fig. 1). To do so, my benchmark has been the CTS URNs model as implemented by the Perseus Catalog9. Each work in the Perseus Library (and in the new Scaife Viewer as well) has a string that identifies it. For example, the greek edition of Soph., Aj. 1-332 is referenced by urn:cts:greekLit:tlg0011.tlg003.perseusgrc2:1-332, where tlg0011 and tlg003 are the traditional codes assigned by the TLG project respectively to the author Sophocles and to the work Ajax, and perseus-grc2 identifies the edition digitized by the Perseus team. The reference goes as far as pointing at a verse or a group of verses (in the example above, verses 1-332). Basing on the work already done on texts from the Perseus Digital Library and the First Thousand Years of Greek Project, I extended the unique reference system down to the word level10. Thus, each word has an identifier with this ideal structure: urn.soph.ajax.pearson@134\u03A4\u03B5\u03BB\u03B1\u03BC\u03CE\u03BD\u03B9\u03B5[1]  Firstly conventional abbreviations of the author, the work, and the edition are listed, separated by a mark (I used a dot); then, after an @, the verse and the word are reported and, finally, a number between square brackets that indicates the occurrence of the same word form in that verse. This formulation of the CTS URN is totally conventional. For our purposes here, it could be cited also in its abbreviated form: 134\u03A4\u03B5\u03BB\u03B1\u03BC\u03CE\u03BD\u03B9\u03B5[1].  Figure 1: A section of the CTS file from Pearson's edition, referencing Soph., Aj. 1: \u1FBF\u0391\u03B5\u1F76 \u03BC\u03AD\u03BD, \u1F66 \u03C0\u03B1\u1FD6 \u039B\u03B1\u03C1\u03C4\u03AF\u03BF\u03C5, \u03B4\u03AD\u03B4\u03BF\u03C1\u03BA\u03AC \u03C3\u03B5. Note the [2] in the cts with id 7, that denotes the second comma in the same verse.  I then divided the material into four sections: an ordered list with vv. 1-332 of Pearson's edition, where each word is assigned with such a CST URN reference (see fig. 2a); a database containing the noticeable readings found in the editions under analysis, and their position in reference to file 1 (see fig. 2b); another database containing the matches between each edition and the readings that could be found in it11 (see fig. 2c); finally, in another database, the broadly meaning commentary material has been linked to the corresponding readings (see fig. 2d). Linking the noticeable passages of each edition to the correct unit of text is not an easy matter. I came up with a conventional set of rules. I considered lexical substitutions, additions, subtractions and movements. For each of them I had to keep in mind that both the reading and the referenced passage could be formed by one word (see fig. 3a) or by a group of words (see fig. 3c). To each reading I added two attributes: from and to. They respectively mark the point in the CTSized text where the variant begins and ends; if they coincide, it means that the reading modifies only a word in the base CTSized and Huskey, 2018). See also the XML structure of the Euripides Scholia Project (Mastronarde, 2010), whose editor chose not to use the TEI module for the Critical Apparatus \u00ABbecause in a project of this kind it seems to me that it would involve an unjustifiably large overhead of markup\u00BB. About it, see Driscoll and Pierazzo, 2016, 213. For a theoretical comparison between inline and standoff markup see e.g. Schmidt, 2012; Eide, 2014; Petersen, 2016; Boschetti, 2007; Monella, 2008. For an overview of the criticalities of the XML TEI module 12, see the report issued by the Critical Apparatus Workgroup (Workgroup, 2014). 8 Pearson, 1924. 9 See the usage of CTS URNs and the Cite Architecture, both developed by the Homer Multitext project, by the Perseus Catalog. See Blackwell and Smith, 2014; Babeu, 2015; Blackwell and Smith, 2019b; Architecture, 2019; Tiepmar and Heyer, 2019; Blackwell and Smith, 2019a; Babeu, 2019. 10 See Celano, 2017 on texts taken from the Perseus Digital Library (Perseus, 2019a,b) and the First Thousand Years of Greek Project (OGL, 2016). See also the new Scaife Viewer (Perseus, 2019c). 11 Thanks to this organization of the material, I reduced the redundance to much less than if, say, I had to list the noticeable readings for each edition.  229  \f(a)  (b)  (c)  (d)  Figure 2: A reading (b) linked to its initial and final CTS URNs (a) and chosen in Dawe's edition (c), with comments on it by Finglass and Dawe (d).  (a)  (b)  (c)  (d)  Figure 3: Types of variation. Interpretive (a), movement (b), substitution (c), subtraction (d).  text. This method works fine for substitutions (see fig. 3a12). For subtractions as well, it was enough to clearly show the reading as empty (see fig. 3d). In the case of the word(s) addition, one needs to use a clear way to show it. I pointed at the space between two words by using the conventional formula 134\u03A4\u03B5\u03BB\u03B1\u03BC\u03CE\u03BD\u03B9\u03B5[1]+1 (to refer to the position after the word \u03A4\u03B5\u03BB\u03B1\u03BC\u03CE\u03BD\u03B9\u03B5) or 134\u03A4\u03B5\u03BB\u03B1\u03BC\u03CE\u03BD\u03B9\u03B5[1]-1 (to refer to the position before it). Finally, movements have been pointed at with the self-closing element movement (see fig. 3b, that also shows the use of +1 and -1). This system has multiple advantages: in the first place, it becomes machine-inferable (but quite clear to the human reader as well) where and how each edition differs from the chosen base text, and from each other. The material is kept separate and clean, with an easy way to add, change and modify parts of it without having to alter the structure of the existing files. Moreover, the overlapping of variants becomes possible without complex systems as it is in the XML TEI. The basic types of intervention adopted by each edition can be easily inferred by an algorithm, by comparing the reading with the from and to attributes and, if necessary, by directing the reader to the comments (see footnote 12 about fig. 3a). Whatismore, in the exact same way as a group of readings is connected to an edition, other groups may be figured out and collected under specific types that go beyond the core distinction between orthographic, 12 When the reading is identical to the \u2018base text', the comment material could tell us if the word is listed as a variant because it is a homograph - like in the case of fig. 3a - or because it is just an interpretive variant on the same word form.  230  \f(a)  (b)  Figure 4: Link to treebank (a). Treebank and aligned translation: Finglass versus Pearson (b)  morphological and lexical variants that is provided, for example, by the Digital Latin Library13. Another advantage of a SR is that it can point to external sources in order to give the reader insights about the differences between texts. The variants chosen by each editor alter the surrounding text in different ways. Some of them may generate syntactic differences, some other may remain on the lexical level. Finally, other variants are only due to different interpretations, and don't affect the texts themselves, but are only visible in the translations. Through the \u2018comments' section, the available online tools can be linked to specific passages in the considered editions to show these differences. For the variants that have an impact on the morphology and the syntax, links to their treebank annotation and graphical visualization on the Arethusa Treebank Editor can be provided in the \u2018comment' database. In this experimental case, the treebanks for each critical edition have been compiled using as a base the file uploaded by the Ancient Greek and Latin Dependency Treebank project14. The comparison between treebanks of corresponding passages in different editions makes us able to encode precisely the difference between editorial choices. Finally, not all variations affect the translation. For the ones that do, links to parallel translation alignments can be provided15 (see fig. 4). A theoretical framework for a SR addresses, on the one hand, some problems that are known to the long-lasting debate over Scholarly Digital Editions (SDEs) and, more broadly, to the field of annotated texts. The comparison between editions, core element of the SR, urges to find a way for handling the textual variation in a digital fashion, i.e. to represent variants and to link them to the base text, which is itself the object of a dispute16. On the other hand, though, the SR's intrinsic differences from SDEs compel us to find new solution. The main distinction is probably the programmatic desultoriness of the provided data. Only the important readings, and not all the text as in SDEs, are named in \u2018printed' reviews, hence the same principle should apply to SRs as well. A model for a SR, besides being a useful improvement of the current printed reviews, can prove to be a valid testing ground for the cooperation and co-existence of various instruments to annotate and encode different features of the texts that are edited in critical editions. Moreover, such a model proves once again that \u2018linguistic' instruments such as the treebank annotation can and should be integrated into strictly speaking philological resources, as precious means to gain a better understanding of the text and the critical editors' choices17. Finally, the possibilities offered by the SR to its users would increase significantly from those of a traditional review, in what we could call a re-purposing of a known instrument through digital means. At the same time, though, its final goal of helping the reader in assessing the degree of innovation or conservativity of an edition, and in evaluating specific editorial choices, would 13See the LDLT Guidelines (Cayless and Huskey, 2018). One could group together, e.g., variants that affect the translation or the staging, or particular types or variants according to one's specific needs. 14 See Alpheios, 2019. For the Guidelines for Greek Treebanking see Celano, 2014. See also Celano and Crane, 2015; Celano, 2019. 15 I used Ugarit, 2019. 16 On the base text see e.g. Andrews and Macé, 2013, p. 506. About variants see e.g. Boschetti, 2007; Monella, 2012; Lana et al., 2017. 17 See Berti, 2019; Passarotti, 2019; Mambrini, 2016; Beaulieu et al., 2012; Bamman et al., 2009.  231  \fnot be altered; quite the opposite, they might be enhanced. From this starting ground, some crucial points need to be addressed. The connections traced between readings, base text and editions could be properly defined semantic. Should the path of semantic annotation be embraced more fully, by developing an ontology18? What can (or should) the role of automated processes both in variant detection and in word analysis be19? What can the visualization and the dissemination of the project be? Which platform will best suit the open source paradigm? The previous pages only provided a first, experimental model that is still under development and that may take various directions. As for now, my hope is that this paper might provide some additional discussion material for some long known questions, more than answers to those very doubts.  Acknowledgements I would really like to thank the members of the Department of Digital Humanities at University of Leipzig and especially professor G. Crane, professor M. Berti and professor T. K\u00F6ntges for their patient teaching and advising throughout my months as a visiting PhD student."
	},
	{
		"id": 37,
		"title": "Strategie e metodi per il recupero di dizionari storici",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Eva Sassolini",
			"Marco Biffi"
		],
		"body": "1 Introduzione Il progetto, nato per strutturare l'intero elenco di voci del dizionario GDLI 1 , ha richiesto un articolato procedimento di estrazione, data la complessità dei dati e la disponibilità di un formato digitale non standardizzato. Il testo digitale da cui siamo partiti era costituito da un formato Word parzialmente strutturato, ottenuto sottoponendo l'originale cartaceo a procedure di OCR2, senza nessun tipo di collazione, parziale o totale. Il processo di acquisizione ha evidenziato caratteristiche stilistiche e scelte di layout derivate dall'originale che hanno reso l'OCR estremamente complicato. La versione edita presenta una suddivisione della pagina in 3 colonne, un colore della carta non sufficientemente bianco, nonché un carattere tipografico relativamente piccolo e una altrettanto minima interlinea. Per ragioni legate a tempo e costi dell'impresa non abbiamo potuto migliorare la qualità dell'OCR, almeno in questa fase del progetto, come viene attualmente proposto in letteratura nei nuovi approcci. Nel caso specifico, utilizzando tecniche di pre- e\/o post-elaborazione dell'output eseguita attraverso l'uso di un singolo o più motori di OCR. Inizialmente abbiamo valutato l'utilizzo di sistemi di estrazione automatici, sia basati su regole che su tecniche di machine learning (Khemakhem et al. 2017) ma l'analisi dei dati ha escluso l'opzione. La complessità strutturale non è l'impedimento maggiore, più rilevante è il numero e la varietà degli errori, a cui si aggiunge la mancanza di un training corpus opportuno per l'addestramento. Tutto questo ci ha spinto verso un approccio sperimentale, basato su strategie di definizione di regole di estrazione dal formato Word. Abbiamo inoltre evidenziato una distribuzione non uniforme delle tipologie di errore nei vari volumi, probabilmente influenzata dalla lunga gestazione dell'opera editoriale complessiva (vedi Tab. 1).  1  2  Grande Dizionario della Lingua Italiana, di Salvatore Battaglia (poi diretto da Giorgio B\u00E0rberi Squarotti), Torino, UTET, 19612002, 21 voll.; con Supplemento 2004, diretto da Edoardo Sanguineti, Torino, UTET, 2004, e Indice degli autori citati nei volumi IXXI e nel Supplemento 2004, a cura di Giovanni Ronco, Torino, UTET, 2004. La Optical Character Recognition è una tecnologia che permette di convertire un'immagine PDF o di altro tipo in testo digitale. 235  \fTabella 1: confronto dei tipi di errore tra volumi diversi  In un lavoro realizzato in un arco temporale di 40 anni, era probabilmente inevitabile la presenza di cambiamenti e aggiustamenti (anche minori), introdotti nel tempo sia a livello delle voci che nel corpus di riferimento del GDLI, che hanno avuto influenze sulle procedure di OCR. Questo contesto ha reso difficile appoggiarsi ad esperienze di altri, pur se indirizzati come noi verso approcci non standard. In alcuni progetti simili si parla di 'digitalizzazione attraverso una procedura primitiva' (Bausi, 2016), ma ci si appoggia poi principalmente alla ri-digitazione manuale da parte di studiosi ed esperti qualificati. Nel nostro caso, data la dimensione e complessità dei dati, è necessario limitare il ricorso alla correzione manuale, per le stesse ragioni di contenimento di tempi e costi indicate sopra.  2 L'approccio Abbiamo impostato un piano di lavoro a lungo termine, che comprendesse \u2018tappe' da raggiungere progressivamente: 1) riconoscimento del lemma; 2) identificazione di tutti i campi del lemma principale; 3) numero di sensi principali; 4) numero di sensi annidati; 5) campi di ogni senso principale; 6) campi di ciascun senso annidato 7) mapping in formato TEI. L'approccio seguito consiste in fasi di riconoscimento successive che, partendo dall'identificazione del lemma dell'entrata lessicale, ne eseguono la segmentazione progressiva individuando, attorno a questo nucleo, gli altri campi dell'entrata. Potremmo definirlo un processo di parsing a più livelli. Ogni campo ha richiesto strategie specifiche per l'identificazione delle caratteristiche distintive, che, tradotte in vincoli di corretta attribuzione e impostati in modo incrementale, hanno portato ad un riconoscimento sempre più granulare della struttura dell'entrata. Oltre a definire procedure software di estrazione e codifica abbiamo implementato metodi di supporto alla correzione manuale e un sistema efficiente di revisione e riallineamento successivo dei dati estratti, per contenere il più possibile l'intervento manuale. L'articolo descrive l'approccio generale e le prime tappe del progetto; la conversione in un formato standard di rappresentazione, pur essendo un impegno rilevante e dall'impatto non trascurabile sul progetto, esula tuttavia dai nostri intenti.  2.1 L'analisi dei dati Il GDLI è il principale dizionario storico dell'italiano pubblicato da UTET. I 21 volumi che lo compongono, terminati di pubblicare nel 2002, sono corredati da due supplementi integrativi, il primo del 2004 e l'altro del 2009, e da un Indice degli autori citati. Recentemente, è stato firmato uno storico accordo tra la UTET e l'Accademia della Crusca, che ha concesso a quest'ultima i diritti per un'edizione elettronica dell'opera, destinata alla consultazione gratuita. Dal maggio 2019 è quindi possibile consultare e interrogare il GDLI con un motore di ricerca per forma applicato al testo in formato Word sopra citato (www.gdli.it). Per quanto il testo elettronico presenti molte debolezze, l'approdo finale di ogni ricerca è la riproduzione in immagine dell'originale a cui si rimane del tutto fedeli, anche in questa edizione, consentendone una comoda lettura con l'ingrandimento a video, a differenza della versione cartacea in cui le dimensioni ridotte dei caratteri non permettono un facile accesso. Nella ricerca si possono certamente perdere alcuni risultati di forme 'occultate' dagli errori di OCR ma, una volta arrivati alla pagina, il consultatore può attingere appieno a tutte le preziose informazioni del dizionario. Questa rappresenta soltanto una fase iniziale del progetto: il contributo scientifico dell'ILC si inserisce a fronte dell'esigenza di fornire un accesso più articolato alle informazioni. Grazie a 236  \fstoriche esperienze nella lessicografia computazionale (Calzolari et al., 1987; Calzolari et al., 1993) stiamo stati coinvolti per implementare il complesso processo di estrazione e riconoscimento della struttura delle entrate. L'analisi dei dati ha evidenziato un input costituito da oltre 23.000 pagine di testo, rappresentate in un formato Word contenente diverse tipologie di errore. Come affermato nell'introduzione, il testo cartaceo originale presenta caratteristiche stilistiche e scelte di layout che hanno condotto il sistema di OCR verso inevitabili problemi di corretta interpretazione. Gli errori di riconoscimento sono stati analizzati su ogni singola caratteristica strutturale del dizionario: lemma, varianti ortografiche, categoria grammaticale, codici d'uso, definizione, etimologia, sensi principali e sensi aggiuntivi (annidati).  Tabella 2: esempi di errori del sistema di OCR  Ciascuno dei campi presenta errori di vario tipo, che vanno dalla mancata segmentazione dei paragrafi, all'interpretazione errata della punteggiatura e dell'ortografia delle parole, al mancato rispetto delle diverse sezioni della voce del dizionario: punti elenco, rientro, dimensione del carattere ecc. (vedi Tab. 2). La presenza di errori ha assunto quindi un peso decisivo nel progetto e ha mostrato come le sole procedure automatiche, per quanto raffinate e puntuali, non sarebbero state sufficienti a produrre un risultato corretto.  2.2 Le fasi di lavoro Siamo partiti da una sommaria classificazione dei problemi relativi all'inesattezza del dato distinguendo tra errori 'bloccanti' e 'non bloccanti', per poi procedere con i casi più specifici. La differenza sta nell'impatto dell'errore sulla procedura di parsing dei dati. Gli errori bloccanti sono costituiti prevalentemente dal mancato riconoscimento di un nuovo lemma. In questo caso, non potendo chiudere correttamente la voce precedente, si inficia il successivo processo di raffinamento, impedendo la definizione dei confini e campi dell'entrata (vedi Tab. 3).  Tabella 3: alcuni tipi di errore nel lemma  Un errore 'non bloccante' interviene invece quando non è possibile separare il codice grammaticale, da quello d'uso, e\/o dalle varianti ortografiche e\/o queste dalla definizione. Questa tipologia di errori producono 237  \fun'entrata non corretta, ma sulla quale si possono impostare le successive fasi di raffinamento progressivo, procedendo in un certo senso a \u2018tappe' nella strutturazione della voce. Mentre per gli errori 'bloccanti' non abbiamo trovato un'efficiente soluzione alternativa alla revisione manuale post-processing, per gli altri è possibile corredare il parser di meccanismi di annotazione puntuale. Segnalare quando mancano campi obbligatori o se il loro ordine non è rispettato, e riferendo puntualmente il caso in un file di report. In alcuni casi, quando è possibile impostare un'indagine più puntuale del dato, i file di report sono finalizzati al controllo delle soluzioni già inserite in fase di parsing, così da alleggerire il lavoro di revisione manuale.  3 Prospettive Nelle fasi successive del progetto le risorse estratte hanno assunto una valenza autonoma, per esempio abbiamo prodotto un confronto tra i lemmi del GDLI e quelli del TLIO3: il primo dizionario storico di tutte le varietà dell'italiano antico fino al 1375. Stiamo pensando di allargare il confronto anche ad altri dizionari, primo fra tutti il Dizionario Macchina dell'Italiano (DMI) che è patrimonio di storiche linee di ricerca dell'ILC. Nel recente passato le ricerche nel settore si sono concentrate principalmente sullo sviluppo di lessici computazionali in applicazioni di elaborazione del linguaggio naturale, ma oggi i metodi e le tecniche sviluppati per estrarre, strutturare e rappresentare dizionari, possono avere un ruolo potenziale per la progettazione e costruzione di risorse orientate all'uomo, nelle attività lessicografiche dell'editoria, soprattutto digitale. I dizionari storici sono in grado di documentare l'evoluzione diacronica della lingua, mostrando la dimensione storica del lessico. I potenziali vantaggi della digitalizzazione e strutturazione di un dizionario monumentale come il GDLI risiedono anche nell'importanza delle citazioni che vi si possono consultare. Come sostenuto da Beltrami e Fornara (2004), il vero fulcro del dizionario è la presenza massiccia di citazioni di testo, che coprono un'ampia varietà di usi linguistici, dalla lingua quotidiana e letteraria, alle lingue regionali e\/o specializzate\/specialistiche, ai neologismi e alle parole straniere. Le citazioni offrono preziose informazioni sulle prime attestazioni delle parole, sulle loro varianti formali\/diacroniche\/diatopiche; sugli autori che le citano e sulle loro etimologie. Per questo motivo stiamo implementando procedure software che da un lato estraggano le varianti dalla struttura della voce e dall'altro, attraverso l'elaborazione delle informazioni estratte dal volume dell'indice degli autori citati, consentano di predisporre filtri su autore ed epoca\/data per le rispettive citazioni.  4 Conclusioni Il nostro impegno è finalizzato a rendere una delle maggiori risorse lessicografiche dell'italiano utilizzabile per il trattamento computazionale, ma l'analisi conclusiva dell'approccio adottato è ancora prematura, soprattutto per quanto riguarda l'estrazione dei sensi annidati. A progetto in corso un'analisi conclusiva del lavoro non è possibile, tuttavia ci sembra di comune utilità descrivere la nostra esperienza, come aiuto per pianificare progetti analoghi, per i quali mancano riferimenti certi in letteratura. Questi progetti, avendo un alto grado di complessità e di incognite, si sviluppano troppo spesso senza un'adeguata divulgazione, il che significa che spesso i ricercatori e gli studiosi devono in un certo senso 'reinventare la ruota'. L'intento di questo articolo è proporre il nostro approccio come caso di studio in contesti in cui non è possibile ricorrere a strumenti e\/o procedure consolidate o sperimentali già note in letteratura e magari offrire spunti per discutere delle strategie specifiche che sono state utilizzate."
	},
	{
		"id": 38,
		"title": "Encoding Byzantine Seals: SigiDoc",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Alessio Sopracasa",
			"Martina Filosa"
		],
		"body": "1  Byzantine Sigillography and Digital Humanities  SigiDoc is an effort providing XML-based and TEI-compliant encoding standards for Byzantine seals. It represents the first attempt to extend the digital approach, which in the past decade has already been applied to inscriptions, coins and papyri1, to Byzantine seals, and, with minor adjustments, to other coin-like objects, such as bread stamps. Already presented to the international community of Byzantine sigillographers during the 12th International Symposium of Byzantine Sigillography held in May 2019 at the Hermitage Museum, St. Petersburg, SigiDoc will be presented at the 9th Annual Conference AIUCD for the first time to a wide community of digital humanists. Seals are the only survivors of the written documents used in daily administration of the Byzantine Empire, the former Eastern Roman Empire (4th to 15th century). These small discs, mostly made of lead, commonly have an iconographic representation on one side (a saint, the Virgin, etc.) and a legend \u2014 with name, at times surname, dignities, functions, places \u2014 on the other. Hence, what we call \"seal\" is both the object bearing an impression and the impression itself, left on the disk by a tool called boulloterion, i.e. the matrix (only six of these tools survive nowadays in comparison to the extant 80\/100,000 seals' impressions). The seals greatly 1  A comprehensive and up-to-date list of projects related to inscriptions, coins and papyri within the Digital Humanities can found at: <https:\/\/wiki.digitalclassicist.org\/Category:Projects>. be 240  \fimproved our knowledge of the central and provincial administrative apparatus of the State, the Church, and the Army, and thanks to them the Byzantine administrative, political, and social history is gradually being rewritten. But the impact of the seals is still limited, since access remains restricted to a very small number of researchers, the great amount of unpublished material is widely scattered across museums, libraries, and private collections, and the paper publications are fragmented and far from being widely available. Despite this situation, unlike sister ancillary disciplines such as epigraphy, papyrology or numismatics, Byzantine sigillography has not received much attention from the Digital Humanities for a long time, although this has been a wish of at least part of the scientific community since the 2000s. Resumed and abandoned several times without significant results, the project has finally been revived by the authors. SigiDoc strives to combine traditional research methods and scholarship in the field of Byzantine sigillography with the technologies offered by the Digital Humanities. In fact, standards for scientific publications within Byzantine sigillography have been established during the 20th century with the works by Vitalien Laurent2, George Zacos3, Nicolas Oikonomides4, Jean-Claude Cheynet5, and Werner Seibt6: hence, one of the main tasks of SigiDoc has been to convert these standard publications into a digital and TEI-compliant form, while partially reassessing them and providing them with more consistency.  2  What SigiDoc is and for what purpose it has been developed  At the time this paper is written, SigiDoc is in an advanced beta version and it will hopefully be close to its 1.0 version by the time the AIUCD annual conference takes place. SigiDoc is largely based on the ongoing experience of EpiDoc, a collaborative effort that provides guidelines and tools for the encoding of scholarly editions of ancient documents; alongside the scientific objectives, EpiDoc developed also an educational asset, with the didactic experiences of EpiDoc training weeks, showing great potential in teaching traditional epigraphy and papyrology.7 EpiDoc uses a subset of the TEI standard for the representation of texts in digital form8, having clearly established itself as the most robust and widely supported format for encoding editions of ancient texts on different text-bearing objects. Far from being only a simple adaptation and implementation of existing solutions, from a technical point of view, SigiDoc is: 1) a schema, merged with EpiDoc's one; 2) a template, i.e. SigiDoc's edition structure; 3) a stylesheet for web visualisation; 4) a set of stylesheets for the critical edition of the legends on seals, derived from EpiDoc but adapted to the needs of Byzantine sigillography; 5) a highly customised version of EFES (see below); 6) a set of guidelines (for metadata, leidenisation, indexing, etc.); 7) a set of files intended to be shared among all the future SigiDoc projects and expanded through the time and the experience, in order to avoid superfluous duplications on one hand, and to ensure consistency on the other (IDs' lists, controlled vocabularies, authority lists, ontologies, etc.).9 SigiDoc is intended for both the creation of digital-born editions of Byzantine seals as well as for the digital conversion of paper publications, and it has been conceived in order on one hand, to provide the users with a common ground for developing their projects, on the other, also to give them the freedom to customise their approach. During its presentation in St. Petersburg, the community of Byzantine sigillographers gave an enthusiastic feedback to SigiDoc, and several projects aimed at the creation of online corpora are now waiting for SigiDoc 1.0 to be released, thus ensuring both its dissemination and implementation. These projects involve leading scholars in the field of Byzantine studies \u2014 such as Prof. Jean-Claude Cheynet, Prof. Claudia Sode, Dr. Vivien Prigent \u2014 as well as important cultural institutions, such as the Biblioth\u00E8que Nationale de France (Paris), the 2  See Vitalien Laurent. 1963\u20131981. Le Corpus Des Sceaux de L'empire Byzantin, Voll. 1\u20135, Paris. See George Zacos and Alexander Veglery. 1972. Byzantine Lead Seals, Vol. 1, Basel. 4 See Nicolas Oikonomides. 1986. A Collection of Dated Byzantine Lead Seals. Washington, D.C. 5 See Jean-Claude Cheynet, Turan Gokyildirim, and Vera Bulgurlu. 2012. Les Sceaux Byzantins du Mus\u00E9e Arch\u00E9ologique d'Istanbul, Istanbul; id. and Maria Campagnolo-Pothitou. 2016. Sceaux de la collection Georges Zacos au Mus\u00E9e d'art et d'histoire de Gen\u00E8ve, Geneva. 6 See Werner Seibt. 1978, Die byzantinischen Bleisiegel in \u00D6sterreich I. Teil, Kaiserhof, Vienna; id. and Alexandra-Kyriaki Wassiliou. 2004. Die Byzantinischen Bleisiegel in \u00D6sterreich, Vol. 2, Zentral- und Provinzialverwaltung, Vienna. 3  7  Gabriel Bodard and Simona Stoyanova. 2016. Epigraphers and Encoders: Strategies for Teaching and Learning Digital Epigraphy, in Gabriel Bodard and Matteo Romanello (eds.), Digital Classics Outside the Echo-Chamber. Teaching, Knowledge Exchange & Public Engagement, p. 51\u201368, London, available: <http:\/\/dx.doi.org\/10.5334\/bat>. 8  Tom Elliott, Gabriel Bodard, Hugh Cayless et al. 2006-2016. EpiDoc: Epigraphic Documents in TEI XML. Online material, available: <http:\/\/epidoc.sf.net>, delivers thorough information about history and mission of EpiDoc and offers always up-to-date versions of the EpiDoc Guidelines as well as documentation, software and tools to work with EpiDoc. 9  In section 3, this paper will address only a selection of these topics. 241  \fDumbarton Oaks Research Library and Collection (Harvard's research institute for Byzantine Studies in Washington D.C.), the Epigraphic and Numismatic Museum of Athens and the Geneva Museum of Art. Researchers, museums, public institutions, as well as private collectors will be able to get a definite and stable record of their Byzantine seals, thus preventing deterioration and making their collections available for research, teaching, and presentation to the general public. Thanks to the increasing number of SigiDoc-based projects, the long-term aim is to ensure dissemination, sharing, and sustainability of the data, and to make available a very wide range of published and unpublished material, edited to a high scholarly standard. SigiDoc has not been conceived just to realise individual projects (an interesting, though limited aim): the use of the same guidelines and set of tools is intended to allow the creation of a common search interface, through which all corpora will be virtually unified in a higher-order catalogue, enabling actions going from the simple cross-referencing to the advanced search throughout every corpus published in SigiDoc standard. Through a dedicated website, the developers will ensure a proper dissemination of SigiDoc. The website (http:\/\/sigidoc.huma-num.fr\/) \u2014 which is, as for now, empty \u2014 will host SigiDoc's documentation as well as all the aforementioned materials needed to run it. Through this site the user will be informed about the life of SigiDoc: status of SigiDoc-based projects, training sessions and events, technical updates, etc. In order to use SigiDoc, a formal training is needed. Training weeks as well as shorter training events will take place regularly (once or twice a year): they are inspired by the well-established EpiDoc training weeks, which have repeatedly shown the feasibility as well as the effectiveness of this teaching format. Through in-depth training, SigiDoc will be able to increase the dissemination and continuity of sigillography itself, while the creation of a new professional figure, i.e. the digital sigillographer, will facilitate the integration between the traditional and the digital approach to Byzantine seals. Consequently, the foundation of a more interrelated scientific community will be laid: a network of digital Byzantine sigillographers is still a desideratum within the larger community of Byzantine sigillography; in order to achieve this goal, a common scientific and prac-tical ground delivered by a standard like SigiDoc is much needed.  3 Main Features of SigiDoc 3.1 TEI-XML Template and Data Encoding SigiDoc XML template organises the information in hierarchical mark-up inside three main common ele-ments of the standard TEI structure: 1) <teiHeader\/> for metadata; 2) <facsimile\/> for the digital reproduction of the artefact; 3) <text\/> for the legend's critical edition, commentary and bibliography. teiHeader: among the data nested inside this element, SigiDoc stresses the importance of providing each seal with a unique numerical identifier (being it the first attempt of systematic categorisation in Byzantine sigillography).To preserve consistency, a common file of ID numbers will be shared among all the SigiDoc projects. Several thesauri and controlled vocabularies are being prepared: among them, the classification of the seal (imperial, military, etc.), the milieu of the issuer, the language(s) of the legend, the work type (original impres-sion, drawing, verbal description, etc.), the material, the layout (iconography only, text only, both, etc.), the execution (struck, cast, printed, etc.), the shape, the iconography (see below). All these lists will be provided in different languages (English, French, German, and Italian by default, but each project will be able to cus-tomise their languages). The preservation history of the seal is a major concern, not only in establishing which is the current repos-itory of a seal, but also in being able to follow it through its different displacements, which is of the utmost importance especially when the seal enters a private collection or is sold in an auction. Byzantine seals are increasingly present in online auctions: thanks to SigiDoc it will be easier to follow them before they disappear in private collections; the leading journal in the field \u2014 Studies in Byzantine Sigillography10 \u2014 includes a final section listing the seals sold through auctions, but its biannual publication (without photos of the seals) limits its effectiveness, whereas with SigiDoc the information will be updated without any delay, thus promot-ing its circulation and its scientific study. An important part of the metadata is devoted to the findspot and the find circumstances: these data contribute significantly to the historical interpretation of the seal, helping to establish both the areas directly administrated by the Byzantine Empire and those \u2014 outside the Empire \u2014 of contact or influence. Unfortunately, 10  See <https:\/\/www.degruyter.com\/view\/serial\/36534>. 242  \fthis kind of data is often lacking for Byzantine seals, and this makes particularly valuable the preservation of this information and its linking with similar data among different corpora. The iconography deserves here a special mention: this is a key element to Byzantine sigillography, but also one of the most challenging due the numerous and specific iconographic typologies to be found on Byzantine seals. In SigiDoc 1.0 this topic will be addressed in two ways: a short and general identification of the iconographic theme (according to a shared controlled vocabulary), and a detailed description. However, the degree of details in iconographic description is perhaps the most changing criterion in Byzantine sigillographic editions: this is the reason why a standard tool for the description of images is being developed, in order to introduce consistency in sigillographic editions, and to restore the importance of this feature, too often neglected. This tool will be released after the launch of SigiDoc 1.0. Links and relationships both within the corpora, and between texts and external datasets can be established and realised as hyperlinks in SigiDoc: the data is being enriched and its interoperability is being increased using online resources and authority files such as prosopographies and geographical gazetteers.11 Facsimile: The digital reproduction of the seals will be displayed as a digital facsimile above the edition: in case of seals in bad state of preservation, some of the ongoing projects (especially those based in Cologne and Paris) will provide images created with RTI technology (Reflectance Transformation Imaging).12 Of course, digital reproductions won't be always available, especially for seals edited between the 19th and the 20th century and of which no trace can be found: in this case, we have often drawings or verbal descriptions. Text: As far as the critical edition of the seals is concerned, variant readings and restorations are encoded in TEI, thus enabling the generation of apparatus criticus, parallel texts, and diplomatic editions; moreover, the leidenisation of the legend allows for a full editorial interpretation based on the Leiden conventions (especially Panciera), but adapted according to the sigillographic editorial standards.13 For the diplomatic edition, SigiDoc uses AthenaRuby14, a Unicode-compliant font based on the lettering\/epigraphy of Byzantine coins and seals, and designed by Joel Kalvesmaki at the Center for Byzantine Studies at Dumbarton Oaks (Washington D.C.). AthenaRuby is currently the most accurate Greek font in terms of the lettering of Byzantine coins and seals. It is not yet widely used within the sigillographic (and numismatic) community due to the preference accorded by them to more abstract yet more approximate fonts, such as New Athena. Being the lettering of the legend a key factor in dating and contextualising a specimen, SigiDoc's developers promote and strongly recommend the use of AthenaRuby for the encoding of Byzantine seals: the diplomatic edition carried out with this font delivers a more accurate representation of the seal's lettering, thus facilitating its understanding even without the observation of the digital reproduction. 3.2 Web Visualisation and Data Valorisation: Contribution from (and to) EFES (EpiDoc Front-End Services) EFES (EpiDoc Front-End Services)15, which builds upon existing tools such as Kiln16, is a highly customisable platform which allows expert and less expert users to get, in a relatively easy and fast way, four main critical features for a TEI-XML based corpus: multiple indices, multilingual options, faceted search interface, and a (raw) webpage. TEI-XML files created in SigiDoc, their XSLT stylesheets, as well as part of their tagging have been designed to be best dealt with in EFES. In 2018 SigiDoc became one of the pilot projects using and testing EFES and, in this way, actively contributing to its development, being also the only non-strictly epigraphical project. The most notable contribution of EFES to SigiDoc is certainly the creation, based on Authority Lists, of automatic indices. The lemmatisation of words and the identification of relevant entities within the legend, as well as the encoding of key words and terms, enable the indexing of words, personal names, geographical entities, offices, titulatures, and other features of philological, epigraphical, and historical interest at large. 11  See, for example, Prosopography of the Byzantine World (<http:\/\/pbw2016.kdl.kcl.ac.uk\/>); Prosopographie der mittelbyzan-tinischen Zeit (<http:\/\/www.degruyter.com\/view\/db\/pmbz\/>); Pleiades (<http:\/\/ pleiades.stoa.org\/>). 12  For further information regarding RTI technology and its application in sigillographic studies, see Franz Fischer and Stephan Makowski. 2017. Digitalisierung von Siegeln mittels Reflectance Transformation Imaging (RTI), Paginae historiae \u2013 Sborn\u00EDk N\u00E1rod-n\u00EDho archivu, 25\/1, p. 137\u2013141, available: <http:\/\/kups.ub.uni-koeln.de\/id\/eprint\/7882>. 13  For example, the rendering after transformation of several kinds of <gap\/> tags has been changed. AthenaRuby is an OpenType and Unicode-compliant font. For documentation, tools, and selected bibliography visit: <https:\/\/www.doaks.org\/resources\/athena-ruby>.  14  15 See <https:\/\/github.com\/EpiDoc\/EFES\/wiki> for the technical documentation and <https:\/\/github.com\/ EpiDoc\/EFES\/wiki\/User-Guide> for detailed guidelines and user guide. 16 See <https:\/\/github.com\/kcl-ddh\/kiln> for the documentation.  243  \fSigiDoc users will be able to potentially index every feature deemed relevant for their corpus: the previous listing of indexed features \u2014 featuring the most common indices in Byzantine sigillographic publications \u2014 is what the authors recommend to all future SigiDoc projects in order to harmonise their indices. Nonetheless, a higher degree of specialisation will be enabled: for example, thanks to the consistent use of AthenaRuby, it will be possible to index \u2014 and, ultimately, to search for \u2014 single variant letters within the legend. The using made by SigiDoc of EFES is essentially based on the customisation of the solutions offered by it: this is especially true for the use of Athena Ruby instead of generic Greek capital letters in the diplomatic edition; the XSLT stylesheet organising the webpage, in order to create a list of fields appropriate for Byzantine sigillography; the EpiDoc stylesheets used for the edition of the legends; and, of course, the customisation of the indices and the search interface. During the next months some improvements related to EFES \u2014 mainly concerning further indexing features and automatic bibliographic references \u2014 will be delivered.  Figure 1. From the seal (left) to the EFES-generated webpage (right), through the SigiDoc XML template (middle).  Conclusions Digital Byzantine sigillography is an entirely new discipline: SigiDoc has therefore been designed for users with no prior computer skills but with a background in Byzantine sigillography or Byzantine history at large. SigiDoc's aim is to deliver a (reasonably) easy and ready-to-use tool, with a template ready to be filled in according to the need of each project; but this also means that a more advanced user will be able to go further and customise it to a larger extent. The visibility and availability of data coming from an increasingly number of seals, jointly with the possibility of establishing relations among them, will push further our knowledge of several aspects of Byzantine history, allowing the specialists to carry out analysis in several directions (i.e. the structure of Byzantine society and the relations among individuals and families, the organisation of the administration, the role of personal piety in the choice of the iconography, art history, but also sigillographic epigraphy or lettering, the \"writing uses\", the Greek language, etc., including the evolution of all these aspects throughout the centuries). Albeit most of these research directions (and many others) already existed before SigiDoc, thanks to it their analysis will be greatly enhanced, in a way and to an extent extremely difficult to reach without this tool.  Acknowledgments The authors would like to thank the anonymous reviewers for their suggestions, which have been taken into account where appropriate."
	},
	{
		"id": 39,
		"title": "Preliminary results on mapping digital humanities research",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Gianmarco Spinaci",
			"Giovanni Colavizza",
			"Silvio Peroni"
		],
		"body": "1  Introduction  It is a scientometrics trope to consider humanities research as poorly indexed in citation databases, and thus poorly understood in terms of research outputs (Hammarfelt, 2016). Several studies have pointed out to the limitations of indexes such as Web of Science and Scopus with respect to the humanities, both in terms of quantity and quality (e.g., lack of books) (Nederhof, 2006). Nevertheless, in recent years more indexes have become available, such as Dimensions and Microsoft Academic, while coverage has been improving (Harzing and Alakangas, 2016). In view of these developments, a comprehensive and cross-index map of research in the (digital) humanities is still pending. Some previous work has considered the intellectual and social organization of the digital humanities using bibliometrics. Nyhan and Duke-Williams (2014) focused on collaboration patterns in the journals Computers and the Humanities and Literary and Linguistic Computing (up to 2011), finding a propensity to collaborate within small, tight groups and a persisting tendency for single-author publishing. It is worth noting that more recent work on the humanities as a whole found an increasing propensity for  246  \fcollaboration, albeit with high variation among different disciplines\/departments (Burroughs, 2017). Citation analyses based on Computers and the Humanities, Digital Scholarship in the Humanities and Digital Humanities Quarterly highlighted instead a sparser organization, around thematic areas such as information studies, historical literature, linguistics, natural language processing and statistical text analysis (Gao et al., 2017, 2018). Further work based on the Journal of Digital Humanities, Digital Humanities Quarterly, International Journal of Humanities and Arts Computing, Digital Medievalist, Digital Studies, Literary and Linguistics Computing assessed co-authorship, co-citation and bibliographic coupling networks. The authors found a sustained growth in digital humanities publications, coupled with increasing integration with respect to citation networks, and persisting fragmentation with respect to collaborations as mapped by co-authorship relations (Tang et al., 2017). A recent bibliometric comparison considered the annual conference of the Italian Association of Digital Humanities and Digital Culture (AIUCD) and the annual Italian conference on computational linguistics (CLiC-it). Results show how collaborations are sparser in digital humanities, how research methodologies usually are introduced in the computational linguistics conference and then readopted in the digital humanities one, and how the citation behaviour in the latter one closely resembles that of humanities scholarship (e.g., higher ratio of references to books) (Sprugnoli et al., 2019). Altmetrics data has also been used to map the digital humanities community worldwide. In particular, Twitter follower and co-retweet networks were used to show how the community is organized around few 'influencers' and according to language and geographical region (Grandjean, 2016; Gao et al., 2018). Finally, some scholars attempted to position the digital humanities within the broader context of humanities scholarship (Leydesdorff and Akdag Salah, 2010; Salah et al., 2015).1 In this paper, we address the following research questions: a) what qualifies as digital humanities research, from a bibliometric point of view? b) What is the coverage of citation indexes with respect to digital humanities research? c) What is the organization of the resulting map of research? We propose an iterative method to individuate digital humanities publications by combining manual journal classification and automatic citation clustering. One of the outcomes of our work is the first version of a list which includes digital humanities journals. We use this list to assess the number of digital humanities journal publications indexed by Web of Science (WoS), Scopus, Crossref and Dimensions. Finally, we use the citation data included in the index with most digital humanities publications, i.e., Dimensions (Hook et al., 2018), to present a map of digital humanities research based on journal articles. It is worth noticing that our results are still preliminary and stem from ongoing work to create a comprehensive map of humanities research.  2  Data and methods  Database coverage limitations notwithstanding, individuating digital humanities (DH) publications is problematic in itself. First of all, there is little agreement on what constitutes DH research among practitioners. Secondly, DH research tends to be highly interdisciplinary, so much so that clear-cut classifications would be intrinsically arbitrary. We adopt here a combination of top-down journal level classification, in view of expanding the ERIH-Plus journal list 2, and a bottom-up clustering approach, where we use citation clusters to find candidate journals to be added to the list. More in detail, we perform the following steps: 1. create a seed list of known journals in DH, by disseminating a survey to the participants to DH 2019 and in the Humanist mailing list, which resulted in obtaining 14 replies; 2. consider a fine-grained clustering of all publications within each citation index, obtained by using the Leiden algorithm (Traag et al., 2019) and following the heuristics proposed in Waltman and van Eck (2012); 1A reasoned review of quantitative analyses of the digital humanities is maintained by Scott Weingart at http:\/\/ scottbot. net\/dh-quantified 2See https:\/\/dbh.nsd.uib.no\/publiseringskanaler\/erihplus  247  \fFigure 1: DH articles in Dimensions according to their journal category: 'exclusively' (green) and 'significantly' (pink). Only articles with with one citation or more (34.4% of the total) were considered. The network was visualized using Gephi 0.9.2 and the Force Atlas 2 layout. The size of the nodes (i.e., the articles) is proportional to number of citations they receive.  3. detect which clusters contain a relatively high proportion of publications from the journals in the list, in order to identify those which are also highly represented in the detected clusters but that are not part of the list obtained in point 1. We do this by considering the top 5 journals (by number of articles) per cluster with more than 5% of articles already from DH journals within the list; 4. manually assess each journal in the set identified in the previous point so as to add it in the original list; and 5. iterate again from point 2 until a convergence criterion is met.3 As convergence criterion, we iterate the proposed method twice (i.e., seed plus first iteration) and focus exclusively on research articles or review articles as publication typologies, published from the year 2000. This approach follows previous work in citation indexing for the humanities (Colavizza et al., 2018). 2.1 Journal classification Given the highly interdisciplinary character of most publications in DH, we classify journals in the list using three categories: exclusively, if we deem a journal to be solely devoted to DH; significantly, if we deem that at least 50% of publications in the journal can be considered DH; marginally, if the journal contains an estimated 5% to 50% publications in DH. Categories were assigned by survey participants (iteration 1) or the authors (iteration 2) independently, and disagreements solved by majority. We acknowledge as a limitation the subjective perspective and biases this approach might have introduced in the resulting list of journals.  3 Results The first outcome of our study is a list of DH journals (Spinaci et al., 2019), arranged according to the proposed categories, and containing 19 'exclusively', 17 'significantly' and 64 'marginally' classified journals.4 3Due to data access constraints, we worked with the following versions of the citation indexes under consideration: Web of Science: December 2018, Scopus: May 2019, Dimensions: December 2018, Crossref: August 2018. Coverage results might be affected accordingly. 4The list has been also made available in a Google sheet (https:\/\/tinyurl.com\/y6rfrsuw) which can be commented for further feedback.  248  \fFigure 2: DH articles in Dimensions according to their related journal, if it contained more than 2% of the visible articles. The underlying network and layout is created as in Figure 1. The journal names are: 1) Language Resources and Evaluation; 2) AI & Society; 3) Literary and Linguistic Computing; 4) D-Lib Magazine; 5) Computational Linguistics; 6) Journal of Quantitative Linguistics; 7) International Journal of Humanities and Arts Computing; 8) Enthynema; 9) International Journal on Digital Libraries; 10) Digital Scholarship in the Humanities; 11) Virtual Archaeology Review; 12) Journal on Computing and Cultural Heritage. Table 1: Database coverage for journals in the three categories. Journal Exclusively Significantly Marginally Total 3.1  WoS 1243 1096 39,655 41,994  Scopus 1858 4421 40,439 46,718  Crossref 3989 5395 55,126 64,510  Dimensions 2751 7259 117,782 127,792  Coverage  The overall database coverage, expressed as the number of indexed articles per category, is shown in Table 1. Crossref has the best coverage with respect to 'exclusively' journals, while Dimensions has better coverage in the 'significantly' and 'marginally' categories. The publication coverage of journals in the 'exclusively' category is shown in Table 2. Only few journals show a good coverage, including some non-active ones: Computers and the Humanities, Digital Scholarship in the Humanities, International Journal of Humanities and Arts Computing, Journal on Computing and Cultural Heritage, and Literary and Linguistic Computing. Many other DH journals we collected in (Spinaci et al., 2019) were either poorly represented or even not present in the indexes we used for the analysis. Crossref appears to be the most comprehensive database in this respect. 3.2  Map of research  We further present a preliminary map of DH research, focusing on journal articles from the 'exclusively' and 'significantly' categories. We chose Dimensions for this analysis in order to better explore its apparently complementary coverage with respect to both categories and with respect to previous work (Gao et al., 2017; Tang et al., 2017; Gao et al., 2018). Coverage in the 'significantly' category is mostly due to work in computational linguistics and digital libraries: Journal of Quantitative Linguistics (574 articles), Computational Linguistics (759), D-Lib Magazine (1054), Language Resources and Evaluation (2076). We also highlight the presence of almost 1500 articles from the journal AI & Society, a topic of increasing interest in DH. The map shown in Figures 1, 2, 3 considers all articles with at least one (given or received) citation, that is to say articles with a degree of one or more. The network initially contains 10,010 articles and 5,283 citation edges, while the number of articles with citations is 3,446 (34.4% of the  249  \fFigure 3: DH citation clusters calculated using the citations available in Dimensions, considering only the clusters containing more than 2% of the visible articles in the dataset. The legend contains the five most frequently occurring words in the titles of the articles within each cluster, after filtering out uninteresting ones. The underlying network and layout is created as in Figure 1. The journal coverage for each cluster is as follows (we only include journals accounting for more than 10% of the articles within a cluster): 1) Literary and Linguistics Computing (39.7%), Language Resources and Evaluation (19.3%), Digital Scholarship in the Humanities (17.6%), Journal of Quantitative Linguistics (17%), 2) Computational Linguistics (55.4%), Language Resources and Evaluation (34.3%), 3) Journal of Quantitative Linguistics (91.2%), 4) Language Resources and Evaluation (91.1%), 5) Language Resources and Evaluation (55%), Computational Linguistics (32.5%), 6) AI Society (99%), 7) International Journal on Digital Libraries (50.8%), D-Lib Magazine (37.1%), 8) Computational Linguistics (53.6%), Language Resources and Evaluation (34.8%) total). Two thirds of the articles are not connected to any other article through citations. The network's layout was created using Force Atlas 2 (Jacomy et al., 2014). Figure 1 shows the articles assigned to the categories 'exclusively' and 'significantly'. The bulk of the articles in 'exclusively' journals include, somewhat predictably, the articles in Literary and Linguistic Computing or its successor, Digital Scholarship in the Humanities. However, we noticed that, when considering the most represented journals in our dataset (i.e., those with most articles) in Figure 2, the articles are visually arranged by journal and tend to follow field-specific patterns: digital libraries (IJDL, D-Lib), computational linguistics (Computational Linguistics, the Journal of Quantitative Linguistics), artificial intelligence and society (AI & Society), and DH (Literary and Linguistic Computing, Digital Scholarship in the Humanities). Instead, the articles in Language Resources and Evaluation are more evenly spread across different field clusters. When we consider citation clusters detected using a modularity-maximizing method (Blondel et al., 2008), in Figure 3, we observe a modular structure with a high correlation with respect to the publication venue. The main focus of the DH cluster (number 1 in Figure 3) are quantitative literary studies, e.g., stylometry and authorship attribution. Other clusters cover the DH-related areas of computational linguistics and natural language processing (2,3,4,5,8), digital libraries (7) and AI and society (6). As far as we could notice from this graph, DH publications tend to connect to related disciplinary areas, even if each area maintained its distinctiveness. The publication venue remains a key trait of the intellectual structure of the DH.  4  Conclusion  In this article, we proposed an approach to find digital humanities publications by iterating between a list of journals (top-down) and its expansion using citation clustering (bottom-up). In this way, we were able to propose a first version of a list of digital humanities journals split in three categories: those that are 'exclusively', 'significantly' and 'marginally' related to the digital humanities. We assessed the  250  \fTable 2: Database coverage for journals exclusively devoted to digital humanities scholarship. Journal Computers and the Humanities Digital Humanities Quarterly (DHQ) Digital Medievalist Digital Scholarship in the Humanities (DSH) Digital Studies \/ Le champ num\u00E9rique Digit\u00E1lis B\u00F6lcs\u00E9szet \/ Digital Humanities Frontiers in Digital Humanities International Journal of Digital Humanities International Journal of Humanities and Arts Computing Journal of Cultural Analytics Journal of Data Mining and Digital Humanities Journal of Digital Archives and Digital Humanities Journal of Digital Humanities Journal of the Japanese Association for Digital Humanities Journal of the Text Encoding Initiative Journal on Computing and Cultural Heritage (JOCCH) Literary and Linguistics Computing Revista de humanidades digitales Umanistica Digitale Total  WoS 663  Scopus 806 38  254  237  15  75 251  178 584  1243  1858  Crossref 1465  Dimensions  67 327 230  66 388  58  19 66  253 13  475 27  15 73  21  1465 23  202 1450 37  3989  2751  coverage of Web of Science, Scopus, Crossref and Dimensions in this respect, finding that Crossref has the best coverage of 'exclusively' digital humanities journals, while Dimensions has the best coverage of the number digital humanities-related articles overall. We discussed a first map of research using citation data from Dimensions, highlighting how just one third of the articles in Dimensions are connected with each other via citations. We further found that digital humanities articles are connected via citations to computational linguistics and natural language processing, digital libraries and other developing areas such as AI and society. Nevertheless, we also found that the venues (i.e., the journals) strongly overlap with citation clusters, and are a key trait of the intellectual organization of digital humanities research. We acknowledge that our work is still in progress, and thus it has a set of limitations which we plan to address in the future. In particular, we plan to include additional bibliographic entity types in addition to journal articles (e.g., books), and to also include the COCI (Heibi et al., 2019) and Microsoft Academic citation indexes to the comparison. Coverage will also be assessed at the article level (i.e., which citation index contains which articles) and chronologically. Lastly, we will elaborate on the map of research by including a comparison across all indexes.  Acknowledgements The authors would like to thank the Centre for Science and Technology Studies (CWTS), Leiden University, for providing access to their databases and computing facilities. This work was in part conducted when Spinaci was visiting CWTS."
	},
	{
		"id": 40,
		"title": "Epistolario De Gasperi: National Edition of De Gasperi’s Letters in Digital Format",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Sara Tonelli",
			"Rachele Sprugnoli",
			"Giovanni Moretti",
			"Stefano Malfatti",
			"Marco Odorizzi"
		],
		"body": "1 Introduction Alcide De Gasperi (1881-1954) was one of the most important statesmen in European history, recognized as the father of the Italian Republic and one of the founding fathers of Europe. Despite the rich historiography about him, many aspects of his biography are still waiting to be enlightened. Indeed, up until now, the historical analysis on De Gasperi has been based almost exclusively on institutional sources, on his published works and on his public speeches (De Gasperi, 2006, 2008a,b, 2009), also available in digital format1 (Sprugnoli et al., 2016). What is still missing is his extraordinary correspondence, which covers all the critical steps of his biography and often offers a different point of view on many events of the time. So far, only very few letters written or received by De Gasperi have been made available through anthological publications, see (De Gasperi, 1955, 1974) among others, or through the publication of the correspondence with specific people, e.g. (Antonazzi, 1999). Nevertheless, a large number of letters has not been published yet. This has paved the way to this ambitious project, coordinated by a scientific committee that brings together all the major scholars of De Gasperi, whose goal is to provide an exhaustive collection of De Gasperi's correspondence in digital format, covering both his public and private life. A preliminary search of the letters to be digitized and edited lists about 5,000 documents written in multiple languages, currently stored in 114 archives all over the world. More than 1,500 of these letters are already available online at the time of writing, all searchable and accessible through an online platform: https:\/\/epistolariodegasperi.it\/#\/archivio_digitale\/lettere. The Italian Ministry of Culture supports the project, which has been recognized as a 'National Edition'. Significantly, this is the first Italian National Edition exclusively conceived through digital tools. 1http:\/\/alcidedigitale.fbk.eu\/  253  \f2  Related Work  Editorial projects involving the digitization of documents are always increasing and the standards that characterize them are constantly evolving (Pierazzo, 2014). The literature reports a rich and varied panorama of digital editions of works by writers, intellectuals and historical figures (Franzini et al., 2016). In recent years, also the digitization of letters has received much attention (Hankins, 2015) with projects dedicated to individual characters, such as Darwin2 and Van Gogh3, or that aim to reconstruct large epistolary networks (Ravenek et al., 2017; Baillot and Seifert, 2013). A centralized web service to search within the metadata of diverse scholarly editions of letters has been also developed (Dumont, 2016). If we look at the Italian landscape, we can see that the development of digital editions of letters is rather limited and that current initiatives have mostly focused on the collection and digitization of literary documents created in the Late Medieval and modern era, such as in the Progetto Datini4 and in the digital edition of the letters by Vespasiano da Bisticci (Tomasi, 2012). In our opinion, the project presented in this paper has at least four significant aspects to be highlighted if compared with other works and that constitute our contribution: (i) the strong interdisciplinary synergy that led to define a rich set of metadata and editorial choices taking into consideration different aspects (i.e., linguistic, historical, philological); (ii) the development of a transcription infrastructure, characterized by three access levels and a graphical interface that is both intuitive and compliant with existing editing standard and that we plan to release for the research community so to be adopted in other projects; (iii) the subject of the edition, that is the exhaustive collection of correspondence related to contemporary history until now closed in archives and difficult to reach; (iv) the attention to different types of final users so that mechanisms have been implemented in order to allow a customization of the reading complexity.  3  Editorial Practice  Several transcribers, mostly history scholars, have been involved in the project to digitally acquire De Gasperi's correspondence. They have been provided with an ad-hoc tool (Moretti et al., 2018), through which they have inserted various metadata for each document, including: sender, recipient, chronotopical data, type of document (letter, telegram etc.)5 (Malfatti, 2019). They also specify if the document is an original, a copy or a draft and if the topic is of a personal or institutional nature. Presence of envelope, letterhead, autograph signatures, attachments have to be reported as well. Transcribers must also indicate for each document the number of cards, writing technique (manuscript\/typewritten), conservation status, as well as the reference code for the correct identification of the document. Transcriptions must accurately reflect the text and the layout of the document, including possible mistakes, illegible words and cancellations, punctuation and paragraph division, etc. Margin notes have to be reported in a specific field, possibly with the corresponding position and author. Historical (or commentaries) notes clarify and integrate the content of the text. The transcript must also be preceded by an abstract to contextualise and summarize its content.  4  Transcription Tool  For the acquisition of De Gasperi's letters we developed a new infrastructure that allows different transcribers to work in parallel and to smoothly publish the transcribed documents online as soon as they have been revised by a small pool of experts. Specifically, the infrastructure functionalities are the following: \u2022 possibility to work both offline and online, so that the transcriptions can be performed also in archives without an Internet connection; \u2022 use of a wide set of metadata defined by the Scientific Committee and compliant with the Dublin Core standard and the standard for the cataloging of manuscripts in Italian libraries; 2https:\/\/www.darwinproject.ac.uk\/ 3http:\/\/vangoghletters.org\/ 4http:\/\/datini.archiviodistato.prato.it\/ 5The editorial guidelines are available online: https:\/\/epistolariodegasperi.it\/#\/risorse  254  \f\u2022 clear division of roles into three types of contributions: (i) the transcriber, who creates a digital picture of the original document and uploads it in the transcription tool, inserts metadata, transcribes and annotates the text, (ii) the supervisor, who adds the critical apparatus, and (iii) the editor, who is in charge of validating the correctness of the transcription and publishes the letter in the final online platform; \u2022 automatic upload and update of each letter on a central database; (v) easy-to-use transcription interface not requiring explicit knowledge of tag annotation and coding schemes; \u2022 easy-to-use transcription interface not requiring explicit knowledge of tag annotation and coding schemes; \u2022 presence of autocomplete features to reduce the risk of mistakes in the insertion of metadata. From the technical point of view, the infrastructure includes a MySQL database and three software applications (one for each role) written in Javascript\/ECMAScript 6. The interface is implemented using the ReactJS framework.  5  Digital Archive  Currently, the digital archive allows to search by title, sender, recipient, type (i.e., letter, telegram, postcard, note, other), theme (i.e. private life, national politics, international politics, local politics, religion, culture, economy, society, political party), free text. A slider can be used to select a specific time period from 1902 to the year of De Gasperi's death in 1954. The interface responds in real time to what the user types, modifying the list of letters that correspond to the search key. Searches can also be combined: for example, it is possible to search for all the notes sent by Giulio Andreotti after 1950 or for all the postcards containing the word 'augurio'. When clicking on a letter, all the metadata compiled by the transcriber are displayed together with an abstract written by a historian that summarizes the content of the document. The photograph and the transcription are placed next to each other and the transcription respects the annotation made by the transcribers showing, among others, words that are underlined, erased parts, spelling errors. Footnotes explain content that could be unclear to the reader adding contextualization. In addition, 380 proper names of persons are accompanied by a biographical note written by historians. Given that the aim of the project is to attract both general public and experts, readers can access the National Edition from different perspectives. More specifically, with a simple click the user goes from a complete transcription of the letter containing all the annotations to a simplified one that is more suitable for the general public: in this reader-friendly view of the letter the deleted parts are not displayed, the typos are corrected and the abbreviations are extended.  6  Current State of the Project  The transcription process started in August 2018 and it currently involves 38 transcribers and 10 supervisors. At the moment of writing (September 2019), 1,549 letters from 106 different private and public archives in Italy and abroad have been transcribed, annotated, revised and published online. The number of letters written in each year currently present in the digital archive is shown in Figure 1. As expected, most letters are exchanged between 1945 and 1954, when De Gasperi was a very prominent political figure with key roles in the Italian government. However, we observe an interesting element for the years between 1926 and 1942, which historians describe as De Gasperi's internal exile, because he did not cover official roles under fascism: while the archive of public documents of Alcide De Gasperi6 contains only few documents for that period (Moretti et al., 2016), 132 letters have been found for the same years, and 24 of them have been tagged as being about Politics. This shows that, even if De Gasperi did not have any official role, he was still involved in political discussions and was expressing his opinion on the current situation. 6Available at http:\/\/alcidedigitale.fbk.eu\/  255  \fFigure 1: Letter distribution over time Additional statistics are reported in Table 1, where we list the five senders and receivers that are currently most present in the correspondence. De Gasperi appears in most letters either as sender and as receiver. Only in few examples he is not explicitly mentioned, for instance when the receiver is a collective entity (e.g. \u2018Governo italiano', \u2018Soci della Tridentum') where De Gasperi was included. Some persons appear in the table because a version of their correspondence had already been curated and published before, see for example Sturzo (Antonazzi, 1999), so that the Epistolario project could take advantage from already transcribed blocks of letters. Senders Alcide De Gasperi Luigi Sturzo Piero Malvestiti Luigi Granello Guido de Gentili Agostino Gemelli  # 816 143 71 39 28 22  Receivers Alcide De Gasperi Luigi Sturzo Giulio Delugan Piero Malvestiti Giuseppe Micheli Amintore Fanfani  # 704 107 55 41 34 31  Table 1: Top-5 senders (left) and receivers (right) in the digital collection at the moment of writing.  Figure 2: Topic distribution in the letters Another interesting analysis is related to the topic(s) of the letters. Indeed, each transcriber has to assign one or more topics to each letter chosen among the following categories: Private Life, National Politics, International Politics, Local Politics, Political Party, Economy, Society, Culture, Religion. The choice to allow multiple annotations was guided by domain experts, who identified in each letter multiple  256  \ftopics and opted not to select only the main one. We report in Figure 2 the distribution of topics in the letters transcribed so far. As expected, national politics is the most present topic. The fact that private life is the second most frequent topic, instead, is rather surprising, since the majority of De Gasperi's letter exchanged with other family members are still kept private by his descendants, and have not been included in the Epistolario. This means that De Gasperi tended to mention personal information and tell about private aspects of his life in letters to members of the christian-democratic party, politicians and other public figures, mixing political, cultural and private topics.  Figure 3: Keyphrases extracted from eighteen English letters sent by\/to De Gasperi. Another intriguing aspect of the collection is the fact that, given the international role played by De Gasperi, especially after WWII, several letters are either in English or in German (resp. 33 and 10 at the moment of writing). Figure 2 shows a preliminary content analysis of the English transcribed letters performed with the keyword extraction tool KD (Moretti et al., 2015). The letters, dated 1945-1948, were exchanged between De Gasperi and important US personalities, such as President Truman and the Secretary of Defense Robert A. Lovett. The focus of these letters is on the international cooperation between the two nations, the treaty after WW2, the necessity of peace for Italy reconstruction and the role of supranational communities.  Figure 4: Network of letters Figure 4 presents the correspondence network of the collection at the time of writing, considering only exchanges of at least two letters. The network is a direct graph in which node and edge size is related to the amount of exchanged letters and edge colour represents different types of document. We can notice  257  \fthat the strong majority are letters (75%, in red) but there are also telegrams (20%, in blue) and short notes (4%).  7  Conclusion and Future Work  A digital edition presents, with respect to a print edition, new opportunities both for curators and users. First of all, there are virtually no limits to the amount of material that can be included in a digital collection; in addition, archiving on a web platform makes the organisation and management of documentary material more flexible, and provides users with interactive tools that are easier and more attractive for non-experts. At the same time, however, a digital project also requires an interdisciplinary approach. In particular, it is necessary to involve different competences already in the planning phase to create effective synergies between historians, DH experts, computer scientists, archivists and publishers. The National Edition of De Gasperi's letters in digital format is an example of this interdisciplinary approach: all the choices about the documentary editing are the results of discussions among the project members that have guided the choices implemented in the platform, for example the possibility to annotate the letters from different perspectives (linguistic, historical, philological). The development of the digital archive is still ongoing thus several improvements and additions are planned for the future. Search options will be extended to take advantage of the many metadata provided by the transcribers through the transcription tool: e.g. language used in the letter, presence or absence of headed paper, presence or absence of a handwritten signature. Thematic paths will also be designed to guide the user in discovering the collection of letters, for example by navigating the various biographical phases of De Gasperi's life or the relationship between De Gasperi and his correspondents. Moreover, it will be possible to browse the letters stored in a specific archive or library using an interactive map. Users will also be allowed to download the letters in different format, including pdf and TEI-XML. In addition, we plan to release to the research community the transcription infrastructure with a Creative Commons 4.0 license.  Acknowledgments Marco Odorizzi wrote Section 1 of this paper, Stefano Malfatti wrote Section 3, while Rachele Sprugnoli, Sara Tonelli and Giovanni Moretti equally contributed to the analyses and the writing of the other sections. "
	},
	{
		"id": 41,
		"title": "Visualizing Romanesco; or, Old Data, New Insights",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Gianluca Valenti"
		],
		"body": "1 The sociolinguistic background In the past decades, dialectology studies have exponentially increased. All linguists nowadays acknowledge the importance of the dialects in the social and cultural history of humanity, and put them on an equal footing with standard languages. In particular, in the context of Italian sociolinguistics, the study of the dialect of Rome is considered as an extremely relevant topic. Since the beginnings, Italian dialects are divided into three main groups: Northern, Tuscan, and Southern varieties. The medieval and renaissance periods marked an irreversible revolution in the Roman social background, and consequently, in the Roman language\u2014the Romanesco. Although Romanesco formed part of the Southern dialects, before the second half of the 16th century at the latest, it came to resemble the Tuscan varieties\u2014a process called \u2018Tuscanisation' (Ernst, 1970). This change, whose dynamics remain largely unclear, represents a unique episode in the history of the Italian language. The uniqueness of the \u2018Rome case' has been stressed on many occasions, and several explanations have been proposed for it, but no agreement has been reached yet.1 Indeed, the mutation of Romanesco was so deep (and without similar precedents) that scholars tend to refer to it as \u2018disintegration, decay' (Migliorini, 1932), instead of \u2018evolution' (as is the usual case for most of the languages). More than fifty years after Migliorini's work, Mancini (1987:41) argued that, until then, scholars \u2018placed too much emphasis on some demographically macroscopic events,' such as the Sack of Rome of 1527. Instead, according to him, the mutation of Romanesco was a slow event, already in place, at least in its germinal stages, in the Trecento and the Quattrocento. Trifone (1990:92) quickly reacted with a detailed linguistic analysis of new documents, and concluded that the demographic de-southernisation of Rome (caused by the sack) and the ensuing repopulation of the city post-1527 were the main reasons for the de-southernisation of the spoken language of its lowest social classes. The debate has continued for several years (cf. also Trifone 1992 and Mancini 1993) without a consensus. From one side, it is unquestionable that, as De Caprio (1988:453) states, \u2018the sack of Rome of 1527 is a traumatic caesura in Roman cultural history,' but, from the other side, its role in the context of the Tuscanisation is admittedly unclear. Even at the present time, instead of positioning themselves on one side or  1  Cf. e.g., Vignuzzi (1988, 1995), De Mauro (1989), Palermo (1991), and, recently, Coluccia (2011), who argues that the exceptionality of the phenomenon lies in the untimely Tuscanisation at a spoken level. 260  \fthe other, scholars tend to report both opinions, at most trying to harmonise them into a single whole.2 Currently, and maybe because no one has established a definitive answer to the issue, researchers seem to prefer to focus on the currently spoken Romanesco, a language that has still many points of contact with its renaissance variety.3 Interest in the old Romanesco has not waned, though, as is clear by the recent organisation of the roundtable \u2018Rome in history, in linguistics and in literature' (Rome, 23th July 2016) and the international conference \u2018Il romanesco tra ieri e oggi', which I organised in Li\u00E8ge the 9th September 2019.  2  Old data, new insights  The old epistemological framework\u2014where single scholars tried to explain the evolution of the Roman language by studying analytically one or few texts\u2014has proved to be unfit for the task of understanding the dynamics of the Tuscanisation of Romanesco. At the present time, it is necessary to look at a broader picture, and consider the Roman texts as if they were a single whole. Indeed, up to now, scholars essentially based their findings on qualitative research, but did not exploit the potentialities of databases and digital tools. Most of the current papers focused on Romanesco analyse the language of a specific text (or a bunch of texts), while a general overview that takes into account the huge amount of data pertaining to the Roman sources collected by the scientific community during the past hundred years is still missing. I recently made the first step in order to fill this gap, by building and putting online a database that allows users to make queries into the whole corpus of texts written in Romanesco from the Origins to 1550. It is available online, free of charges, at the address http:\/\/www.romanesco.uliege.be\/. Regularly updated, the database makes available metadata concerning not only the texts, but also the physical supports (printed books, manuscripts, and places, such as churches or catacombs) that transmit them. Working on digital data leads scholars to new findings, and allows them to answer old research questions, which could not be solved with the traditional approach. In this paper, with the help of some visualisations, I show that\u2014due to the scarcity of the sources\u2014we do not have sufficient data to get new insights about the language of Rome and its evolution through time if we only look at the languages of the texts. Therefore, to have a sharp understanding of the Tuscanisation process, we need to reanalyse the linguistic features of the epigraphs, whose language has been often defined, maybe too quickly, as generically \u2018Vernacular'. 2.1 Corpus and tool To conduct this study, I put in plain text files the metadata of 372 texts, written from 800 to 1550, with at least some features of Romanesco in them. I took the data from D'Achille and Giovanardi (1984). With regard to the languages, notice that a text can contain some features that do not belong to its original linguistic system. In consequence, for each text, I identified its primary language and all the potential secondary languages (i.e., the languages that occur to a lesser extent). The condition for a text to be included in this corpus is to have Romanesco as its primary or, at least, its secondary language. The corpus is thus composed of texts written in Romanesco\u2014which may contain some pieces of Tuscan or Latin\u2014, but also of texts that contain only a small amount of features of Romanesco, while their primary language is Tuscan, Latin, or Vernacular. Each text is transmitted by one physical support: a) \u2018places' transmit epigraphic texts; b) \u2018manuscripts' transmit handwritten texts; and c) \u2018printed books' transmit printed texts. All the visualizations are made with the software Tableau. 2.2 Results Figures 1\u20134 show the total number of occurrences of primary and secondary languages, and their evolution over time. The figures provide some interesting insights that, in a way, strengthen both hypotheses of Mario Mancini and Pietro Trifone.  2 3  Cf. e.g., Vignuzzi (1994), Giovanardi (1998:61), D'Achille and Petrocchi (2004:122\u2013123). Cf. e.g. the projects VRC. Vocabolario del Romanesco Contemporaneo and ERC. Etimologie del romanesco contemporaneo. 261  \fFigure 1. Primary languages  Figure 2. Primary languages over time (I)  Figure 3. Primary languages over time (II)  Figure 4. Secondary languages  We notice that the total amount of occurrences of texts written in Romanesco and Tuscan starts to diverge (in favour of the former) in the 2nd half of the 14th century, maybe due to the success of Anonimo Romano's Cronica (1357\u20131358), the most important Roman text of its century. However, the Tuscan language is attested all over the centuries, from the 14th to the 16th century. I do not register any dramatic increase right after 1527 (cf. figure 2). This observation seems to endorse Mancini's view of the Tuscanisation as a slow process, already in place in the Quattrocento. On the other hand, if we look at figure 3 we notice, in the first half of the 16th century, a slight decrease of texts written in Romanesco, and a parallel growth of texts written in the Tuscan language. Admittedly, this outcome may be related\u2014as Trifone states\u2014to the de-southernisation of the Roman population after the Sack, and the subsequent increase in the number of Tuscan people moving to the town. These visualisations improve significantly our perception of the evolution of Romanesco through time. Nonetheless, they do not provide any irrefutable evidence that would end the debate on the timing and modalities of its Tuscanisation. Therefore, we need to look at the problem from another perspective. Indeed, an aspect has escaped the scrutiny of most of the past scholars: observing the physical supports that transmit Roman texts over the years, we notice some interesting insights.  Figure 5. Physical supports  Figure 6. Physical supports over time  262  \fThe prevalent supports of literary texts are, up until and including the 15th century, manuscripts, and afterwards, printed books. In the present corpus, though, the total number of epigraphs is significantly high (cf. figure 5). Furthermore, if we consider only the sources ranging from 800 to 1550, texts transmitted by epigraphs are even more than texts transmitted by manuscripts (cf. figure 6).4 This is due to the fact that most of the texts of this corpus are practical documents (such as receipts, letters, and private notes), and have no literary value. Indeed, texts that are transmitted by perishable material\u2014such as pieces of paper\u2014get easily lost, while texts that are carved on the column of a church are more likely to be preserved. By their very nature, epigraphs are dramatically short, and in consequence, linguistic features that are typical of a given area are less likely to be detected in epigraphs than in other textual typologies. The high number of epigraphs included in this corpus may be related to the high number of occurrences of texts written in a language that has been defined, generically, as \u2018Vernacular'. Therefore\u2014maybe because of their apparently low linguistic value\u2014the past surveys on the Tuscanisation of Romanesco did not take enough into account epigraphic texts. However, the low number of handwritten documents of Romanesco and, in contrast, the high number of epigraphic texts, make the latter a critical source to understand the linguistic mutations in the medieval and renaissance Rome. Moreover, we should not forget that, as a starting point for the research, we have at disposal a solid documentary basis, the volume on Vernacular texts found in churches, edited by Sabatini et al. (1987). There, the authors provide detailed linguistic analyses, which could serve as a model for further studies focused on the language of the newly discovered epigraphic texts of the past thirty years. Once we have collected a significant amount of new data, it will be possible to look again at the linguistic features of the Roman sources\u2014including but not limited to manuscripts and printed books\u2014thus refining our theories and reaching new conclusions about the Tuscanisation process of Romanesco.  3 Conclusions and new perspectives Within the traditional approach, scholars tried to explain timing and causes of the Tuscanisation of Romanesco by analysing the linguistic features of a small selection of texts. The results of this approach\u2014albeit essential in many respects\u2014did not lead to a sharp understanding of this particular linguistic process. I have shown that the reason for this failure is not entirely due to the little number of texts analysed. Indeed, even though we consider all texts at our disposal, we are not able to recognise a clear pattern in favour of one or the other theory. In order to resolve this issue, we need more data, i.e., we need to look back at those texts that\u2014until now\u2014have been catalogued as written in \u2018Vernacular' language. The high number of texts that we did not assign to any specific linguistic system is probably related to the high number of epigraphs that transmit them. Nowadays, scholars should approach the epigraphic texts with renewed attention, looking for pieces of evidence of their linguistic features. While awaiting additional archival findings, this is the only way to increase the number of texts whose language is known, which is our only chance to make new assumptions that could explain the Tuscanisation of Romanesco."
	},
	{
		"id": 42,
		"title": "What is a last letter? A linguistics/preventive analysis of prisoner letters from the two World Wars",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Giovanni Pietro Vitali"
		],
		"body": "1 Introduction This paper aims to report some of the first results of the Marie Curie project entitled Last Letters from the World Wars: Forming Italian Language, Identity and Memory in Texts of Conflict, which started in September 2018. The project analyses the linguistic and thematic features of the last letters of people sentenced to death during the two World Wars, and is conducted with digital humanities tools. The documents concerning the First World War have been collected mainly in the Central Archives of the Italian State in Rome and thanks to a kind donation by Professor Giovanna Procacci, who offered her letters of Italian prisoners (Procacci, 2000), published and unpublished, for analysis in the Last Letters project. The letters from the Second World War were collected in close collaboration with the Ferruccio Parri National Institute (ex INSMLI) and the Centre for Contemporary Jewish Documentation (CDEC), both in Milan. The majority of the Second World War texts were collected through these two organisations, although some also come from other Italian institutes of resistance connected to the Ferruccio Parri National Institute, which is the central organisation of the Italian Network of Institutes for the History of the Resistance and the Contemporary Age. Other letters were found thanks to the National Association of Ex-Deportees in the Nazi Camps (ANED), again in Milan. In other words, we were able to collect letters from Italian prisoners captured by the Austrian or German armies as far as the First World War is concerned, whereas we composed a corpus of letters from partisans and Jewish deportees for the Second World War. The total number of letters is 1203 for letters from WWII and 960 for WWI. I selected those documents, which were analysed for this paper, from a total of approximately 3500 letters collected in the first six months of my archival research. 2 Objectives The objective of this paper is to display the main differences between these two corpora (WWI-WWII). In fact, as far as the Second World War is concerned, the prisoners who wrote those letters were mainly partisans who knew for certain that they were going to be executed, whereas for the First World War the writers did not have a precise notion of what their fate was going to be, despite the precariousness of their situation. This dichotomy between letters from prisoners who knew they were sentenced to death and prisoners who still believed in a chance of survival, is underlined by textual and extra-textual elements. These elements were made evident by a digital analysis that allowed a greater understanding of these letters. The purpose of this paper is not to provide a complete interpretation of the subject but to propose a framework for the genre \u2018last letter' in order to understand if it is possible to give a preliminary answer to such a question. The texts analysed were submitted to a first NLP analysis carried out with TreeTagger (Schmid, 1994, 1995). To conduct this 265  \fpreliminary analysis, I used, in TreeTagger, the Italian parameter file of Professor Achim Stein (University of Stuttgart). I then scanned the text for potential errors. Several writers for example, especially during World War I, often write a group of words as a single word. An example of this is \u2018saperesezicarlo' which means \u2018to know if uncle Carlo\u2026' The transcripts of these texts normally respect the language and spelling of the original documents. This way of writing, combined with dialectal forms, could not be read by TreeTagger. In order to rectify the pos-tagging procedure, I decided to correct manually the repertories of tagged word so that the work could be more precise, considering that I had to divide those groups of words. Regarding the stop words, I decided to include them in the analysis because they represent another characteristic habit of the writers. As I explained, these texts often display groups of united words, and in some cases, this happens because the writers do not know the correct spelling of these phrases, nor the concept of collocating, for instance, a preposition and a noun. An example of this tendency is the recurring intrincea [in the trenches], which should be written in two separate words, or aggorizia, which is a case of syntactic gemination (Repetti, 1991), a typical phenomenon that occurs in spoken Italian. In three different cases, the writers wrote aggorizia instead of a Gorizia [to Gorizia], imitating the sound they reproduce orally. This example is typical of the main category of writers who are part of the two corpora, that is, partially literate people. They often write groups of words attached together, like alacamba, literally alla gamba [to the leg], with the palatalization of the velar consonant (Pellegrini, 1985: 272). Another interesting example is the assimilation of the verb \u2018have' to the following past participle. In 1,2% of the passato prossimo [present perfect] forms it is possible to read expressions like oreclamato [I reclaimed] or oscrito [I wrote]. Finally, a third kind of word grouping, which occurs quite rarely (0,02% of the tokens), consists in the writer attaching entire phrases in a unique form. Some examples of this tendency are to be found in cases like nosischersa [don't joke about it] that display the low level of education of the writer, who obviously does not know the correct spelling of the verb scherzare, in which there is no \u2018s' before the final suffix. The preliminary analysis that I am proposing in this paper presents the very first results of my ongoing research, namely, the first comparison between WWI and WWII corpora. 3 Letters So what is a \u2018last letter'? Is it the last text written by someone before his or her disappearance or should it have some precise characteristics in terms of language and contents? Can a prisoner who was ignorant of his\/her fate really write a last letter? Can we consider a \u2018last letter' one written by a prisoner who is then pardoned? Traditionally in the history of epistolary memoirs, the last letter has always been vaguely described as the last message that remains to us from someone who died. However, there are several types of documents that fit this description, and yet also have other, distinct features. For example, the Jewish partisan Emanuele Artom (Aosta, 23\/06\/1915 \u2013 Torino, 7\/04\/1944) kept a diary (Artom, 1966) during his imprisonment by the Nazis. His spiritual testament is contained within this diary, but not at the end of the text. Could this message be considered Artom's last letter even though it is a diary page, simply because it contains the last message he wrote? Considering that the World War II corpus is mainly composed by attested last letters, and the World War I corpus comprises letters written by prisoners who were, in most cases, unaware of their possible execution, I will compare them in order to see what the main differences between these two corpora are. Then I will determine whether, among these differences, there are shared, distinctive features that can generally be attributed to a last letter genre, as people's final messages obviously present some recurring peculiarities. One of the main characteristics of last letters is the request to the family for forgiveness. An example of this, taken from the WWI corpus, is the famous letter by Fabio Filzi (Pisino, 20\/11\/1884 \u2013 Trento, 12\/11\/1916), an Italian volunteer and irredentist executed by the Austrians. In his last message to his parents, one can immediately recognise a request for forgiveness which is one of the common traits of letters in both WWI and WWII corpora. Cari genitori, prima di morire non posso fare a meno di esprimere il mio profondo rincrescimento, per il fatto che mi sovrasta, invero non per la mia esistenza, ma per voi che avete fatto tanto per me e che non approvaste i miei sentimenti italiani. Io ho sempre adempito il mio dovere con scrupolosità seguendo sempre l'impulso della mia coscienza. Prima di morire rivolgo il pensiero a voi e alla mia cara Emma, che si trova a Padova, e contro i cui consigli ho agito arruolandomi. Addio per sempre, baci ai miei fratelli. Fabio Filzi.1  1 Dear parents, before I die I cannot help but express my deep regret, for the fact that it overwhelms me, indeed not for my existence, but for you who have done so much for me and who did not approve of my Italian feelings. I have always fulfilled my duty with scrupulousness, always following the impulse of my conscience. Before dying I turn my thoughts to you and to my dear Emma, who is in Padua, and against whose advice I acted by enlisting. Goodbye forever, kiss my brothers. Fabio Filzi.  266  2  \fThis letter stands out as one of the few examples of surviving letters written by Italian WWI soldiers who were executed. The red part shows the request for forgiveness to his parents for the pain that his own death will cause them. This sentence could also be easily contained in the letter of a partisan sentenced to death. On the contrary, the green part is something totally different compared to the letters of the Second World War. In the letters of this period there are no ideological clashes between the people sentenced to death and their families. The contrast is always linked to the desire of the condemned person's loved ones not to lose them. Another feature that differentiates the two corpora is that First World War letters were written only by adult men while in the WWII corpus there are several other categories of writers, like teenagers and women. An example is Renato Mantovani (Treviso, 16\/12\/1928 \u2013 Pieve di Teco (IM), 26\/01\/1945), one of the youngest partisans in the corpus, who was 16 years old. Notizia ai genitori. \u2018Sono accusato di appartenere alle bande comuniste, vi domando perdono, ora mi fucilano'. Renato.2  Again, as you can see in green, Mantovani apologizes to his family for the pain that his shooting will cause them, as in the previous case of Fabio Filzi. These two texts show that there are characteristics in common among the last letters, although these two texts were produced 20 years apart and were written by different profiles of writers. Another common feature of all these letters, even those not written with full awareness of certain death, is to entrust one's family to the care of the recipient of the message. This is because the writer thinks that they will never be able to see their loved ones again. An example from the First World War is the following excerpt from the letter of a soldier who is writing from a trench shortly before an assault, with an obvious fear of dying. [\u2026] e bacia i banbini ch'io sara dificile a poterli rivedere ancora una volta; mediante il mio scritto la lagrime cadono dalli occhi che una volta ti rimirava. adio. [\u2026]. (Procacci, 2000: 414).3  We only know the name of this infantryman, Beppe, and we cannot say with certainty that this is his last letter even if it is plausible to think so. In red, it is evident that the letter refers to never seeing one's loved ones again with the probable intention of exorcising the fear that this soldier had that this eventuality would actually occur. An example of a letter from World War II with the same kind of tone is that of Vanda Abenaim (Pisa, 6\/05\/1907 - Auschwitz, unknown), a Tuscan Jew who did not survive Auschwitz. The case of Abenaim is very interesting because it is one of the few that presents coded messages in a last letter. Based on her son's accounts, we know that the family had devised a coded means of communicating in case they should find themselves in a dangerous situation (Pacifici, 1993: 129). Firenze 30\/11\/1943 Gent.ma Signora, Mi farebbe tanto la gentilezza di consegnare a mio fratello la presente perché purtroppo sono ferita gravemente e non so quale destino mi sono destinata. Sono molto avvilita perché non so se potrò essere salva e rivedere più i miei cari. Già sono in camerata. Pregata tanto per me. I bimbi sono stati salvati. Per ora sono sempre a Firenze. Mando tanti baci al mio caro Carlo e mando baci alla mia mamma e chissà quando la rivedrò. Saluto tanto anche lei e pure la sua signorina. Sua aff.m a nipote Vanda.4  The letter is theoretically addressed to a woman, but in truth it is addressed to her brother. In red you can see the parts where the woman asks her brother in code to take care of the children because she fears she will never see them again. The Abenaim family had established a secret code if they were captured by the Germans (Abenaim, 2015). The examples in the text are in green. With the sentence sono gravemente ferita e non so quale destino mi sono destinata [I am seriously injured and do not know what fate I am destined for], she warns her family that she has been taken prisoner by the Germans. Moreover, with the expression Già sono in camerata [I'm already in my dormitory], she informs her brother that she is already on the train to the concentration camp. Considering that these texts have some common traits despite their many differences, I tried to keep the characteristic elements of the original documents to better underline the differences between 2  Notify the parents. I am accused of belonging to communist gangs, I ask your forgiveness, now they shoot me \u2018Renato'. And kiss the children as it will be difficult for me to see them again; through my writing the tears fall from the eyes that once gazed at you. Farewell. 4 Dear Madam, could you be so kind as to bring this letter over to my brother because unfortunately I am seriously injured and do not know what fate I am destined for. I am very discouraged because I do not know if I will be saved and I will be able to see my loved ones again. I'm already in my dormitory. Please pray a lot for me. The children have been saved. For now I'm still in Florence. I send many kisses to my dear Carlo and I send kisses to my mother and who knows when I will see her again. I also greet you and your lady. Your affectionate nephew Vanda. 3  267  3  \fone corpus and the other, through a semantic and morpho-syntactic analysis of the two. I mainly used TreeTagger and I displayed some results through the use of the Links tool of Voyant-Tools. The next step of the project will consist in the comparative analysis of another group of texts carried out with TreeTagger and other lemmatizers such as Tint (tint.fbk.eu), UDPipe (http:\/\/ufal.mff.cuni.cz\/ udpipe) and T2K (http:\/\/www.italianlp.it\/demo\/t2k-text-to-knowledge\/). 4 Part-of-Speech analysis It is the first time that such an analysis is applied to these texts, despite the fact that in some cases these letters have already been studied and analysed, both for the WWI corpus (Spitzer, 2016) as well as the WWII one (Bozzola, 2013), but in any case, this is the very first digital analysis that has been applied to these texts. The possibilities given by digital tools have enabled us to clearly see the differences between World War I and World War II, and to establish some of the characteristics of the last letter genre. The result of the pos-tagging and the following manual corrections enabled me to gain a better understanding of the language of these letters. The following table summarises all the characteristics of these texts. World War I  World War II  Letters  960  1203  Tokens  63.637  134.103  Types  9.953  12.912  Lemmas  6122  8031  Type-Token Ratio (TTR)  0,156  0,096  Lemma-Token Ratio (LTR)  0,96  0,6  Average Words Per Sentence  19,2  32,3  Accuracy Corrected Tokens  89,8%  95,5%  6505 (10,2% of the total)  6044 (4,5% of the total)  The data concerning the accuracy are very interesting, and are linked to the linguistic nature of the two corpora. As a matter of fact, the World War I corpus is linguistically more problematic, and Treetagger had a harder time analysing it. As you can see, on World War I texts, it had an index of accuracy of 89,8%, which is 5.7 percentage points less than the accuracy score it had on World War II letters. A manual correction confirmed these data, but I also noticed, thanks to a close reading approach to World War I letters, some linguistic peculiarities that I did not think were canonical peculiarities, such as local and dialectal traits. As I explained in the introduction, World War I writers were not confident about the spelling of Italian, owing to their education and their being dialect speakers. These characteristics, which TreeTagger cannot tag, are extremely representative of the World War I corpus, but totally absent from the other one. Moving from these general comments about parts of speech to a more detailed analysis, it is possible to verify the differences between the two corpora in terms of what grammatical categories are used, and where TreeTagger struggled the most.  World War I  Corrected  TreeTagger Occurrences  Accuracy  Occurrences  Percentage  Percentage  Abbreviations  308  0,48%  0,07%  45  0,413%  Adjectives  4843  7,61%  8%  5093  0,392%  Adverbs  5842  9,18%  8,65%  5504  0,531%  Conjunctions  4268  6,7%  6,5%  4141  0,199%  Articles  4283  6,73%  7,84%  4988  0,568%  Nouns  12038  18,91%  21,95%  13970  3,03%  Proper Names  1550  2,43%  1,91%  1217  0,523%  Punctuation  5263  8,27%  1,99%  1269  6,276%  Prepositions  9674  15,20%  13,12%  8349  2,082%  Pronouns  9078  14,26%  13,11%  8347  1,148%  Verbs  15890  24,97%  20,10%  12792  4,868%  Accuracy 0,055% 0,118%  Occurrences 84 10809  TreeTagger Percentage 0,063% 8,06%  Corrected Percentage Occurrences 0,12% 159 8,18% 10968  268  World War II Abbreviations Adjectives  4  \f0,428% 0,051% 0,040% 2,427% 1,191% 0 0,255% 0,548% 9,994%  11213 8629 8806 26940 3237 3515 16990 19935 27154  8,36% 6,43% 6,57% 20,09% 2,41% 2,62% 12,67% 14,86% 20,25  8,79% 6,49% 6,6% 17,66% 3,61% 2,62% 12,41% 15,41% 30,19%  11787 8698 8860 23685 4835 3515 16647 20670 40485  Adverbs Conjunctions Articles Nouns Proper Names Punctuation Prepositions Pronouns Verbs  WORL WAR I  WORL WAR II  The biggest differences between the two corpora lie in the use of verbs. If the present, the most common tense of the indicative, appears in both corpora with almost the same frequency (WWI: 7,82 \u2013 WWII: 8%), the same cannot be said of the past tenses. The use of the past \u2013 simple past, imperfect, and present perfect \u2013 in World War II letters is more than twice as frequent (11,41%) as in WWI texts (4,9%). Combining these data with a close reading approach, it is possible to affirm that this linguistic trait is one of the peculiarities of the last letter genre. As I said, WWI letters were written by prisoners who wanted to tell their families about their everyday lives. On the other hand, WWII letters were written by people sentenced to death, who often used the memories of their past experiences as a way of exorcising the fear of capital punishment and entrusting their loved ones with the memories of happy times when they were together. The use of parts of speech being so meaningful, I decided to highlight the correlation between these grammatical elements by using a graphic tool. Then I submitted these tagged texts to Voyant-tool. The application then showed which parts of speech are the most common (blue rectangles) and which combinations they form (orange rectangles).  It is immediately evident that the texts from the First World War give greater attention to textual construction. For instance, they display a greater use of punctuation. As Spitzer (2016: 108) noticed, this use is often made incorrectly (Cortelazzo, 1972: 119-123). Nevertheless, the writers demonstrate awareness of the fact that punctuation must be there as an indispensable element of the text, (Restivo: 2018, 249), while in the Second World War letters, due to the strong emotionality of the moment, language becomes mimetic of speech. First World War morpho-syntactic chains highlight a higher number of nouns, prepositions and adverbs with an indirect relationship between substantives and adjectives. Analysing the letters with a close reading approach, I can suppose that sentences are more complex and present a higher number of indirect objects. On the other hand, the Second World War corpus, with a higher number of nouns and adjectives, displays a more prominent use of direct objects or nominal sentences.5 Indeed texts of the First World War show a greater hypotaxis, therefore complexity, in comparison to those of the Second, precisely because of the different emotional conditions of writing but not only. In fact, the writing of the last letters of deportees and condemned to death of the resistance is very often clandestine and can literally be visualised on the page as a stream of consciousness, as the writers were trying to make the most of every moment in which they could find time to write. In a possible definition of the last letter, it will therefore be necessary to consider the morphological construction of the discourse as one of the discriminating factors in the reflection on genre. 5 Most Frequent\/ Characteristic words In order to better display the use of the lexicon, I used Voyant as a tool to visualize collocations and links within the texts. The representation of the connections of the most frequent words (blue rectangles) with the others (orange rectangles) is the following:  5  In any case, in the next phases of the project I will go further in the syntactic analysis using tool like CohMetrix (http:\/\/terence.fbk.eu\/services\/api\/computeReadability\/v2\/). 269  5  \fWorld WAR II  WORLD WAR I  On the green background is the corpus of the First World War while on the red one is the corpus of the Second.6 This analysis is surprising because it clearly shows what the nature of the two corpora is. The words sempre, ora and guerra, on the left, show a descriptive lexical approach to war writing, which aims to tell the stories of the front to the families of the writers. On the contrary, the most frequent words in the last letters corpus of the Second World War illustrate a familiar lexicon that reveals the emotional character of this writing. All this information allows us to say more about the generic features of the last letter. In fact, both corpora, WWI and WWII, are texts written in a tragic moment and, if we bracket the substantial differences between a person condemned to death and a soldier in prison, the condition of deportees of the Second World War can in fact be compared to that of prisoners of the First World War. The substantial difference lies in the codification that the subject makes of the reality he or she is living. A soldier learns to experience the daily realities of war as part of a group of like-minded people; imprisonment and death become a codified consequence of a tragic but commonly accepted situation. Those sentenced to death, on the other hand, find themselves alone before death, in some cases feeling incredulity, such as the case of fourteen or fifteen-year-old boys who do not expect to be shot or tortured; in other instances, they cling onto the hope that their comrades can make an exchange for a Fascist or a Nazi prisoner. On the other hand, deportees, especially for racial reasons, are faced with the unknown while in chains, considering that their imprisonment is not the result of their actions but of their personal identity, and they are kept in the dark as to their fate. The letters of many deportees also contain appeals to hope. A very interesting fact is that this concerns the lexicon in its entirety. If we observe in fact the lemma\/token ratio (Jurafsky, Bell, Girand, 2002) it is evident that the WWI corpus is more lexically varied (0.96) than the WWII corpus (0,6). This is due to the fact that the letters written a few hours before execution with the certainty of having to die, are characterized by a basic lexicon that often returns. I preferred lemmatoken to type-token because it is better suited to treat inflected forms of a word as the same type (McCarthy, 1990: 73). To give an example of how the lexicon of these texts works, we can for example cite the use of the word dolore [pain] which is often used in phrases in which the writer apologizes for the pain that death will cause his\/her loved ones as in the previous case of Filzi and Mantovani. In the corpus of the First World War there are 43 occurrences of the word dolore and in no case is it collocated with adjectives. By contrast, in the corpus of the Second World War, the word \u2018pain' appears 185 times, 57 of which are accompanied by a demonstrative adjective such as: grande [big], immenso or immane [immense], tremendo [tremendous], profondo [deep], accorato [heartfelt], straziante [heartbreaking] or ultimo [last]. The language in these last letters is therefore more descriptive, especially when it describes the feelings and therefore distinguishes the story of imprisonment or of life in the trenches from an inner narrative that must condense a greater communicative intent into a few lines. It should also be noted that of the 57 occurrences of the noun pain with these adjectives, 7 have the adjective post-placed to the noun while 50 have it placed before the noun (Serianni, 1989: 199-205). This is typical of the syntactic structures commonly found in literary texts (Scarano, 2000: 5). It is no wonder that there should be a similar lexicon as well as sentence construction in the letters, given that their authors learnt how to write in Italian through the example of literature, for instance Dante. These letters are as diverse as the materials on which they were written. During my research in the archives, I never found a single letter from a soldier on the front of the First World War that was written on a precarious medium. In contrast, the partisans and deportees wrote their last messages really wherever they could (Bozzola, 2013: 26). There are also, for instance, \u2018letters' composed of three words on the edge of a book or even a list of names engraved on a loaf of dry bread. These letters represent in essence what the two wars were, and testify to their differences. Thanks to the function \u2018oppose' of the R package Stylo, I identified the most characteristic words of each of the corpora. For World War I it found austriaci [Austrians] (34), macello [slaughterhouse] (21), 6  In the rectangles the numbers are the occurrences of the given word. An English translation is provided in italics. The blue rectangles contain the most frequent words, or parts of speech, and the orange the ones those that appear most frequently in connection with them. 270  6  \flicenza [license] (19), francesi [French] (19), stanchi [tired] (18). On the other hand, for World War II, muoio [I die] (212), sii [be] (117), chiedo [ask] (105), perdonatemi [forgive me] (51), non piangete [don't cry] (45), ricordatevi [you remember] (43). It is interesting to note how, even in these few cases, the most characteristic words of World War I are related to the conflict, describing it as a slaughterhouse, its protagonists \u2013 the Austrian enemies and the French allies \u2013 the authors' desires to escape the war while on leave and to one of the most common feelings of the soldiers: tiredness. 6 Conclusion These \u2018last' letters focus on the content of the message, whereas World War II letters are most concentrated on the emotive and conative functions because the language focuses on the sender and the addressee (Jakobson, 1960). The sentenced to death ask to be remembered by the people they love. They want to be forgiven and for their families to be happy. We could therefore assume that one of the most salient peculiarities of a last letter is when the message mainly focuses on the sender himself\/herself and on the addressee. The last letters aim to describe emotions rather than facts, and to tell about the past more than about the present, because there is no future for the sentenced to death. In the next future, I will include other letters in the corpus and I will cross the analysis done until now with TreeTagger with the use of other lemmatizers and tools. The analysis I have conducted revealed some problems in comparing World War I and World War II letters but it also highlighted changes in the writing of Italian. Most importantly, this phase of my research proved that a \u2018last letter' was thematically, linguistically and pragmatically definable. Acknowledgements I would like to thank Dr Rachele Sprugnoli for having shared with me her scientific expertise, and Professor Matthew Reynolds and Dr Anita Jorge for their precious help, advice and patience."
	},
	{
		"id": 43,
		"title": "L’organizzazione e la descrizione di un fondo nativo digitale: PAD e l’Archivio Franco Buffoni",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Paul Gabriele Weston",
			"Primo Baldini",
			"Laura Pusterla"
		],
		"body": "1 Introduzione Il progetto PAD-Pavia Archivi Digitali dell'Università di Pavia è nato nel 2009 con lo scopo di preservare dalla scomparsa gli archivi delle memorie digitali di autori contemporanei. L'Università, che attraverso il Centro per la tradizione manoscritta di autori moderni e contemporanei dal 1969 salvaguarda i documenti cartacei di scrittori e giornalisti italiani, ha voluto estendere questa esperienza ai documenti nativi digitali. Fu il giornalista e scrittore Beppe Severgnini, ex alunno dell'Università, che, partendo dalla constatazione che una parte maggioritaria della produzione culturale letteraria si basa ormai sull'utilizzo di supporti informatici, sollecitò questo ampliamento di prospettive. Per rendere tali documenti ricercabili e leggibili è necessario servirsi di infrastrutture hardware e software in continua evoluzione, un ostacolo che rende le procedure della conservazione progressivamente più impegnative con il passare degli anni. La volontà di arginare questa perdita di testimonianze della nostra storia culturale è stata alla base della creazione del progetto PAD. PAD conserva diverse tipologie di materiali digitali, garantisce la tutela a lungo termine dei fondi ed eventualmente può essere accessibile agli studiosi, nel pieno rispetto, come è ovvio, delle disposizioni ricevute dagli autori. Fino ad ora il progetto si è focalizzato soltanto sulla conservazione a lungo termine degli archivi digitali in locale, ospitati cioè sui dispositivi di scrittura correntemente utilizzati dagli scrittori o su apparecchiature non più utilizzate, ma da essi conservate, nonché sui supporti di archiviazione utilizzati dagli stessi nel corso degli anni (nastri magnetici, floppy di diverse dimensioni e densità di archiviazione, cd, dvd, unità compatte di archiviazione massiva). La crescente tendenza ad avvalersi della rete per comunicare ed archiviare dati ha reso necessario mettere a punto una strategia e dei dispositivi finalizzati alla salvaguardia di risorse digitali, siti web e contenuti sui social media. A questo modulo del sistema è stato dato nome PAD Web Archiving. L'intento di PAD non è, ovviamente, quello di competere con analoghi progetti internazionali di ben altro respiro, ma mantenersi come un progetto sostenibile, sia tecnologicamente, sia finanziariamente, che garantisca, ad onta delle sue dimensioni contenute, dei risultati di qualità. Spetta agli autori stessi o alle istituzioni culturali alle quali fanno capo i siti interessati richiedere espressamente che anche questa componente venga inserita nel piano complessivo di preservazione dell'archivio. L'accordo è indispensabile al fine di interagire direttamente con il committente per stabilire tempi e metodi per il salvataggio e la consultazione. Tutto il materiale resta ovviamente di proprietà dell'autore, che può in ogni momento decidere di rimuoverli dall'archivio e di rinunciare al prosieguo del progetto. 273  \f2 Il fondo Franco Buffoni Franco Buffoni, anglista, poeta, prosatore e traduttore, il cui archivio cartaceo si trova già in deposito presso il Centro Manoscritti della stessa Università, avendolo lui conferito in anni precedenti, è uno degli autori che, nel corso degli anni, hanno conferito a PAD i propri archivi digitali. Nel 2016, nel rispetto dell'iter messo a punto allo scopo da PAD, una copia del suo ampio archivio è stata riversata in PAD. L'iter a cui si fa qui riferimento prevede che, dopo la firma di un contratto legale, un operatore di PAD si rechi presso la residenza dell'autore per prelevare una copia dei file che lui stesso ha selezionato per la conservazione. Il riversamento dell'archivio Buffoni ha riguardato 1065 elementi, per complessivi 758 MB, comprendenti tipologie di file di diversa natura: documenti di testo, immagini, video, audio, link. Nel 2019, con l'intenzione di sperimentare lo strumento messo a punto per la salvaguardia a lungo termine delle risorse web, PAD ha concordato con l'autore di utilizzare il suo sito personale (www.francobuffoni.it), ritenendolo particolarmente idoneo allo scopo a motivo della ricchezza di contenuti e della varietà di formati e tipologie. Per prima cosa, su richiesta esplicita del proprietario del sito, PAD ne ha prelevato una copia. Dato che i siti web possono essere modificati o aggiornati anche molto di frequente, si è concordato con l'autore di procedere con l'effettuazione di salvataggi a cadenza prestabilita. In questo modo si possono conservare le varie versioni del sito, che possono essere messe a disposizione dell'utenza secondo la volontà del proprietario. Attraverso un software per il web scraping, il sito dell'autore è stato riprodotto in locale, in modo da garantirne il browsing offline. Così l'utente futuro potrà navigare liberamente nella copia dell'intero sito. Per progettare questa implementazione, si è dovuto tenere conto della struttura anche molto complessa che i siti possono talvolta presentare, comprendente riferimenti numerosi ad altre pagine, interne o esterne nel web. Per questo di ogni pagina che compone il sito web, PAD memorizza, oltre alla pagina stessa, i link anche alle pagine esterne, con un'immagine della pagina a cui il link conduce, nonché i documenti allegati. In questo modo si può tenere meglio traccia dei path che il creatore del sito ha voluto valorizzare. Se, ad esempio, un link a una pagina esterna non fosse più funzionante o se la pagina non risultasse più esistente, una parte di ciò che l'autore intendeva comunicare, una componente probabilmente significativa del suo pensiero, andrebbe perduta. Quando il progetto ha avuto inizio è stata presa in considerazione l'idea di utilizzare principalmente il formato di archiviazione WARC (Web ARChive). Sebbene questo formato sia stato standardizzato nel 2009 (ISO 28500:2017) il suo utilizzo da parte delle grandi aziende informatiche (Microsoft, Apple, Google ecc.) non ha mai goduto negli anni della diffusione che sarebbe stata auspicabile. Un'accurata serie di verifiche ha permesso di accertare che i browser più diffusi non lo riconoscono. Allo stesso tempo il sistema di memorizzazione sembra essere stato realizzato per essere installato e usato solamente da un sistemista esperto, ciò che rischia di creare notevoli problemi agli utenti comuni. Si è preferito quindi adottare un prodotto di più facile utilizzo per l'elaborazione del sito. Il formato WARC è, invece, stato mantenuto per la parte del progetto che si occupa di preservazione a lungo termine. Quindi in PAD per il Web Archiving vengono gestiti due sistemi diversi. Il primo utilizza il software Heritrix, sviluppato da Internet Archive, mentre il risultato dei processi di crawling viene memorizzato in file con formato WARC. Per l'elaborazione delle pagine il software che PAD utilizza prevalentemente si chiama HTTrack, un Web crawler open-source. Consente di scaricare un sito web da internet in una directory locale, ottenendo HTML, immagini e altri file dal server al computer. HTTrack mantiene la struttura originale del sito, compresi i link, permettendo all'utente di navigare da una pagina all'altra, come se la stesse visualizzando online. Esso preleva anche tutte le altre tipologie di documenti e immagini che si trovano allegate alle pagine web. Pur non basandosi su uno standard, HTTrack presenta il vantaggio della semplicità nell'aprire le pagine web o nell'estrarre il testo per elaborarlo. Alcuni autori, come ad esempio Francesco Pecoraro, hanno richiesto di creare una copia offline del proprio sito, con l'intenzione di rimuoverlo successivamente dalla rete. Questa copia resta ovviamente a loro disposizione per l'accesso, qualora ne facciano richiesta, anche a distanza. Si è visto come la versione \u2018mirror' del sito è stata considerata come quella più semplice da inviare e da consultare da parte di utenti non particolarmente esperti. La possibilità di navigare all'interno del sito locale, senza l'obbligo di installare preventivamente specifici software, ha reso questo servizio estremamente semplice da gestire. Al contrario, per le questioni ricordate in precedenza, l'utilizzo di un archivio WARC avrebbe comportato la necessità di assistere l'utente nel corso della procedura di consultazione. Tutto il materiale così raccolto è stato sottoposto alle procedure di conservazione sperimentate da PAD nel corso degli anni. La prima operazione è creare più copie dell'archivio, ubicate su diversi server. Oltre che sul server interno di PAD infatti, esso viene replicato sui server dell'Università di Pavia e su quello della sede 274  \fdistaccata dell'Università a Cremona, città distante da Pavia circa 70 chilometri, in modo da salvaguardare la sicurezza delle informazioni in caso di disastro ambientale. Un'ulteriore copia viene memorizzata su supporto hardware esterno. Una volta assicurata la preservazione dell'originale, l'archivio passa in un'area di working. Vengono estratti i metadati, fondamentali per poter poi svolgere l'operazione di normalizzazione, che comporta il salvataggio di ogni documento in diversi formati, a seconda della tipologia. Terminate le operazioni preliminari, si procede a descrivere i file.  3 La descrizione Rispetto al trattamento di un archivio cartaceo, un archivio nativo digitale presenta peculiarità, come ad esempio il numero dei file che lo costituiscono, che talvolta possono assommare a molte migliaia, che rendono la descrizione effettuata seguendo consuetudini e procedure tradizionali inadeguata e persino non sostenibile. \u00C8 stato, perciò, necessario individuare strategie che potessero consentire di sfruttare al massimo le potenzialità offerte dall'informatica. Al contempo, si registrano problematiche simili, come la questione dell'accessibilità e della riservatezza. Trattandosi di documentazione prodotta molto di recente, si è dovuto tener conto del fatto che, probabilmente, una parte anche significativa dei file non possano essere resi disponibili all'utenza senza che ciò comporti la violazione di disposizioni legislative, come quelle sulla privacy o sulla proprietà intellettuale. Anche il fatto che i documenti conferiti o salvati dal web siano stati indicati espressamente dal conferente, non è sufficiente a garantirne la libera consultazione da parte degli studiosi. Si rende perciò necessario, durante le fasi preliminari della descrizione, sottoporre ogni singolo file ad una attenta disamina volta ad escludere che contenga dati sensibili o creazioni intellettuali la cui responsabilità non sia in capo al conferente, al soggetto produttore dell'archivio o al titolare del sito. Anche lo scrittore che ha conferito il proprio archivio potrebbe richiedere, per ragioni personali, che alcuni documenti siano secretati e di conseguenza esclusi dalla consultazione, anche per motivi di studio, per un determinato periodo di tempo, il cosiddetto embargo. Per tener conto di queste evenienze, l'operatore di PAD, nel vagliare ogni file dell'archivio digitale, gli assegna una categoria di rischio, in base alla quale esso viene automaticamente reso o meno consultabile da parte degli utenti. Si passa poi alla fase del riordino. Come per gli archivi cartacei, viene creata una struttura ad albero rovesciato che comprende le diverse serie, alle quali vengono poi assegnati i singoli file. Già in questa procedura si manifestano quelle potenzialità dell'informatica, prima ricordate, che offrono un significativo contributo agli archivisti e nuove opportunità agli utenti. In primo luogo, il sistema offre la possibilità di assegnare un singolo file a più di una sezione dell'archivio. In secondo luogo, se nell'archivio cartaceo il riordino comporta la modifica della sistemazione pensata dallo scrittore, l'archivio digitale può consentire di mantenere ad un tempo l'aspetto originale e contemporaneamente collocare i documenti in un ordinamento che segua altri criteri. Questi criteri possono anche essere più di uno, quando le esigenze lo richiedano. Durante la descrizione, infatti, l'archivista assegna a ogni documento dei tag, che hanno lo scopo di aiutare l'utente a comprendere meglio la tipologia del materiale in questione (se, ad esempio, si tratta di un testo, di una recensione, di un'immagine, di un video e così via). In ogni archivio, ovviamente, non tutti i file sono prodotti da colui o colei che conferisce l'archivio stesso. Sono molto frequenti i casi di documenti frutto del lavoro intellettuale di terze persone. La possibilità di collegare a ogni file uno o più nomi di persona che abbiano in qualche modo contribuito alla sua produzione è funzionale anche a questo scopo. Al tempo stesso, il nome della persona o dell'ente viene associato ad una tipologia di responsabilità intellettuale, espressa attraverso un vocabolario controllato, implementabile a seconda delle esigenze mediante l'inserimento di nuovi termini in una tabella. Ricorre, poi, il caso di documenti - il termine viene qui utilizzato in senso generale, senza cioè fare riferimento a funzioni di natura amministrativa - che siano stati estratti o ricavati da pubblicazioni più ampie (ad esempio, un capitolo da un libro, una poesia da una raccolta, un brano da un'intervista o da una recensione e così via). Qualora il collegamento tra i due documenti sia riconosciuto dall'archivista, il sistema consente di esplicitare la relazione anche a beneficio dell'utente, dal momento che è possibile stipulare un collegamento tra l'oggetto e il titolo della risorsa che lo contiene o del quale è parte. L'inserimento del codice ISBN o DOI nel caso di un libro o dell'ISSN per una rivista è funzionale a rendere più riconoscibile e in modo inequivoco la fonte e, di conseguenza, a consentirne più facilmente il recupero. La presenza di tutte queste informazioni inserite dall'archivista (tag, nomi, responsabilità, identificativi univoci), unitamente ai metadati tecnici estratti direttamente dalla macchina, consente a PAD di mettere a disposizione dell'utente percorsi di ricerca alternativi, o, per meglio dire, complementari e trasversali, rispetto a quelli consueti. Infatti, vi è la possibilità di estrarre i dati dei documenti secondo l'ordinamento per serie 275  \farchivistiche, oppure secondo la disposizione originale dello scrittore, o ancora ordinati per soggetto produttore o per provenienza. In questo modo PAD cerca di venire incontro alle variegate necessità dell'utente che si trova a consultare l'archivio.  4 La descrizione del sito web Analizzando i conferimenti effettuati dai diversi autori ci si è resi conto che spesso l'autore ha conservato i testi che poi vengono riversati sul web sotto forma di file allegati alla pagina o come link. In altri casi, i testi della pagina web si ispirano, prendono spunto oppure sono almeno in parte i medesimi di quelli presenti nell'archivio. Operando sui metadati estratti, avendo selezionato quelli più importanti per identificare in modo puntuale la risorsa web, tali correlazioni tra nativo digitale e web possono venire ricercate ed individuate. Le procedure occorrenti non si discostano molto da quelle normalmente messe in atto in PAD al momento dei conferimenti per analizzare la struttura e la consistenza dell'archivio. Come primo passo, si procede a creare una mappa del sito; in secondo luogo si estraggono i metadati, il cui numero viene ridotto in seguito alla scrematura di quelli di scarsa utilità; infine, i documenti vengono sottoposti ad operazioni di normalizzazione. Conclusa questa fase, i documenti del web possono essere descritti. Il sistema di descrizione è in parte automatizzato, dato che la grande quantità di materiale disponibile sul web richiederebbe tempi eccessivamente lunghi e un'attività dispendiosa, se ad occuparsene fosse un operatore. \u00C8 evidente, infatti, che, una volta ricevuto il comando, il computer possa elaborare per diverse ore il materiale, fino ad arrivare alla conclusione. Per ottenere un risultato molto simile, un operatore dovrebbe lavorare per giorni. Il software PAD Web Analyzer confronta ogni pagina web - prendendo in considerazione sia il testo vero e proprio della pagina, sia eventuali file allegati - con i documenti presenti nell'archivio. Procede, quindi, a mostrare, affiancati, i testi del web e i corrispondenti testi del nativo digitale, sulla base di indicatori di similitudine. Per ciascuna corrispondenza viene stabilito un indice di similitudine, che indica in percentuale quanta parte del testo sul web sia uguale a quella di un documento presente nell'archivio nativo digitale. Un risultato molto basso, indicativamente inferiore al 50 %, non viene tenuto in considerazione dall'operatore. Al contrario, come hanno permesso di accertare le prove effettuate, la certezza di aver individuato il medesimo documento si ha quando l'indice presenta un valore intorno al 98%. In presenza di valori intermedi sarà compito dell'operatore effettuare i riscontri necessari, ma anche in questo caso il sistema è in grado di offrire assistenza. Se il valore è relativamente elevato, il computer, assumendo di aver individuato due file \"probabilmente\" corrispondenti propone l'assegnazione ad una delle serie inserite con la descrizione manuale dell'archivio. Se, viceversa, il valore è relativamente baso, la casella delle assegnazioni resta vuota e deve essere quindi l'operatore a riempirla sulla base di una ricognizione autoptica. Per effettuare queste valutazioni si è tenuto conto del fatto che talvolta i documenti in archivio possono essere in formati differenti rispetto a quelli pubblicati o allegati sul web (ad esempio un file Word solitamente viene convertito in PDF per essere messo sul sito) e questo comporta un ragionevole abbassamento dell'indice. Poiché la macchina non ha le stesse capacità di discernimento di un operatore, è opportuno che il controllo finale sia effettuato esaminando in parallelo i due testi. Una specifica funzione del software di PAD mostra i due testi affiancati in modo che la ricognizione possa procedere speditamente. \u00C8 ovviamente è possibile che il testo sia stato prodotto direttamente sulla rete, oppure che il file originale sia stato eliminato o non conferito: in tal caso non si evidenziano corrispondenze. A questo scopo PAD ha implementato all'interno del software l'algoritmo Levenshtein distance, adattandolo alle proprie esigenze. La procedura appena descritta risulta attualmente vantaggiosa solo se la parte dell'archivio digitale nativo sia stata già trattata e viene quindi utilizzata in questo momento unicamente per la descrizione dei siti web o in caso di secondi (o comunque successivi) conferimenti. Le informazioni ottenute attraverso questo procedimento serviranno all'archivista per assegnare il documento a una particolare serie inventariale, riguardante una specifica opera dell'autore o una specifica tipologia documentale. Integrando i documenti provenienti dai siti web con quelli nativi digitali si ottiene un archivio il più completo possibile. Se all'interno di una serie viene inserito del materiale proveniente dal web, viene creata una sottoserie apposita, in modo che l'utente possa trovare aggregato tutto il materiale avente lo stesso argomento e al contempo conoscerne la provenienza.  5 Prospettive per il futuro Come nella tradizione delle realizzazioni informatiche, il sistema PAD viene costantemente implementato per fornire nuove funzionalità e migliorare quelle attuali. La scelta di configurarsi come un progetto di piccola 276  \fscala, limitato a autori o a istituti culturali selezionati, permette di dedicare grande cura nel perfezionare le soluzioni tecniche, secondo le necessità dell'archiviazione e della descrizione. L'architettura di PAD è stata pensata per la conservazione, ma negli anni si è evoluta con la finalità di consentire lo studio del materiale conferito. La stretta collaborazione con gli scrittori conferenti garantisce il rispetto delle loro decisioni sul trattamento e la gestione del loro archivio e permette di andare incontro alle esigenze che essi manifestano. Seguendo le tendenze dei cambiamenti sociali nell'uso di internet e delle sue risorse, PAD sta sperimentando una funzionalità che permette la conservazione a lungo termine e la consultazione delle pagine personali di social network, come Facebook, Twitter o Instagram, dei canali YouTube e delle e-mail. Questa tipologia di documenti informatici, che potrebbero anche essere di rilevante importanza per studi futuri, sono per ora solo pensati in funzione della conservazione. Su richiesta esplicita di un autore, verranno raccolti i dati direttamente dai social e preservati. Con lo stesso criterio verrebbero trattate le e-mail, utilizzando criteri analoghi a quelli che negli archivi tradizionali si applicano ai carteggi e agli epistolari. Attualmente è al vaglio la possibilità di affidare un'ulteriore copia degli archivi al progetto nazionale Magazzini Digitali, avviato nel 2006 dalla Fondazione Rinascimento Digitale, dalla Biblioteca nazionale centrale di Firenze e dalla Biblioteca nazionale centrale di Roma. La conservazione digitale assicurata dai depositi digitali affidabili o fidati (trusted or trustworthy digital repositories) di un servizio pubblico è una ulteriore garanzia che archivi digitali di autore così preziosi e che rischiano l'oblio vengano adeguatamente conservati nel lungo termine."
	}
]