[
	{
		"id": 1,
		"title": "EcoDigit-Ecosistema Digitale per la fruizione e la valorizzazione dei beni e delle attivit√† culturali del Lazio",
		"abstract": {
			"it": "EcoDigit \u00E8 uno dei progetti del \u201CCentro di Eccellenza DTC Lazio\u201D, che ha\r\nl\u2019obiettivo di aggregare e integrare varie competenze nell\u2019ambito del patrimonio culturale\r\ndigitale. EcoDigit intende aricchire il sistema dell\u2019Anagrafe delle Competenze, altro progetto del\r\nDTC Lazio, con una piattaforma middleware che faciliti l\u2019integrazione di nuove risorse di dati\r\ne permetta la pubblicazione e riuso di servizi per la valorizzazione e fruizione del patrimonio\r\nculturale nella regione Lazio. Il progetto \u00E8 finalizzato all\u2019integrazione dei dati censiti attraverso\r\nformati aperti e tecnologie semantiche basate sul modello Linked data.",
			"en": "EcoDigit is one of the projects of the \u201CCentro di Eccellenza DTC Lazio\u201D, which intends\r\nto aggregate and integrate expertises in the field of technologies applied to cultural heritage.\r\nEcoDigit aims at enriching the Anagrafe delle Competenze, another project of the DTC Lazio,\r\nwith a middleware platform that is able to facilitate the integration of new data sources and\r\nto allow the publication and reuse of services for the enhancement and fruition of the cultural\r\nheritage in the Lazio region. The project is designed to integrate the collected data through open\r\nformats and semantic technologies based on Linked Data model."
		},
		"authors": [
			"Luigi Asprino",
			"Antonio Budano",
			"Marco Canciani",
			"Luisa Carbone",
			"Miguel Ceriani",
			"Ludovica Marinucci",
			"Massimo Mecella",
			"Federico Meschini",
			"Marialuisa Mongelli",
			"Andrea Giovanni Nuzzolese",
			"Valentina Presutti",
			"Marco Puccini",
			"Mauro Saccone"
		],
		"body": "Introduzione Il progetto Ecosistema Digitale per la fruizione e la valorizzazione dei beni e delle attivit\u00E0 culturali del Lazio (EcoDigit)1 \u00E8 una delle iniziative del Centro di Eccellenza del Distretto Tecnologico per i beni e le attivit\u00E0 Culturali (DTC)2, costituito dalle cinque universit\u00E0 statali del Lazio - Sapienza, Tor Vergata, Roma Tre, Cassino e Tuscia - in rete con CNR, ENEA e INFN per aggregare e integrare competenze nel settore delle tecnologie per i beni e le attivit\u00E0 culturali. Il progetto (luglio 2018 - gennaio 2020) si rivolge alle organizzazioni operanti nel settore della cultura le quali producono e mantengono basi di dati anche di grandi dimensioni archiviati usando formati, modelli e processi diversi. Questo fenomeno comporta la necessit\u00E0 di identificare processi, tecnologie e modelli di integrazione semplici e sostenibili che consentano la fruizione del patrimonio in modo globale e collegato. Le tecnologie semantiche, e in particolare i Linked Open Data (LOD), sono state ampiamente sfruttate con successo nel campo del patrimonio culturale al fine di migliorare l\u2019accesso e l\u2019esperienza di fruizione dei beni culturali da parte dei cittadini, cos\u00EC come di facilitare la reperibilit\u00E0, l\u2019integrazione e l\u2019arricchimento dei dati (Dijkshoorn et al., 2018; Daquino et al., 2017; Lodi et al., 2017). Infatti, il paradigma dei LOD \u00E8 utilizzato per collegare dati provenienti da diverse istituzioni culturali, aumentando cos\u00EC la possibilit\u00E0 di raggiungere i dati culturali disponibili nel Web of Data. L\u2019interconnessione dei contenuti delle organizzazioni collaboratrici ha anche contribuito ad arricchire le informazioni in modo efficace e finalizzato alla valorizzazione del patrimonio culturale (Hyv\u00F6nen, 2009). La collaborazione tra organizzazioni culturali ha portato anche allo sviluppo collaborativo di ontologie che descrivono il patrimonio culturale a livello internazionale, ad esempio CIDOC-Conceptual Reference Model (CRM) (Doerr, 2003)3, in modo tale che i requisiti di interoperabilit\u00E0 semantica potessero essere soddisfatti all\u2019interno dei loro sistemi. Inoltre, l\u2019uso di ontologie comuni ha facilitato lo scambio di dati e la creazione di enormi biblioteche digitali, ad esempio l\u2019Europeana Data Model (EDM) (Isaac and Haslhofer, 2013)4. EcoDigit, inoltre, non solo ha preso ispirazione da esperienze gi\u00E0 consolidate sia degli enti partner e non del progetto sia maturate nell\u2019ambito delle Pubbliche Amministrazioni (PA), ma intende anche instaurare collaborazioni con tali realt\u00E0. Ad esempio, il progetto ReCAP5 dell\u2019Universit\u00E0 Sapienza ha creato una rete di condivisione di conoscenze, strumenti e sperimentazioni che permette di elaborare linee guida e modelli per la costruzione di processi conservativi del patrimonio digitale. Inoltre, il Sacher Project6 sta affrontando problematiche simili, ma non a livello di progettazione di una piattaforma regionale. Molte istituzioni regionali e nazionali che gestiscono il nostro patrimonio culturale stanno adottando il modello dei dati aperti seguendo le linee guida dell\u2019 Agenzia per l\u2019Italia Digitale (AgID); tuttavia per le organizzazioni pubbliche e private non \u00E8 semplice adeguarsi per la mancanza di una piattaforma che ne faciliti il compito. La missione del progetto Data & Analytics Framework (DAF)7, di cui il network di ontologie OntoPia8 \u00E8 uno dei risultati, \u00E8 analoga a quella di EcoDigit, ma insiste sull\u2019intero territorio nazionale con focus sull\u2019interoperabilit\u00E0 tra dati tra PA. In tale contesto, EcoDigit pu\u00F2 contribuire a rendere efficace e coordinata l\u2019integrazione con i sistemi nazionali. 2 Obiettivi EcoDigit ha l\u2019obiettivo di arricchire il sistema Anagrafe delle Competenze9 con una piattaforma middleware che faciliti l\u2019integrazione di nuove sorgenti di dati e consenta la pubblicazione e il riuso di servizi per la valorizzazione e la fruizione del patrimonio culturale del Lazio. Nello specifico, EcoDigit fornisce (i) l\u2019architettura di riferimento per l\u2019integrazione di servizi modulari e per la loro pubblicazione e il riuso; (ii) una componente software sviluppata in forma prototipale per l\u2019orchestrazione dei servizi, l\u2019integrazione 1http:\/\/ecodigit.dtclazio.it\/ 2https:\/\/dtclazio.it\/ 3http:\/\/cidoc-crm.org\/ 4https:\/\/pro.europeana.eu\/page\/edm-documentation 5http:\/\/digilab.uniroma1.it\/attivit\/recap 6http:\/\/www.sacherproject.com\/progetto 7https:\/\/teamdigitale.governo.it\/it\/projects\/daf.htm 8https:\/\/github.com\/italia\/daf-ontologie-vocabolari-controllati 9http:\/\/dtc.si.cnr.it\/anagrafe-delle-competenze 2 \fFigure 1: Schema architetturale in cui si inserisce il Middleware di EcoDigit. e l\u2019aggregazione delle interfacce e dei dati; (iii) la versione prototipale di servizi orientati alla fruizione e valorizzazione del patrimonio culturale. Con queste caratteristiche EcoDigit si configura come un progetto di ricerca e trasferimento tecnologico collegato in maniera significativa agli altri progetti del DTC Lazio. Con Anagrafe \u00E8 in relazione fornendo un livello software intermedio, detto middleware, che consente al sistema di aggregare nuove sorgenti di dati, servizi, strumenti innovativi sia industriali che accademici. In questa prospettiva EcoDigit estende il sistema Anagrafe e gli conferisce la capacit\u00E0 di evolvere ed essere esteso. Per gli altri progetti di ricerca, EcoDigit svolge il ruolo di mediatore nei confronti del sistema Anagrafe. Ci\u00F2 significa che i risultati dei vari progetti potranno essere integrati grazie all\u2019interfacciamento con il middleware di EcoDigit. In Figura 1 \u00E8 rappresentato uno schema architetturale di alto livello in cui si inserisce il middleware di competenza di EcoDigit. Sulla sinistra sono rappresentate le risorse, ovvero le banche dati (ciascuna col il proprio formato sintattico e modello concettuale di rappresentazione dei dati) e i servizi messi a disposizione dai vari enti operanti sul territorio che, per\u00F2, mancano di un punto di accesso unico. Ci\u00F2 ostacola potenziali utenti interessati a consultare le risorse o creare applicativi basati su esse. Il sistema, a cui stanno lavorando congiuntamente i gruppi di EcoDigit e Anagrafe, ha l\u2019obiettivo di superare questa frammentariet\u00E0. Esso \u00E8 composto da vari livelli: subito al di sopra del Data Layer, c\u2019\u00E8 (i) l\u2019Acquisition Layer, a cui lavorano congiuntamente Anagrafe ed EcoDigit, che si occupa di creare dei flussi di dati i quali saranno acquisiti, uniformati secondo uno schema comune e memorizzati dal sistema. Questi flussi dati verranno creati dal (ii) Middleware, che offre alle applicazioni client delle funzionalit\u00E0 per l\u2019elaborazione semantica dei contenuti, facilities per il mashup dei servizi indicizzati dal sistema, oltre a definire le linee guida che servizi e dataset esterni dovranno seguire per garantire l\u2019interoperabilit\u00E0 con il sistema. Nella parte pi\u00F9 alta, (iii) il Client Application rappresenta una qualunque applicazione che intende utilizzare i dati acquisiti e offerti dal sistema. Essa potr\u00E0, attraverso il Middleware, interrogare il sistema per raccogliere dati, recuperare i metadati di dataset esterni o descrizioni dei servizi disponibili. 3 Metodologia Gli obiettivi sovraesposti sono stati realizzati attraverso l\u2019esecuzione di attivit\u00E0 svolte in maniera collaborativa e sinergica tra i partner del progetto e sintetizzabili in tre fasi: 1. Censimento delle risorse relative ai beni culturali presenti nella regione Lazio (cf. Sezione 3.1); 2. Elaborazione del modello di integrazione delle sorgenti nel Middleware (cf. Sezione 3.2); 3. Realizzazione di prototipi volti a verificare la validit\u00E0 dei risultati e dell\u2019approccio seguito (cf. Sezione 3.3). Tali attivit\u00E0 sono state svolte considerando una prospettiva almeno quinquennale di sostenibilit\u00E0. 3 \f3.1 Attivit\u00E0 di censimento delle risorse presenti nel Lazio Fondamentali nel corso del progetto sono state tre tipi di attivit\u00E0 preliminari di censimento consistenti in: (a) un\u2019analisi dei requisiti del sistema, con relativa ricognizione degli attori e dei casi d\u2019uso, e dello stato dell\u2019arte su tecnologie middleware e architetture per l\u2019integrazione di servizi; (b) l\u2019individuazione delle sorgenti dei dati presenti nel Lazio, e dei modelli ontologici e tecniche per la loro integrazione e standardizzazione basata su formati aperti e semantici. In questo contesto, tramite la formalizzazione e disseminazione \u00E8 stato possibile anche individuare potenziali stakeholder del progetto; (c) una valutazione dei tool allo stato dell\u2019arte per la creazione e la gestione di ambienti virtuali 2D\/3D. 3.2 Modello di integrazione di una sorgente Il modello di dati e metadati, che le sorgenti devono rispettare per poter essere acquisite dal middleware EcoDigit, si basa su metodi e tecniche di \u201Cmetadatazione\u201D e di Semantic Web, comprendendo anche tecnologie proprie delle openAPI e tutto ci\u00F2 che viene comunemente classificato come Open Data. Per la sua elaborazione, si \u00E8 proceduto a un\u2019analisi delle sorgenti censite nel Lazio al fine di evidenziare tanto i domini di conoscenza coperti dalle sorgenti quanto il dettaglio dei vari campi che il modello deve rappresentare. Per ognuno di essi sono stati ricercati gli schemi concettuali considerati standard di riferimento per la modellazione dei dati inerenti a un certo dominio, quali FOAF10, DOAP11, Org Ontology12, OntoPia network, SPAR Ontologies13, ArCo14, ecc. Il modello deve essere pensato come l\u2019unione di standard tecnologici, schemi concettuali e di metadatazione allo stato dell\u2019arte per i vari domini di conoscenza, relativi in particolare all\u2019ambito dei beni culturali. Successivamente, quando i modelli esistenti allo stato dell\u2019arte sono stati ritenuti non in grado di rappresentare semanticamente campi peculiari presenti nei dataset di input, \u00E8 stata effettuata una modellazione ex novo, utilizzando una metodologia di ingegneria ontologica (Blomqvist et al., 2016), basata su un\u2019estensione di eXtreme Design (XD) (Blomqvist et al., 2010). XD \u00E8 un metodo di progettazione agile di ontologie che si basa sul riuso di Ontology Design Patterns (ODP) (Gangemi and Presutti, 2009) al fine di risolvere problemi di modellazione ontologica noti e ricorrenti. Il workspace del progetto EcoDigit \u00E8 disponibile su Github15. Di seguito, si elencano le ontologie definite ex-novo nel corso del progetto: \u2022 Ontologia delle Organizzazioni16 lo scopo di definire un vocabolario condiviso ditermini per la descrizione delle organizzazioni che partecipano al Centro di Eccellenza-DTC Lazio. Essa estende: OntoPiA-COV, FOAF, W3C\u2019s Organization Ontology. \u2022 Ontologia delle Valutazioni17 ha lo scopo di definire un vocabolario di termini per la descrizione di qualsiasi cosa abbiamo una valutazione associata che \u00E8 espressa rispettouna certa scala. \u2022 Ontologia delle Esperienze e Competenze18 ha lo scopo di definire un vocabolario condiviso di termini per la descrizione delle esperienze e competenze di una persona. Estende: OntoPiA-CPV, OntoPiA-COV, FOAF, BIBO; importa l\u2019Ontologia delle Valutazioni. 10http:\/\/xmlns.com\/foaf\/spec\/ 11http:\/\/usefulinc.com\/ns\/doap# 12https:\/\/www.w3.org\/TR\/vocab-org\/ 13http:\/\/www.sparontologies.net\/ontologies 14http:\/\/w3id.org\/arco 15https:\/\/github.com\/ecodigit\/workspace 16https:\/\/w3id.org\/ecodigit\/ontology\/organization 17https:\/\/w3id.org\/ecodigit\/ontology\/grade 18https:\/\/w3id.org\/ecodigit\/ontology\/eas\/ 4 \f3.3 Prototipi A seguito delle attivit\u00E0 sovraesposte di censimento dei dataset, degli schemi concettuali e dei tool allo stato dell\u2019arte per la fruizione del patrimonio culturale, sono in fase di elaborazione due prototipi: \u2022 una Proof-of-Concept, volta a mostrare la validit\u00E0 del modello attraverso l\u2019integrazione di alcune delle sorgenti identificate nel task di censimento. L\u2019obiettivo principale \u00E8 quello di creare una best practice che mostri sia la semplicit\u00E0 dell\u2019approccio di integrazione, e la sua scalabit\u00E0 nel tempo, sia gli strumenti hardware e software che una sorgente deve adottare per aderire al modello di ingresso di EcoDigit; \u2022 un prototipo di un servizio avanzato per la fruizione dei beni culturali nel dominio della formazione. Si sta lavorando ad una soluzione che permetta di raggiungere e fruire da una singola interfaccia i dati acquisiti dal sistema. I partner del progetto, sulla base delle loro expertise, hanno collaborato alla definizione di una tassonomia di categorie che popolino l\u2019interfaccia utente a fini didattici. Anche questo prototipo fa uso di tecnologie semantiche per migliorare la ricerca delle risorse e collegarle ai contenuti di carte tematiche (GIS based) e ricostruzioni 3D per permettere la fruizione in ambiente virtuale dei beni mostrati dall\u2019educatore. In particolare, sar\u00E0 possibile gestire completamente modelli 3D attraverso l\u2019implementazione ad hoc (cfr. il Workspace GitHub di EcoDigit19) del software 3DHOP20 e di mostrare dati cartografici tematizzati grazie all\u2019uso della libreria Openlayer21. I due prototipi utilizzano e rappresentano due viste diverse sugli stessi contenuti ai quali applicare le tecniche semantiche di integrazione dei dati, indicandone la provenienza: (i) il dataset della S&TDLScience & Technology Digital Library del CNR; (ii) il dataset della Sapienza Digital Library; (iii) il modello 3D di Porta Latina (Mura Aureliane), implementato da Roma Tre; (iv) la mappatura GIS con modelli 2D e 3D di alcune chiese della citt\u00E0 di Viterbo, curata dalla Tuscia; (v) la ricostruzione 3D del Trono Corsini e del busto in Terracotta di Alessandro VII Chigi, fornita da ENEA; (vi) i Linked Open Data del progetto Arco-Architettura della Conoscenza22, progettati dal CNR-ISTC in collaborazione con il MiBAC-ICCD. 4 Avanzamento tecnologico e impatto del progetto Il modello di integrazione delle sorgenti nel quale riveste un ruolo fondamentale lo sviluppo delle tecniche di interoperabilit\u00E0 semantica dei vari dataset censiti nel Lazio consente l\u2019arricchimento delle informazioni dei dati esistenti secondo una fruizione integrata. I risultati di questo studio possono essere applicati a qualsiasi progetto che potenzialmente abbia necessit\u00E0 di integrare sorgenti eterogenee. Data la rilevante diversit\u00E0 dei tipi di dati rinvenuti finora, la riusabilit\u00E0 delle tecniche ideate e sperimentate in EcoDigit rappresenta uno degli aspetti principali di avanzamento tecnologico prodotto. Ci\u00F2 comporta la possibile partecipazione di stakeholder eterogenei alla fruizione ed evoluzione di servizi e contenuti, dando supporto all\u2019inclusione di patrimoni gi\u00E0 esistenti e alla loro filiera di gestione. L\u2019utilizzo di formati e strumenti aperti garantisce tanto la sostenibilit\u00E0 nel tempo quanto la diffusione massiva dei contenuti. Inoltre, il progetto stimola la creazione di nuove figure professionali e la conseguente richiesta formativa sull\u2019uso e sulla divulgazione delle tecnologie semantiche basate sul modello dei Linked Data per la fruizione e la valorizzazione del patrimonio culturale. Ringraziamenti Si ringrazia i seguenti enti per il loro supporto istituzionale: la Galleria Corsini, l\u2019Associazione CIVITA, l\u2019Istituto per il Catalogo e la Documentazione (ICCD) del MiBAC, la Sovrintendenza xx. 19https:\/\/github.com\/ecodigit\/3dhop-react 20http:\/\/vcg.isti.cnr.it\/3dhop\/ 21https:\/\/openlayers.org\/en\/latest\/doc\/ 22http:\/\/dati.beniculturali.it\/arco\/; http:\/\/dati.beniculturali.it\/"
	},
	{
		"id": 2,
		"title": "Encoding the critical apparatus by domain specific languages: The case of the Hebrew book of Qohelet",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Gianfranco Bandini",
			"Andrea Mangiatordi"
		],
		"body": "Introduction1 The work of the textual philologist consists of two main parts: 1. the gathering and the systematic analysis of all the available documents (the witnesses) of a literary work (recensio); 2. the removing of all the errors due to the textual transmission process (emendatio) (Timpanaro 2005). During the recensio phase, the scholar proceeds to a comparison of the witnesses with the purpose of detecting textual differences (the variant readings or simply variants). This procedure, named collatio, is one of the most important and delicate phase within the workflow of the text-critical praxis and represents a preliminary step to the preparation of a critical edition. The variants are presented in the critical apparatus, an instrument devised to show the reader the results of both the recensio and the emendatio by means of a conventional and formalized language, specific for the domain of textual philology (Domain Specific Language, cf. section 3). One of the tasks of the digital philologist consists in encoding variant readings and conjectural emendations. The encoding enables the creation of dynamic critical apparatuses: as with databases, the user can decide which data to extract and present, to combine the result of different queries, to transform the philological data into numerical format suitable for quantitative analysis and, finally, to prepare a digital 1Even if both authors contributed equally to this work, L. Bambaci is responsible for sections 1-3 and 5-6 and F. Boschetti is responsible for section 4. 7 \fversion of the work. Unlike traditional, printed critical apparatuses, where the information is stored in a predefined, static way, a digital apparatus allows to retrieve, from the same encoded file, different types of information according to different research purposes and needs (Driscoll and Pierazzo 2016). The guidelines provided by the Text Encoding Initiative (TEI)2 are among the best practices in the domain of the digital philology. TEI markup schemes pursue interoperability and reusability, making available for digital philologists a common interchange language covering a large set of text-critical phenomena.3 TEI digital framework, moreover, is flexible enough to enable the user to add new tags and attributes, thus allowing to shape customized encoding vocabularies suitable for specific text-critical problems and for different literary traditions. The verbosity and complexity of XML language, however, combined with the necessity of being adherent to standards, is at risk of distracting the traditional philologist from his or her critical activity. Goal of this paper is to show how it is possible to encode variant readings by exploiting the nature of DSL which characterizes the language of the critical apparatus, without requiring from the philologist to deal with TEI technicalities and with problems of conformity to standards. Our case studies are represented by a sample digital collation of one book of the Hebrew Bible, the book of Qohelet also known as Ecclesiastes, conventionally dated to V-III BC, and by the digital version of the collation of Hebrew Medieval manuscripts of the same book, carried out by Benjamin Kennicott at the end of the XVIII century (Kennicott 1776). Both the collations are part of a forthcoming doctoral dissertation, which aims to prepare a digital critical edition of the literary work. 2 Background As we have already discussed in Bambaci et al. 2018, 2019, many tools for encoding critical apparatuses are already available or currently developing.4 The strategies adopted by these tools to avoid or facilitate the encoding process are mainly based on ad hoc graphical user interfaces or on annotation systems through abbreviated markers. Tools that allow to create digital TEI apparatuses directly from printed, traditional ones are lacking or not fully developed, such as in the case of the Classical Text Editor (Hagel 2007). The methodology we propose is implemented on Euporia, a web annotation system based on DSLs developed at the CoPhiLab of the CNR-ILC. Euporia has been formerly used for interpretative tasks, such as the identification of ritual frames in the ancient Greek tragedies (Mugelli et al. 2016), and also for educational purposes, namely in teaching Ancient Greek both in secondary school (Liceo Classico) and with BA students in Classics (first year students) at the University of Pisa. 3 Methodology As stated in the introduction, the critical apparatus is the part of a critical edition or collation devoted to the collection of textual variants and conjectural emendations. There are no fixed guidelines for compiling a critical apparatus. The different methodologies are indeed the result of different traditions of study and research practices, which depend not only on the literary domain (a critical edition of a classical text will necessarily be different from a critical edition of a medieval text, Varvaro 1970), but also on internal developments and trends within each discipline: even different editions of the same text may vary in the choice of vocabulary or typographical standards, according to the different scholarly orientations or the editor\u2019s critical insights. Despite this extreme degree of variability, the critical apparatuses, however different the shapes they may assume, all strive for the same goal: to overcome the verbosity of the natural language and present the information in a way which is as concise as possible. The result of this process of departure from the natural language is an artificial (or planned) language (Blanke 2011, Libert 2018), specific to the domain of the textual philology (Domain Specific Language, DSL). Unlike a General Purpose Language (GPL), which is applicable across domains, a DSL is a language of limited 2https:\/\/tei-c.org\/ 3Cf. in particular chapter 12 of the TEI Guidelines, devoted to the encoding of the critical apparatus: TEI Consortium, eds. \u201C12 Critical Apparatus.\u201D TEI P5: Guidelines for Electronic Text Encoding and Interchange. [3.5.0.]. [16th July 2019]. TEI Consortium. https:\/\/tei-c.org\/release\/doc\/tei-p5-doc\/en\/html\/TC.html ([29\/11\/2019]). 4Cf. https:\/\/wiki.tei-c.org\/index.php\/Category:Editing_tools and https:\/\/wiki.tei-c.org\/ index. php\/Editors for a list of the main editing tools. 8 \fexpressiveness optimized for a particular domain of knowledge or domain of application (Fowler 2010).5 Figure 1: A sample apparatus entry from Qohelet 3:17 Let\u2019s take an example of apparatus entry from the third chapter of Qohelet, verse 17 (Fig. 1). Concepts such as \u201Clocation\u201D, \u201Clemma\u201D, \u201Cwitness\u201D, \u201Creading\u201D and \u201Cvariant reading\u201D are encoded here by means of: 1. numbers (chapters and verses); 2. strings (the Latin sigla indicating the witnesses; the words of the readings); 3. characters (separators such as the square bracket \u201C]\u201D, found after the lemma; a vertical line \u201C|\u201D which divides the readings and a double vertical line \u201C||\u201D which marks the end of a reading group). The function of the apparatus components is determined by their position within the sentence (the first reading group shows the readings supporting the lemma; the following groups contain the variants).6 Similarly, in the apparatus entry of Qohelet 5:1 in Kennicott\u2019s collation (Fig. 2) we find integers for the Figure 2: A sample apparatus entry of Qohelet 5:1 from Kennicott\u2019s collation identification of the location (\u201Cverse 1\u201D), of the manuscripts\u2019 sigla and of the word occurrence in the reference text (the numero sign following the roman number after the reading, e. g. \u201C1\u00B0\u201D); Hebrew words identifying lemmas and readings; special symbols for describing the variant typology (e. g. the symbol \u201C\u2038\u201D standing for omission); annotations concerning the source description (\u201Cprimo\u201D for \u201Cfirst copyist\u2019s hand, \u201Cnunc\u201D for \u201Csecond copyist\u2019s hand\u201D) and finally special separators for discriminating reading groups (\u201C\u2014\u201D) and apparatus entries (\u201C.\u201D). A language of this sort, in which all the constituents are characterized in a concise, non-redundant and unambiguous way, is a formal language. From a computational point of view, a formal language is a language whose structure (its syntax) and meaning (its semantics) is clearly defined (Grishin 1989). A computer, therefore, is able to check that sentences are grammatically correct (well-formed) and to recognize their meaning and function (Reghizzi 2009). 4 4.1 Workflow The Context Free Grammar In order to allow the interpreter (or compiler) to parse the apparatus written in our DSL, we wrote the Context Free Grammar (CFG). A CFG is a formal grammar consisting of a set of rules describing a formal language (Parr 2010). An example of CFG suitable for analysing the lemma is shown in Fig. 3. Thanks to the parsing system provided by ANTLR software (Parr 2012), the CFG allows to tokenize and parse the whole apparatus entry, identifying the vocabulary symbols (token rules) and the syntactic structure (parser rules). The token rules (in the example, the rules in capital letters) allow to tokenize integers (NUM), alphabetic characters (ALPHA_SEQ), Hebrew words (HEBW) and separators (R_BRACKET). The parser rules allow to check the syntax: the lemma (lem), for example, is encoded as a sequence of Hebrew words (w+), witnesses sigla (wit) and separators (LemSep). The result of the parsing of the whole apparatus entry is an Abstract Syntax Tree (AST), as shown in Fig. 4 below. The AST attaches 5Examples of DSL can be considered the language of algebra for stating numerical relationships, the language of boolean logic for propositional calculus and, more in general, any kind of notation systems that allows, within a particular community of practice, the description of problems and solutions in a specific area of interest. In computer science, examples of DSLs are HTML for web pages, SQL for relational databases, LaTeX for text processing, XSLT for XML transformations and so forth. In software engineering, these languages can be called, more properly, Domain Specific Programming Language (DSPL), as opposed to General Purpose Programming Language (GPPL), such as Java, C, etc. 6Both vocabulary and structure of the critical apparatus of the sample edition have been shaped following the more recent critical edition of the book (Goldman 2004). 9 \f(a) CFG for the sample edition of Qohelet (b) CFG for Kennicott\u2019s collation Figure 3: Examples of Context Free Grammars (a) AST from the sample edition (b) AST from Kennicott\u2019s collation Figure 4: Examples of syntactic trees (AST) 10 \fto the textual items (the nodes) a label which remind the function assumed in the context and shows the syntactic hierarchical relationships existing between them (the branches). 4.2 The Visitor In order to convert any DSL in XML, we wrote a software component, named \u201CAstToXmlVisitor\u201D, which generates an XML file structured on the AST. The Visitor passes through the tree nodes and slavishly translates the parser rules into XML markers (Fig. 5). The result is a structured, well-formed XML file, whose elements take the name from the parser rules and the hierarchical structure from the AST. The Visitor has been implemented in Java language, through the set of tools available in ANTLR4 software. (a) XML output of Qoh. 3:17 from the sample edition (b) XML output of Qoh. 5:1 from Kennicott Figure 5: Visitor\u2019s XML outputs 4.3 From XML to TEI-XML A final conversion from XML code to a TEI compliant critical apparatus has been carried out through an XSLT stylesheet (Fig. 6). During the transformation phase from XML to TEI-XML, the philologist can choose which elements represent in the encoding and which to rule out (punctuation, separators and so forth). The encoding of both the apparatuses rely on the TEI model of critical apparatus: the apparatus of the digital edition of Qohelet has been encoded with the parallel segmentation method, while the encoding of Kennicott\u2019s collation follows the location-referenced method. 5 Results So far, the first three out of twelve chapters of Qohelet have been collated. Kennicott\u2019s collation has been totally digitalized and automatically encoded. The critical apparatus of both the collations hosted in Euporia have been successfully converted into a compliant TEI file. Using XSLT stylesheets, it is possible to re-convert the TEI file back to our DSL, without loss of information. TEI encoding schemes and our DSL are therefore isomorphic. The implementation of AstToXmlVisitor in JavaScript language represents an important point of the work-flow. It allows to automatically create a well-formed XML file from any AST and is therefore applicable to different DSLs. It is written once and for all by the computer scientist and needs no further customization according to the input file. Such a division of tasks meets the needs and habits of digital philologists, who are generally more accustomed to manipulating XML code, rather than working with general-purpose programming languages. 11 \f(a) TEI apparatus of Qoh. 3:17 (b) TEI apparatus of Qoh. 5:1 Figure 6: TEI compliant XML encoding 6 Discussion There are several advantages in using a DSL for ecdotic purposes. First of all, the compactness of the DSL respect to TEI encoding. The annotation through a DSL is significantly less verbose than TEI annotation, as it can be seen by comparing the number of characters of the traditional apparatus shown in Fig. 1 and the TEI counterpart of Fig. 4 (on the right). Compactness is an important feature: the verbosity of XML language may compromise human readability and make the encoding difficult to handle, especially for traditional scholars not accustomed with long in-line encoded files. Manually encoding is a time-consuming task. Markup vocabularies require a long apprentice time to be mastered; once the encoding is complete, moreover, the encoder must check whether it is internally coherent, perfectly TEI conformant and in line with best practices. The cognitive stress derived from such a mixture of disciplinary content and cross-disciplinary formalism may stray away the world of humanistic academic research from the potentialities of computational technologies, thus contributing to increase the gap between the respective communities of scholars. A DSL-based approach, on the contrary, is entirely domain-centered: the scholar is not compelled to acquire skills which fall outside his or her cultural background, nor to make his or her research practices adhere to external, technology-conditioned standards. It is up to the digital philologist, who best knows how to organize the data according to standards, to create a perfectly conformant TEI encoding from the results of the XML general exporter. Finally, the DSL may represent a good way to exercise tighter control not only on transcriptional errors, but also on semantic errors. Thanks to the tokenization, indeed, the parser is able to assign a semantic value to each apparatus component directly from the data type to which it belongs: so, for example, tokens such as \u201Comit\u201D (abridgement for Latin \u201Comittit\u201D, non-capital character), \u201CK1\u201D (capital alphabetic character + integer), \u201CMcNeile(1904)\u201D (string of alphabetic characters, integers and punctuation), will always be parsed differently and automatically assigned to different XML tags or attributes (In TEI encoding, respectively, @ana, @wit, @resp). In a manual encoding, on the other hand, the encoder must decide, each time, which tags or attributes are more suitable for expressing his or her interpretations of textual phenomena: this may often lead to an incoherent or erroneous choice of markers and increase the possibility of semantic errors, which are very difficult to be detected. In a DSL-based approach, on the contrary, the choice of markers is not entrusted to human decision, but it is determined by the form of the apparatus components and automatically performed by the Visitor."
	},
	{
		"id": 3,
		"title": "600 maestri raccontano la loro vita professionale in video: un progetto di (fully searchable) open data",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Gianfranco Bandini",
			"Andrea Mangiatordi"
		],
		"body": "1 Tematiche e obiettivi della ricerca Il progetto \u201Cmemorie di scuola\u201D \u00E8 basato sul Content Management System \u201CWordPress\u201D (https:\/\/wordpress.org), una piattaforma sulla quale si basa \u2013 secondo le stime dei suoi autori e sviluppatori \u2013 circa un terzo dei siti web mondiali. La nostra implementazione, utilizzabile attraverso qualsiasi web browser, \u00E8 stata adattata a esigenze molto specifiche, in modo da consentire la costruzione di una memoria collettiva della vita scolastica nella scuola primaria in Italia. Al momento raccoglie oltre 600 interviste a maestre e maestri in pensione (o prossimi ad essa) che raccontano con molti dettagli e grande passione la loro vita professionale, a partire dagli anni \u201840. Il nostro intento \u00E8 quello di migliorare la piena disponibilit\u00E0 dei video (che costituiscono un ampio insieme di open data) e la loro completa fruibilit\u00E0 attraverso un sistema di indicizzazione analitico. I video, cos\u00EC come le attivit\u00E0 formative e didattiche connesse, sono indirizzati in primo luogo agli insegnanti in servizio e in formazione, ma anche a un pi\u00F9 ampio pubblico interessato agli aspetti educativi della nostra storia sociale. In questa sede presentiamo quindi l\u2019implementazione di una particolare feature del sito web che consente di effettuare una ricerca full-text all\u2019interno di tutte le parole pronunciate in tutti i video che compongono il progetto (e di collegarsi ad essi nel preciso istante durante il quale il soggetto pronuncia la parola cercata). 1  Gli autori hanno lavorato alla stesura del testo confrontandosi e concordando ogni sua parte. Gianfranco Bandini si \u00E8 dedicato in particolare alle sezioni 1, 2 e 4; Andrea Mangiatordi ha curato in particolare le sezioni 3 e 3.1.  14  \f2 Quadro teorico di riferimento Nel corso del Novecento la storia orale ha affermato, non senza contrasti e opposizioni, la sua legittimit\u00E0 e utilit\u00E0, soffermandosi soprattutto sulla storia dal basso, degli esclusi dalla storia tradizionale (Gardner, & LaPaglia, 2006). Il settore di studi che si occupa della storia della scuola, all\u2019interno della storia dell\u2019educazione (McCulloch, 2011), ha utilizzato con sempre maggiore convinzione fonti non testuali, come le fotografie o i dipinti. Una piccola parte di queste ricerche ha inoltre scoperto, anche se con un certo ritardo, l\u2019utilizzo delle fonti orali, cio\u00E8 la raccolta delle testimonianze (Gardner, 2003). La memoria personale ha consentito di spostare l\u2019accento degli studi sulle percezioni e sui sentimenti delle persone, sugli aspetti interiori e comunitari della vita sociale. La storia dell\u2019educazione e la storia orale, in questa forma congiunta, hanno trovato un campo di nuova e eccezionale sperimentazione nell\u2019ambito della digital public history, nel quale il presente progetto si colloca (Bandini, 2017). Nel contesto digitale, l\u2019incrocio di queste diverse tradizioni di ricerca d\u00E0 la possibilit\u00E0 di potenziare la comune aspirazione a un maggiore contatto tra il mondo accademico e la societ\u00E0, soprattutto per quanto riguarda il rapporto con le professioni educative e di cura (cfr. Depaepe, 2001; Linn\u00E9, 2001; Vinovskis, 2015). La trasformazione delle classiche fonti storiche in open data risponde proprio a questo ambizioso obiettivo. Il progetto consente, oltre alla piena e completa disponibilit\u00E0 online delle testimonianze, di superare una delle principali limitazioni che qualsiasi tradizionale ricerca che fa uso di storia orale si trova a affrontare (cfr. Ritchie, 2011): la numerosit\u00E0 delle testimonianze e la gestione della massa dei dati che possono esserne ricavati (intesi anche come momenti di titubanza del testimone, silenzi, salti temporali, ecc.). A questo proposito, uno dei punti di discussione metodologica consiste, ad esempio, nel riflettere sulla questione del numero ottimale di testimonianze. In alcuni casi (cfr. l\u2019ampio studio di Johnson & Reuband, 2008), in abbinamento a ricerche di tipo quantitativo, si \u00E8 stabilito un vero e proprio piano di campionamento, come di consueto nelle ricerche statistiche. In generale, tuttavia, nelle ricerche storiche si sostiene che la ricerca di nuove testimonianze si pu\u00F2 arrestare nel momento in cui si satura il campo concettuale che stiamo indagando, cio\u00E8 quando nuove testimonianze non porterebbero pi\u00F9 nulla di nuovo, o insignificanti dettagli, rispetto a quanto gi\u00E0 detto. Nel contesto della storia digitale il panorama risulta ampiamente modificato perch\u00E9 la raccolta delle testimonianze pu\u00F2 essere espansa senza eccessiva fatica e scarso dispendio di risorse (cfr. Thomson, 2007). Questo aspetto ci consente di progettare una raccolta di testimonianze aperta, nel senso che pu\u00F2 essere incrementata nel tempo e offrire sempre nuove opportunit\u00E0 di conoscenza e formazione. La cassetta degli attrezzi dello storico e l\u2019insieme delle sue fonti vengono cos\u00EC trasportate da uno spazio privato a uno spazio pubblico. Nell\u2019ottica della public history, il contesto digitale pu\u00F2 quindi essere progettato non soltanto per la collocazione online di fonti primarie (come sono le interviste), ma per consentire lo sviluppo delle interazioni tra gli utenti. Per superare l\u2019impostazione tradizionale, che trasforma le fonti online in musei statici (per quanto digitali e pi\u00F9 facilmente accessibili) c\u2019\u00E8 bisogno di alcuni strumenti di base, che sono volti a aumentare la significativit\u00E0 delle fonti. La trascrizione automatica dei video, in questo senso, rappresenta un tassello fondamentale della strategia comunicativa, a vantaggio della piena fruibilit\u00E0 da parte degli utenti.  3 Metodologia La raccolta delle testimonianze video presenta delle particolari caratteristiche rispetto alla raccolta di documenti testuali (per esempio diari o autobiografie). Il repository \u201Cmemorie di scuola\u201D attualmente supera le 5.000 ore di filmato e rende oggettivamente molto difficile sia l\u2019ascolto integrale, sia una completa attivit\u00E0 di indicizzazione manuale basata sull\u2019inserimento di tags (per esempio quelli contenuti in TESE, Thesaurus Europeo dei Sistemi Educativi). Nella prospettiva della public history il numero delle ore \u00E8 inoltre destinato a crescere ancora: in questa situazione la messa a disposizione del pubblico, con una piccola serie di tags di indicizzazione, di fatto non consente l\u2019accesso alla ricchezza documentaria contenuta nei video che diventa quasi casuale. Mantenendo la prospettiva fin qui esposta, il progetto si \u00E8 indirizzato a cercare di produrre un archivio video liberamente accessibile attraverso l\u2019uso di risorse sostenibili \u2013 per lo pi\u00F9 software Open Source \u2013 e l\u2019automazione di una serie di operazioni che permettono di facilitare la ricerca all\u2019interno della grande mole di contenuti raccolti. Il servizio online YouTube (https:\/\/www.youtube.com), famoso per essere tra i principali  15  \frepository gratuiti di contenuti video online, consente la trascrizione automatica del parlato dalla lingua italiana al fine di produrre sottotitoli. La quantit\u00E0 di errori di trascrizione \u00E8 altamente variabile e dipendente da molteplici fattori, primo tra tutti la qualit\u00E0 della traccia audio (cfr. Alberti e altri, 2009). Tuttavia, un sistema in grado di estrarre i sottotitoli creati automaticamente dal servizio e di inserirli in un database permette di rendere le interviste esplorabili e ricercabili in modo simile a un corpus testuale. Questo sistema, pur non presentando un\u2019innovazione dal punto di vista degli algoritmi e del software, consente di ottenere un significativo vantaggio rispetto ai sistemi attuali e offre un modello di funzionamento facilmente replicabile e applicabile a grandi masse di dati. Bisogna ricordare che quando sono in gioco grandi quantit\u00E0 di interviste, la trascrizione umana (ovviamente pi\u00F9 accurata dei sistemi automatici) solo in rarissimi casi riesce a raggiungere il risultato: \u00E8 il caso, quasi unico nel suo genere, della grande raccolta di testimonianze condotta dalla Shoah Foundation2.  3.1  Struttura del Software e flussi di lavoro  L\u2019architettura della soluzione software messa in atto per sostenere il progetto Memorie di Scuola consta di servizi e applicazioni Open Source che si interfacciano con il servizio proprietario YouTube per l\u2019hosting dei materiali video e per la produzione di trascrizioni rese disponibili nella forma di sottotitoli e in formati diversi. In particolare, lo stack software utilizzato \u00E8 basato su: \u2022  Un web server dotato in grado di eseguire codice scritto in linguaggio PHP \u2013 nel caso specifico del progetto \u00E8 stato utilizzato il software NginX (https:\/\/www.nginx.com\/), ma a questo livello non ci sono requisiti particolarmente stringenti;  \u2022  Un database MySQL (https:\/\/www.mysql.com\/);  \u2022  Il CMS WordPress (https:\/\/wordpress.org);  \u2022  Il plugin Meks Video Importer per WordPress (https:\/\/wordpress.org\/plugins\/ meks-video-importer\/), che permette la ricerca di video disponibili pubblicamente su YouTube e l\u2019importazione automatica del contenuto testuale della loro descrizione;  \u2022  Il plugin Advanced Custom Fields per WordPress (https:\/\/wordpress.org\/plugins\/ advanced-custom-fields\/), che facilita l\u2019inserimento di metadati ai contenuti dei post;  \u2022  Il software YouTube Transcript\/Subtitle API (https:\/\/github.com\/jdepoix\/youtubetranscript-api), uno script in grado di estrarre i sottotitoli da video YouTube pubblicamente disponibili;  \u2022  Lo script \u201CYouTube transcript to WordPress\u201D (https:\/\/github.com\/andreamangia\/ youtube-transcript-to-wp), progettato specificamente per automatizzare le operazioni.  Il software si inserisce nel workflow descritto di seguito e lo sostiene, minimizzando la necessit\u00E0 di intervento umano ma rendendola comunque possibile in una fase successiva di revisione dei contenuti: \u2022  L\u2019intervistatore effettua l\u2019intervista e esegue l\u2019upload su YouTube, aggiungendo al video una semplice descrizione testuale; \u2022 Un operatore del sito web www.memoriediscuola.it importa il video, nella forma di un nuovo post WordPress, con l\u2019aggiunta di tag ed eventuali altri termini tassonomici, la verifica della congruenza tematica e l\u2019indicazione di dati quali il nome dell\u2019intervistatore, il luogo; \u2022 L\u2019operatore del sito web esegue lo script \u201CYouTube transcript to WordPress\u201D indicando il numero identificativo del post WordPress generato al punto precedente. Lo script si occupa di: \u25E6 Estrarre in formato JSON la trascrizione dell\u2019audio; La Shoah Foundation gestisce il Visual History Archive, disponibile all\u2019indirizzo https:\/\/ sfi.usc.edu\/vha, con lo scopo di documentare e rinforzare l\u2019empatia verso le memorie di persone che hanno vissuto genocidi e altri drammi. 2  16  \f\u25E6  \u2022 \u2022  Trasformare ciascun frammento della trascrizione (in genere, ma non sistematicamente, corrispondente a una breve frase) in un elemento HTML contenente l\u2019indicazione del momento temporale in cui il testo viene pronunciato nel video; \u25E6 Salvare l\u2019intera trascrizione come valore per un campo personalizzato di WordPress associato al post contenente il video. L\u2019utente effettua una ricerca libera all'interno del repository, che \u00E8 stato opportunamente configurato per consentire la ricerca anche all\u2019interno dei campi personalizzati, normalmente ignorati dal motore di ricerca interno di WordPress; Il sito web elenca tutti i video che contengono la parola ricercata. Accedendo alla pagina di ciascun video \u00E8 possibile raggiungere il momento in cui una frase \u00E8 pronunciata attraverso un click sulla porzione di trascrizione corrispondente.  La selezione degli strumenti operativi che rendono possibile il procedimento \u00E8 dunque stata pensata per favorire la replicabilit\u00E0 totale dell\u2019esperienza in qualunque sistema basato su WordPress, indipendentemente da elementi quali il tema grafico utilizzato o dalla necessit\u00E0 di modificare l\u2019architettura del database sul quale poggia il sistema. Non \u00E8 previsto supporto in questa fase per altri CMS, ma non \u00E8 da escludere che le stesse funzionalit\u00E0 e la stessa logica di lavoro possano essere applicate anche altrove, data la presenza di diversi layer basati su tecnologie standard e interoperabili.  4 Risultati attesi e interventi futuri Il progetto ha come obiettivo principale la costruzione di un set di open data costituiti da testimonianze video, liberamente accessibili sul web e esplorabili in profondit\u00E0 attraverso gli strumenti di ricerca del parlato sopra descritti. Questo obiettivo, del resto, \u00E8 propedeutico a molte azioni formative che possono essere svolte proprio grazie alla possibilit\u00E0 di trovare all\u2019interno dei video esattamente ci\u00F2 che stiamo cercando, siano esse parole generiche o identificatori di luogo o di persona. L\u2019insieme delle trascrizioni si presta inoltre a successive analisi e esplorazioni con software di data mining (per esempio T-Lab) o di categorizzazione dei testi (per esempio Nvivo), all\u2019interno del paradigma di ricerca della Grounded Theory. Per quanto l\u2019accuratezza delle trascrizioni possa essere migliorata, gi\u00E0 allo stato attuale i testi dei video appaiono ben comprensibili e di grande aiuto per effettuare delle analisi approfondite. Nel caso di uno studio volto alla categorizzazione dei testi, un ulteriore passaggio manuale di correzione potrebbe portare a un corpus assestato e corretto in tempi ragionevolmente brevi. Tenendo conto che i software di analisi dei testi sono di fatto degli strumenti semi-automatici, questo tipo di operazione risulta essere di carattere ordinario. Bisogna inoltre considerare che il presente progetto, in modo del tutto automatico (attraverso il citato plugin Meks Video Importer per WordPress), si giover\u00E0 dei miglioramenti nel riconoscimento del parlato che verranno implementati nella piattaforma YouTube. Lo sviluppo dei sistemi di Intelligenza Artificiale ha dato prova, in questi ultimi anni, di consentire dei progressivi e tangibili miglioramenti nella comprensione del parlato, a partire dalla lingua inglese (riconosciuta nelle sue molte varianti di pronuncia). L\u2019insieme delle trascrizioni, infine, pu\u00F2 costituire la base per una piattaforma partecipativa che permetta di dare avvio a progetti di crowdsourcing di correzioni e integrazioni alle trascrizioni automatiche, aumentando ulteriormente la sostenibilit\u00E0 del progetto; inoltre, in linea con l\u2019approccio di public history fin qui adottato, sar\u00E0 possibile dare la possibilit\u00E0 agli utenti di apporre commenti ai video, commenti che a loro volta verranno trascritti e inseriti in una mappa esplorabile per concetti chiave. Dal punto di vista tecnico \u00E8 possibile pensare in tempi brevi all\u2019integrazione di altre due tecnologie Open Source. La prima di queste \u00E8 la piattaforma di ricerca Apache Solr (https:\/\/lucene.apache.org\/ solr\/): questa mette a disposizione un motore di indicizzazione pi\u00F9 raffinata rispetto alla semplice ricerca testuale disponibile nel CMS WordPress e permetterebbe di avere risultati di ricerca rispondenti a query di tipo diverso (con operatori booleani, stemming, proximity search). La seconda tecnologia potenzialmente utilizzabile \u00E8 Hypothes.is (https:\/\/web.hypothes.is\/), software Open Source di annotazione di pagine web, che renderebbe possibile appunto l\u2019annotazione dei contenuti, in modalit\u00E0 aperta e collaborativa da parte di ricercatori diversi.  17  \fRingraziamenti Si ringrazia sentitamente il prof. Gianfranco Crupi per gli utili consigli e suggerimenti; Inclusive Cloud S.r.l.s. per aver messo a disposizione competenze e infrastrutture informatiche; si ringraziano vivamente tutti i maestri che hanno cortesemente messo a disposizione il racconto della loro vita professionale e tutti gli studenti del corso di laurea in scienze della formazione primaria (universit\u00E0 di Firenze) che hanno raccolto, con pazienza e seriet\u00E0, le testimonianze in video."
	},
	{
		"id": 4,
		"title": "Ripensare i dati come risorse digitali: Un processo difficile?",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Nicola Barbuti"
		],
		"body": "Introduzione  L'art. 2 delle Conclusioni del Consiglio dell'UE del 21 maggio 2014 sul patrimonio culturale come risorsa strategica per un\u2019Europa sostenibile (2014 \/ C 183\/08) recita1: Il patrimonio culturale \u00E8 costituito dalle risorse ereditate dal passato in tutte le sue forme e aspetti \u2013 tangibile, intangibile e digitale (nato digitale e digitalizzato), inclusi monumenti, siti, paesaggi, abilit\u00E0, pratiche, conoscenze ed espressioni della creativit\u00E0 umana, nonch\u00E9 raccolte conservate e gestite da enti pubblici e privati come musei, biblioteche e archivi. Ha origine dall'interazione tra persone e luoghi nel tempo ed \u00E8 in continua evoluzione. Queste risorse sono di grande valore per la societ\u00E0 dal punto di vista culturale, ambientale, sociale ed economico e quindi la loro gestione sostenibile costituisce una scelta strategica per il 21\u00B0 secolo. Partendo da questo presupposto, dobbiamo necessariamente cambiare il nostro approccio al digitale e alla digitalizzazione iniziando a considerarli rappresentazioni qualificanti l'et\u00E0 contemporanea e la digital transformation che la connota. Ci\u00F2 implica l\u2019urgenza di individuare e classificare tra le risorse digitali prodotte fino a oggi e in produzione, siano esse singoli oggetti, o complesse digital libraries, o sistemi 3D, quelle che possono essere identificate come il nuovo Digital Cultural Heritage (DCH), distinguendole da quelle prodotte per mera semplificazione di processi gestionali o per la fruizione estemporanea e immediata di scadenti rappresentazioni relative a entit\u00E0 tangibili e intangibili. https:\/\/eur-lex.europa.eu\/legal-content\/EN\/TXT\/?uri=CELEX%3A52014XG0614%2808%29  1  19  \fDa diversi anni, la digitalizzazione e la qualit\u00E0 e preservazione delle risorse digitali sono riconosciute tra le principali emergenze da affrontare in tutto il mondo. Nel 2012 l'UNESCO ha tenuto la sua conferenza a Vancouver con il significativo titolo The Memory of the World in the Digital Age: Digitization and Preservation (Duranti and Shaffer [ed. by], 2012), nel cui ambito \u00E8 stata redatta ed emanata la Vancouver Declaration on Digitisation and Preservation2, con l\u2019IFLA e l\u2019International Council of Archives (ICA) tra i principali responsabili. Da allora, la situazione non sembra essere molto cambiata, nonostante gli sforzi intrapresi per accelerare l\u2019elaborazione di soluzioni a criticit\u00E0 di una complessit\u00E0 che, forse, non hanno precedenti storici. Gli attuali approcci ai processi di creazione delle risorse digitali, infatti, sembrano non recepire l\u2019evoluzione che negli ultimi anni ha riguardato la digitalizzazione, ancora oggi associata semplicisticamente alla riproduzione fotografica, mentre, invece, \u00E8 diventata un processo complesso guidato da regole definite e condivise. Anche l\u2019importanza della qualit\u00E0 dei dati digitali \u00E8 del tutto sottovalutata nel relegarne la funzione a meri strumenti di mediazione per la fruizione del il reale in forma virtuale, sebbene da pi\u00F9 parti si riconosca che questi dovrebbero rispondere a requisiti di intellegibilit\u00E0, affidabilit\u00E0, pertinenza, persistenza, e registrare le trasformazioni delle funzioni legate al loro riutilizzo nel tempo. L\u2019interpretazione strumentale, infatti, ancora orienta e condiziona negativamente soprattutto la strutturazione degli schemi di metadati con cui indicizzare gli oggetti digitali prodotti e la composizione delle descrizioni loro associate, formulate per essere meri codici funzionali esclusi-vamente alla ricerca e al recupero dei dati in rete. Proprio i metadati e i contenuti descrittivi, invece, dovrebbero essere oggetto di particolare attenzione, in quanto sono la sola possibilit\u00E0 di registrare e rappresentare in modo intellegibile i processi di digitalizzazione, creazione e trasformazione che caratterizzano il ciclo di vita dei dati, e di conservare cos\u00EC le informazioni necessarie a conoscerli e a qualificarli come risorse digitali con funzioni anche culturali. Il tema della funzione essenziale dei metadati nel management e nella fruizione dei dati digitali \u00E8 il focus dei FAIR Guiding Principles for Scientific Data Management and Stewardship3, le linee guida per il management dei dati scientifici pubblicate nel 2016, da qualche tempo uno dei temi di maggior interesse nell\u2019ambito del pi\u00F9 ampio dibattito sulle possibilit\u00E0 di applicare le metodologie del data science alla creazione e gestione dei data humanities4. A riguardo, nella CIDOC Conference 2018 si \u00E8 tenuto un workshop sull\u2019effettiva efficacia dei FAIR Principles rispetto agli scenari che oggi la digitalizzazione propone, e soprattutto rispetto a quelli che gi\u00E0 si preannunciano imminenti5. Il presente lavoro sintetizza alcune riflessioni maturate da quel proficuo confronto, relative alla necessit\u00E0 di provvedere a un ampliamento del requisito R: Reusable nei requisiti R5: Reusable, Readable, Relevant, Reliable and Resilient, finalizzato a facilitare l\u2019applicabilit\u00E0 dei FAIR Principles ai data humanities e, conseguentemente, l\u2019identificazione e certificazione come DCH dei dati rispondenti a tali requisiti nell\u2019informe magma digitale in cui oggi fluttuiamo.  2  Verso un ampliamento dei FAIR da R a R5  L\u2019assunto di partenza per avviare la riflessione \u00E8 che i dati digitali non possono pi\u00F9 essere creati finalizzandoli alla mera funzione di strumenti di mediazione per una fruizione della realt\u00E0 alternativa a quella fisica: \u00E8 necessario ripensarli quali risorse digitali che si qualificano come record, entit\u00E0 dinamiche e diacroniche che registrano e conservano nelle descrizioni i processi di digitalizzazione che li hanno creati e quelli che hanno caratterizzato il loro successivo ciclo di vita.  http:\/\/www.unesco.org\/new\/fileadmin\/MULTIMEDIA\/HQ\/CI\/CI\/pdf\/mow\/ unesco_ubc_vancouver_declaration_en.pdf 2  https:\/\/www.go-fair.org\/fair-principles\/ https:\/\/www.rd-alliance.org\/open-consultation-fair-data-humanities-until-15thjuly-2019; https:\/\/www.gofair.org\/implementation-networks\/overview\/co-operas\/; https:\/\/ operas.hypotheses.org\/ 5 http:\/\/www.cidoc2018.com\/sites\/default\/files\/CIDOC2018-BookOfAbstracts-Final-v-1-2.pdf 3  4  20  \fI metadati descrittivi diventano perci\u00F2 fondamentali e inscindibili dagli oggetti digitali, in quanto sono proprio l\u2019accuratezza e la qualit\u00E0 delle descrizioni a qualificarli come record e, quindi, a renderli risorse digitali pensate e strutturate per essere fruite diacronicamente dagli utenti del futuro, che devono comprendere cosa il dato rappresenta alla pari degli utenti contemporanei. L\u2019adozione dei FAIR Principles lascia aperte alcune questioni. Innanzitutto, non siamo del tutto persuasi che Ricercabilit\u00E0 (Findable), Accessibilit\u00E0 (Accessible, che assolutamente non \u00E8 identificabile con Open) e Interoperabilit\u00E0 (Interoperable) siano requisiti idonei a qualificare i dati come record e risorse digitali, conferendogli funzioni nuove e pi\u00F9 evolute da quelle strumentali attualmente riconosciute. Un dato che sia ricercabile, accessibile e interoperabile con altri non fornisce alcuna garanzia di qualit\u00E0, sufficienza e affidabilit\u00E0 dei contenuti informativi che contiene. Inoltre, la ricercabilit\u00E0 e, di conseguenza, l\u2019accessibilit\u00E0 e interoperabilit\u00E0 che sono a essa vincolate hanno senso nella misura in cui un dato sia oggetto di interesse da parte dei fruitori. E l\u2019interesse per un dato \u00E8 legato strettamente non alla sua mera funzione di chiave d\u2019accesso a un oggetto digitale semplice o complesso, ma all\u2019essere risorsa informativa e cognitiva per la quantit\u00E0 e qualit\u00E0 dei contenuti descrittivi che mette a disposizione dell\u2019utente gi\u00E0 in fase di lettura dei suoi metadati. Siamo perci\u00F2 del parere che il requisito che conferisce significato e senso ai primi tre, e dal quale questi dipendono indissolubilmente, sia la Riutilizzabilit\u00E0 (Reusable). L\u2019utilizzo e, soprattutto il riutilizzo dei dati, infatti, sono secondo noi i fattori che ne garantiscono la sostenibilit\u00E0 nel tempo e, quindi, la sopravvivenza, in quanto requisiti caratterizzati da dinamismo e diacronia che, quasi sem-pre, implicano trasformazioni nelle funzioni delle entit\u00E0 che ne sono oggetto: per avere un\u2019idea uti-lizzando un paradigma analogico, si pensi al Colosseo e al suo ciclo di vita. Le registrazioni descrittive dei metadati sono perci\u00F2 fondamentali per garantire qualit\u00E0 e persistenza delle risorse digitali, qualora siano improntate a equilibrate soluzioni quantitative\/qualitative e rispondano a ulteriori requisiti che, secondo noi, sono altrettanto essenziali quanto la riutilizzabilit\u00E0. Anche la Reusability, infatti, di per s\u00E9 non costituisce una garanzia di qualit\u00E0 del dato e del suo valore quale risorsa informativa e cognitiva. Anzi: proprio le variabili cui una risorsa \u00E8 soggetta perch\u00E9 riutilizzabile possono essere fonte di distorsione e difformit\u00E0 dei contenuti, il cui valore informativo e cognitivo pu\u00F2 perci\u00F2 non essere pi\u00F9 certificabile come affidabile. La R di Reusable andrebbe perci\u00F2, secondo noi, ampliata in R5 con i seguenti requisiti: -  -  -  -  Readability: da intendersi non nell\u2019accezione semantica di leggibilit\u00E0, ma in quella concettuale di intellegibilit\u00E0 della risorsa digitale per tutte i possibili target di utenti interessati a fruirne; \u00E8 requisito fondamentale per conferire ai metadati la funzione informativa e cognitiva necessaria a qualificarli come risorsa culturale, e si basa sull\u2019equilibrato rapporto quantitativo\/ qualitativo dei contenuti descrittivi e sull\u2019accuratezza formale, stilistica e linguistica dei contenuti; Relevance: la persistenza nel tempo \u00E8 legata all\u2019interesse degli utenti per i contenuti informativi e cognitivi registrati nella risorsa; essa \u00E8 strettamente legata al suo riutilizzo e alle possibili trasformazioni di funzione registrate nelle descrizioni; \u00E8, quindi, requisito indispensabile affinch\u00E9 la risorsa, di solito creata con funzioni e scopi non necessariamente culturali, possa essere identificabile e riconoscibile nella sua struttura formale e descrittiva anche se varia nel tempo le proprie funzioni evolvendosi in fonte di conoscenza sui processi che registra e, quindi, in risorsa culturale digitale; Reliablility: l'affidabilit\u00E0 \u00E8 la certificazione e validazione della qualit\u00E0 della risorsa digitale rilevabili dalle registrazioni delle sue descrizioni durante tutto il suo ciclo di vita, in relazione a tutte le possibili trasformazioni ed evoluzioni funzionali cui pu\u00F2 essere stata soggetta; \u00E8, dunque, strettamente connessa alla capacit\u00E0 dell'entit\u00E0 digitale di registrare e preservare gli elementi qualificanti la qualit\u00E0 informativa e cognitiva dei suoi contenuti descrittivi, anche nell\u2019evoluzione delle funzioni e nelle variazioni di forme e funzioni nel tempo; Resilience: come l\u2019intellegibilit\u00E0, anche la resilienza applicata ai dati e, soprattutto, ai metadati \u00E8 requisito fondamentale per conferire alle risorse digitali la nuova dimensione culturale;  21  \fchiosando la definizione comunemente in uso in ambito informatico6, essa va intesa come la capacit\u00E0 di una risorsa digitale di adattarsi alle condizioni di utilizzo e riutilizzo, di resistere all\u2019usura, di essere duttile nelle trasformazioni e nell\u2019evoluzione delle sue funzioni, al fine di garantire la disponibilit\u00E0 del proprio potenziale cognitivo e informativo nello spazio e nel tempo; \u00E8, quindi, indispensabile per garantire la sostenibilit\u00E0 e il riutilizzo delle risorse digitali nel medio-lungo termine, provvedendo a preservare sia le informazioni utili a conoscere i processi della loro creazione, sia quelle sulla loro funzione originale, sia, infine, le registrazioni delle trasformazioni ed evoluzioni funzionali che ne hanno caratterizzato il ciclo di vita.  3  Conclusioni  Tirando le conclusioni su quanto sopra sinteticamente delineato, \u00E8 nostra opinione che l\u2019adozione dei requisiti FAIR con la R ampliata in R5 sia prerequisito indispensabile nel processo di creazione dei dati digitali e, soprattutto, dei metadati che li descrivono, in quanto gli conferirebbero le funzioni di potenziale DCH, rendendoli sostenibili, permanenti, affidabili e, nel contempo, storicizzandoli come fonti di conoscenza dei processi e delle complessit\u00E0 che caratterizzano la rapidissima evoluzione della digital transformation. Non il dato in s\u00E9, infatti, ma l\u2019interesse degli utenti presenti e futuri per la fruizione del dato in quanto risorsa informativa e cognitiva deve diventare il prerequisito su cui fondare l\u2019intero processo di creazione, pubblicazione e preservazione di risorse digitali. L\u2019applicazione dei requisiti R5, dun-que, deve diventare oggetto di attenzione fin dalla fase di analisi e progettazione dei processi sia di digitalizzazione che di creazione di qualsiasi schema di metadati con cui descrivere e gestire gli oggetti digitali in produzione. Solo cos\u00EC si potr\u00E0 dare un serio inizio, nel medio termine, all\u2019individuazione di quanto possa essere identificato come DCH nella massa di dati che oggi sovrabbonda nel web e, nel contempo, si potranno definire linee guida omogenee e condivise che presiedano alla creazione di nuove risorse avendo chiaro fin dal principio se gli si voglia conferire il potenziale valore di entit\u00E0 culturali. In questo modo, nel giro di pochi anni le Conclusioni EU del 2014 potranno finalmente essere sostanziate con un nuovo DCH ufficialmente riconosciuto. Diversamente, continueremo a considerare digitalizzazione e digitale solo come un modo diverso e accattivante di fruire il tangibile, perdendo di vista quanto invece tutto ci\u00F2 sia gi\u00E0 oggi l\u2019humus identitario che, pur a livelli diversi, identifica l\u2019era digitale contemporanea."
	},
	{
		"id": 5,
		"title": "Verso il riconoscimento delle Digital Humanities come area scientifica: Il catalogo online condiviso delle pubblicazioni dell‚ÄôAIUCD",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Nicola Barbuti",
			"Maurizio Lana",
			"Vittore Casarosa"
		],
		"body": "Introduzione  Nell\u2019ambito della Conferenza AIUCD 2019, uno dei temi di maggior confronto, anche in seno all\u2019Assemblea annuale dei soci, \u00E8 stata la necessit\u00E0 ormai non pi\u00F9 rinviabile di attivare iniziative finalizzate a riconoscere alle Digital Humanities \u2013 o Humanities Computing che si vogliano ridefinire \u2013 la dignit\u00E0 di Area Scientifica, emancipandole definitivamente dalla dimensione di come nebulosa in cui fluttuano in modo frammentario e caotico ricercatori e studiosi rinvenienti da una pletora di SSD tradizionali delle Humanities, in cui sono considerati delle entit\u00E0 scientifiche ancipiti. L\u2019analisi dello stato dell\u2019arte della disciplina ha evidenziato la consistenza decisamente ampia di contributi scientifici che, pur riferendosi ad ambiti umanistici riconducibili a singoli SSD tradiziona-li, sono confluenti nella comune prospettiva di ricerca su digitale e computazionale applicati alle humanities e, perci\u00F2, assolutamente associabili in un un\u2019unica area di contenimento. Pertanto, al fine di sostanziare l\u2019istanza di riconoscimento delle DH come Area con una sua digni-t\u00E0 scientifica, si \u00E8 scelto un indirizzo operativo \u201Cdal basso\u201D: produrre un catalogo digitale delle pub-blicazioni nazionali di settore che possa essere riconosciuto dall\u2019AIUCD quale proprio riferimento ufficiale e, in prospettiva, possa diventare nodo di un pi\u00F9 ampio e condiviso catalogo internazionale di pubblicazioni sulle DH. Il Gruppo di Lavoro individuato per occuparsi di progettare e strutturare il catalogo \u00E8 composto dai soci AIUCD Maurizio Lana, Vittore Casarosa e Nicola Barbuti. Il GdL ha intrapreso le attivit\u00E0 immediatamente dopo la chiusura del Convegno 2019 e attualmen-te sta provvedendo agli ultimi passaggi per la realizzazione esecutiva di quanto progettato e proposto all\u2019Associazione agli inizi dell\u2019estate 2019. 24  \f2  Il Catalogo delle DH AIUCD  Primo tema di riflessione \u00E8 stato definire i limiti geografici e cronologici delle pubblicazioni da inserire in prima istanza. Si \u00E8 deciso di limitare inizialmente l\u2019inserimento agli autori italiani con priorit\u00E0 per i soci AIUCD, con un orizzonte temporale non superiore agli ultimi 10 anni. \u00C8 stata ipotizzata la creazione dello spazio online con credenziali di accesso da condividere con tutti i soci in modo che, per quanto possibile, ognuno possa inserire da s\u00E9 i record bibliografici relativi alle proprie pubblicazioni, e di consentire anche l\u2019associazione dei pdf ai record, ove legalmente disponibili. Tuttavia, per conferire al catalogo veste ufficiale, pur consentendo a ciascuno di inserire i dati direttamente, sembra opportuno stabilire regole definite e condivise, nominando un organismo deputato a eseguire un controllo annuo sulla coerenza delle nuove risorse bibliografiche caricate per evitare l\u2019insorgere di situazioni caotiche. Dal momento che una delle principali criticit\u00E0 connesse con la multiforme produzione scientifica delle DH \u00E8 proprio l\u2019elevata variet\u00E0 di fonti e quindi di formati citazionali bibliografici, si \u00E8 passati ad analizzare software open source per la gestione e la fruizione di record bibliografici digitali che avessero le caratteristiche necessarie a favorire un import di risorse di diversa tipologia e struttura. Primi a essere presi in considerazione sono stati Zotero, in considerazione sia del fatto che l\u2019AIUCD utilizzando questo software aveva gi\u00E0 attivato un repository per l\u2019allocazione di risorse digitali relative alle DH sia dell\u2019ampio uso che se ne sta facendo per la creazione di bibliografie on line con pubblicazioni relative ad altri ambiti di ricerca scientifica, e Zenodo per il repository delle fonti non altrimenti online. Zotero nacque proprio come strumento per la gestione di bibliografie di area storica (Cohen, 2008) ed \u00E8 attualmente usato, per esempio, dall\u2019associazione tedesca di Digital Humanities1 per uno scopo simile a quello cui stiamo pensando anche per AIUCD o dalla American School of Classical Studies at Athens (ASCSA) per catalogare e gestire i metadati di tutte le pubblicazioni della Scuola stessa (libri e articoli)2. Tuttavia, quest\u2019ultimo \u00E8 risultato un open repository per \"prodotti della ricerca\" generici ed ha evidenziato il limite non secondario che ogni oggetto caricato deve essere linkato a mano al record corrispondente. Zotero invece \u00E8 mirato a collezioni di articoli e bibliografie di varia natura e i dati caricati in uno suo spazio on line non sono soggetti al problema rilevato per Zenodo (O\u2019Donnell, Manola, Manghi, Porter, Esau, Viejou, Rosselli Del Turco, and Singh, 2018; Peters, Kraeker, Lex, Gumpenberger and Gorriaz, 2017). \u00C8 sufficiente, infatti, che il responsabile del caricamento tagghi la pubblicazione con l'indicazione del\/dei SSD in cui si colloca e con le keywords che egli ritiene ne identifichino correttamente il contenuto per rendere il record interrogabile anche in questo modo oltre che con i consueti criteri di autore, titolo, etc. Inoltre, altre caratteristiche interessanti di Zotero sono la separazione dei (meta)dati dalla loro presentazione secondo uno stile citazionale piuttosto che un altro, la possibilit\u00E0 di esportare in RDF e altri formati open il database della bibliografia. Altro punto a favore di Zotero \u00E8 la possibilit\u00E0 di definire modalit\u00E0 di recupero delle informazioni bibliografiche incrociando i seguenti dati: - dati dei titoli; - dati delle keywords inserite ufficialmente nei lavori o, in mancanza, indicate espressamente dagli autori rilevando parole poi riscontrabili nel testo; - dati che possono essere estratti dagli abstract o dai full text dei contributi inseriti; - SSD degli autori. Queste rilevazioni possono essere utilizzate per produrre report annui sullo stato dell\u2019arte della ricerca scientifica sulle DH da ufficializzare per rimarcare la fertilit\u00E0 produttiva del settore. Per iniziare a popolare il catalogo, si \u00E8 concordato di proporre ai soci (e non, purch\u00E9 italiani) vari modi per inserire i dati nella bibliografia (Vahdati, Arndt, Auer and Lange, 2016): https:\/\/www.zotero.org\/groups\/372575\/dhd_ag_publikationen https:\/\/www.zotero.org\/groups\/80651 american_school_of_classical_studies_at_athens  1  2  25  \f-  coloro che usano un Bibliographic Reference Software (BRS: Zotero, Mendeley, Endnote, Bibref, Refworks, etc.), possono esportare i (meta)dati citazionali delle loro pubblicazioni completi di URL a ciascuna pubblicazione, quindi inserirli in Zotero e dare l'accesso pubblico per consentire la fruizione diretta delle risorse caricate; - coloro che possono utilizzare l\u2019ISBN per le monografie o il DOI per gli articoli possono inserire direttamente nella bibliografia online, utilizzando Zotero, i dati delle loro pubblicazioni, completandoli con i tag che indicano il SSD e quant\u2019altro pu\u00F2 essere necessario al recupero del record. Per tutte le forme di pubblicazione grigia (presentazioni, abstract, raccolte di dati, etc.) Zotero non pu\u00F2 gestire in modo ottimale i dati perch\u00E9 gestisce per lo pi\u00F9 risorse bibliografiche. Giunge utile a questo punto Zenodo, poich\u00E9 assegna automaticamente un DOI alle pubblicazioni o alle fonti in genere che non lo hanno gi\u00E0, e quindi permette di salvare nel suo repository aperto anche altri prodotti della ricerca, come dati e software (Potter and Smith, 2015), oltre alle classiche pubblicazioni. Si \u00E8 dunque concluso di creare il catalogo sfruttando al massimo le diverse opportunit\u00E0 offerte dai due software, creando una soluzione che, non comportando attivit\u00E0 di sviluppo software ad hoc, dia ragionevoli prospettive di sostenibilit\u00E0 e permetta agevoli importazione, esportazione e migrazione dei dati. Di seguito riportiamo l\u2019articolazione dell\u2019ipotesi progettuale, che si articola nelle seguenti fasi. Base di partenza sar\u00E0 il catalogo AIUCD gi\u00E0 esistente in Zotero, sebbene esso presenti alcuni fisiologici punti di debolezza (duplicazioni, item incompleti, assenza di rimando con link alle fonti dove disponibili in OA, etc.). I cataloghi Zotero sono accessibili in lettura-scrittura per gli editors, in sola lettura per tutti quelli che hanno il link. Per facilitare l'acquisizione e l\u2019inserimento dei dati si partir\u00E0 dalle pubblicazioni che hanno gi\u00E0 il DOI, quindi seguiranno, in ordine progressivo, quelle che hanno ISBN, poi quelle con ISSN, infine le pubblicazioni i cui dati devono essere raccolti e inseriti manualmente. Le pubblicazioni cosiddette \u201Cgrigie\u201D prive di DOI (a es.: presentazioni) saranno invece preliminarmente caricate in Zenodo, di modo che il sistema le renda accessibili assegnandovi un DOI. Scegliere di gestire le risorse digitali associando Zotero e Zenodo, oltre a presentare il vantaggio di rendere le pubblicazioni inserite facilmente ricercabili utilizzando chiavi di accesso uniformi, consente di collocare il catalogo in un contesto di ricerca aperta. Ciascun socio\/autore, quindi, potr\u00E0 provvedere all\u2019inserimento dei propri dati nel modo che segue: - chi ha pubblicazioni in OA ma senza DOI le carica in Zenodo per ottenere il DOI; chi ha pubblicazioni gi\u00E0 provviste di DOI comunica la lista dei DOI, uno per riga, mandando un messaggio all'indirizzo email dedicato pubblicazioni@aiucd.it; se la pubblicazione non ha keywords internamente, si possono indicare fino a un massimo di 5 keyword (parole o espressioni) accanto al DOI, separate da virgole; a riguardo, una buona chiave identificativa pu\u00F2 essere il (o i) SSD in cui l\u2019autore ritiene si collochi la sua pubblicazione nello spazio cloud di zotero; - chi ha libri manda gli ISBN; - chi ha articoli senza DOI manda l'URL. Le pubblicazioni provviste di DOI saranno caricare in Zotero utilizzando il codice. Il DOI dovr\u00E0 essere presente e ben visibile nel catalogo pubblico, in quanto \u00E8 la chiave di accesso principale alla pubblicazione inserita. Relativamente alle necessit\u00E0 di gestione del catalogo, si prospettano le seguenti soluzioni. Sar\u00E0 necessario ridefinire chi avr\u00E0 accesso in scrittura al catalogo sia per inserire i DOI in Zotero e costruire l\u2019elenco, sia per intervenire a correggere eventuali errori nei dati inseriti. Un accesso indiscriminato, infatti, creerebbe rischi di rumore notevole e soluzioni caotiche e difformi nell\u2019organizzazione dei dati. Ogni inizio d\u2019anno, i soci saranno invitati a inviare all'indirizzo sopra indicato i DOI e gli ISBN delle nuove pubblicazioni dell'anno precedente.  26  \fDal momento che esiste un problema concreto di autodefinizione sulla pertinenza delle pubblicazione al campo DH, al fine di evitare l\u2019afflusso nel catalogo di pubblicazioni che poco hanno a che fare con il settore sar\u00E0 necessario che il Direttivo AIUCD definisca delle linee guida sui temi di ricerca coerenti con esso. Una volta analizzato l\u2019impatto del catalogo nell\u2019ambito della ricerca scientifica (Sample, 2011), si potranno prendere in considerazione altri aspetti che renderanno necessarie opportune implementazioni: si pensi, nello specifico, alle pubblicazioni senza codici, che sono in linea di massima quelle pi\u00F9 datate e richiedono notevole lavoro per essere inserite nel catalogo. Resta da definire come agire operativamente, cio\u00E8 chi si occuper\u00E0 di inserire i DOI in Zotero per costruire l'elenco: potrebbe essere un\u2019attivit\u00E0 laboratoriale per studenti di biblioteconomia\/scienze biblioteconomiche e dell\u2019informazione o per i futuri digital librarians?  3  Conclusioni  Tirando le conclusioni sul catalogo progettato, siamo del parere che un\u2019integrazione Zotero-Zenodo in un\u2019unica soluzione on line, sfruttando al massimo l'assegnazione DOI e, in prospettiva, la funzione di repository del secondo, prende il massimo da due mondi open source. Ci\u00F2 apre opportunit\u00E0 di ana-lisi della ricerca del settore prima impossibili da attuare (Winslow, Rains, Skripsky and Kelly, 2016), e si configura come un elemento qualificante il riconoscimento delle DH come settore di primo livel-lo nella ricerca italiana."
	},
	{
		"id": 6,
		"title": "Il trattamento automatico del linguaggio applicato all'italiano volgare. La redazione di un formario tratto dalle prime dieci Lettere di Alessandra M. Strozzi",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Ottavia Bersano",
			"Nadezda Okinina"
		],
		"body": "1 Introduzione\r\nIl presente contributo si propone di coniugare le tecniche offerte dal Trattamento automatico del Linguaggio\r\n(TAL) allo studio dell'italiano volgare, e nella fattispecie a testi di origine fiorentina del sec. XV. Esso nasce\r\nin seno ad alcune difficolt\u00E0 emerse durante la selezione dei dati testuali di una tesi di dottorato \u2013 ancora in\r\ncorso di stesura \u2013 tesa a fornire una nuova edizione dell'Epistolario di Alessandra Macinghi Strozzi (14061471): un documento assai conosciuto e apprezzato da chiunque si occupi della civilt\u00E0 italiana del Quattrocento\r\ne che siamo ancora costretti a leggere nell'edizione curata da Cesare Guasti nel 1877 (Guasti, 1877). Una nuova\r\nedizione e una nuova analisi linguistico-interpretativa, condotta con gli strumenti pi\u00F9 aggiornati, porter\u00E0\r\nindiscutibili vantaggi anche agli studi di ambito storico e letterario.\r\n\r\n2 Le Lettere di Alessandra Macinghi Strozzi\r\nLe Lettere \u2013 in tutto settantatr\u00E9 \u2013 furono scritte in un arco temporale compreso tra il 1447 e il 1470 e\r\nindirizzate, da Alessandra Macinghi \u2013 vedova di Matteo Strozzi \u2013 ai figli Filippo, Lorenzo e Matteo. Questi,\r\nraggiunta la maggiore et\u00E0, ereditarono l'esilio paterno e furono costretti a lasciare Firenze: un provvedimento\r\nlegislativo, infatti, stabiliva che tutti i figli maschi degli esuli, al compimento del diciottesimo anno, ne\r\ndovessero ereditare la condizione; lasciata dunque Firenze ed esercitando il mestiere della mercatura, Filippo,\r\nLorenzo e Matteo viaggiarono per tutta Europa, trovandosi a soggiornare nei maggiori centri politicocommerciali del tempo, in particolare a Palermo, Napoli, Barcellona, Madrid, Londra e Bruges.\r\nIl presente Epistolario \u2013 che nasce da una naturale quanto necessaria esigenza comunicativa, quella che\r\nintercorre tra una madre e i suoi figli \u2013 costituisce un raro esempio di scrittura femminile privata del sec. XV\r\ne rappresenta non solo una delle prime testimonianze in lingua volgare, ma anche una delle prime\r\ntestimonianze scritte da una donna laica, le cui possibilit\u00E0 di scolarizzazione, com'\u00E8 noto, erano al tempo\r\nlimitatissime.\r\n\r\n28\r\n\r\n\f2.1 La rilevanza linguistica dell'Epistolario\r\nLa figura di Alessandra Macinghi Strozzi desta interesse non solo perch\u00E9 dimostra una certa dimestichezza\r\nnel padroneggiare la penna, ma anche perch\u00E9 riesce con disinvoltura a cimentarsi in partite e ragioni: un ambito\r\nlessicalmente e storicamente maschile. La prosa di Alessandra si contraddistingue per autenticit\u00E0, schiettezza\r\ne onest\u00E0 di valori pedagogici, tanto che Contini (1970) la defin\u00EC la \"paradigmatica\" tra le prose domestiche del\r\nQuattrocento; la sua rilevanza linguistica \u00E8 data anzitutto dal genere letterario cui va ascritta, ovverosia il\r\ngenere epistolografico: \u00ABLe settantatr\u00E9 Lettere pervenuteci rispondono a un bisogno immediato, urgente di\r\ncomunicazione, non hanno n\u00E9 preoccupazione n\u00E9 destinazione letteraria [...]. Sono scritti assolutamente privati,\r\nin cui la stessa grammatica \u00E8 quella parlata; avvisi, massime, ricordi, notizie, resoconti [...] di fatti,\r\navvenimenti, azioni, proposte relative al nucleo familiare, all'intimit\u00E0 della casa, segreti che non devono andare\r\noltre il mittente e il destinatario [...]\u00BB (Doglio, 1984, p. 487).\r\nPer quanto concerne il lessico delle Lettere, si intende redigere \u2013 in appendice e a completamento della\r\nnuova edizione \u2013 un Glossario, che recepir\u00E0 e metter\u00E0 prontamente a disposizione del LEI \u2013 Lessico\r\netimologico italiano (Pfister, 1979-) \u2013 numerosi termini specialistici, fra cui primeggiano quelli relativi\r\nall\u2019attivit\u00E0 commerciale e finanziaria, che ebbe nell\u2019Italia del tardo Medio Evo e del Rinascimento uno\r\nstraordinario sviluppo. E poich\u00E9 taluni di questi termini si sono trasmessi anche alle altre lingue europee (si\r\npensi alla fortuna di termini come banco, banchiere, capitale, credito, debito, polizza, assicurazione, lettera\r\ndi cambio, ecc.), ne deriver\u00E0 un contributo molto utile alla storia della terminologia economica sovranazionale\r\ntuttora in uso.\r\nCome messo in evidenza pi\u00F9 volte dallo Stussi (2000), i documenti mercantili sono una fonte di inestimabile\r\nvalore per la conoscenza della storia della lingua e offrono talvolta la possibilit\u00E0 di retrodatare parole o,\r\naddirittura, intere espressioni.\r\nUna nuova edizione delle Lettere della Macinghi Strozzi metter\u00E0 finalmente a disposizione degli studiosi\r\nun testo filologicamente corretto e linguisticamente affidabile, che potr\u00E0 sostituire l\u2019edizione curata da Cesare\r\nGuasti nel 1877, di cui pi\u00F9 volte ne sono stati segnalati i limiti (Trifone, 1989). Verr\u00E0 cos\u00EC per la prima volta\r\nalla luce la grafia realmente utilizzata dall\u2019autrice, sistematicamente modernizzata dal Guasti, e si potranno\r\nrecuperare i diversi caratteri relativi ai suoni e alle forme, che non risultano nella precedente edizione. Le\r\nLettere si prestano inoltre a interessanti rilievi di ordine sociolinguistico, essendo la scrivente una donna di\r\nceto mercantile, assimilabile alla classe dei cosiddetti \"semicolti\".\r\n\r\n3 L'applicazione del TAL per la redazione del formario\r\nPer il presente contributo \u00E8 stato realizzato un primo modello di formario tratto dalle prime dieci Lettere di\r\nAlessandra M. Strozzi, teso a mettere in luce i tratti grafici, fonetici e morfologici caratterizzanti la lingua della\r\nscrivente, autentica espressione del fiorentino argenteo (cfr. Manni, 1979); trattandosi di un archetipo, lo\r\nstudio si \u00E8 limitato all'analisi delle prime dieci lettere dell'Epistolario: ci\u00F2 ha consentito di eseguire un lavoro\r\naccurato, ma soprattutto di riflettere sui benefici e sulle criticit\u00E0 date dall'utilizzo degli strumenti offerti dal\r\nTAL.\r\nAlcune rese grafiche dell'originale (del tipo x per s, y\/j per i, cha per ca, cho per co, chu per cu) sono state\r\nanzitutto normalizzate conformemente alla grafia moderna; \u00E8 stata successivamente realizzata una\r\ntokenizzazione attraverso l'impiego del modello TreeTagger per l'italiano contemporaneo elaborato da Achim\r\nStein (Schmid, 1994). Si \u00E8 quindi proceduto con l'assegnazione di un POS per ogni forma delle prime dieci\r\nLettere, il cui corpus consta, in tutto, di 13.782 occorrenze. Il tagset impiegato, elaborato sulla base del tagset\r\nELRA, \u00E8 stato semplificato e adattato alle esigenze dell'analisi linguistica eseguita, della quale vengono esposti\r\ngli esiti al \u00A7 4; il tagset adottato \u00E8 illustrato nella tabella sottostante, la quale pone inoltre in evidenza le\r\ncorrispondenze tra le etichette impiegate da chi scrive e quelle proprie degli analizzatori adoperati.\r\nCome si evince dalla tabella, \u00E8 stata operata una distinzione tra articolo determinativo e indeterminativo;\r\nper quel che riguarda le congiunzioni, invece, si \u00E8 preferito non introdurre alcuna distinzione tra congiunzione\r\nsubordinante e coordinante, dato che nella lingua del tempo la congiunzione assume una funzione che sovente\r\nnon \u00E8 possibile classificare con certezza, giacch\u00E9 polivalente. Le preposizioni, ripartite in semplici e articolate,\r\nconstano di una terza etichetta, \"prep.\", sotto la quale sono state fatte confluire le preposizioni improprie.\r\n\r\n29\r\n\r\n\fSPIEGAZIONE\r\n\r\nETICHETTA\r\n\r\nCORRISPONDENZE\r\nELRA\r\n\r\nCORRISPONDENZE\r\nTREETAGGER\r\n\r\nCORRISPONDENZE\r\nTINT\r\n\r\naggettivo\r\n\r\nagg.\r\n\r\nAS, AP,AN, DP,\r\nDN, DS\r\n\r\nADJ\r\n\r\nA, AP, DI, PI\r\n\r\nantroponimo\r\n\r\nantrop.\r\n\r\nSPN\r\n\r\nNPR\r\n\r\nSP\r\n\r\nart.det.f.\r\n\r\nRS, RP\r\n\r\nDET:def\r\n\r\nRD\r\n\r\nart.det.m.\r\n\r\nRS, RP\r\n\r\nDET:def\r\n\r\nRD\r\n\r\nart.indet.f.\r\n\r\nRS, RP\r\n\r\nDET:indef\r\n\r\nRI\r\n\r\nart.indet.m.\r\n\r\nRS, RP\r\n\r\nDET:indef\r\n\r\nRI\r\n\r\navverbio\r\n\r\navv.\r\n\r\nB\r\n\r\nADV\r\n\r\nB\r\n\r\ncongiunzione\r\n\r\ncong.\r\n\r\nC*\r\n\r\nCON\r\n\r\nCC, CS\r\n\r\ninteriezione\r\n\r\nint.\r\n\r\nI\r\n\r\nINT\r\n\r\nI\r\n\r\nnon verbale\r\n\r\nx\r\n\r\nX*\r\n\r\nSENT, SYM, PON,\r\nFW, LS\r\n\r\nFB, FF, FS\r\n\r\nnumerale\r\n\r\nnum.\r\n\r\nN\r\n\r\nNUM\r\n\r\nNO\r\n\r\npreposizione\r\n\r\nprep.\r\n\r\nE\r\n\r\nPRE\r\n\r\nE\r\n\r\npreposizione articolata\r\n\r\nprep.art.\r\n\r\nES, EP\r\n\r\nPRE:det\r\n\r\nE+RD\r\n\r\npreposizione semplice\r\n\r\nprep.sempl.\r\n\r\nE\r\n\r\nPRE\r\n\r\nE\r\n\r\npronome\r\n\r\npron.\r\n\r\nP*, Q*\r\n\r\nPRO*\r\n\r\nPC, PD, PE, DQ\r\n\r\npronome femminile\r\n\r\npron.f.\r\n\r\nP*, Q*\r\n\r\nPRO*\r\n\r\nPC, PD, PE, DQ\r\n\r\npronome maschile\r\n\r\npron.m.\r\n\r\nP*, Q*\r\n\r\nPRO*\r\n\r\nPC, PD, PE, DQ\r\n\r\nsostantivo femminile\r\n\r\ns.f.\r\n\r\nSS, SP, SN\r\n\r\nNOM\r\n\r\nS\r\n\r\nsostantivo maschile\r\n\r\ns.m.\r\n\r\nSS, SP, SN\r\n\r\nNOM\r\n\r\nS\r\n\r\ntoponimo\r\n\r\ntop.\r\n\r\nSPN\r\n\r\nNPR\r\n\r\nSP\r\n\r\nverbo\r\n\r\nv.\r\n\r\nV*\r\n\r\nVER*\r\n\r\nV*\r\n\r\narticolo determinativo\r\nfemminile\r\narticolo determinativo\r\nmaschile\r\narticolo\r\nindeterminativo\r\nfemminile\r\narticolo\r\nindeterminativo\r\nmaschile\r\n\r\nTabella 1. Corrispondenze delle etichette grammaticali.\r\nSi segnalano mediante l'asterisco (*) le etichette trascritte nella forma base, senza i dettagli offerti dai tools.\r\nL'etichetta TreeTagger \"PRO:demo\", per esempio, \u00E8 stata riportata \"PRO*\".\r\nDal momento che nessun POS tagger dispone di un modello per l'italiano volgare, il processo di POS\r\ntagging \u00E8 risultato piuttosto articolato e, per ovviare all'assenza del modello, sono state sperimentate due\r\ndiverse strategie: la prima \u00E8 consistita nell'applicare il modello POS tagger addestrato sul corpus D(h)ante\r\n(Basile e Sangati, 2016), la seconda nel normalizzare il corpus delle Lettere, cos\u00EC da poter impiegare un POS\r\ntagger predisposto per l'italiano contemporaneo. Al fine di eseguire un valido confronto fra le due strategie,\r\nsono state manualmente etichettate le prime 1.000 parole del corpus delle Lettere, le cui assegnazioni sono\r\nstate comparate con gli esiti dati dai due processi sopra descritti. Impiegando dunque TreeTagger e il parser\r\nStanford CoreNLP \u2013 addestrati da A. Basile e F. Sangati sul corpus D(h)ante \u2013 sono state assegnate le parti\r\ndel discorso. TreeTagger ha dato un risultato migliore rispetto al parser Stanford: il primo ha dato infatti il\r\n59% dei tag corretti contro il 54% del secondo. Prima di approdare alla seconda strategia, il testo delle Lettere\r\n\u00E8 stato normalizzato utilizzando il correttore ortografico GNU Aspell (http:\/\/aspell.net) e quindi nuovamente\r\netichettato mediante il modello TreeTagger elaborato da Achim Stein e addestrato sull'italiano contemporaneo,\r\nil cui impiego ha consentito di approdare a una percentuale di tag corretti pari al 69%. I migliori risultati di\r\nPOS tagging, tuttavia, sono stati ottenuti utilizzando il tagger per l'italiano contemporaneo Tint (Aprosio e\r\nMoretti, 2018) che, una volta applicato al testo precedentemente normalizzato, ha restituito il 72% delle\r\netichette corrette.\r\n30\r\n\r\n\fProgramma\r\n\r\nStanford\r\nD(h)ante\r\n\r\nTreeTagger\r\nD(h)ante\r\n\r\nAspell+TreeTagger\r\n(Stein)\r\n\r\nAspell + Tint\r\n\r\nAccuratezza\r\n\r\n54%\r\n\r\n59%\r\n\r\n69%\r\n\r\n72%\r\n\r\nTabella 2. Descrizione dell'accuratezza dei programmi impiegati nell'assegnazione dei tag alle prime 1000\r\nparole del corpus delle Lettere.\r\nNell'attribuzione delle etichette le parti del discorso che hanno presentato maggiori difficolt\u00E0 sono state\r\naggettivi e pronomi, indipendentemente dal programma impiegato. L'etichettatore Stanford \u00E8 inoltre risultato\r\nparticolarmente debole e impreciso nel riconoscimento dei verbi, attribuendo erroneamente tale etichetta a\r\nmolte altre parti del discorso.\r\nAl fine di incrementare la percentuale di tag corretti, il sistema \u00E8 stato perfezionato attraverso l'impiego\r\ndei dati derivanti dal dizionario TLIO (Tesoro della Lingua italiana delle Origini, http:\/\/\r\ntlio.ovi.cnr.it\/TLIO\/), che ha consentito di attribuire le etichette grammaticali a 5.194 forme e\r\ngrazie al quale \u00E8 stato possibile ricavare, inoltre, una puntuale distinzione di genere per 606 sostantivi. Per i\r\nsostantivi restanti, che non hanno trovato riscontro all'interno del dizionario TLIO, sono state elaborate\r\nalcune regole, basate su una serie chiusa di articoli e aggettivi, a seconda che questi accompagnino\r\nsostantivi femminili o maschili; tali regole \u2013 per l'elaborazione delle quali sono state manualmente redatte\r\ndelle liste, comprensive di serie di articoli e aggettivi \u2013 hanno consentito di assegnare ad altri 120 sostantivi\r\nla distinzione di genere, precedentemente mancante.\r\nProssimamente si ritiene opportuno impiegare un analizzatore morfologico per l'identificazione automatica\r\ndel genere delle parole, cos\u00EC da verificarne il grado di correttezza; si intende inoltre sperimentare una terza\r\nstrategia \u2013 simile a quella applicata per il POS tagging del corpus MIDIA (Iacobini et al., 2014) \u2013, finalizzata\r\na perfezionare il lessico di TreeTagger per l'italiano contemporaneo attraverso le voci provenienti dal\r\ndizionario TLIO.\r\nPer favorire il riconoscimento di antroponimi e toponimi, inoltre, sono stati impiegati i dati ricavati dal\r\nGlossario del Libro dei debitori, creditori e ricordi che Alessandra Macinghi Strozzi tenne tra il 1453 e il\r\n1473, un testo dunque coevo all'Epistolario oggetto del presente studio e di mano della stessa Alessandra\r\nMacinghi Strozzi (Bersano, 2015-16, pp. 271-294).\r\nDal corpus annotato \u00E8 stato tratto automaticamente un formario, strutturato in ordine alfabetico; sulla base\r\ndi queste due fonti \u00E8 stata eseguita una breve quanto esaustiva analisi linguistica, di cui si riportano gli esiti\r\npi\u00F9 significativi nel paragrafo successivo.\r\n\r\n4 Esiti\r\nPer garantire la completa affidabilit\u00E0 dei risultati, \u00E8 stato necessario confrontarsi costantemente con la\r\ntrascrizione originale \u2013 specie nella fase iniziale del TAL \u2013 onde evitare errori nel processo iniziale di\r\ntrasmissione dei dati \u2013 input \u2013, e poter essere certi dell'attendibilit\u00E0 dei dati in uscita, output.\r\nIl formario tratto dal testo delle Lettere e realizzato grazie al processo sinora descritto, ha consentito a colpo\r\nd'occhio di cogliere i fenomeni e i tratti grafici, fonetici e morfologici tipici del fiorentino argenteo, cos\u00EC come\r\ndi porre in luce quelli che ne esulano, presentandosi inaspettatamente 'controcorrente'. Qui di s\u00E9guito una breve\r\nillustrazione degli esiti linguistici grafici e morfologici pi\u00F9 interessanti ricavati dal corpus annotato e dal\r\nformario.\r\nGRAFIA: si rileva un'oscillazione costante per la resa della l palatale, con il primeggiare del tipo gl, anche\r\ndinanzi a i (seppure risultino consistenti le rese grafiche lgl, rare invece quelle in li); meno incerta sembrerebbe\r\nla resa grafica della n palatale, per la quale primeggia il tipo ngn (38 occorrenze), pi\u00F9 sparute le rese ngni, gn;\r\nrarissima gni (con 3 sole occorrenze); dinanzi a i risulta nuovamente schiacciante il tipo ngn (16 occorrenze)\r\nrispetto a gn (2 sole occorrenze).\r\nAncora, \u00E8 da evidenziare l'uso della grafia k per l'occlusiva velare sorda dinanzi ad a nella voce Karisimo;\r\ntale resa grafica per l'occlusiva velare \u00E8 fatto notevole: essa risulta infatti del tutto assente nel Libro dei debitori,\r\ncreditori e ricordi di Alessandra Macinghi Strozzi (Bersano, 2015-16, p. 166). Scorrendo gli item presenti alla\r\nlettera h, spicca la scrizione etimologica homo, impiegata due volte in queste prime dieci Lettere (8 attestazioni\r\nin tutto, invece, per uomo, conforme alla grafia moderna).\r\n31\r\n\r\n\fMORFOLOGIA: notevoli sono i plurali in -(l)gli < -li; alcuni esempi: begli (-lgli) con 3 occorrenze e fanciugli\r\n(-lgli) con 2 occorrenze, senza esiti contrari.\r\nPer l'art. det. masch. sing. prevale la forma il, sebbene risulti considerevole anche la presenza della variante\r\nel (si contano in tutto, rispettivamente, 88 e 13 occorrenze), penetrata nel fiorentino intorno alla seconda met\u00E0\r\ndel sec. XIV per influsso dei dialetti occidentali e meridionali (Manni,1979). Per il plurale dell'art. det. \u00E8\r\nattestata la forma e in luogo di i, il sistema ha tuttavia etichettato tutte le e presenti (550 occorrenze) come\r\ncongiunzioni; occorrer\u00E0 senz'altro ovviare all'errore, insegnando alla macchina che e pu\u00F2 essere anche articolo\r\nse posta dinanzi a un sostantivo masch. pl., cos\u00EC che il sistema offra la possibilit\u00E0 di scegliere fra le due\r\netichette: cong. oppure art.det.m.\r\nPer l'uso di mie, tuo, suo invariabili (del tipo, mie bisongni) e mia, tua, sua pl. masch. e femm. (del tipo,\r\nmia figluoli), \u00E8 risultato maggiormente utile consultare il file contenente il testo delle Lettere etichettato\r\nautomaticamente dal sistema piuttosto che il formario, poich\u00E9 quest'ultimo \u00E8 privo dei contesti, essenziali al\r\nfine di verificare a quale sostantivo pronomi e aggettivi si accordino e dunque comprendere, come nel caso\r\npresente, l'uso dei possessivi sopraccitati.\r\nMolto agevole \u00E8 stata la ricerca delle occorrenze per i numerali duo (prevalente) e dua in luogo di due: le\r\n12 occorrenze di duo sono state erroneamente etichettate come sostantivi; le 3 occorrenze di dua sono state\r\ncorrettamente etichettate come num. (numerale) secondo il tagset elaborato da chi scrive; le 7 occorrenze di\r\ndue sono state invece etichettate come aggettivi.\r\nVERBI: \u00E8 attestata una volta soltanto la forma sete per siete; anche in questo caso, \u00E8 stato essenziale\r\nconsultare il file contenente il testo delle Lettere etichettato automaticamente dal sistema piuttosto che il\r\nformario, cos\u00EC da verificare il contesto e sciogliere ogni perplessit\u00E0 rispetto al valore semantico della parola\r\n(sete sostantivo vs. sete verbo).\r\nSchiacciante \u00E8 la presenza dei tipi ar\u00F2, arei per avr\u00F2, avrei, che non presentano esempi contrari; tutte le\r\nattestazioni, inoltre, sono state correttamente etichettate come verbi, a eccezione dei tipi aresti (4 occorrenze\r\nin tutto) e arebe (un'occorrenza) classificati erroneamente come aggettivi.\r\nNon sono presenti esempi contrari ai tipi dia e stia; il primo, che consta di 18 occorrenze in tutto, risulta\r\ndue sole volte erroneamente etichettato come sostantivo; il tipo stia occorre una volta soltanto ed etichettato\r\nregolarmente.\r\nInfine, a riprova della conformit\u00E0 della lingua di Alessandra al fiorentino argenteo, compaiono senza\r\nesempi contrari i tipi fussi per fossi e fusti per fosti.\r\nPer quel che concerne le desinenze verbali, non \u00E8 possibile in questa sede offrirne una panoramica esaustiva;\r\nbasti dire, tuttavia, che grazie al supporto informatico, l'analisi linguistica tesa a individuare le forme nonch\u00E9\r\nil numero di occorrenze per ciascuna desinenza verbale \u00E8 stata di facile esecuzione, oltre che rapida e accurata.\r\nSi segnala una desinenza atipica per la lingua di Alessandra, riscontrata nel formario e verificata nell'originale:\r\nil tipo preghiamo per la 1\u00B0 pers. pl. del pres. ind.; tale occorrenza \u00E8 un unicum in tutto il testo delle Lettere,\r\npoich\u00E9 la scrivente adotta uniformemente la desinenza -no per la 1\u00B0 pers. pl. (del tipo noi laviano), tanto nelle\r\nLettere quanto nel Libro dei debitori, creditori e ricordi di Alessandra Macinghi Strozzi (cfr. Bersano, 201516, p. 233).\r\nSi riscontrano 3 attestazioni per la forma metatetica drento (Manni, 1979) \u2013 di cui una con raddoppiamento\r\ndell'occlusiva postconsonantica: drentto \u2013 in luogo di dentro e un'attestazione per la forma metatetica\r\ngrillanda, tipiche del fiorentino argenteo. Sono inoltre attestate le forme metatetiche adrieto, adrietro (con\r\nmancata dissimilazione r-r in r-\u00F8), indrieto e, pi\u00F9 rara per questo periodo, dirieto, derivanti dall'influsso\r\nesercitato da altri dialetti toscani (Manni, 1979, pp. 167-168).\r\nSignificativa \u00E8 ancora l'attestazione di sun in luogo di su nel tipo in sun un, con l'inserzione della n eufonica\r\nin sun.1\r\nPer quel che concerne antroponimi e toponimi, ne sono stati riconosciuti in tutto 346, grazie ai dati ricavati\r\ndal Glossario del Libro di debitori, creditori e ricordi di Alessandra (Bersano, 2015-16); 127, invece, non\r\nsono stati riconosciuti secondo le etichette prestabilite ('antrop.' e 'top.'); di questi 127, tuttavia, 43 sono stati\r\nriconosciuti ed etichettati come nomi propri; 112 sono stati invece etichettati pi\u00F9 genericamente come\r\nsostantivi e solo 4, erroneamente, come verbi.\r\n\r\n1\r\n\r\nNon si hanno esempi, in queste prime dieci Lettere, per il tipo sur, che trae origine da sun per dissimilazione (in sun un > in sur un).\r\n32\r\n\r\n\f5 Conclusioni\r\nIl corpus annotato, da cui \u00E8 stato tratto il formario impiegato per l'analisi linguistica, costituisce non solo\r\nuna proficua base di lavoro per la redazione del futuro Glossario delle Lettere di Alessandra Macinghi\r\nStrozzi, ma anche un primo quanto fondamentale strumento di analisi; esso ha infatti consentito di selezionare\r\ncelermente i dati pi\u00F9 significativi che andranno opportunamente registrati nel Glossario finale, che sar\u00E0 posto\r\nin appendice alla nuova edizione delle Lettere di Alessandra Macinghi Strozzi.\r\nIl lavoro di revisione effettuato a mano \u00E8 stato senz'altro ingente; si ritiene tuttavia che il supporto\r\ninformatico sia stato essenziale al fine di porre in evidenza taluni tratti, cos\u00EC come la quantit\u00E0 di occorrenze\r\nper ogni forma riscontrata: fattore indispensabile, quest'ultimo, per chiarire quanto un fenomeno, o un\r\ntratto, fosse frequente nella lingua dell'epoca."
	},
	{
		"id": 7,
		"title": "Annotazione semantica e visualizzazione di un corpus di corrispondenze di guerra",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Beatrice Dal Bo",
			"Francesca Frontini",
			"Giancarlo Luxardo"
		],
		"body": "Introduzione\r\n\r\nIl progetto Corpus 14, iniziato in concomitanza con il centenario della Grande Guerra, nasce dalla volont\u00E0\r\ndi studiare la lingua delle persone comuni all\u2019inizio del XX secolo, ed in particolare degli scriventi peu\r\nlettr\u00E9s, che potremmo tradurre, seguendo la letteratura italiana, con semicolti (D\u2019Achille, 1994). In questo\r\ncontesto si tratta dei Poilus, soldati francesi della Grande Guerra, spesso provenienti dalle campagne e\r\nda contesti rurali, ancora in parte dialettofoni, che si confrontano spesso per la prima volta con il testo\r\nscritto. Se lo studio delle competenze linguistiche e pragmalinguistiche \u00E8 alla base della raccolta delle\r\nloro corrispondenze, tali documenti si dimostrano essere una fonte interessante anche per altre discipline,\r\ncon interessanti informazioni di carattere storico, geografico e culturale. In particolare l\u2019interesse si \u00E8\r\nfocalizzato su due ambiti:\r\n\u2022 lo studio della Grande Guerra e della sua eredit\u00E0 in termini di memoria sociale, e delle trasformazioni\r\nda essa prodotte,\r\n\u2022 l\u2019evoluzione degli usi linguistici, in particolare per quanto riguarda l\u2019influenza delle variet\u00E0 regionali\r\n(in particolare per le zone come il Sud della Francia o la Bretagna, caratterizzate da diglossia), o lo\r\nsviluppo di un socioletto comune, il cosiddetto argot des poilus .\r\nUtilizzando materiale proveniente da archivi pubblici, nonch\u00E9 documenti donati da eredi al progetto,\r\nCorpus 14 si compone ad oggi (versione 2.01) di 37 scriventi, provenienti da 11 regioni diverse, per un\r\ntotale di 1.797 lettere e circa 500.000 parole. I criteri di selezione del corpus sono stati i seguenti:\r\n\u2022 la selezione di scriventi che non hanno completato la formazione elementare,\r\n\u2022 la preferenza per le corrispondenze complete, o che per lo meno permettessero di seguire gli scriventi\r\nsu un arco temporale lungo, e che potessero dunque dare luogo a reti di corrispondenze complesse.\r\nAl momento Corpus 14 \u00E8 costituito di 11 reti di corrispondenze, raggruppate per zona geografica e\r\nnominate secondo i luoghi di origine (si veda Figura 1)\r\n\r\n34\r\n\r\n\fFigure 1: Localizzazione dei luoghi di origine dei soldati di Corpus 14.\r\nTali criteri di selezione fanno dei fondi di Corpus 14 una collezione unica nel suo genere. Tuttavia la\r\nsua realizzazione si ispira anche a progetti faro nella comunit\u00E0 delle edizioni digitali di corrispondenze,\r\nmolti dei quali dedicati agli epistolari di personaggi illustri 2, con poche eccezioni, come: \"Digitising\r\nexperiences of migration: the development of interconnected letter collections\" di Moreton e Nesi, 20132014 3. Inoltre, per la tematica il progetto pu\u00F2 essere accostato ad altri omologhi sviluppati in diversi paesi\r\neuropei in occasione del centenario della Prima Guerra Mondiale, come l\u2019italiano \"Voci della Grande\r\nGuerra\"4 ed il britannico \u201CLetters from the First World War\"5.\r\n\r\n2\r\n\r\nL\u2019edizione digitale\r\n\r\nL\u2019edizione digitale si \u00E8 avvalsa di pratiche gi\u00E0 ben stabilite, come la trascrizione diplomatica del testo,\r\nl\u2019allineamento tra i facsimile delle cartoline o delle lettere e la loro codifica precisa (con precisazioni\r\nsulla leggibilit\u00E0 del testo).\r\nPer quanto riguarda la codifica, si \u00E8 fatto appello allo standard della Text Encoding Initiative (TEI6).\r\nIn particolare le trascrizioni sono state effettuate in modo da permettere la descrizione della struttura\r\nlogica del testo, nonch\u00E9 delle caratteristiche di leggibilit\u00E0 del supporto fisico. Per ogni lettera sono state\r\nrealizzate due versioni (fedele e normalizzata all\u2019ortografia corrente).\r\nL\u2019applicazione di questo schema di annotazione XML alla tipologia testuale in oggetto \u00E8 stato facilitato\r\ndall\u2019esistenza di un gruppo di lavoro sulle corrispondenze in seno alla comunit\u00E0 TEI7. In particolare si \u00E8\r\nfatto ricorso agli elementi TEIheader, correspDesc e CorrespAction (introdotti nella versione 2.8.0 delle\r\nspecifiche TEI P5).\r\nPer quanto riguarda la distribuzione, Corpus 14 \u00E8 reso disponibile in diverse modalit\u00E0 di accesso\r\nche garantiscono la fruizione da parte di tipologie di utenti diverse. Da una parte si \u00E8 voluto fornire\r\nun\u2019interfaccia di esplorazione8 ed analisi del testo attraverso la piattaforma di testometria TXM (si\r\n1https:\/\/hdl.handle.net\/11403\/corpus14\r\n2Uno dei progetti pi\u00F9 noti in questo senso \u00E8 Mapping the Republic of Letters, http:\/\/\r\nrepublicofletters.stanford. edu\/; per una ricognizione pi\u00F9 completa di tali progetti si veda (Stadler et al.,\r\n2016).\r\n3http:\/\/lettersofmigration.blogspot.com; per ulteriori informazioni si veda (Moreton et al., 2014;\r\nMoreton, 2016)\r\n4http:\/\/www.vocidellagrandeguerra.it\/\r\n5https:\/\/www.nationalarchives.gov.uk\/education\/resources\/letters-first-worldwar-1915\/\r\n6https:\/\/www.tei-c.org\r\n7Special Interest Group della TEI sulle corrispondenze https:\/\/tei-c.org\/activities\/sig\/\r\ncorrespondence\/\r\n35\r\n8http:\/\/textometrie.univ-montp3.fr\/\r\n\r\n\fveda la Figura 2)9. Allo stesso tempo i sorgenti TEI sono scaricabili dalla piattaforma Ortolang10, che\r\ngarantisce l\u2019interoperabilit\u00E0 dei dati, la loro preservazione e la loro reperibilit\u00E0 (tramite il protocollo\r\nOAI-PMH).\r\n\r\nFigure 2: L\u2019interfaccia di esplorazione del corpus TXM.\r\n\r\n3 L\u2019indicizzazione semantica dei testi\r\nUna volta realizzata la prima versione dell\u2019edizione digitale, si \u00E8 posto il problema di arricchire e indicizzare i testi, ed in particolare di creare indici a persone, luoghi, organizzazioni citate. Tali indici, collegati\r\nai riferimenti nel testo, dovranno poi essere arricchiti e collegati con l\u2019informazione corrispondente\r\ndisponibile.\r\nL\u2019indicizzazione dei testi, che per ora esiste solo su due reti di corrispondenze (Chazeaux e Le Souli\u00E9),\r\n\u00E8 stata condotta secondo le buone pratiche della codifica in TEI, che si sono delineate anche nel contesto\r\ndi gruppi di lavoro francesi facenti riferimento al consorzio CAHIER11, come il progetto Testaments de\r\npoilus12.\r\nIn particolare le menzioni di luoghi, persone e organizzazioni sono state dapprima annotate nel testo\r\ndi ogni lettera (sia nei metadati della corrispondenza che nel corpo della lettera), utilizzando gli elementi\r\nTEI persName, placeName, orgName. Si \u00E8 inoltre scelto di annotare oltre ai nomi propri anche stringhe\r\ndi testo aventi nel contesto della lettera dei referenti univoci, usando l\u2019elemento rs. L\u2019annotazione \u00E8 stata\r\neffettuata in maniera ricorsiva, dunque un\u2019espressione come \"les cousins de Cicignan\" \u00E8 stata annotata\r\ncome una rs, contenente un placeName.\r\nIn seguito ogni menzione \u00E8 stata referenziata con l\u2019attributo ref e un codice univoco. Tale codice rinvia\r\na tre indici, file separati contenenti delle liste di persone, luoghi, organizzazioni (listPerson, listPlace,\r\nlistOrg). Per il referenziamento a DBpedia si \u00E8 utilizzato il sistema di riconoscimento automatico di entit\u00E0\r\nnominate REDEN Online (R\u00E9solution et D\u00E9sambigu\u00EFsation d\u2019Entit\u00E9s Nomm\u00E9es) (Frontini et al., 2016),\r\ncon postcorrezione manuale.\r\nInfine, tali liste sono state dove possibile arricchite con informazioni addizionali in nostro possesso\r\n(come le date e i luoghi di nascita e morte delle persone, scriventi o solo menzionate, il loro grado di\r\n9TXM \u00E8 uno strumento per l\u2019esplorazione e l\u2019analisi statistica di corpora testuali, sviluppato dall\u2019ENS di Lione. Permette\r\ntra le altre cose l\u2019import di testi annotati in TEI. Si veda http:\/\/textometrie.ens-lyon.fr.\r\n10ORTOLANG, Outils et Ressources pour un Traitement Optimis\u00E9 de la LANGue \u00E8 la piattaforma francese per la pubblicazione delle risorse linguistiche, ora integrata all\u2019infrastruttura CLARIN ERIC. https:\/\/www.ortolang.fr\/\r\n11CAHIER, Corpus d\u2019Autueur pour les Humanit\u00E9s Num\u00E9riques, \u00E8 un consorzio di progetti affiliati all\u2019infrastruttura HumaNum, che si occupa di edizioni digitali principalmente in TEI. Si veda https:\/\/cahier.hypotheses.org\/\r\n12https:\/\/testaments-de-poilus.huma-num.fr\/\r\n\r\n36\r\n\r\n\fparentela, ecc.). Per quanto riguarda i luoghi si \u00E8 fatto ricorso alla georeferenziazione e all\u2019aggiunta di\r\nlink al database geografico esterno GeoNames, oltre a quanto gi\u00E0 referenziato su DBpedia. In alcuni casi,\r\ntoponimi non presenti nelle basi sono stati individuati e localizzati. In alcuni casi, toponimi non presenti\r\nnelle basi sono stati individuati e localizzati.\r\n\r\n4\r\n\r\nVisualizzazione\r\n\r\nAttualmente in corso \u00E8 lo sviluppo di una piattaforma di visualizzazione, che permetter\u00E0 di esplorare le\r\ncorrispondenze in maniera geolocalizzata13. Come si pu\u00F2 vedere dalla Figura 3, l\u2019interfaccia permette di\r\nselezionare gli scambi epistolari di una stessa rete familiare per data, proiettando sulla carta ad esempio\r\nla lettera di un soldato e la risposta della moglie. Nella visualizzazione i segnaposto indicano il luogo\r\ndi invio della lettera, mentre le bandierine indicano i luoghi citati nella lettera. La visualizzazione \u00E8\r\nrealizzata in modo da sfruttare al massimo lo standard TEI recuperando i placeName con interrogazioni\r\nbasate su XQuery (sia all\u2019interno dei metadati correspDesc che nel corpo della lettera) e utilizzando la\r\ngeocodifica degli indici. In questo modo, una volta terminata, la piattaforma potr\u00E0 essere riutilizzata\r\ncome base per altri progetti con lo stesso formato di annotazione14.\r\n\r\nFigure 3: L\u2019interfaccia di visualizzazione e geolocalizzazione delle corrispondenze.\r\n\r\n5 Analisi\r\nNumerose analisi sono state condotte su Corpus 14, si cita in particolare il volume collettivo curato da\r\nAgn\u00E8s Steuckardt (Steuckardt, 2015a), nel quale sono analizzati vari aspetti linguistici di queste corrispondenze, fra cui la punteggiatura (Steuckardt, 2015b), l\u2019ortografia (Pellat, 2015), il lessico (Luxardo,\r\n2015) e la lingua regionale (G\u00E9a, 2015). Ricordiamo inoltre altri studi riguardanti aspetti morfosintattici\r\n(Steuckardt and Dal Bo, 2018) o discorsivi (Dal Bo and Wionet, 2018). Una tesi di dottorato \u00E8 attualmente\r\nin corso di completamento (Dal Bo, 2019)).\r\n13La rappresentazione delle reti di corrispondenze attraverso visualizzazioni \u00E8 una componente tipica di questo tipo di\r\nprogetti. Si vedano ad esempio (O\u2019Leary and Moreton, 2017); Visual Correspondence, http:\/\/\r\nwww.correspondence.ie; Mapping the Republic of Letters, http:\/\/republicofletters.stanford.edu\/;\r\nEarly Modern Letters Online, http:\/\/emlo.bodleian.ox.ac.uk\/home; Clavius on the Web, http:\/\/\r\nclaviusontheweb.it\/.\r\n14Il progetto di interfaccia \u00E8 stato realizzato da studenti del corso di laurea specialistica in informatica dell\u2019Universit\u00E0 di\r\nGenova, sotto la supervisione della prof. Marina Ribaudo.\r\n\r\n37\r\n\r\n\fPer quanto riguarda gli aspetti spaziali, l\u2019analisi dei luoghi citati nelle corrispondenze dei soldati ha\r\npermesso di mettere in evidenza il fatto che questi evochino nelle lettere in maniera prevalente luoghi\r\nlegati alla loro vita familiare, alla casa e agli affetti, e molto meno luoghi legati alla guerra e al fronte\r\n(come gi\u00E0 messo in evidenza da (Dal Bo and Wionet, 2018; Gibelli, 2016)). La proiezione su una carta\r\ngeografica delle informazioni geografiche e temporali delle corrispondenze permette inoltre di seguire\r\ngli spostamenti dei soldati al fronte e delle donne rimaste all\u2019interno del Paese. Gli spostamenti di\r\nquest\u2019ultime, pi\u00F9 raramente studiati, potranno essere inoltre paragonati a quelli delle donne appartenenti\r\na classi sociali superiori durante lo stesso periodo storico."
	},
	{
		"id": 8,
		"title": "The use of parallel corpora for a contrastive (Russian-Italian) description of resource markers: new instruments compared to traditional lexicography",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Anna Bonola",
			"Valentina Noseda"
		],
		"body": "1 Parallel corpora2 and linguistic research\r\nDespite the skepticism of early corpus linguists, who refused to use translated texts to draw conclusions about\r\nthe functioning of a language3, nowadays the scientific community has produced countless works that\r\ndemonstrate how the use of parallel corpora (PC) can have a greater impact in several areas4:\r\n1) in linguistic research (contrastive, but not only) PC provide a rather solid empirical basis for comparing two\r\nor more languages (Johansson, 2003); moreover, the \u2018translation method\u2019 allows to deepen the semantics and\r\nfunctions of a given linguistic structure (No\u00EBl, 2003)5;\r\n2) in Translation Studies, since Baker\u2019s work (1993), PC have become a fundamental tool for the study of\r\ntranslated texts, treated as a linguistic variety in its own right, worthy of analysis;\r\n3) finally, PC have allowed computational linguistics to make progress in the programming of translation\r\nsoftware and, more generally, they have favored the development of NLP (Calzolari and Lenci, 2004).\r\nHowever, these 3 points must be integrated with a further aspect: PC are in fact very useful for the heuristic\r\nphase of a contrastive analysis on polyfunctional linguistic elements that are strongly influenced by the context.\r\n\r\n1 This paper is the result of a research in which the authors have equally contributed; however, Valentina Noseda is the author of\r\n\r\nsections 1, 1.1, 3.1 and 4, Anna Bonola of sections 2, 3 and 3.2.\r\nA parallel corpus consists of texts in a language A, aligned (usually at the sentence level) with the corresponding translations in a\r\nlanguage B. If bidirectional, the corpus will also contain language B originals alongside translations in language A.\r\n3\r\nThe reasons for this skepticism can be traced to the generally recognized existence of the so-called universals of translations, to the\r\ninfluence that the source text often exerts on translators and their final product, and to the freedom with which a translator can interpret\r\nthe source text while transferring its contents into a target text (Olohan, 2004; von Waldenfels, 2012; Zanettin, 2012).\r\n4\r\nAnother field where parallel corpora have proved to be useful is the teaching of second languages, since bilingual corpora, first of all,\r\nallow students to grasp equivalences and differences between L1 and L2, thus acquiring greater awareness of the structures of a studied\r\nlanguage (Granger, 2003), and secondly help them learn unknown words (Bernardini, 2004).\r\n5\r\nNo\u00EBl (2003) was among the first to promote the use of PC not only for contrastive analysis but also to deepen the semantic\r\ninvestigation of one of the two aligned languages. In Russian studies, many works have been carried out following No\u00EBl\u2019s example,\r\nincluding (Zaliznjak, 2015; Levontina and Denissova, 2017; Zaliznjak, Denissova and Mikeljan, 2018).\r\n2\r\n\r\n39\r\n\r\n\fIn this paper we will show an example of such further use of PC, applying it to the contrastive study of two\r\nDMs.\r\n\r\n1.1 The Italian-Russian parallel corpus of the Russian National Corpus\r\nIn Russian studies, the active use of language corpora fell slightly behind the spread of Corpus Linguistics\r\naround the world and it has been directly linked to the creation of the Russian National Corpus (Nacional\u2019nyj\r\nkorpus russkogo jazyka, from now on: NKRJa) in 2004. With its 500 million words, its numerous specialized\r\nsub-corpora and a highly sophisticated search engine, NKRJa has quickly become an essential tool for the\r\nstudy of Russian6.\r\nIn 2005 NKRJa already presented a section dedicated to PC, although for the Russian-Italian pair there was\r\nonly a small pilot corpus, not very balanced and almost useless for any type of research. A first expanded\r\nversion \u2013 resulting from the collaboration between Catholic University of Milan (Universit\u00E0 Cattolica del Sacro\r\nCuore di Milano), the University of Bologna (Universit\u00E0 di Bologna) and the Russian Language Institute in\r\nMoscow (Institut Russkogo Jazyka imeni V.V. Vinogradova) \u2013 became available in 2015. Now the ItalianRussian PC (it-ru PC) exceeds 4 million words and has become a sufficiently large tool allowing to conduct\r\nscientifically valid and statistically relevant research.\r\nThe corpus, compiled according to precise criteria, has the following features7: i) is bidirectional: it contains\r\nRussian originals translated into Italian and vice versa; ii) it includes several literary works and essays (from\r\n19th, 20th and 21st centuries) as well as some newspaper articles written in the last decade (and this variety\r\ndistinguishes it from other parallel corpora in NKRJa); iii) like all the other sections of NKRJa, it has three\r\ntypes of annotation: metatextual8, morphological and semantic.\r\n\r\n2. The use of parallel corpora for the analysis of discourse markers\r\nA field in which parallel corpus linguistics seems to have great potential, especially if compared to more\r\ntraditional research methods, is that of discourse markers (DMs), i.e. multi-functional linguistic elements of\r\nvarious origins (adverbs, verbs, particles, etc.) that can operate at a textual, discursive, interactive, modal,\r\nsocial and contextual level9. DMs have come to the attention of researchers especially during the eighties, as\r\na result of a new pragmatic direction in language studies, and since then considerable progress has been made\r\nin this area10. However, the use of electronic corpora in the description of DMs is still in its initial phase.\r\nThe difficulty of producing a fully automatic tool for the analysis of DM is due to the fact that these are\r\nprocedural and multifunctional elements expressing pragmatic and discursive functions which are clarified\r\nonly in relation to the context or to the communicative situation, whose automatic annotation is still\r\ndeveloping11. Moreover, syntactically, DMs are optional (can be removed), relatively mobile in the utterance\r\nand come from diverse grammatical classes, on which depends their syntactical integration (Crible, 1917: 106).\r\nTherefore, the discussion on the automatic processing of DMs is currently still focused on the \u201Cneed for\r\nfunctional paradigmatic studies that include every kind of DMs, possibly in multifunctional approaches for\r\nbetter generalization\u201D in order to \u201Cprovide a solid basis for comparative or contrastive analysis between\r\nlanguages and frameworks\u201D (Crible, 2017: 100).\r\nSome recent experiments for the identification and annotation of DMs are worth noting, like for example\r\n(Bolly et al., 2017), even though the empirical method they present is still matching manual and automatic\r\nannotation. For a fully automatic cross-linguistic analysis of DMs, which takes into account not only syntaxis\r\n6\r\nA detailed description of the corpus and its sub-corpora (including all the information about the available annotations) can be\r\nfound on the corpus website (www.ruscorpora.ru). See also (Aa.Vv 2005) and (Plungjan 2009).\r\n7\r\nFor a description of the Russian-Italian PC and its design criteria, see (Noseda, 2018).\r\n8\r\nIt provides various pieces of information about a text: author, date, genre, number of words, etc.\r\n9\r\nThis list of functional areas summarizes the results of the debate on the classification of DMs \u2013 for a review see (Schiffrin 2001;\r\nFrediani and Sans\u00F2 2017); for the discussion in Italy see (Bazzanella 2001: 41-42) \u2013 although we avoid entering into the discussion on\r\nlabels, whose boundaries are subject to change and have a graduated character (Molinelli, 2018: 277).\r\n10\r\n\r\nFor a review of the features of the DMs highlighted by the research, up to the most recent studies see (Frediani and Sans\u00F2, 2017).\r\nAs far as Russian is concerned, in the NKRJa only the multimodal sub-corpus (4 million words) is pragmatically annotated: the\r\nsearch engine can be interrogated on the basis of specific contexts (at the doctor\u2019s, at the restaurant, etc..) and linguistic acts (complaint,\r\nprohibition, apology, etc..).\r\nAmong Italian corpora, we can name the AVIP corpus (http:\/\/www.parlaritaliano.it\/index.php\/it\/corporadi-parlato\/673-corpus-avip-api) and PraTID (http:\/\/www.parlaritaliano.it\/index.php\/it\/\r\nprogetti\/35-pratid-un-sistema-di-annotazione-pragmatica-di-dialoghi-task-oriented), which\r\nare fully or partially annotated at a pragmatic level.\r\n11\r\n\r\n40\r\n\r\n\f(Cinque 1999) but also semantics and pragmatics, the annotation of PC, according to Crible (2017: 107),\r\nshould consider the following levels: ideational (the relation between real-world events), rhetorical (the\r\nrelation between epistemic and speech-act events), sequential (the shaping of discourse segments) and\r\ninterpersonal (speaker-hearer relationship). Therefore, the large amount of data that can be consulted today\r\nthrough electronic corpora, as far as DMs are concerned, has yet to find a way to be processed employing a\r\ntargeted annotation. More specifically, concerning the automatic analysis of DMs in Russian, some\r\nsupracorpora databases (SCDB), resulting from the processing of some bilingual parallel corpora within\r\nNKRJa, have recently been developed (Zatsman, Inkova, Kruzhkov and Popkova, 2016). Their aim is to\r\nincrease the functionality of parallel corpora for goal-oriented cross-linguistic research on various linguistic\r\nelements. For the moment, there is one SCDB for French-Russian contrastive analysis of verbs (Zatsman and\r\nBuntman, 2015) and one for textual connectors (In\u2019kova, 2018).\r\nThe it-ru PC used for our research does not have an annotation that takes into account pragmatic and\r\ndiscursive parameters; moreover, we still do not have Russian-Italian SCDB for such particular linguistics\r\nelements as DMs. Therefore, for the moment, we tested the effectiveness of it-ru PC as a tool for linguistic\r\nanalysis in the heuristic phase, as it provides a significant number of examples in a short time, allowing\r\nresearchers to clarify and adjust their intuition regarding a given research question (corpus-based approach) or\r\nto formulate new hypotheses (corpus-driven approach) (Mikhailov and Cooper, 2016: 15-16). If this is\r\ngenerally helpful, it is even more useful for DMs, i.e. linguistic elements that both in Italian and Russian have\r\nbeen developing textual, discursive, modal and pragmatic functions that make them multifunctional and often\r\nlanguage-specific, but frequently still lacking an adequate description (Proietti, 2000: 227) (Benigni and\r\nNuzzo, 2019: 152\u2013154)12, especially if we consider current lexicography.\r\nAs we will show in this paper, the effectiveness of our PC (in its current form) for the heuristic phase of a\r\ncontrastive corpus-driven or corpus-based approach \u2013 well described in (Crible, 2017) \u2013 lies in the fact that:\r\n1) it makes the multi-functionality of DMs easily emerge, clarifying it by contrast with another language, as\r\ndescription through linguistic comparison, \u201Crend le dispositif d\u2019analyse plus puissant: elle peut sugg\u00E9rer, d\u2019une\r\npart, de nouvelles hypoth\u00E8ses pour les faits constat\u00E9s; elle peut, d\u2019autre part, inciter \u00E0 r\u00E9examiner des\r\nhypoth\u00E8ses existantes\u201D (Lamiroy, 1984: 224);\r\n2) if a given DM presents recurrent functional equivalences in the language compared, it is possible to\r\ndetermine if in the L2 there are DMs associated with specific functions as well;\r\n3) finally, analyzing quantitative data (even with a relatively small number of examples), we can see the\r\npreferential strategies of each language to express certain functions, and in some cases, as illustrated in section\r\n3, it is also possible to make some assumptions about possible structural differences between the two compared\r\nlanguages.\r\nIn section 3 we will exemplify the abovementioned points by analyzing Italian allora and Russian ved\u2019,\r\ntwo of the most frequently used DMs in the respective languages.\r\n\r\n3. The DMs allora and ved\u2019\r\nConcerning the pragmatic-textual multi-functionality of DMs (section 2, point 1), both Russian and Italian\r\nlexicographic descriptions are particularly poor and often do not distinguish contextual elements from the\r\nfunctional core meaning of the DM under investigation.\r\nFor example, as far as allora is concerned, DISC 2008, among the several dictionaries that we have\r\nconsulted, is the only one providing some clear categories about the discursive use of this word, which can be\r\na temporal adverb, a conjunction or an actual DM. According to DISC, allora, as DM, refers to shared\r\nknowledge in dialogues (Allora?) or in exhortative, imperative and interrogative sentences (e allora sei\r\npronto?). This brief description, although correct, is rather uncomplete and it uses contextual categories, such\r\nas sentence or text type, without specifying how their role interacts with the functionality of the DM.\r\nAs for traditional Russian lexicography, the description of DMs is not better: in both traditional (U\u0161akov,\r\n1935) and recent dictionaries (Kuznecov, 2000; Efremova, 2001; O\u017Eegov and \u0160vedova 2003) the particleconjunction ved\u2019, whose various meanings are summarized in (Morozov, 2014: 259), is defined as follows: 1)\r\nconjunction in those sentences that indicate the cause or the motivation of a previous statement; 2) concessive\r\nconjunction; 3) it expresses a hypothetical or possible state; 4) particle that underlines or contradicts what has\r\n\r\n12\r\n\r\nIn particular, in the article, dedicated to the use of corpora for teaching DMs, the authors underline how even in this field has emerged\r\nso far \u201Ca lack of contextualization of pragmatic phenomena and a shortage of natural conversational models, exemplifying the real use\r\nof language\u201D (Benigni and Nuzzo, 2019: 154).\r\n41\r\n\r\n\fbeen said; 5) it emphasizes adversative conjunctions such as no [but]13, \u0430 [but, and], da\u017Ee [even]; 6) in\r\nconditional clauses it means togda [then], v takom slu\u010Dae [in this case]; 7) it indicates a statement from which\r\na conclusion will be drawn; 8) it gives emotional color to spoken language; 9) in questions and exclamations\r\nit means neu\u017Eeli ne?, razve ne [really\/indeed].\r\nSuch a functional heterogeneity, as well as the variety of aspects involved, shows that the core meaning of\r\nved\u2019 provided by lexicographic descriptions is quite vague and even confused. Moreover, as in the case of\r\nallora, the problem of distinguishing the function of connector from that of DM remains.\r\nThanks to our corpus-driven analysis in the it-ru PC, a much more precise and complex description has\r\nsurfaced.\r\n\r\n3.1 Allora\r\nOur analysis took into account the first 200 occurrences of allora automatically extracted from the corpus (100\r\nin Italian originals and 100 in texts translated from Russian)14.\r\nFirstly, we considered Russian DMs corresponding to allora both in Russian translations and in Russian\r\noriginal texts; secondly, we examined their different functions. Our goal was, on the one hand, to clarify the\r\nmulti-functionality of allora by contrast with Russian, and on the other to compare our results with the\r\ndescriptions of this DM provided by traditional lexicography and some linguistic research works. This allowed\r\nus to verify if our PC, even in its current form, can be useful to integrate these resources towards a more precise\r\ndescription.\r\nAs Allora is highly polysemic (it combines temporal, logical and pragmatic values) and multifunctional (it\r\ncan be an adverb, a connector or a DM), we found out that it does not have full functional equivalents in\r\nRussian; in fact, quite frequently (25 occurrences) allora does not show any equivalent at all: either it is omitted\r\nin the Russian translation or it is inserted in the Italian translation without a corresponding DM in the Russian\r\noriginal; its adverbial and connective values are rather carried out by different and thus highly specialized\r\nmarkers with metatextual\/metanarrative, interactive and pragmatic functions (this distinction is provided in\r\nBazzanella, 2001) (see section 2, point 2). More precisely:\r\n\u2013 togda [then] mostly conveys adverbial and connective meanings;\r\n\u2013 zna\u010Dit and vychodit [so] (connectives) express conclusion by inference or deduction;\r\n\u2013 (i) tut [and then] often expresses temporal correlation and is combined with the metanarrative function typical\r\nof allora, that marks the different phases of narration.\r\n\u2013 Tak\/itak [so] add two pragmatic functions to the basic consequential meaning: i) interactional function\r\n(beginning or end of the interaction, and turn-taking in a conversation); ii) metanarrative function (restarting\r\nthe narration or marking the narrative phases);\r\n\u2013 Nu and \u017Ee [well] never have temporal meaning, but they carry out pragmatic functions, emphasizing the\r\ninteractional process as well as turn-taking. Nu and \u017Ee do not seem to express any consequential component.\r\nThese results are summarized in Figure 1, which shows, in addition, quantitative data. In this regard we\r\nmust point out that we had to leave out some examples due to translation errors, omissions etc.; as a result, the\r\nnumber of examples that we could actually take into account amounts to 164 (including the 25 cases of zero\r\ncorrespondence which are not showed in Figure 1):\r\n\r\n13\r\n\r\nThe translations that we provide in brackets are approximate since even in English there is never a single equivalent for these words.\r\nThis bidirectional approach allows determining if the behavior of a given linguistic unit differs according to text type (i.e. original\r\nversions vs translations). In this sense, a bidirectional parallel corpus has proved to be an extremely helpful resource.\r\n14\r\n\r\n42\r\n\r\n\fFigure 1: Data resulting from a corpus-driven analysis of DM allora\r\n\r\n3.2 Ved\u2019\r\nBiagini and Bonola (2019, in press) have recently applied to ved\u2019 a similar heuristic method of investigation\r\nusing the it-ru PC. They examined the first 100 occurrences automatically extracted from the corpus (both in\r\noriginals and translated texts). In this case, the analysis was carried out considering first of all the contexts of\r\noccurrence. The primary goal was to identify the core meaning of ved\u2019, in order to distinguish it from other\r\nperipheral values. The results of the analysis are summarized in Figure 2, followed by a brief explanation:\r\nFigure 2: Data resulting from a corpus-driven analysis of DM ved\u2019\r\n\r\n43\r\n\r\n\f- unlike what is stated in dictionaries, Biagini and Bonola would not attribute to ved\u2019 the encoding function of\r\nclause linking, even though in our corpus the group of contexts which exhibit an interphrastic relation is the\r\nsecond in terms of entity: ved\u2019 in fact occurs in sentences that express very different kinds of relations (such\r\nas adversative and causal), which, nevertheless, in almost all the examples are codified by conjunctions or are\r\ninferable from the propositional content of the statements, instead of directly depending on ved\u2019.\r\n- secondly, in more than 50% of the analyzed contexts, ved\u2019 occurs in the presence of two sentences, the second\r\nof which expresses a \u2018reason to say\u2019 (i.e. a reason why something was previously said) instead of a mere causal\r\nrelation. Strengthening the illocutionary force of the second sentence by referring to a shared background that\r\nthe speaker wants to recall, ved\u2019 provides the listener with useful hints to overcome the inferential process. In\r\nthese contexts, ved\u2019 realizes the macro-functions of expressing textual cohesion (discourse marker), social\r\ncohesion and personal attitude (pragmatic marker).\r\n- thirdly, the semantic core of ved\u2019 (if used when referring to a shared knowledge) persists in particular when\r\nit functions as a pragmatic marker that manages social cohesion and modulates illocutionary force (in questions\r\nand \u2018reasons to say\u2019) or as an element which favors the inferential process (in \u2018reasons to say\u2019). When, on the\r\nother hand, it carries out the role of discourse marker favoring textual cohesion, it still refers to shared\r\nknowledge, but apart from this, nothing else is presupposed.\r\nIf the results described above exemplify points 1 and 2 of section 2, concerning the multi-functionality of\r\nallora and ved\u2019 or the specialization of their equivalents in Russian (for allora) and in Italian (for ved\u2019), for\r\npoint 3 \u2013 i.e. the preferential strategies of Russian and Italian regarding the expression of certain discursive\r\nfunctions \u2013 it was very useful to analyze the asymmetries emerged from the it-ru PC, i.e. the cases of omission\r\nor addition of allora and ved\u2019 in target texts compared to the originals. This analysis showed that both DMs\r\nare sometimes omitted in translation or they are added in the absence of a correspondent marker in the original.\r\nIn addition to this, neither of the two DMs has a perfect functional equivalent in the target language, but they\r\ndistribute their many functions on partial equivalents. This demonstrates a certain language-specificity of both\r\nDMs (on the relationship between the number of translation variants and language-specificity of DMs see\r\nInkova, 2017).\r\nMoreover, as far as allora is concerned, we observed that using this marker we tend to give a logical\r\n(consequential) interpretation to the temporal relationship between two circumstances: \u201Cin that moment\/that\r\ncircumstance\u201D can, in fact, be interpreted through allora also as a consequential relationship. Here we can see\r\nthe preference of Italian for logical cohesion in the text. Russian, on the contrary, often simplifies this temporalconsequential relation in a strictly temporal sense, translating allora with temporal adverbs or adverbial\r\nphrases (on this difference between Russian and Italian, a consequence of Latin syntax, see Govorucho, 2007).\r\n\r\n4 Conclusions: a hypothesis on the differences between Italian and Russian regarding the use\r\nof DMs\r\nOur conclusions regard, firstly, the evaluation of the tool we adopted for our corpus-based contrastive analysis\r\nof DMs, i.e. the Russian-Italian bidirectional parallel Corpus of NKRJa. At the moment we can say that this\r\ncorpus is suitable for the heuristic phase, but it does not yet provide sufficient data to draw general conclusions\r\nfrom a systemic or typological point of view. Any assumption about possible structural differences related to\r\nthe use of DMs in Russian and Italian should be supported by a larger number of data. Nevertheless, a heuristic\r\nanalysis allowed us to formulate some preliminary hypotheses.\r\nMore precisely, we were able to register the tendency of Russian to express purely pragmatic functions,\r\nboth cognitive and interactive15, through an ancient group of primitive particles, such as ved\u2019, nu, \u017Ee, which\r\nare more specialized if compared to DMs of more recent origin, such as togda, which maintains an adverbial\r\nand connective function as well. On the contrary, Italian tends to form multifunctional DMs of verbal or\r\nadverbial origin which combine their pragmatic features with the task of guaranteeing logical cohesion in the\r\ntext and interphrastic relations.\r\nThis is a broad \u2013 and according to us new \u2013 observation on a structural difference between the two\r\nlanguages, which deserves to be further explored by investigating \u2013 both from a diachronic and a synchronic\r\npoint of view \u2013 more Russian and Italian DMs. All this demonstrates how a heuristic corpus-driven study\r\nallows, on the one hand, to quickly obtain linguistic descriptions on the functioning of DMs that are more\r\nprecise than those provided by traditional tools and, on the other, to open up new hypotheses for wide-ranging\r\nresearch.\r\n15"
	},
	{
		"id": 9,
		"title": "PhiloEditor: simplified HTML markup for interpretative pathways over literary collections",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Claudia Bonsi",
			"Angelo Di Iorio",
			"Paola Italia",
			"Francesca Tomasi",
			"Fabio Vitali",
			"Ersilia Russo"
		],
		"body": "Introduction\r\n\r\nDescriptive markup has been introduced three decades ago as the main mechanism to provide structured\r\nannotations over arbitrarily organized text documents. The rigidity of SGML and XML-based languages\r\nand the complexity of their editing tools made HTML an attractive markup language for text\r\nrepresentations, as well as web pages, academic articles, structured documents and literary collections.\r\nHowever, the syntax of HTML is too flexible and its vocabulary too rich and unspecific. So the idea of\r\nchanneling and restricting this richness into smaller and simplified subsets of the language was tried:\r\nScholarly HTML, RASH and ADF, for instance, are simple and rigid subsets of HTML5 provided with\r\nsimilar approaches towards a restricted vocabulary and syntax. PhiloEditor\u00AE1 started as a web-based\r\ntool that identifies variants and versions of literary texts using such descriptive richness as a mechanism\r\nfor creating and perusing complex, multiform and coexisting interpretative pathways over literary\r\ncollections. It has been used for three years now at the Departments of Italian Studies of the University\r\nof Rome and at the University of Bologna, in order to study and characterize the differences between\r\nearlier and later versions of the same literary texts. PhiloEditor\u00AE was originally meant to provide a\r\nvisualization of the differences between two versions of the same text, as a display of the output of a\r\ndiff tool over two text documents. To ease the development of this tool, a simplified form of HTML5\r\nimmediately displayable in a browser window was used. Over time, in addition to the mere display of\r\ndifferences, a few additional interpretative tools were added to the interface to provide for highlighting\r\nof some semantically relevant textual features, rendered with different colors of the text. Finally, a few\r\n1\r\n\r\nA demo is available at http:\/\/site1705.web.cs.unibo.it\/phed6.2\/#.\r\n47\r\n\r\n\fextractors of data were created to collect and graphically represent some statistics of features in the\r\nannotations and in the very text, exploiting the regularities in the controlled use of simplified HTML\r\nmarkup. The purpose of the tool, therefore, has shifted from the merely philological characterization of\r\nliterary texts (i.e., to represent their temporal evolution) to a critical and hermeneutic approach to them,\r\nby providing support for custom, discipline-specific pathways over complex, multidimensional and\r\ntemporally complex collections of (literary) documents. At the same time, the minimality of the markup\r\nlanguage of the tool (relying exclusively on a well-behaved and extremely simplified subset of HTML)\r\nallows an extreme homogeneity and control over markup without the necessity of giving up in richness\r\nof features. Texts annotated through this simple format are extremely regular and can be exported to a\r\nwell-known literary XML vocabulary for literary texts such as XML\/TEI.\r\n\r\n2\r\n\r\nMarkup patterns in simplified HTML\r\n\r\nThe pattern theory of documents (Di Iorio et al., 2014) has been created to provide a generative approach\r\nto useful patterns in document structures. The advantages in restricting elements to patterns are to\r\nachieve orthogonality, as each pattern has a specific goal, and fits a specific context with no overlap\r\nwith other patterns, and assemblability, as it is clearly associated to precise nesting rules. By limiting\r\nthe possible choices, patterns prevent the spread of arbitrary structures and allow authors to create\r\nunambiguous, manageable and well-structured markup languages and, consequently, documents which\r\ndrive reusability and homogeneity (Di Iorio et al., 2012; Di Iorio et al., 2013).\r\nIn our theory of patterns applied to PhiloEditor\u00AE, elements are divided into four categories\r\naccording to two axes: text\/no-text content and element\/no element content. This creates a basic\r\nscaffolding of four categories: marker is the class of elements that can contain neither text nor other\r\nelements (i.e., empty elements), flats are the elements that can only contain text, buckets are the elements\r\nthat can only contain other elements, and mixed is the class of elements that can contain both text and\r\nother elements.\r\nCannot contain text\r\n\r\nCan contain text\r\n\r\nCannot contain\r\nelements\r\n\r\nEmpty element (Marker)\r\n\r\nPlain text element (Flat)\r\n\r\nCan contain\r\nelements\r\n\r\nPlain structural element\r\n(Bucket)\r\n\r\nBoth elements and text\r\n(Mixed)\r\n\r\nFigure 1. Main categories of our pattern theory of documents.\r\nWe now switch to consider where these elements can be used, that is the context model that can\r\naccommodate them. Since only two categories can contain other elements, we have exactly eight\r\npatterns of elements.\r\nMarker\r\n\r\nFlat\r\n\r\nBucket\r\n\r\nMixed\r\n\r\nMarker\r\n\r\n-\r\n\r\n-\r\n\r\n-\r\n\r\n-\r\n\r\nFlat\r\n\r\n-\r\n\r\n-\r\n\r\n-\r\n\r\n-\r\n\r\nBucket\r\n\r\nMeta\r\n\r\nField\r\n\r\nContainer\r\n\r\nBlock\r\n\r\nMixed\r\n\r\nMilestone\r\n\r\nAtom\r\n\r\nPopup\r\n\r\nInline\r\n\r\nFigure 2. The basic set of patterns.\r\n\r\n\u2022\r\n\r\nAccording to this model:\r\nA meta element is a marker (i.e., an empty element) placed within a bucket, i.e., never close to text\r\nfragments. Usually their real position is not relevant, and only its existence has some relevance: it is the\r\n\r\n48\r\n\r\n\f\u2022\r\n\r\n\u2022\r\n\r\n\u2022\r\n\r\n\u2022\r\n\r\n\u2022\r\n\r\n\u2022\r\n\r\n\u2022\r\n\r\nperfect candidate for metadata structures, hence its name. Element <meta> in HTML is clearly and\r\nrightly a meta element.\r\nA milestone is a marker (i.e., an empty element) placed within a mixed, i.e., close to the text fragments.\r\nIts location in the document is often its most important contribution, i.e. it represents a special position\r\nwithin of the document, hence its name. Elements <br> and <img> in HTML are milestones.\r\nA field is a flat element (text allowed, elements not allowed) placed within a bucket, i.e., never close to\r\ntext fragments. It is just a container of data whose position is not particularly relevant. Its proper use is\r\nas a field of a record, hence the name. Element <title> in HTML is a field.\r\nAn atom is a flat element (text allowed, elements not allowed) placed within a mixed, i.e., close to text\r\nfragments. Its location in the document is often its most important contribution. Since it allows no\r\nelement content, it is an atomic container of text, hence its name.\r\nA container is a bucket (text not allowed, elements allowed) placed within a bucket. It is a container of\r\nelements and it provides the fundamental scaffolding of the document structure. Containers are arguably\r\nthe most important pattern of the eighth. For instance, elements <html> and <table> in HTML are\r\ncontainers, but there are many others.\r\nA popup is a bucket (text not allowed, elements allowed) placed within a mixed element. It provides an\r\ninterruption of the usual flow of inline and text elements within a mixed element, creating a separate\r\ncontext for buckets such as containers. It acts as a frontier element between mixed and buckets, and lets\r\nthe contained elements jump out of the constraints of mixed elements, hence its name. Element <figure>\r\nin HTML is a popup.\r\nA block is a mixed element (text and elements are both allowed) placed within a bucket. It is the earliest\r\nelement in a hierarchy of containment of buckets that can contain text, and therefore acts as a frontier\r\nelement between buckets and mixed. The most important type of blocks is the paragraph, i.e. a basic,\r\nindependent container of text and inline elements of a document, a block of text vertically separated\r\nfrom the others of the same type, hence its name. Element <p> in HTML is therefore a block.\r\nAn inline is a mixed (text and elements are both allowed) placed within a mixed. An inline element\r\ncharacterizes text fragments according to several criteria, such as style, typography, semantics, etc. Since\r\nit does not break the paragraph structure, it stays on the same line as the text, hence its name. Elements\r\n<b> or <a> in HTML are inlines, as well as many others.\r\nThese eight patterns represent by construction the complete set of possible element types, and no others\r\ncan exist without loosen the rules. However we have identified some specific regularities within\r\ncontainers, which are very important and complex elements. Rather than creating independent patterns,\r\nwe identified sub-patterns of containers, inheriting all their characteristics and adding others: record,\r\ntable, hierarchical container (hcontainer).\r\nThis theory shows that although HTML does not recognize nor use patterns in a systematic way, it\r\nis possible to restrict HTML by selecting a reasonable subset of elements expressive enough to capture\r\nthe typical components of domain-specific documents while being also well-designed, easy to reuse and\r\nrobust.\r\n\r\n3\r\n\r\nSimplified HTML for scholarly, technical and literary texts\r\n\r\nThe discussion to adopt HTML as the markup language of choice even for specialized domaindependent documents started in the academic publishing domain: through the unification of data formats\r\nwe can homogenize the process of drafting, submitting and publishing documents. HTML easily\r\nsupports the embedding of semantic annotations to improve the sharing of communication results,\r\nthanks to already existing W3C standards such as RDFa (Sporny, 2015) or JSON-LD (Sporny et al.,\r\n2014), so that discoverability, interactivity, openness and usability of the scientific works are increased\r\n(Shotton et al., 2009). The HTML language has been widely used as a full-fledged markup language to\r\nencode texts, not only plain Web pages but also academic articles, technical documentation, literary\r\nartefacts and data reports. The ability to embed semantics within HTML pages, for instance by using\r\nRDFa (Herman et al., 2015), is a key factor for this success since it allows designers to express rich\r\n49\r\n\r\n\finformation about any domain and to overcome the limitations of a language developed and used for\r\nlong time for other purposes. HTML has recently gained importance as a markup language for writing\r\ntechnical documentation as well. ADF (Caponi et al., 2018) is a pattern-based subset of HTML that\r\nallows designers to express information about the authoring process such as change-tracking data,\r\ntemplates and reusable fragments and authorship attribution data. The format can be manipulated by a\r\nWeb editor that relies on the pattern-based structure of the language and its ability to express finegrained semantic information on each piece of content.\r\n\r\n4\r\n\r\nPhiloEditor\u00AE for the scholarly markup of literary documents\r\n\r\nPhiloEditor\u00AE, as a web-based environment for reading and annotating variants, is aimed to be a valuable\r\nsupport for scholars in the criticism of variants. Its main characteristics are the display of differences\r\n(deltas) between two versions of the same document and the classification of semantically relevant\r\nfragments of the text (colouring), according to a parametric set of features provided by a scholar in a\r\nspecified domain. PhiloEditor\u00AE's data model is based on a highly simplified version of HTML5 that is\r\nfully based on the pattern model described in section 2. PhiloEditor\u00AE\u2019s features make it particularly\r\nsuitable for the study of literary texts in multiple versions, in particular those resulting from the shifts of\r\nvolition of the author (authorial philology). In fact, it has been tested first on the two printed editions of\r\nI Promessi Sposi by Alessandro Manzoni (Bonsi et al., 2015), then on the internationally-known\r\nPinocchio by Carlo Collodi (Gargano and Italia, 2018), comparing the version published periodically\r\nbetween 1881 and 1883 on \"Giornale per i bambini\" magazine with the printed edition published by\r\nPaggi in 1883.\r\n\r\nFigure 3. Color coding of the edits in I Promessi Sposi.\r\nFor versions and variants to be comparable, it\u2019s necessary for a delta to be created. Many algorithms\r\nexist that perform such tasks, but the kind of delta is also relevant for our purpose. Most algorithms\r\nmeant for computer code assume that the relevant atomic entity to be differentiated is the individual\r\nline, since most programming languages use lines as their atomic semantic unit of production. In other\r\ntypes of documents, atomic entities may be single characters or structural nodes (e.g., a paragraph or a\r\nwhole chapter). These are not particularly appropriate for literary texts, where long paragraphs with\r\nmany differences (as in the case of our documents) would make the understanding of the variants\r\ndifficult. The best choice for literary texts in our opinion is word-based diffing, which identifies whole\r\nwords as the basic perceivable and understandable difference between variants. We thoroughly tested\r\ntwo Javascript libraries for word-based diffing, Javascript diff algorithm (Resig, 2005) and wikEd diff\r\n(Cacycle, 2017). The final choice was the latter, which is more precise and better parameterized. This\r\nalgorithm reads two text files and outputs a document comparing them by placing the differences as\r\nspans of an HTML document. The output needs to be further synthetized and qualified: in particular,\r\n50\r\n\r\n\fwikEd diff generates a simple list of insertions and deletions. Literary variants do in fact contain\r\nrelatively few simple deletions and insertions, and mostly replacements (where an old fragment is\r\nsubstituted by a new fragment). A replacement is seen by the diff algorithm as a deletion of the old text\r\nfollowed, in the same position, by an insertion of the new text. Correctly identifying and characterizing\r\nreplacements is important for a better description of the actual editing operation occurred and this is\r\ngenerated by appropriately grouping the list of insertions and deletions provided by the diff algorithm\r\nbefore the result is displayed to the user. PhiloEditor\u00AE provides different ways to display variants: the\r\nuser can choose an exclusive, a synoptic or a layered representation of the text. In these views, all\r\nmodifications are characterized as replacements. Complex diff issues can be handled manually: the\r\nsegment of the variants automatically created can be modified thanks to a simple interface operation that\r\nmakes possible to divide or combine groups of elements according to the user\u2019s needs. Word-based\r\ndiffing doesn\u2019t work with macro-variants though, so for now PhiloEditor\u00AE can only manage microvariants.\r\nThe increased familiarity with the temporal evolution of the text brought a growth in curiosity\r\ntowards the types of modifications. For this reason, we devised a two-layer model of modification types\r\n(Italia, 2018): \u201Ccategories of corrections\u201D to identify the actual methodology of correction (i.e. shifts of\r\ntextual passages, additions, deletions, inversions of words, etc.) and \u201Clinguistic categories\u201D to identify\r\nthe linguistic categories that drove the correction. In order to describe the text in these terms, a colorbased coding schema of the fragments of text affected by each phenomenon was created. Since the two\r\nclasses could overlap, two separate types of styles were created using the color of the text (categories of\r\ncorrection) and the color of the background (linguistic categories). Displaying these changes visually\r\nenables us to reach a more complex perception of linguistic features of literary texts and facilitates not\r\nonly the representation of important syntactic structures but also their hermeneutic implications.\r\nThe variety of textual features that need to be expressed in the markup of documents edited inside\r\nPhiloEditor\u00AE is limited: one needs to support the hierarchical structure of the document (chapters,\r\nparagraphs, text), very little typography (just a few words in italic here and there), plus the applicationspecific requirements of describing replacements (e.g., the edits generated by the diff engine as\r\ninsertions and deletions and converted by an internal algorithm) and the colors, i.e., the linguistic\r\ncategories described by scholars.\r\n\r\nFigure 4. The HTML source of I Promessi Sposi.\r\nCorrespondingly, we have created an extreme simplification of the HTML5 used by the application:\r\n<section>, <p> and <i> are used for the document, and <span> is used for both modifications and color\r\ncoding (since most of the colors are applied to modifications anyway). HTML classes are created and\r\n51\r\n\r\n\fused both to provide semantics and rendering characteristics, and an additional data-* attribute is used\r\nto provide basic provenance attribution. Given that the same span can be associated to multiple classes,\r\nclashes in categorization are impossible. On the other hand, the HTML that is being created is rigorously\r\nwell-formed and homogeneous, and generating a correct and valid XML\/TEI source is immediate and\r\nstraightforward.\r\nUsing HTML class names for semantic characterization has both advantages and issues. Of course,\r\nclasses make the association of special rendering to fragments easy. On the other hand, it is extremely\r\ndifficult to impose constraints on the list of classes that can be used, e.g., to specify within a schema that\r\nelements of the class \"replace\" must first contain an element with the class attribute containing \"new\"\r\nand then an element with the class attribute containing \"old\".\r\n\r\n5\r\n\r\nPhiloEditor\u00AE and Scholarly Editions: markup tools for literary texts\r\n\r\nMost of the available online editions of literary texts are based on full-text transcriptions of original\r\ntexts into electronic form (Franzini, 2012), typically using the XML\/TEI model, where the sources\r\n(witnesses in philology jargon) are traditional primary sources. The infrastructure is generally based on\r\nstandard web programming languages, both client- and server-side. Users have limited access to the\r\ndigital sources without any awareness of the backend software employed. Most of the editions, as said,\r\nare XML\/TEI documents, written by hand with a stand-alone application such as Oxygen, and converted\r\nas needed into HTML\/CSS documents using XSLT stylesheets. All projects in this domain are built on\r\nthe same basic structural components: they consist of a set of files (assets) stored inside an information\r\narchitecture such as a database or file system (structure) where they can be accessed (services) and\r\nshown on a browser (use\/display) (Druker at al., 2014). Different phases (assets, structure, services,\r\nuse\/display) mean different tools. Thus, the DiRT Directory is a registry of digital research tools for\r\nscholarly use, while the Taxonomy of Digital Research Activities in the Humanities (TaDiRAH) \u201Cbreaks\r\ndown the research lifecycle into high-level \u2018goals\u2019, each with a set of \u2018methods\u2019 \u201D.\r\nIn the field of literary texts, editing is the first important step of the process: editors need to use\r\nsimple applications in order to describe their documents and their features. TextGrid, for instance, is a\r\nreal research environment with all the necessary tools and services to support the entire research process,\r\nespecially in digital scholarly editing. The downloadable \u201CTextGridLaboratory\u201D is an editor for\r\nXML\/TEI markup with a view on the source code and a traditional visualization mechanism. Catma\r\n(Computer Assisted Text Markup and Analysis) is an online environment that manages analysis, manual\r\nand automatic annotations, and visualizations of documents. Juxta is an open-source tool for comparing\r\nand collating multiple witnesses of a text work, useful for an analytic visualization of textual variants.\r\nTapas is meant for visualizing, storing and sharing XML\/TEI documents, while EVT (Edition\r\nVisualization Technology) is a downloadable open source tool that creates digital editions from XMLencoded texts. The final styling of documents is entrusted to CSS style-sheets and is easily customizable.\r\nAs shown, XML\/TEI is the fundamental data model for all documents, and it is used as a work\r\nformat rather than as an output format, requiring scholars to learn it in depth. Simplified HTML could\r\nbe an alternative that, while maintaining complete exportability to XML\/TEI, allows application\r\ndesigners to rely on web technologies much easier to work with, and avoiding exposing literary scholars\r\nto angle brackets altogether.\r\n\r\n6\r\n\r\nConclusions\r\n\r\nPhiloEditor\u00AE is but one of the activities in which we are pushing for the use of a simple HTML instead\r\nof a full-fledged custom XML vocabulary, although striving to identify a well-formed, well-behaved,\r\ntotally descriptive and specialized subset of HTML. In PhiloEditor\u00AE two specific markup needs, the\r\ndescription of modifications and the coloring of semantic characterization of the texts, have been\r\nexpressed within a standard and very simple subset of HTML. Similar activities, such as RASH or ADF,\r\nare used in other completely different, but still very specialized, domains. Overall the response to the\r\nfeatures and the flexibility of PhiloEditor\u00AE has been overwhelmingly positive. The objective of the next\r\n52\r\n\r\n\ffuture is to slowly convert PhiloEditor\u00AE into a full-scale environment for all the needs of the collection,\r\ndigitization and publication of scholarly editions of literary texts."
	},
	{
		"id": 10,
		"title": "An empirical study of versioning in Digital Scholarly Editions",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Martina B√ºrgermeister"
		],
		"body": "Digital publishing and a textuality that is dynamic, collaborative, distributed and interdependent lead to the\r\ndigital scholarly edition (DSE) facing additional technological challenges in contrast to print editions. The\r\nMLA Committee von Scholarly Editions takes care of this and recommends the use of technologies appropriate\r\nto the goals of the edition, \u201Cin recognition of the fact that technologies and methods are interrelated in that no\r\ntechnical decisions are innocent of methodological implications and vice versa (MLA 2016, 1).\u201D The\r\ncommittee also suggests a design of the DSE that will be as durable and sustainable as possible. This applies\r\nnot only to DSE, but also to the entire scientific practice. In the \u201CGuidelines of good scientific practice\u201D, like\r\nthe one from the University of Parma, it says: \u201CI ricercatori dell\u2019Ateneo devono aver cura che tutti i dati,\r\nprimari e secondari, generati dalle loro attivit\u00E0 di ricerca siano archiviati e conservati in modo corretto ed\r\nappropriato, garantendone la sicurezza e l\u2019accessibilit\u00E0 [\u2026]\u201D Besides being responsible for data safety and\r\naccess, researchers should support a prompt data exchange and reuse: \u201CCon l\u2019intento di rendere la ricerca pi\u00F9\r\naperta, globale e collaborativa e garantirne un controllo di qualit\u00E0, i dati dovrebbero essere messi a disposizione\r\ndei colleghi che vogliano replicare lo studio o elaborare nuove ricerche a partire da essi [\u2026] (Universit\u00E0 di\r\nParma, 2018)\u201D.\r\nThe reuse of data is a matter of trust in data itself and the IT infrastructure as maintainer and provider. In\r\nthis case, transparency is a key factor. In the following it will be argued, that versioning is more than a measure\r\nto guarantee data authenticity, integrity and accessibility. Versioning as an integrated part of the DSE creates\r\ntransparency in an ongoing scholarly discourse. This objective is achieved, firstly, when new versions of the\r\nDSE or components of it are created if changes are made to it. Second, if what has been changed is also\r\ncommunicated; and third, if previous versions are made available. These three criteria form the basis of my\r\nanalysis of versioning DSEs. Using an empirical approach, I would like to find out why versioning plays a role\r\nin these projects and how it is implemented.\r\nThe implementation of versioning in a DSE is not yet state-of-the-art. In order to find out which DSEs have\r\na versioning strategy and what it looks like, the \"Catalogue of digital editions\" (\u00D6AW) was used as a finding\r\naid. Since the list is very extensive, the analysis includes edition projects mainly from the past 15 years, which\r\nhave developed over a relatively longer period of time. Since there are far more DSEs without versioning than\r\nwith it, the empirical basis was mainly provided by Bleier's preliminary work (2019). He has evaluated 100\r\nDSEs on criteria such as citation recommendation, permalinks and versioning. Finally, those cases that assign\r\nversion numbers, but neither convey what has changed, nor make past versions available, were excluded from\r\nfurther consideration. Their exclusion from this analysis is justified by the fact that they fulfil only one of three\r\nconditions for versioning DSEs discussed here. The review has shown that so far three different versioning\r\nstrategies have been applied, which meet at least two of the three conditions formulated above. An overview\r\nof the discussed strategies is provided in section 5 (Table 1).\r\n\r\n55\r\n\r\n\f1 Versioning as documentation\r\nDSE projects of this type implement the assignment of sequential version numbers for changes and document\r\nwhat has been changed. A list of past version numbers with descriptions of the revisions and where they were\r\nmade is provided. In these projects, the DSE is seen as an ongoing process, which is documented.\r\nThe strategy is pursued in the DSE \"Auchinleck Manuscript\", edited by David Burnley and Allison Wiggins.\r\nThe project started in 2000 and was launched in July 2003 with version 1. The edition has a citation\r\nrecommendation with version number indication. The editors recommend citing version 1.1 because it\r\ncorresponds to the current version (March 15, 2004). The version number applies to the entire project. There\r\nis a version documentation with the title \u201CArchive of site updates\u201D. It consists of three lists: One with version\r\nnumber and date, another with \"Corrections to texts and to textual notes\" and an extra column \"Record of other\r\nchanges\", which lists changes to the bibliography and the side menu. Allison Wiggins explains the version\r\ndocumentation theme as follows:\r\nAll changes made to the content of this site are recorded here in the archive of site updates.\r\nEach time a batch of updates is added, the site is designated a new version number. This\r\nsystem ensure that users can keep track of any changes made and can reference the site\r\nmaterials accurately (Burnley and Wiggins, 2019).\r\nAt about the same time (March 2003), the DSE \"The Old Bailey Proceedings Online\" was published. It is an\r\neditorial long-term project that presents London processes from the period 1675-1913 and Tim Hitchcock,\r\nRobert Shoemaker, Clive Emsley, Sharon Howard and Jamie McLaughlin are responsible for the edition. A\r\nversion documentation can be found in the \"What's New Archive\". As in the previous example, an introductory\r\ntext refers to the non-final, process-like state of the DSE. It is followed by a listing and description of what has\r\nchanged in previous versions: the current status is March 2018 with version 8.\r\nA version documentation is also available in the edition project \"The Online Froissart\". The editors Peter\r\nAinsworth and Godfried Croenen cite another reason for documenting versions:\r\nThe Online Froissart is a collaborative, interdisciplinary and incremental project. Given the\r\nsheer size of Froissart\u2019s Chroniques, the number of surviving manuscripts and their\r\ndispersal across many libraries in several different countries, it has been decided not\r\nto delay publication of available transcriptions until all of these materials have been\r\ntranscribed and annotated. The Online Froissart, therefore, publishes all currently available\r\ntranscriptions and other materials produced by the project team, plus updates, and augments\r\nthe website and its underlying datasets on a regular basis (annually from 2012 onwards).\r\n(Ainsworth and Croenen 2018)\r\nDue to the large amount of material and its scattered provenance, an intentional decision was made that the\r\npublication of the DSE should be as early as possible, but the edition will be regularly updated.\r\n\r\n2 Versioning as version management\r\nVersioning is implemented in the same way in the DSE as it can be found in software development. Version\r\nmanagement systems save all changes to text documents as versions. All versions can be restored, and these\r\nsystems are also designed to handle collaborative writing processes. Examples of DSEs that implement this\r\nversioning strategy are \"Papyri.info\" and \"The Devonshire Manuscript\".\r\n\u201CThe Devonshire Manuscript\u201D is a Wikibook edition, whose main editor is the Devonshire Manuscript Group.\r\nThe aim is to discuss the edited sources as widely as possible and to change the role of the scientific editor\r\nfrom the sole authority for the text to that of a moderator: \u201CThe social edition is a work that brings communities\r\ntogether to engage in conversation around a text formed and reformed through an ongoing, iterative, public\r\neditorial process.\u201D (Wikibooks, 2014) One of the main contributors to this project is Ray Siemens (2012, 453),\r\nwhose motivation to support the DSE with the help of social media is as follows:\r\nSuch tools facilitate a model of textual interaction and intervention that encourage us\r\nto see the scholarly text as a process rather than a product, and the initial, primary editor\r\nas a facilitator, rather than a progenitor, of textual knowledge creation. (ibid.)\r\nIt is therefore about the editorial process as an iterative and collaborative activity. In this respect, it is\r\nessential for the progress of the project to keep all iterations available in the form of commented versions.\r\nLike all pages of Wikipedia, all pages of a Wikibook are based on the same software MediaWiki (since version\r\n1.5) and have a revision history (under the tab View history). The revision history in the form of a table contains\r\nall edits of a page in the wiki. Each change to a page creates a change line that contains information about the\r\nperson who made the edit, the time when the edit was made, and a reference to the new wiki text in the text\r\ntable. Elements of the revision table are preserved permanently, unless the page is deleted.\r\n56\r\n\r\n\fThe versions of Papyri.info are accessible in a similar way, but via a different interface. Papyri.info\r\naggregates papyrological resources from different databases and makes them available for editing. This DSE\r\nhas been in existence since 2006 and interested editors can still add or change data today. A peer review of the\r\nrevisions ensures the quality of the content. The implementation of the strongly social and collaborative project\r\napproach is made possible by a version management software that manages different users and their\r\ncontributions. A method that was developed to facilitate the software development process is used here to\r\nmanage the editorial processes. The DSE is stored in a Git repository. All editorial processes are recorded,\r\nversioned and recoverable. A look at the repository in Github shows 100 contributors1 and more than 100,000\r\ncommits2.\r\nGit is a version control system that is used for collaborative software development. As already mentioned,\r\nthe changes to the files are tracked. These programs provide access to any version of the file so that any changes\r\ncan be undone. Each version has a timestamp and an author. It is always possible to see who changed what\r\nand when.\r\nIn the case of Papyri.info, the edited texts are saved as XML files in Git. Via the platform Github the\r\nrepository can be viewed and the different versions of the texts can be displayed. The version comparison is\r\ndone line by line and any changes to the file will be recognized by the software and automatically saved as a\r\nnew version.\r\nIt makes no difference which version management software is used, the understanding of what a version is\r\nremains the same in the software development domain. In the examples mentioned so far, which have version\r\ndocumentation, the same version number stood for a whole series of revisions. If versioning takes place via\r\nversion control systems, every saved change becomes a new version, which has no further semantic meaning.\r\nIt makes no difference for the system if you make many changes or just fix a typo, it is always a new version.\r\nThis strategy certainly has its advantages especially in a collaborative editing process.\r\n\r\n3. Versioning as retrievable milestone versions\r\nIn contrast to the open DSEs mentioned above, which are geared towards a high frequency of changes, DSEs\r\nof this type are updated at intervals under new version names. These versions can also be called milestone\r\nversions, because the question of when to publish the next update is a project specific decision. Former versions\r\nare findable via a permalink and can be retrieved. A version name or number applies to the entire edition. This\r\nversioning strategy is evident in teams of editors who deliberately publish changes, revisions, and\r\nenhancements collectively. Every single editing step is not shown. What is desired as a research process in the\r\ncase of Wikibooks is not part of the intention to publish an edition in this case.\r\nThis group includes the DSE \"Der Sturm. Digital Source Edition on the History of the International AvantGarde\", developed and edited by Marjam Trautmann and Torsten Schrade since 2018. This edition project is\r\nstill a \"work in progress\": the team of editors will gradually open up new sources and publish them promptly.\r\nThe motive for this approach is explained as follows:\r\nDies kommt den interessierten Forschungscommunities zugute, da somit ein schneller Zugriff auf eine\r\nbest\u00E4ndig wachsende Gesamtedition gew\u00E4hrleistet ist. Ein weiterer Vorteil dieses iterativen Vorgehens\r\nist, dass sich somit auch das Forschungsdatenmodell und die ben\u00F6tigten Softwarekomponenten\r\nkontinuierlich und in Einklang mit den hinzukommenden Quellen und ihren jeweiligen Spezifika\r\nweiterentwickeln k\u00F6nnen. (https:\/\/sturm-edition.de\/projekt\/methodik.html)\r\nAll developed sources have several permalinks that represent the versions. The permalinks are constructed in\r\nsuch a way that the identifier ends with the version information:\r\nVersion 1: https:\/\/sturm-edition.de\/id\/Q.01.19140115.FMA.01\/1\r\nVersion 2: https:\/\/sturm-edition.de\/id\/Q.01.19140115.FMA.01\/2\r\n\u201CHumboldt Digital\u201D also integrates this type of versioning. The edition \"Humboldt Digital\" is a publication of\r\nthe Academy Project \"Alexander von Humboldt auf Reisen \u2013 Wissenschaft aus der Bewegung\" at the BerlinBrandenburg Academy of Sciences and Humanities. In this project, each entry offers the possibility to view\r\npast milestone versions. The specific version number is inserted after the domain name. E.g. \u201Chttps:\/\/\r\nedition-humboldt.de\/v4\/H0002656\u201D.\r\n1\r\n\r\nGithub Glossary: \u201CA collaborator is a person with read and write access to a repository, who has been invited to contribute by the\r\nrepository owner.\u201D\r\n2\r\nGithub Glossary: \u201CA commit, or \"revision\", is an individual change to a file (or set of files). It's like when you save a file, except\r\nwith Git, every time you save it creates a unique ID (a.k.a. the \"SHA\" or \"hash\") that allows you to keep record of what changes were\r\nmade when and by who. Commits usually contain a commit message which is a brief description of what changes were made.\u201D\r\n57\r\n\r\n\f4. Mix type: Versioning as retrievable, documented milestone versions\r\nIf milestone versions are also documented, then all three of the above criteria for versioning DSEs are met:\r\nchanges will be published at intervals as new versions. Old versions remain viewable and what the version\r\nstands for and what has been changed are documented.\r\nAn example of this is the \u201CAugust Wilhelm Schlegel Edition\u201D. This DSE is about bringing together August\r\nWilhelm Schlegel's correspondence. The project runs until 2020 under the direction of Jochen Strobel and\r\nClaudia Bamberg. The first beta version was published on 2 June 2014; version-07-19 was published on July\r\n1, 2019. Specific versions can be addressed by entering the name after the domain. For example, a letter in the\r\nup-to-date version has a Permalink like this:\r\n\u201Chttps:\/\/august-wilhelm-schlegel.de\/version-07-19\/briefid\/1599\u201D.\r\nUnder the menu item \u201CVersions\u201D is stated that every three months a new version with numerous resources will\r\nbe published, while all previous versions remain fully accessible in the version archive.\r\nThe DSE \"Johann Wolfgang Goethe: Faust\", edited by Anne Bohnenkamp, Silke Henke and Fotis Jannidis,\r\nalso has a version archive. The entire project is versioned at regular intervals and the current version is 1.2. In\r\nthis case, the version is specified in the subdomain:\r\n\u201Chttp:\/\/v1-2.faustedition.net\/document?sigil=B.a&page=59&view=print&section=5#l813\u201D\r\n\r\n5. Overview of applied versioning strategies\r\nversion name\r\n\r\nwork\r\nin\r\nprogress\r\n\r\nX\r\n\r\nX\r\n\r\nX\r\n\r\nX\r\n\r\nVersioning\r\nas\r\nretrievable\r\nmilestone versions\r\n\r\nX\r\n\r\nX\r\n\r\nMix\r\ntype:\r\nVersioning\r\nas\r\nretrievable,\r\ndocumented\r\nmilestone versions\r\n\r\nX\r\n\r\nX\r\n\r\nVersioning\r\nas\r\ndocumentation\r\n\r\nVersioning\r\nversion\r\nmanagement\r\n\r\nas\r\n\r\ncollaborative\r\n\r\ndocumented\r\nchanges\r\n\r\nformer\r\nversions\r\navailable\r\n\r\nX\r\n\r\nX\r\n\r\nX\r\n\r\nX\r\n\r\nX\r\n\r\nX\r\n\r\nX\r\n\r\nX\r\n\r\nTable 1. Overview of applied versioning strategies. X means: is provided.\r\n\r\n6. Conclusion\r\nThe empirical analysis of DSE has shown that integrating versioning is still relatively rare in DSE projects.\r\nThis is the case despite the fact that there are manifold practical motives for implementing a versioning\r\nstrategy. Such as the idea that the editorial process should be shared with the public; the project has a long\r\nduration and editors would like to publish interim results; the material to be edited is too extensive and will be\r\nmade accessible in publishable stages; or the editorial team and the collections are so scattered that it would\r\nmake sense and be beneficial for the overall project to publish partial results. For whatever reason, all projects\r\nintend to communicate changes and to be transparent in such a way that makes the DSE more reliable and\r\ntrustworthy.\r\nAll projects presented here clearly pursue the strategy of making the DSE available to the public, although\r\nthe editions are work in progress. The most technologically undemanding practice to communicate the changes\r\nis to maintain a version documentation. Many more ways to make changes transparent are given when the\r\nedition is linked to a version control system. In these cases, it is also easy to make a version comparison and\r\n58\r\n\r\n\fto understand the contributions of individual editors. Each resource has its own version history and can be\r\nretrieved. In the presented projects of the type \u2018versioning as retrievable milestone version\u2019 this is instead not\r\nthe aim of the editors. The version number refers to the complete edition. Individual contributions can be\r\nsearched under the milestone version number. A version control system is not absolutely necessary for the\r\ntechnical implementation, but it is an advantage. It is part of the workflow to have a published version and at\r\nthe same time to have a new version in processing. The encapsulated data storage in the system is important\r\nfor the addressability and availability of past versions. In addition, the information resources gain in quality if\r\nthe traceability of the changes is also collectively available as documentation. By this means, versioning works\r\nlike an apparatus in a broader sense, and one which verifies editorial decisions by making the work in progress\r\ntransparent to the users."
	},
	{
		"id": 11,
		"title": "ELA: fasi del progetto, bilanci e prospettive",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Emmanuela Carb√©",
			"Nicola Giannelli"
		],
		"body": "Introduzione\r\n\r\nTra le cinque fasi che dovrebbero caratterizzare ogni progetto, ovvero avvio, pianificazione, esecuzione,\r\ncontrollo e chiusura, l\u2019ultima viene talvolta sottovalutata o non messa in atto, dimenticando che un progetto\r\nper definirsi tale deve avere le caratteristiche di unicit\u00E0 e durata limitata, con la produzione di un risultato il\r\npi\u00F9 possibile in linea con il piano di costi, tempo e qualit\u00E0 stabiliti. Ne consegue il rischio di trasformare un\r\nlavoro in un pericoloso work-in-progress, che pu\u00F2 subire improvvisi arresti per mancanza di copertura\r\nfinanziaria, per lo scioglimento del gruppo di lavoro, o per altre cause interne ed esterne. La qualit\u00E0 di un\r\nprogetto andrebbe dunque valutata anche nella sua capacit\u00E0 di arrivare a una conclusione con la\r\nformalizzazione delle lezioni apprese (Mastrofini, 2017), incluse quelle meno positive (Dombrowski, 2019),\r\nal fine di costruire un patrimonio comune di conoscenze non meno importante del progetto stesso. Questo\r\npatrimonio, se conservato e condiviso, pu\u00F2 essere utile per realizzazioni future e per la costruzione di nuove\r\nreti di collaborazione.\r\nIl presente contributo si propone di fare il punto su un progetto di start-up, DAS-MeMo1, avviato a marzo\r\n2018 dall\u2019Universit\u00E0 di Siena sotto la direzione di Francesco Stella e in collaborazione con l\u2019azienda\r\nQuestIT e la casa editrice Pacini. Questa fase del progetto prevede la realizzazione della piattaforma ELAEurasian Latin Archive, che raccoglie documenti in lingua latina e multilingua dal XII al XVIII secolo\r\nriguardanti l\u2019Estremo Oriente. Il progetto \u00E8 inserito all\u2019interno di un contesto pi\u00F9 ampio, caratterizzato da\r\nmetodologie e strumenti di lavoro gi\u00E0 collaudati grazie a numerose esperienze del PI e del suo gruppo di\r\nlavoro nell\u2019ambito dei progetti digitali. Viene dunque tracciata la storia del progetto in tutte le sue fasi,\r\ndall\u2019avvio del lavoro alla sua chiusura, prevista per febbraio 2020, e presentato un bilancio di ci\u00F2 che \u00E8 stato\r\nrealizzato, delle lezioni apprese, e degli sviluppi futuri con l\u2019avvio della fase di consolidamento e\r\nimplementazione.\r\n\r\n1\r\n\r\nDAS-MeMo (Data Mining e analisi statistica su fonti testuali storiche del periodo medievale e moderno,\r\nwww.dasmemo.unisi.it) ha ricevuto il contributo di Regione Toscana per un assegno di ricerca cofinanziato con le risorse\r\nPOR FSE 2014-2020 nell\u2019ambito del progetto Giovanis\u00EC.\r\n61\r\n\r\n\f2\r\n\r\nMetodologia di progetto\r\n\r\nNella fase di avvio \u00E8 stata elaborata la documentazione per il piano del progetto, basato su obiettivi, tempi\r\ne risorse. Nel corso della pianificazione sono stati individuati i passi da realizzare per il raggiungimento\r\ndegli obiettivi, sono stati assegnati i ruoli e decise le tempistiche delle consegne. \u00C8 stata condotta un\u2019analisi\r\nSWOT per identificare fin da subito le possibili problematiche e i fattori di rischio (si veda, a titolo di\r\nesempio, l\u2019analisi applicata al caso BEIC di Consonni e Weston, 2015). La fase di start-up, della durata di\r\n24 mesi, aveva i seguenti obiettivi: 1. Creazione di un modello e di un workflow di lavoro, includendo\r\nmomenti di revisione e di controllo della qualit\u00E0. 2. Definizione e creazione del corpus, con relativa codifica.\r\n3. Progettazione, analisi dei requisiti e realizzazione, in collaborazione con l\u2019azienda partner del progetto,\r\ndi un prototipo della piattaforma, con: a. pagine informative gestibili tramite un comune CMS; b. digital\r\nlibrary; c. tool di analisi linguistiche e semantiche; d. backend per la gestione della Digital Library. 4.\r\nIndagine preliminare di una parte del corpus con primi risultati, pubblicati in e-book grazie all\u2019editore\r\npartner del progetto. 5. Disseminazione e comunicazione dei risultati su pi\u00F9 livelli; 6. Realizzazione di un\r\npiano di sostenibilit\u00E0. Dalla pianificazione del progetto \u00E8 nata una lista di specifiche basate su metodo\r\nMoSCoW (vd. par. 3). Il progetto \u00E8 stato monitorato sia internamente, con un controllo costante dello stato\r\ndi avanzamento dei lavori anche per eventuali modifiche del cronoprogramma, sia esternamente, con\r\nrelazioni intermedie inviate ai soggetti cofinanziatori del progetto.\r\nIn questa ultima fase, ormai prossima alla chiusura dei lavori, rimane dunque da compiere una revisione\r\ngenerale e una valutazione di ci\u00F2 che \u00E8 stato fatto. In un intervento sul sito del King\u2019s Digital Lab, Arianna\r\nCiula (2019a) spiega le motivazioni che hanno portato alla realizzazione della Checklist for Digital Outputs\r\nAssessment (Ciula, 2019b), una guida pubblicata per facilitare la revisione dei progetti in vista della prossima\r\nvalutazione REF (Research Excellence Framework), utilizzata nel Regno Unito per monitorare la qualit\u00E0\r\ndella ricerca. Ciula rileva che se in altre discipline una valutazione e autovalutazione del lavoro parte da\r\nbasi piuttosto definite, quando si opera nel contesto di progetti digitali stabilire dei criteri condivisi comporta\r\ncriticit\u00E0 e incertezze. La checklist propone dodici punti tematici (con relativi esempi) per la valutazione di\r\nprodotti digitali, che abbiamo deciso di adottare in questa fase finale. I punti servono per verificare vari\r\naspetti del progetto: i credits, con la corretta attribuzione dei lavori svolti (incluse le realizzazioni dei data\r\nmodel e dell\u2019architettura) e degli enti che hanno partecipato e\/o finanziato il progetto, che devono essere\r\ncorrettamente menzionati con i loro loghi; il controllo delle licenze e copyright, che devono essere esplicite\r\ne il pi\u00F9 possibile conformi ai principi FAIR; la messa a disposizione della documentazione relativa al\r\nprogetto, che includa esplicitamente una riflessione sul valore aggiunto dei risultati conseguiti; l\u2019attenzione\r\nall\u2019accessibilit\u00E0, alla user experience e alla funzionalit\u00E0 dell\u2019interfaccia; controllo delle versioni del\r\nprodotto, con la conservazione delle eventuali versioni precedenti; la presenza di indicazioni su come citare,\r\ndi identificatori persistenti e DOI; il piano di sostenibilit\u00E0 e accessibilit\u00E0 e delle informazioni sull\u2019utilizzo\r\ndel prodotto.\r\n\r\n3\r\n\r\nAnalisi del corpus e creazione della piattaforma\r\n\r\nDopo una valutazione dei casi di studio, e in particolare di ALIM \u2013 Archivio della Latinit\u00E0 Italiana del\r\nMedioevo (Russo, 2005; Ferrarini, 2017; Manos, 2018), si \u00E8 proceduto alla definizione del corpus attraverso\r\nun censimento di trecento testi, effettuato sulla base di alcuni repertori cartacei e online (tra questi:\r\nBibliotheca Sinica 2.0 e CCT-Christian Texts Database). Il censimento \u00E8 stato allestito e arricchito con\r\nl\u2019utilizzo di OpenRefine (Hooland, Verborgh and De Vilde, 2013; Williamson, 2017), includendo i\r\nriferimenti bibliografici di ogni record, l\u2019eventuale presenza di digitalizzazioni online e i diritti di utilizzo\r\ndella risorsa. In corso d\u2019opera sono stati aggregati metadati relativi allo stato di avanzamento del progetto:\r\nse un testo \u00E8 preso in carico dal gruppo di lavoro, sono aggiunte informazioni circa il software utilizzato per\r\nl\u2019eventuale digitalizzazione o trattamento degli OCR, il responsabile della trascrizione e codifica (con\r\nrelativo ORCID), i tempi di realizzazione, la licenza di pubblicazione, il grado di attendibilit\u00E0 della risorsa.\r\n\u00C8 stato poi creato un modello di lavoro per la realizzazione della piattaforma, basato sulla MoSCoW\r\nanalysis, che ha permesso di focalizzare meglio gli obiettivi prioritari (Must have), i desiderabili ma non\r\nessenziali (Should have), i desiderabili ma non strettamente necessari (Could Have), e quelli che possono\r\n62\r\n\r\n\fessere pianificati per il futuro (Would have). Si d\u00E0 qui conto dei contenuti essenziali: 1. Must have: una\r\npiattaforma che raccoglie testi latini contenenti inserti multilingua (cinese, giapponese, coreano, ma anche\r\npinyin); un solido motore di ricerca, in grado di realizzare ricerche mirate anche con filtri e indicizzazioni\r\nin base a una lista predefinita di metadati; un modello di codifica in XML TEI; possibilit\u00E0 di visualizzare il\r\ntesto e scaricarlo in pi\u00F9 formati (TXT; PDF; XML); un backend per il caricamento e la gestione dei\r\ndocumenti; un identificativo persistente per ciascun documento; un set base di tool per analizzare i\r\ndocumenti (indici di parole, lemmi, type, stopwords; frequenze assolute e relative; concordanze;\r\nType\/Token Ratio, N-grams; numero totale di parole per documento e altre analisi quantitative di semplice\r\nacquisizione); definizione della user policy; messa a punto di metodi di lavoro collaborativi per future\r\nimplementazioni del progetto con pi\u00F9 unit\u00E0 di ricerca (es. Wiki). 2. Should have: tecniche di\r\ngeoreferenziazione; codifica di luoghi, date e persone menzionati nei testi; strumenti pi\u00F9 raffinati per\r\nl\u2019analisi linguistica dei testi (PoS, Burrows Delta, frequenze in base a sillabe) anche in considerazione del\r\nproblema specifico che pongono i documenti multilingua; analisi semantica dei testi; topic modeling; un\r\nprogetto pi\u00F9 complesso di backend per i collaboratori, con la possibilit\u00E0 di modificare i documenti attraverso\r\nun editor di testo e di gestire il flusso di lavoro. 3. Could have: un progetto pi\u00F9 specifico per alcune tipologie\r\ndi documenti, come ad esempio le lettere, numerose all\u2019interno del corpus; la possibilit\u00E0 di creazione, da\r\nparte dell\u2019utente, di un corpus personalizzato con analisi testuali comparate attivando processi in tempo\r\nreale; integrazione della codifica TEI con modelli semantici (Ciotti et al., 2016; Ciotti, 2018); Named Entity\r\nRecognition per luoghi, persone e date (Erdmann et al., 2016; Simon et al., 2017); 4. Would have: inclusione\r\ndelle digitalizzazioni dei documenti (Rosselli Del Turco, 2015; 2019), che talvolta contengono disegni,\r\nmappe e altre rappresentazione grafiche di particolare rilevanza.\r\n\r\nFigure 1. Il prototipo della piattaforma.\r\n\r\nIl prototipo della piattaforma \u00E8 stato realizzato con pi\u00F9 componenti: per l\u2019interfaccia \u00E8 stato utilizzato il\r\nCMS Wordpress, scelto soprattutto per una facile gestione delle pagine informative e per la pubblicazione\r\ndella documentazione del progetto; la Digital Library \u00E8 sviluppata in Java EE, con una UI basata su\r\ntecnologie web e realizzata utilizzando Javascript; si basa sul motore di ricerca Elastic Search con possibilit\u00E0\r\ndi effettuare ricerche full text anche con l\u2019utilizzo della sintassi Lucene. Al suo interno \u00E8 stato aggiunto un\r\ntool dedicato alle analisi linguistiche, attualmente alla sua versione 1.0: si tratta di un strumento realizzato\r\nin Python e basato su CLTK (Burns, 2019) e NLTK (Bird et al., 2015), chiamato ELA-tool, che restituisce\r\nin formato JSON i risultati delle analisi linguistiche previste dai requisiti primari del progetto (Must Have).\r\nNel momento in cui avviene l\u2019upload del documento, ELA-tool processa il testo. I risultati vengono poi\r\nmemorizzati e utilizzati sia per la visualizzazione delle analisi linguistiche, sia da Elastic Search per raffinare\r\nle possibilit\u00E0 di ricerca. Nell\u2019ottica di poter effettuare continue migliorie del tool procedendo con il rilascio\r\n\r\n63\r\n\r\n\fdi nuove versioni perfezionate, \u00E8 stato previsto che nel backend per l\u2019upload e la gestione dei documenti sia\r\npresente una funzione di refresh che esegue i processi di ELA-tool sui testi gi\u00E0 caricati in precedenza su\r\nesplicita richiesta di un utente amministratore della piattaforma.\r\n\r\nFigure 2. Visualizzazione dei risultati processati dal tool di analisi linguistica.\r\n\r\nI testi sono codificati in XML seguendo lo schema TEI P5 (Tei Consortium 2015), con un TEI header\r\nmodellato grazie al censimento realizzato con OpenRefine; si pone una particolare attenzione alla codifica\r\ndi luoghi, date, persone (Wikidata, VIAF e, per i luoghi, primariamente Pleiades Gazetteer) e l\u2019eventuale\r\npresenza di inserti in lingue diverse dal latino (\u00E8 il caso, ad esempio, di Sapientia Sinica di Costa e\r\nIntorcetta).\r\n\r\n4\r\n\r\nBilanci, prospettive\r\n\r\nAlla conclusione della fase di start-up, ELA ospiter\u00E0 i primi cento testi tratti dal censimento, scelti per\r\nimportanza e per variet\u00E0 nelle caratteristiche, per permettere una verifica sul campo del modello di codifica\r\nscelto e intervenire con correzioni in corso di implementazione. La piattaforma sar\u00E0 ospitata presso il centro\r\ndi calcolo dell\u2019Universit\u00E0 di Siena, con un progetto di business continuity e disaster recovery, con un piano\r\nprogrammato di snapshot e backup.\r\nRispetto agli obiettivi del progetto, i \u201Cmust have\u201D risultano oggi completamente raggiunti, cos\u00EC come quasi\r\ntutti i \u201Cshould have\u201D. In questo contesto, pare utile una revisione e valutazione del lavoro sulla base di\r\nindagini qualitative e quantitative. Tra queste, appare prioritaria un\u2019analisi dei risultati ottenuti dal sistema\r\ndi lemmatizzazione di CLTK, attraverso una comparazione con altri lemmatizzatori, sulla scorta di\r\nMambrini e Passarotti (2019) e Eger et al. (2015, 2016). La revisione del lavoro potr\u00E0 includere anche una\r\nvalutazione sulla user experience della piattaforma, con un questionario per gli utenti che utilizzano il\r\nprototipo.\r\nSe la fase di start-up \u00E8 prossima alla sua conclusione, quella di consolidamento e implementazione di\r\nEurasian Latin Archive va ora pianificata nell\u2019ottica della sostenibilit\u00E0 sul medio e lungo periodo per il\r\nraggiungimento di risultati pi\u00F9 ambiziosi: l\u2019avvio a regime della piattaforma e la programmazione del suo\r\nincremento in termini di numero di documenti trattati, con un piano redazionale che comprenda anche la\r\npubblicazione e il mantenimento di tutte le sezioni della piattaforma; l\u2019ampliamento dell\u2019utenza prevista:\r\nallo stato attuale ELA \u00E8 pensato soprattutto per un pubblico specializzato, ma non si esclude, in futuro, la\r\npossibilit\u00E0 di rivolgersi a una comunit\u00E0 pi\u00F9 ampia di utenti, anche attraverso nuovi percorsi di\r\ndisseminazione e riutilizzo dei materiali; infine l\u2019integrazione con altri progetti: ELA \u00E8 stato primariamente\r\npensato per essere interoperabile con ALIM, tuttavia si ritengono necessari, anche per la sostenibilit\u00E0 del\r\n\r\n64\r\n\r\n\fprogetto stesso, la condivisione dei dati e degli strumenti realizzati (che saranno messi a disposizione su\r\nGitHub) e un dialogo costante per operare a fianco di altri progetti in corso (Passarotti et al., 2019).\r\n"
	},
	{
		"id": 12,
		"title": "Digitized and Digitalized Humanities: Words and Identity",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Claire Clivaz"
		],
		"body": "1 Introduction: Words and Identity in Digital Humanities\r\nAs the 2020 AIUCD conference topic underlines, the identity and definition of the Humanities that has met\r\nthe computing world, is in constant reshaping (Ciotti, 2019)1. The English language has acknowledged the\r\nimportant turn from humanities computing to digital humanities at the beginning of the 21st century\r\n(Kirschenbaum, 2010), whereas French-speaking scholarship is wrestling between humanit\u00E9s num\u00E9riques\r\n(Berra, 2012; Doueihi, 2014) and humanit\u00E9s digitales (LeDeuff, 2016; Cormerais\u2013Gilbert, 2016; Clivaz,\r\n2019). Moreover, new words are often tested to express the intensity of what is at stake: if Jones has chosen\r\nthe term \u201Ceversion\u201D for describing the present state of the digital turn (Jones, 2016), the French thinker Bernard\r\nStiegler focuses on \u201Cdisruption\u201D (Stiegler, 2016). German and Hebrew link digital humanities naming with the\r\nvocabulary of spirit\/mind, whereas the outmoded word humanit\u00E9s has come back in French through the naming\r\nof the humanit\u00E9s num\u00E9riques, recalling the presence of the body (Clivaz, 2017).\r\nInscribed in this linguistic effervescence, a phenomenon has so far not drawn the attention of the humanist\r\nscholarship: the difference between digitization and digitalization, or between digitized and digitalized\r\nHumanities. The present paper will explore, as far as possible, the emergence of this dualistic vocabulary,\r\ninside and outside of digital humanities scholarship, looking for its meanings and implications. It represents\r\nonly a first overview about the scare definitions and occasional uses of \u201Cdigitalization\u201D, even if the debate\r\nbetween digitization and digitalization can sometimes inform implicitly the discourse, as we will see in Section\r\n4 (Smithies, 2017). Section 2 will first comment similarity and difference between both words, looking for\r\n\u201Cdigitalization\u201D definitions, and its uses. Section 3 discusses in detail the only definition article we have so far\r\ndebating these two concepts. Section 4 considers more broadly the digitalization perspective and presents the\r\nauthor\u2019s point of view on the issue, including its articulation to the AIUCD 2020 topic.\r\n\r\n2 Looking for \u201Cdigitalization\u201D definition and uses\r\nEnglish native speakers would surely ask first if there is really a difference between \u201Cdigitization\u201D and\r\n\u201Cdigitalization.\u201D \u201CDigitalization\u201D does not benefit from its own entry in Wikipedia or in the Collins Dictionary\r\n\r\n1\r\n\r\nMany thanks are due to the reviewers for their remarks, to Andrea Stevens for her English proof-reading, and to Elena Giglia for her\r\ntranslation of the Italian abstract.\r\n67\r\n\r\n\fonline.2 However, the Oxford English Dictionary (OED) dates the first use of digitalization as equivalent to\r\ndigitization in 1959, 3 whereas the medical sense appeared in 1876. 4 OED presents also digitalization as\r\nmeaning \u201Cthe adoption or increase in use of digital or computer technology by an organization, industry,\r\ncountry, etc.\u201D5 In the Wikipedia entry \u201Cdigital transformation\u201D, a similar definition is given for \u201Cdigitalization\u201D:\r\n\u201Cunlike digitization, digitalization is the \u2018organizational process\u2019 or \u2018business process\u2019 of the technologicallyinduced change within industries, organizations, markets and branches.\u201D6 A most decisive shift in the sense of\r\na difference between the two words can be seen in the International Encyclopedia of Communication Theory\r\nand Philosophy, which published an entry on \u201CDigitalization\u201D by J. Scott Brennen and Daniel Kreiss in 2016.\r\nThey argue in favour of a distinction from \u201Cdigitization\u201D (Brennen\u2013Kreiss, 2016). This publication is in itself\r\na quite clear signal, according to our cultural and scholarly habits, that \u201Cdigitalization\u201D exists with its own\r\nmeanings, since it has been defined in an encyclopedia. As far as I have been able to determine, it is the only\r\narticle trying to define both concepts and is discussed in detail in Section 3.\r\nAs we see, references to digitalization\u2019s definition are quite scare. So far, there it is not even possible to do\r\na systematic overview of its theoretical background based in the scholarly literature because it is not discussed,\r\nwith the exception of the Brennen\u2013Kreiss article. But if we look at its uses, some aspects clearly emerge.\r\n\u201CDigitalization\u201D is mainly used in the business and economical world, and very infrequently in digital\r\nhumanities. For example, according to Jari Collin in a 2015 Finnish volume of collected essays, digitalization\r\nrefers to the understanding of \u201Cthe dualistic role of IT in order to make right strategic decisions on IT priorities\r\nand on the budget for the coming years. IT should not be seen only as a cost center function anymore!\u201D (Collin,\r\n2015, 30). Digitalization seems to be \u201Cone of the major trends changing society and business. Digitalization\r\ncauses changes for companies due to the adoption of digital technologies in the organization or in the operation\r\nenvironment\u201D (Parvianien et al., 2017, 63).\r\nAccording to M\u00E4enp\u00E4\u00E4 and Korhonen, \u201Cfrom the retail business point of view, the \u2018digitalization of the\r\nconsumer\u2019 is of essence. People are increasingly able to use digital services and are even beginning to expect\r\nthem. To a certain extent, this is a generational issue. The younger generations, such as Millennials, are\r\ngrowing up with digitalization and are eagerly in the forefront of adopting new technology and its affordances\u201D\r\n(M\u00E4enp\u00E4\u00E4\u2013Korhonen, 2015, 90). In 2018, Toni Ryyn\u00E4en and Torsti Hyyryl\u00E4inen, members of the Helsinki\r\nInstitute of Sustainability Science at the Faculty of Agriculture and Forestry, published an article seeking to\r\nfill the gap between the digitalization process and digital humanities, by focusing on the concern for \u201Cnew\r\nforms of e-commerce, changing consumer roles and the digital virtual consumption\u201D (Ryyn\u00E4en \u2013 Hyyryl\u00E4inen,\r\n2018, 1). In this process, the role of digital humanities is described in a way that is quite hard to recognize for\r\nDHers, at least for those not involved in digital social sciences: \u201CA challenge for digital humanities research is\r\nhow to outline the most interesting phenomena from the endless pool of consumption activities and practices.\r\nAnother challenge is how to define a combination of accessible datasets needed for solving the chosen research\r\ntasks\u201D (Ryyn\u00E4en \u2013 Hyyryl\u00E4inen, 2018, 1).\r\nIn light of such clear descriptions of what \u201Cdigitalization\u201D means for business and economy, digital\r\nhumanities scholarship demonstrates a deafening silence about this notion. The 2004 and 2016 editions of the\r\nreference work Companion to Digital Humanities do not mention the word. In the established series Debates\r\nin the Digital Humanities, one finds one occurrence in the five volumes, under the pen of Domenico Fiormonte\r\n(2016). As a third example, the collected essays Text and Genre in Reconstruction: Effects of Digitalization\r\non Ideas, Behaviours, Products and Institutions, edited by Willard McCarty (2010), can only surprise the\r\nreader: indeed, \u201Cdigitalization\u201D stands in the title, but the word is then totally absent from the volume. When\r\nquestioned about this discrepancy, McCarty answered that the publisher had requested to have this word in the\r\ntitle. This request has led to a damaging side effect in terms of Google searches: if one searches for\r\n\u201Cdigitalization\u201D and \u201Cdigital humanities\u201D, one gets several book titles that do not contain no mention of this\r\nword other than a reference to Text and Genre\u2019s title. It is also the case in my 2019 book Ecritures digitales.\r\n\r\nEntry \u201Cdigitization\u201D in Wikipedia: https:\/\/en.wikipedia.org\/wiki\/Digitization; entry \u201Cdigitalize\u201D in the\r\nCollins Dictionary online: https:\/\/www.collinsdictionary.com\/dictionary\/english\/digitalize. All\r\nhyperlinks have been last checked on 30\/11\/19.\r\n2\r\n\r\nEntry \u201Cdigitalization n.2\u201D, OED, https:\/\/www.oed.com\/view\/Entry\/242061\r\nEntry \u201Cdigitalization n.1\u201D, OED, https:\/\/www.oed.com\/view\/Entry\/52616: \u201Cthe administration of digitalis or any of\r\nits constituent cardiac glycosides to a person or animal, esp. in such a way as to achieve and maintain optimum blood levels of the\r\ndrug. Also: the physiological condition resulting from this\u201D.\r\n\r\n3\r\n4\r\n\r\nEntry \u201Cdigitalization n.2\u201D in the Oxford English Dictionary online: https:\/\/www.oed.com\/view\/Entry\/24206\r\nEntry \u201Cdigital transformation\u201D in Wikipedia:\r\nhttps:\/\/en.wikipedia.org\/wiki\/Digital_transformation#Digitization_(of_information)\r\n5\r\n\r\n6\r\n\r\n68\r\n\r\n\fDigital writing, digital Scriptures: the unique occurrence of \u201Cdigitalization\u201D occurs in my reference to\r\nMcCarty\u2019s collected essays (Clivaz, 2019).\r\nOne can sometimes meet infrequent uses of digitalization in digital humanities, such as a 2013 article by\r\nAmelia Sanz. She uses the word to describe Google Books and the Hathi Trust\u2019s effect on Spanish literature:\r\n\u201CDigital Libraries as Google Books or Hathi Trust include numerous works belonging to our study period\r\namong its digitalized collections in US universities, because most of these forgotten authors make part of the\r\nSpanish diaspora after the Civil War (1936-39) and during the subsequent dictatorship (1939-1975). In fact,\r\nEuropean copyright legislation has made Google digitalize only works prior to 1870 in Spain, and,\r\nunfortunately for Spanish researchers, those works appear to be in \u2018limited access\u2019 due to the existing\r\ndiffusion\/circulation rights, but available in \u2018full text\u2019 mode for researchers located in the US\u201D (Sanz, 2013,\r\nn.p.). The two italicized words are the unique occurrences of digitalization vocabulary in an article focused on\r\nthe effects of digitization. When asked about her use of these two words, Sanz answered that it was probably\r\na misuse of language, since she is not a native English speaker.\r\nUsually in digital humanities scholarship, one speaks about \u201CHumanities digitized\u201D (Shaw, 2012)7, and the\r\nmutation to the digital sphere is seen as a pre-step before the processes of interpretation.8 Uses of digitalization\r\nand cognate terms remain rare, like Domenico Fiormonte, who is also a non-native English speaker and the\r\nonly one to use digitalization in the series Debates in Digital Humanities: \u201CIn the last ten years, the extended\r\ncolonization, both material and symbolical, of digital technologies has completely overwhelmed the research\r\nand educational world. Digitalization has become not only a vogue or an imperative, but a normality. In this\r\nsort of \u2018gold rush\u2019, the digital humanities perhaps have been losing their original openness and revolutionary\r\npotential\u201D (Fiormonte, 2016, n.p.). Fiormonte compares digitalization to a colonization process: if there is\r\nsome consciousness of the digitalization vocabulary in humanities, it can be indeed found in research about\r\ncultural diversity and colonialism, such as in a 2007 article by Maja van der Velden, \u201CInvisibility and the Ethics\r\nof the Digitalization: Designing so as not to Hurt Others.\u201D\r\nVan der Velden studies \u201Cthe designs of Indymedia, an Internet-based alternative media network, and\r\nTAMI, an Aboriginal database, [...] informed by the confrontations over different ways of knowing\u201D (2007,\r\n81). She points to the fact that, \u201Cif we understand knowledge not as a commodity but as a process of knowing,\r\nsomething produced socially, we must ask about the nature of digitalization itself. As the Aboriginal elders\r\nsay, \u2018Things are not real without their story\u2019\u201D (2007, 82). She documents in this way two examples of nonWestern digital projects, in which the diversity of the source codes and standards has led to recurrent\r\nnegotiations: \u201Cthe confrontations over issues of privacy and control resulted in different ways of organizing\r\naccess and information management\u201D (2007, 89). Van der Velden\u2019s article allows one to understand, from a\r\nhumanist point of view, what is at stake in the concept of digitalization, a perspective that the next section\r\ndevelops. But it should be underlined that, even in this article pointing to cultural and digital control issues,\r\ndigitalization is not discussed as such. The apparent lack of awareness about this binomial vocabulary and its\r\nimplication for DH scholarly literature appears to be a real blind spot that section 4 considers.\r\n\r\n3 Claiming a Critical Use of Digitalization in Humanities\r\nIn their overview article, Brennen and Kreiss give a general definition of \u201Cdigitalization\u201D similar to the one\r\npresented in Section 2: \u201CWe [...] define digitization as the material process of converting analog streams of\r\ninformation into digital bits. In contrast, we refer to digitalization as the way many domains of social life are\r\nrestructured around digital communication and media infrastructures\u201D (Brennen\u2013Kreiss, 2016, 1). They\r\nusefully remind us that \u201Cdigitization is a process that has both symbolic and material dimensions\u201D (2016, 2),\r\nand that \u201Canalog and digital media, [...] all forms of mediation necessarily interpret the world\u201D (2016, 3). The\r\nauthors also consider that \u201Cthe first contemporary use of the term \u2018digitalization\u2019 in conjunction with\r\ncomputerization appeared in a 1971 essay first published in the North American Review. In it, Robert Wachal\r\ndiscusses the social implications of the \u2018digitalization of society\u2019 in the context of considering objections to,\r\nand the potential for, computer-assisted humanities research. From this beginning, writing about digitalization\r\nhas grown into a massive literature\u201D (2016, 5). The reference to Wachal\u2019s article is a very interesting one, and\r\nit deserves more attention than the co-authors devote to it. Moreover, they omit any reference to Maja van der\r\nVelden\u2019s article or to similar approaches in Brennen and Kreiss\u2019s article. The \u201Cwinners\u201D of their digitalization\r\n7\r\nOne can also see uses of digitalization in the humanities in archaeology, notably in conjunction with 3D discussion (Ercek \u2013Viviers\r\n\u2013Warz\u00E9e, 2009).\r\n8\r\nSee Earhart \u2013 Taylor (2016): \u201COur White Violence, Black Resistance project merges foundational digital humanities approaches with\r\nissues of social justice by engaging students and the community in digitizing and interpreting historical moments of racial conflicts.\u201D\r\n\r\n69\r\n\r\n\fdefinition are scholars from the vein of Manuel Castells, who argues that \u201Ctechnology is society, and society\r\ncannot be understood or represented without its technological tools\u201D (Brennen\u2013Kreiss, 2016, 5).\r\nTo get a deeper understanding of the critical potential of digitalization, it is worth reading Wachal\u2019s 1971\r\narticle. He uses digitalization in just one sentence: \u201CThe humanist\u2019s fears are not entirely without foundation,\r\nand in any case, as a humane man he naturally fears the digitalization of the society. He doesn\u2019t like to be\r\ncomputed. He doesn\u2019t want to be randomly fingered by a credit card company computer\u201D (1971, 30). The entire\r\narticle is an ironic confrontation between the habits of a humanist scholar and what a programmer and a\r\ncomputer could do for humanities. As a computer programmer teacher himself, Wachal remembers the term\r\ncoined by Theodor Nelson, \u201Ccybercrud\u201D: \u201Cputting things over on people [by] saying using computers. When\r\nyou consider that this includes everything from intimidation (\u2018Because we are using automatic computers, it\r\nis necessary to assign common expiration dates to all subscriptions\u2019) to mal implementation (\u2018You\u2019re going to\r\nhave to shorten your name - it doesn\u2019t fit in to the computer\u2019), it may be that cybercrud is one of the most\r\nimportant activities of the computer field\u201D (1971, 30). In other words, computer scholars have a clear\r\nawareness about their world, as Nelson and Wachal after him demonstrate. After this captatio benevolentiae,\r\nWachal raises what is for him the main issue with the humanist point of view on computing: \u201CDare we hope\r\nthat the day has come when humanists will begin asking some new questions?\u201D (1971, 33), referring also to\r\nartificial intelligence (1971, 31). His \u201Cpersonal view\u201D, as announced in the article title, is an open call that is\r\nstill worth humanist scholars\u2019 attention.\r\nThe complex elements of the discussion of the digitization\/digitized vs digitalization\/digitalized divide\r\nindicates that it is surely time for DHers to pay attention to this binomial expression, so successfully deployed\r\nin business or economy that a publisher can get it in a title of collected essays that does not contain the word\r\ndigitalization at all. It is time to form an understanding of digitalization that still denounces \u201Ccybercrud\u201D when\r\nneeded, or helps us to pay attention to \u201Cthe confrontations over issues of privacy and control resulted in\r\ndifferent ways of organizing access and information management\u201D (van der Velden, 2007, 89). To express it\r\nin an electronic vocabulary, Brennen and Kreiss present a \u201Cpath of least resistance\u201D to the definition of\r\ndigitalization, according to the path describing the third potential state of an electronic circuit (open, closed,\r\nor not working), because electricity follows the \u201Cpath of least resistance.\u201D 9 But it is a core skill of the\r\nhumanities to renounce the paths of least resistance and to wrestle with words, concepts, and realities. In that\r\nperspective, the last Section will develop some tracks to further the debate.\r\n\r\n4 The effect of the \u201Cdigitalization\u201D perspective\r\nThe binomial expression \u201Cdigitization\u201D versus \u201Cdigitalization\u201D enters in the international debate through the\r\nEnglish language. Such a distinction does not exist in French, Italian, or German, for example. But the inquiry\r\nof this article demonstrates that it this concept is worthy of exploration in an effort to grasp what is at stake in\r\nan explicit way in the English language. It represents surely one further argument in favor of a multilingual\r\napproach to digital epistemology, like the one developed in Digital writing, digital Scriptures (Clivaz, 2019).\r\nI firstly underline how striking it is that even in the few occurrences where humanist scholars consciously\r\nuse the term \u201Cdigitalization\u201D (van der Velden, Fiormonte), it is not discussed per se: a blind point exists in the\r\nscholarly discussion apart of Brennen and Kreiss\u2019s article. After all, the first use of \u201Cdigitalization\u201D in relation\r\nto the computer sphere was by a programmer (Wachal, 1971), but nowadays its use in critical discussion is\r\nmainly found under the pen of scholars outside of humanities who make claims about the \u201Cessence\u201D of \u201Cthe\r\n\u2018digitalization of the consumer\u2019\u201D (M\u00E4enp\u00E4\u00E4\u2013Korhonen, 2015, 90; quoted in Section 2). In light of this\r\nconsumerist perspective, DH scholars are generally confident in the traditional critical impact of their\r\nmethodologies and knowledge. Alan Liu, for example, writes that \u201Cthe digital humanities serve as a shadow\r\nplay for a future form of the humanities that wishes to include what contemporary society values about the\r\ndigital without losing its soul to other domains of knowledge work that have gone digital to stake their claim\r\nto that society\u201D (2013, 410). In the same line, the HERA 2017 call hopes that the humanities, when digitized,\r\nwill be able \u201Cto deepen the theoretical and empirical cultural understanding of public spaces in a European\r\ncontext.\u201D10\r\nBut it could secondly be argued that the blind point of the absent discussion about digitization\/digitalization\r\ndemonstrates an overconfidence of the digital humanities in its capacity to not lose the soul of the humanities\r\nin digital networks. Other voices are indeed more sensitive to the limitations imposed on humanities research\r\nSee \u201CPath of Leaf Resistance\u201D, Wikipedia, https:\/\/en.m.wikipedia.org\/wiki\/Path_of_least_resistance\r\nSee \u201CHERA Public Spaces\u201D, 31.08.17, http:\/\/heranet.info\/2017\/08\/31\/hera-launches-its-fourthjoint-research-programme-public-spaces\/\r\n9\r\n\r\n10\r\n\r\n70\r\n\r\n\fby digital constraints, as we have seen with Maja van der Velden: even if she uses the word \u201Cdigitalization\u201D\r\nwithout discussing it, her article clearly points to digital control issues in the practice of building a database or\r\na virtual research environment. From a more general and theoretical point of view, James Smithies strongly\r\nunderlines in his book The Digital Humanities and the Digital Modern the same issues, even if the word\r\ndigitalization is totally absent in it. He suggests that \u201Cour digital infrastructure [\u2026] has grown opaque and has\r\nextended into areas well outside scholarly or even governmental control\u201D (2017, 11). His discourse becomes\r\novertly political when he affirms the existence of a \u201Cpoint of entanglement between the humanities and\r\nneoliberalism, implicating digital humanists and their critics in equal measure\u201D (2017, 218).\r\nWe are probably reaching here the main root of the silence about the digitization\/digitalization challenge\r\nin DH debates: this binomial expression points to the political dimension of the digital revolution in\r\nhumanities, to its economic and institutional implications, something that we prefer to let aside, consciously\r\nor unconsciously. This fear is also described by Wachal: \u201CThe humanist\u2019s fears are not entirely without\r\nfoundation, and in any case, as a humane man he naturally fears the digitalization of the society\u201D (1971, 30;\r\nquoted in Section 3). Listening to Wachal, and almost fifty years later to Smithies, can begin to lead us beyond\r\nthe \u201Cpath of leaf resistance\u201D of Brennen and Kreiss. We should consider digitalization rather as the top of a\r\nmountain: it can be reached only through the via ferrata of the debates about cultural and multilingual diversity,\r\nabout multiple source codes and standards, a multiplicity that preserves, at the end, diversity in humancomputing knowledge productions.\r\nMoreover, we are probably reaching right now the start of the DH awareness of this linguistic debate. As I\r\nend this article, I have opened the debate in the list Humanist Discussion Group and Simon Tanner has signaled\r\nhis interest in the point, referring to Brennen and Kreiss\u2019 definition: \u201CI have found the difference to be\r\nsignificant enough to seek to define it for my current book and in the past it has been a source of confusion or\r\nconflation that has not been helpful. I make it very clear to our students in the Masters of Digital Humanities\r\nor the MA Digital Asset and Media Management that they should not use the interchangeably\u201D (Tanner, 2019).\r\nThird, since the binomial expression digitization\/digitalization is a vehicle for its own impact and meaning\r\nwithin the DH epistemology, is it possible to tie these concepts to the general challenge raised by the AIUCD\r\n2020 call for papers? Notably, this discussion raises the following questions: \u201Cis it still necessary to talk about\r\n(and make) a distinction between \u2018traditional\u2019 humanists and \u2018digital\u2019 humanists? Is the term \u2018Digital\r\nHumanities\u2019 still appropriate or should it be replaced with \u2018Computational Humanities\u2019 or \u2018Humanities\r\nComputing\u2019? Is the computational dimension of the research projects typically presented at AIUCD\r\nconferences that methodologically distinctive?\u201D11 At the root of these problems stands of course an important\r\ndebate in Italian speaking DH, present in the name itself of the national DH organization, the AIUCD. This\r\nname mentions \u201CHumanities Computing\u201D (informatica umanistica) and \u201Cdigital culture\u201D (cultura digitale):\r\nAIUCD - Associazione per l\u2019Informatica Umanistica e la Cultura Digitale.12 But beyond this specific Italian\r\nperspective, the importance of collaboration between DHers and other humanist scholars concerns all of us.\r\nThe dialectic between Humanities Computing and Digital Humanities will in all cases remain in the\r\nhistorical memory of the DH development. But I am personally not convinced that a \u201Cstep back\u201D in the form\r\nof a return to Humanities Computing, motivated by a desire to keep all the humanists together under the banner\r\nof the informatica umanistica, is viable. Why? When the Harvard Magazine published in 2012 one of its first\r\narticles about the digital humanities, it was entitled \u201CHumanities Digitized\u201D (Shaw, 2012). It has always been\r\nmeaningful for me to think in that direction. As I have argued elsewhere in detail, we could \u201Cbegin to speak\r\nabout the digitized humanities, or simply about humanities again, instead of digital humanities. Such an\r\nevolution might occur, if one looks at the evolution of the expression \u2018digital computer\u2019 which was in common\r\nusage during the fifties, but it has been now replaced by the single latter word \u2018computer\u2019 (Williams, 1984,\r\n310; Dennhardt, 2016). When humanities finally become almost entirely digitized, perhaps it is safe to bet that\r\nwe will once again speak simply about humanities in English or about humanit\u00E9s in French, thus making this\r\noutmoded word again meaningful through the process of cultural digitization\u201D (Clivaz, 2019, 85\u201386).\r\nAccording to this perspective, the debate between \u201Chumanities digitized\u201D or \u201Chumanities digitalized\u201D, with\r\nall its cultural, economic, material, institutional and political dimensions, could signal a third step after\r\nHumanities Computing and Digital Humanities. This third step would stand at the crossroads where all\r\nhumanists could meet up again, in an academic world definitively digitized, but hopefully not totally\r\ndigitalized. It is up to all of us to decide if, in the third millennium, Humanities will be digitized or digitalized.\r\n11\r\n\r\nSee \u201CConvegno annuale dell'Associazione per l'Informatica Umanistica e la Cultura Digitale. Call for\r\npapers\u201D, https:\/\/aiucd2020.unicatt.it\/aiucd-call-for-papers-1683.\r\n12\r\nSee AIUCD, www.aiucd.it.\r\n71"
	},
	{
		"id": 13,
		"title": "La geolinguistica digitale e le sfide lessicografiche nell‚Äôera delle digital humanities: l‚Äôesempio di VerbaAlpina",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Beatrice Colcuc"
		],
		"body": "\r\nTrattare i dati (lessicografici) nell\u2019era delle digital humanities\r\n\r\nLa ricerca dell\u2019era pre-digitale \u00E8 stata contraddistinta da modalit\u00E0 di concezione progettuale fortemente\r\nindividuali: la raccolta, l\u2019analisi e l\u2019illustrazione dei dati venivano effettuate da persone (o gruppi di persone)\r\nche operavano singolarmente. La comunicazione scientifica si compiva attraverso le pubblicazioni dei libri\r\ncartacei, i quali venivano conservati in luoghi circoscritti come ad esempio le biblioteche, e ciascuno studio\r\nrappresentava un progetto concluso in s\u00E9 (cfr. Krefeld, 2016). Inoltre, molti dati rimanevano nelle mani degli\r\nstessi ricercatori che li avevano raccolti e, in generale, la comunit\u00E0 scientifica non compiva molti sforzi per\r\nmettere a disposizione del grande pubblico i dati provenienti dai diversi progetti scientifici.\r\nOrmai, l\u2019avvento delle nuove tecnologie non \u00E8 pi\u00F9 un fenomeno di recente datazione. Il passaggio dal\r\ncartaceo al digitale \u00E8 avvenuto in maniera sempre pi\u00F9 repentina, contribuendo al mutamento dell\u2019approccio\r\nscientifico nei confronti delle modalit\u00E0 di ricerca. La rivoluzione digitale ha rinsaldato l'idea della condivisione\r\ne, allo stesso tempo, provveduto a fornire gli strumenti necessari per favorire e facilitare l\u2019interscambio di dati\r\ncome pure, pi\u00F9 in generale, l\u2019interazione tra diversi progetti. Ciononostante, la creazione di una rete di progetti\r\ne la condivisione dei relativi dati non sono fornite da una mera digitalizzazione degli strumenti di ricerca. La\r\ncollaborazione rappresenta l\u2019essenza primaria del fare scienza, poich\u00E9 \u00E8 sulla base delle conoscenze gi\u00E0\r\npresenti che si costruisce il progresso. Per operare in maniera collettiva nell\u2019era digitale \u00E8 per\u00F2 necessario che\r\ni dati di ricerca soddisfino alcuni requisiti fondamentali. In primo luogo i dati devono essere strutturati, descritti\r\n\r\n74\r\n\r\n\fed eventualmente etichettati in maniera tale da prestarsi a essere maneggiati in sedi esterne al loro progetto\r\noriginario e devono poter continuare a essere accessibili anche in un momento successivo all\u2019eventuale\r\nchiusura del progetto. In questo contesto si inserisce l\u2019idea di Web Semantico (e successivamente dei Linked\r\nOpen Data), nata con l\u2019obiettivo di rimodellare l\u2019ambiente virtuale di internet. Numerosi sono i progetti che si\r\ncollocano all\u2019interno di questo pensiero: essi formano la cosiddetta Linked Open Data Cloud (cfr. Cyganiak e\r\nJentzsch, 2007 -) e costituiscono, al contempo, una comunit\u00E0 interconnessa attraverso relazioni basate sui loro\r\ninsiemi di dati (cfr. Bizer et al. 2009, 154).\r\nLe esigenze di condivisione e connessione dei dati sul web sono inoltre state formulate in maniera esplicita\r\nnel 2016, quando un grande numero di ricercatori provenienti da diversi Paesi ha pubblicato le linee guida per\r\nla gestione moderna dei dati di ricerca (cfr. Wilkinson, Dumontier et al., 2016). Tali raccomandazioni sono\r\nstate racchiuse nell\u2019acronimo FAIR, una sigla che raccoglie i quattro princ\u00ECpi fondamentali sui quali\r\ndovrebbero essere basate la comunicazione e la cooperazione scientifiche nell\u2019era digitale. Secondo tali\r\nprinc\u00ECpi, i dati della ricerca dovrebbero essere rintracciabili (findable), accessibili (accessible), interoperabili\r\n(interoperable) e riutilizzabili (reusable). Per essere rintracciabili, i progetti ai quali i dati appartengono,\r\ndevono essere reperibili attraverso portali centrali quali, ad esempio, i cataloghi delle biblioteche. I dati di\r\nricerca che non sono soggetti ad alcuna restrizione giuridica (come potrebbero essere ad esempio i dati\r\nstrettamente personali) devono essere messi a disposizione del grande pubblico e rinunciare, di conseguenza,\r\nal diritto d\u2019autore. Al fine di poter essere interoperabili, inoltre, i dati devono essere innanzitutto scissi,\r\nsuccessivamente strutturati ed essere descritti in maniera precisa. Il riutilizzo dei dati si rende infine possibile\r\nattraverso una corretta applicazione dei tre princ\u00ECpi precedenti: si tratta di princ\u00ECpi estensibili che non si lasciano\r\nmettere in contrapposizione l\u2019un l\u2019altro (cfr. Force11, 2011-2017; GoFair, 2011-2017; L\u00FCcke, 2018).\r\nLe possibilit\u00E0 offerte dalla digitalizzazione in termini tecnici hanno altres\u00EC consentito di valutare da un nuovo\r\npunto di vista una delle questioni storiche relative al trattamento dei dati lessicografici. In linea di massima, le\r\nopere lessicografiche possono essere strutturate in maniera semasiologica (le parole di un dato idioma sono\r\nelencate seguite dal loro significato) oppure onomasiologica (si descrivono i significati e vi si collegano le\r\ndiverse parole che ad essi conducono). Nell\u2019era analogica tali opere erano strutturate secondo uno o secondo\r\nl\u2019altro modo: in ambito romanzo, si ricordino, tra gli altri, l\u2019atlante linguistico ed etnografico dell\u2019Italia e della\r\nSvizzera meridionale (AIS), di stampo onomasiologico, oppure, il Dicziunari Rumantsch Grischun (DRG),\r\nstrutturato, invece, in maniera semasiologica. Una concezione in entrambi i sensi non era possibile a causa di\r\nlimiti pratici imposti dalle modalit\u00E0 di pubblicazione del passato, mentre oggigiorno, le possibilit\u00E0 fornite dalla\r\ndigitalizzazione offrono nuovi strumenti e la concezione di un\u2019opera lessicografica che vada in due direzioni\r\n\u00E8 realizzabile. Ciononostante, bench\u00E9 l\u2019aspetto tecnico non rappresenti pi\u00F9 alcun problema alla messa in pratica\r\ndi tale approccio bidirezionale, le sfide si aprono soprattutto dal punto di vista contenutistico, come si evincer\u00E0\r\ndai capitoli che seguiranno.\r\nLa compilazione digitale di opere lessicografiche quali i dizionari sembra essere oggi un\u2019attivit\u00E0\r\nrelativamente consolidata. Non sono rari i dizionari che offrono la possibilit\u00E0 di essere consultati in rete, anche\r\nse, alcuni di essi, non si sono mai realmente distanziati da una concezione cartacea. Tali opere, presentano\r\nancora un grande margine di sviluppo e ampliamento per potersi definire digitalizzate in modo interoperabile\r\nsecondo i gradi definiti in L\u00FCcke (2016). A titolo illustrativo, ma non esaustivo, si pensi alla versione online\r\ndel Romanisches Etymologisches W\u00F6rterbuch (Meyer-L\u00FCbke, 1935): il contenuto dell\u2019opera \u00E8 messo a\r\ndisposizione online in formato PDF, ma, ai sensi della digitalizzazione, in essa potrebbero essere implementate\r\ndiverse funzioni, tra cui, ad esempio, la ricerca di singoli lemmi. Lo scopo di una lessicografia basata sul web\r\nnon si limita alla mera presentazione virtuale di una determinata opera. Lo sviluppo digitale \u00E8 bens\u00EC\r\nrappresentato da un pi\u00F9 ampio tentativo di costituzione di reti lessicali e semantiche messe in relazione tra di\r\nloro. Progetti volti a fornire tali interconnessioni sono stati iniziati gi\u00E0 verso la met\u00E0 degli anni Ottanta. A titolo\r\nesemplificativo si pensi a WordNet, il database lessicale per la lingua inglese, ma anche a EuroWordNet nato\r\nnegli anni Novanta come rete semantica per le lingue europee (cfr. Fellbaum 2006, 665, 669). Tali progetti\r\ncostituiscono un primo tentativo di strutturare il materiale in maniera semantica e non solo lessicale come si \u00E8\r\nsoliti fare nei dizionari cartacei e, soprattutto, si inseriscono nel panorama dei lavori relativi all\u2019elaborazione\r\ndel linguaggio naturale multilingue. In tempi pi\u00F9 recenti si colloca la concezione di\r\nBabelNet (https:\/\/babelnet.org\/), una rete semantica multilingue, automatizzata e di ampia\r\ncopertura, ovvero un dizionario enciclopedico costituito unendo il contenuto lessicale di WordNet al\r\nsapere enciclopedico di Wikipedia attraverso processi automatizzati di integrazione dei contenuti di\r\nambedue i database (cfr. Navigli e Ponzetto, 2010, 216).\r\n\r\n75\r\n\r\n\f2 VerbaAlpina: geolinguistica e lessicografia digitali\r\nMentre la digitalizzazione dei primi dizionari e corpora lessicali si colloca tra gli anni Ottanta e gli anni\r\nNovanta (cfr. Chiari 2012, 98), pi\u00F9 recente risulta invece essere il passaggio dal cartaceo al digitale per quanto\r\nriguarda gli atlanti linguistici. Relativamente all\u2019area alpino-romanza, diversi atlanti sono oggi disponibili in\r\nrete in formati PDF o JPG ma non hanno percorso tutte le tappe del passaggio dal cartaceo al digitale (cfr.\r\nL\u00FCcke, 2016; Knapp, 2017). \u00C8, a titolo esemplificativo, il caso di NavigAIS (Tisato, 2009-2018) all\u2019interno del\r\nquale, nonostante la sua presenza su internet sia lodevole, potrebbero essere implementate diverse funzionalit\u00E0,\r\ntra le quali ad esempio la rintracciabilit\u00E0 di forme attestate oppure una visualizzazione quantificata dei dati, la\r\npossibilit\u00E0 di consultare singoli gruppi di dati linguistici in prospettiva onomasiologica o semasiologica, come\r\npure l\u2019esportazione dei dati.\r\nAlle esigenze della ricerca lessicografica e atlantistica in chiave moderna, cerca di dare una risposta il progetto\r\nVerbaAlpina dell\u2019Universit\u00E0 Ludwig-Maximilian di Monaco di Baviera, nato nel 2014 con l\u2019intento di\r\nindagare lo spazio linguistico delle Alpi nella sua storica unit\u00E0 linguistico-culturale (cfr. Krefeld e L\u00FCcke, 2014\r\n-). Fin dalla sua concezione, completamente digitale e pensata non solo sul web, ma per il web, il progetto\r\nVerbaAlpina ha operato nel pieno rispetto dei princ\u00ECpi FAIR (che sarebbero stati formulati solamente due anni\r\ndopo) e promosso un\u2019idea innovativa di lessicografia e atlantistica linguistica (cfr. Krefeld, 2018). Oltre\r\nall\u2019aspetto linguistico, una parte consistente del progetto \u00E8 specificatamente dedicata alla creazione di\r\nstrumenti per la gestione dei dati di ricerca nei progetti digitali e pensati per il web.\r\n2.1 Concezione e presentazione del progetto\r\nNucleo centrale dell\u2019attivit\u00E0 di VerbaAlpina \u00E8 la raccolta strutturata di una precisa cornice semasiologica e\r\nonomasiologica, costituita dagli ambiti terminologici alpini, alla quale \u00E8 possibile accedere attraverso una\r\ncartina interattiva. La ricerca prende in esame il lessico dialettale degli idiomi alpini, in modo particolare le\r\nparole relative agli ambiti della natura (flora, fauna, formazioni paesaggistiche), della cultura alpina storica\r\n(lavorazione del latte) e di quella corrente (turismo). I dati raccolti e analizzati da VerbaAlpina sono puramente\r\ndialettali, mentre i termini relativi alle lingue standard non sono presi in considerazione. Diversi sono gli scopi\r\nperseguiti da VerbaAlpina: in primo luogo il progetto intende documentare e analizzare in prospettiva\r\nlinguistica e storico-etimologica la regione alpina, uno spazio fortemente frammentato per quanto riguarda le\r\nlingue e i dialetti ivi parlati. I confini dell\u2019area di ricerca sono definiti dalla Convenzione delle Alpi\r\n(http:\/\/www.alpconv.org\/), un trattato tra i Paesi del territorio alpino atto a promuovere e sviluppare questa\r\narea montana in diversi ambiti (cfr. L\u00FCcke 2018a).\r\nI dati sono forniti dagli atlanti linguistici e dai dizionari relativi all\u2019area di ricerca, analogici o digitali,\r\npubblicati nel corso del tempo. In un primo momento, il materiale linguistico georeferenziato proveniente dalle\r\nfonti affronta un percorso di digitalizzazione attraverso un sistema di trascrizione basato esclusivamente sui\r\ncaratteri ASCII (cfr. Krefeld e L\u00FCcke, 2016). In un secondo momento, il materiale trascritto viene sottoposto\r\na una tokenizzazione, un processo che separa in singoli token (parole) il materiale trascritto in un momento\r\nprecedente. L\u2019interesse principale del progetto si esplica nella presentazione dei punti di coesione tra i diversi\r\nidiomi e le diverse famiglie linguistiche presenti sul territorio alpino soprattutto in prospettiva lessicologica.\r\nPer l\u2019adempimento di tale scopo, il materiale linguistico viene raggruppato in tipi di base, ossia secondo la\r\nradice lessicale comune a diverse attestazioni che possono appartenere anche a diverse famiglie linguistiche1\r\ne in tipi morfolessicali, vale a dire in forme di un solo tipo di base, appartenenti a un\u2019unica famiglia linguistica\r\nche presentano caratteristiche grammaticali comuni quali la parte del discorso, il genere e gli elementi di\r\nformazione delle parole (cfr. Krefeld e L\u00FCcke, 2016a). Ad esempio, il tipo di base latino *CASEU(M) \u2018formaggio\u2019\r\n\u00E8 presente sia in area linguistica germanica, sia in area romanza nelle forme deu. K\u00E4se e ita. cacio, i quali, a\r\nloro volta, rappresentano due tipi morfolessicali differenti.\r\nI dati linguistici storici rilevati dagli atlanti e dai dizionari sono completati attraverso una piattaforma\r\ndi crowdsourcing\r\nsviluppata\r\nall\u2019interno\r\ndel\r\nprogetto\r\n(https:\/\/www.verbaalpina.gwi.uni-muenchen.de\/en\/?page_id=1741&db=191). La piattaforma si rivolge\r\ndirettamente ai parlanti dei dialetti delle Alpi al fine di raccogliere materiale linguistico attuale e poter\r\nosservare lo spazio alpino anche in prospettiva diacronica. Una volta aperta la pagina, viene chiesto agli\r\nutenti di scegliere una lingua di navigazione tra quelle proposte (francese, italiano, sloveno, tedesco).\r\nSuccessivamente, vengono mostrate le istruzioni per l\u2019utilizzo\r\n1\r\n\r\nIn molti casi non \u00E8 dato sapere se la radice lessicale comune a diverse parole sia da ricollegare allo stesso sostrato linguistico oppure\r\na un contatto linguistico pi\u00F9 recente. Per questo motivo VerbaAlpina utilizza il termine \u201Ctipo di base\u201D, in quanto \u201Cetimo\u201D si riferisce\r\ngeneralmente allo strato linguistico immediatamente precedente (cfr. Krefeld e L\u00FCcke 2016a).\r\n76\r\n\r\n\fdella piattaforma e gli utenti sono invitati a inserire l\u2019idioma alpino di cui essi sono i parlanti. Nel caso in cui\r\nun idioma non sia presente nella lista, gli utenti hanno la possibilit\u00E0 di segnalarlo direttamente alla redazione\r\nche provveder\u00E0 a inserirlo. Innanzitutto, gli utenti sono chiamati a inserire il nome del comune di cui\r\npadroneggiano l\u2019idioma. Cliccando sull\u2019apposito campo \u201Cconcetto\u201D, appare una lista con tutti i concetti\r\nesistenti nella banca dati di VerbaAlpina. Da qui, gli utenti possono scegliere per quali concetti inviare parole.\r\nI dati raccolti attraverso il crowdsourcing vengono trattati alla pari dei dati provenienti dai dizionari e dagli\r\natlanti, con l\u2019unica differenza che non sono sottoposti al processo di trascrizione. Per questi dati, la\r\ntokenizzazione avviene solamente qualora si tratti di un sintagma costituito da pi\u00F9 elementi. La tipizzazione di\r\nqueste parole avviene alla stregua dei dati raccolti dai dizionari e dagli atlanti linguistici. A livello di database,\r\nle singole attestazioni provenienti dal crowdsourcing ricevono un identificatore e sono collegate ai concetti di\r\ncui rappresentano le diverse forme dialettali. Successivamente al trattamento strutturato, i dati analizzati\r\nda VerbaAlpina possono essere visualizzati sulla cartina interattiva (https:\/\/www.verbaalpina.gwi.uni-muenchen.de\/it\/?page_id=27&db=191). Tramite l\u2019utilizzo di filtri\r\nappropriati, i dati sono accessibili in prospettiva onomasiologica (si rappresentano tutte le attestazioni\r\nlinguistiche collegate a un determinato concetto) oppure semasiologica (si rappresentano i concetti legati a\r\nun preciso tipo morfolessicale). Inoltre, la visualizzazione pu\u00F2 essere impostata in modalit\u00E0 qualitativa,\r\nattraverso la quale \u00E8 possibile evincere la distribuzione generale delle attestazioni linguistiche, oppure\r\nquantitativa, ossia indicante il numero di dati all\u2019interno di una certa area. I dati possono essere visualizzati\r\nin prospettiva geografico-fisica oppure astratta: la prima mostra i dati distribuiti su una cartina fisica,\r\nattraverso la seconda, invece, i dati sono presentati su una mappa a nido d\u2019ape.\r\nParallelamente all\u2019attivit\u00E0 linguistica, il progetto ha profuso un grande impegno nella gestione dei dati\r\ndigitali con l\u2019obiettivo ultimo di promuovere la sostenibilit\u00E0 e la durabilit\u00E0 del progetto anche dopo la sua\r\nchiusura definitiva. La descrizione di una parte delle attivit\u00E0 che sono state intraprese in questo senso avviene\r\nnel corso del presente contributo. VerbaAlpina si impegna a utilizzare strumenti tecnologici adatti al web e\r\napplicabili al pensiero open source, come ad esempio Wordpress per la piattaforma centrale, Leaflet per la\r\ncartina interattiva e le banche dati relazionali MySQL. Dato che VerbaAlpina intende altres\u00EC fungere da creatore\r\ndi nessi tra istituzioni e progetti gi\u00E0 esistenti al fine di interscambiare, integrare e completare i dati linguistici\r\nriguardanti l\u2019area alpina, per i diversi partner sono messe a disposizione banche dati all\u2019interno delle quali i\r\nprogetti cooperanti possono inserire i loro dati e collegarli cos\u00EC a quelli di VerbaAlpina.\r\n2.2 Status quo\r\nL\u2019area di ricerca di VerbaAlpina prende in considerazione tutti gli idiomi parlati nell\u2019arco alpino. Si tratta di\r\nuna superficie di 190.000 km2 comprendente alcune regioni di sei Paesi diversi (Austria, Francia, Germania,\r\nItalia, Slovenia e Svizzera) e due interi stati (Liechtenstein e Montecarlo) per un totale di quasi 6000 comuni,\r\ni quali rappresentano per VerbaAlpina le unit\u00E0 di georeferenziazione. Considerato che in linguistica\r\nl\u2019unanimit\u00E0 di opinioni su una definizione unitaria di dialetto rappresenta ancora una visione remota (e, a dire\r\nil vero, di scarso interesse per la disciplina stessa), non \u00E8 possibile indicare un numero, nemmeno\r\napprossimativo, di variet\u00E0 alpine locali parlate dalla Francia alla Slovenia. Pur concedendo grande importanza\r\nall\u2019aspetto locale delle variet\u00E0, l\u2019analisi linguistica di VerbaAlpina si eleva al livello delle tre famiglie\r\nlinguistiche che occupano il territorio alpino (romanza, germanica e slava). Per questo motivo, attraverso il\r\nprocesso di tipizzazione di cui sopra, il variegato materiale linguistico locale \u00E8 raggruppato in tipi\r\nmorfolessicali etichettati rispettivamente con le sigle ISO 639-5 relative alle famiglie linguistiche: roa. per\r\nromanzo, gem. per germanico e sla. per slavo (cfr. ISO 639-5). La base di conoscenza di VerbaAlpina\r\nracchiude ad oggi (novembre 2019) 55.407 stimoli (si tratta solitamente dei titoli delle carte degli atlanti di\r\nriferimento) ai quali sono collegate 165.521 attestazioni linguistiche, distribuite tra 3.989 concetti e riassunte\r\nin 9.556 tipi morfolessicali. Per quanto riguarda i dati provenienti dal crowdsourcing, si contano 1.065\r\ninformanti diversi e 15.249 parole totali inviate dagli utenti.\r\n\r\n3\r\n\r\nLa lessicografia tradizionale e le sfide per il futuro nell\u2019era delle digital humanities\r\n\r\nCome \u00E8 stato gi\u00E0 accennato, solitamente, i dizionari classici sono strutturati in maniera semasiologica, ovvero\r\nil lessico ivi contenuto viene elencato partendo dall\u2019unit\u00E0 lessicale (parola) alla quale sono collegati i diversi\r\nsignificati. La fortuna di questo modello di dizionario \u00E8 da ricondurre essenzialmente a due ragioni: da un lato\r\ntali opere lessicografiche hanno il compito di raccogliere e illustrare il lessico appartenente a un dato idioma\r\n(e in questo senso fungono da ausili per la documentazione di una lingua); dall\u2019altro lato, concepire un\u2019opera\r\n77\r\n\r\n\flessicografica in prospettiva semasiologica risulta essere un\u2019operazione di pi\u00F9 facile realizzazione. Per\r\nriprodurre una serie ordinata di segni che compongono un\u2019unit\u00E0 lessicale si pu\u00F2 contare su un sistema\r\ncodificato e standardizzato di caratteri: la scrittura stessa. La difficolt\u00E0 di creare un\u2019opera lessicografica\r\npartendo dal contenuto concettuale (prospettiva onomasiologica) \u00E8 invece molto pi\u00F9 estesa. Il contenuto\r\nsemantico dell\u2019unit\u00E0 lessicale non pu\u00F2 essere delineato cos\u00EC facilmente, n\u00E9 tantomeno pu\u00F2 essere\r\nstandardizzato. L\u2019utilizzo di una determinata espressione, non predice nulla sulle caratteristiche intrinseche del\r\nconcetto al quale \u00E8 collegata la parola che si \u00E8 cercato di riprodurre. Fondamentalmente, sia per quanto riguarda\r\nla riproduzione di una singola parola, sia per quanto riguarda la descrizione di un significato, si fa appello alla\r\nscrittura, ovvero, alla lingua. Tuttavia, tale ricorso alla lingua \u00E8 problematico giacch\u00E9 \u00E8 possibile utilizzare\r\nsolamente un determinato idioma alla volta, mentre invece, alla luce di quanto detto poc\u2019anzi e nell\u2019ottica di\r\nuna scienza interconnessa, sarebbe opportuno potersi riferire ai concetti indipendentemente dalla singola\r\nlingua. Il mero ricorso ai codici linguistici ostacola inoltre la condivisione dei dati e la loro connessione ad\r\naltri database, limitando in parte una pi\u00F9 ampia collaborazione tra progetti scientifici.\r\n\r\n4\r\n\r\nL\u2019approccio al problema sull\u2019esempio di VerbaAlpina\r\n\r\nPer l\u2019accorpamento dei contenuti provenienti dai vari atlanti linguistici e dai dizionari, anche il progetto\r\nVerbaAlpina si \u00E8 dovuto misurare con la suddetta questione. All\u2019interno della banca dati relazionale che funge\r\nda base del progetto, \u00E8 stata creata una tabella che racchiude i concetti (si tratta prevalentemente dei contenuti\r\ntematici delle mappe linguistiche). Le singole mappe degli atlanti sono quindi collegate al concetto appropriato\r\ncorrispondente. Per quanto riguarda il contenuto semantico di un concetto, il minimo comune denominatore \u00E8\r\nrappresentato dall\u2019insieme delle informazioni che compongono il dato concetto. Dal momento che\r\nVerbaAlpina tratta dati provenienti da diverse fonti esterne al progetto, la gestione e l\u2019uniformizzazione degli\r\nstessi pu\u00F2 essere concepita solamente attraverso una descrizione accurata dei dati che, nell\u2019insieme, formano\r\nun singolo concetto. Tuttavia, il metodo di gestione appena descritto consente solo la comparabilit\u00E0 all'interno\r\ndel progetto, mentre per collegare tra di loro diversi gruppi di dati, sarebbe auspicabile e necessaria una\r\nsoluzione globale e indipendente dalla lingua.\r\nLa sfida della standardizzazione \u00E8 stata intrapresa da lungo tempo all\u2019interno delle biblioteche, dove\r\nl\u2019esigenza dell\u2019uniformit\u00E0 mira a creare uno standard ad esempio per la gestione dei dati relativi agli autori\r\ndelle diverse pubblicazioni o per la realizzazione di differenti indicizzazioni tematiche. \u00C8 da questa esigenza\r\ndell\u2019ambito bibliotecario che nasce l\u2019idea del cosiddetto authority control, ossia un sistema normato per la\r\ncostituzione di un archivio (authority file) che possa contenere dati organizzati secondo uno stesso modello.\r\nIn Germania, a partire dagli anni Ottanta del secolo scorso sono stati creati diversi sistemi per la\r\nstandardizzazione dei dati relativi a persone (PND: Personennamendatei), enti (GKD: Gemeinsame\r\nK\u00F6rperschaftsdatei) e voci (SWD: Schlagwortdatei). Inoltre, tra il 2009 e il 2013, la Biblioteca Nazionale\r\nTedesca (Deutsche Nationalbibliothek) e altre associazioni bibliotecarie di lingua tedesca hanno intrapreso\r\nun\u2019iniziativa volta a creare il cosiddetto GND (Gemeinsame Normdatei), un sistema di controllo di autorit\u00E0\r\nche riassume tutti gli elenchi sopraccitati in un unico file. Oltre alle tradizionali entit\u00E0 quali organismi o\r\npersone, il GND raccoglie anche concetti.\r\nAnche il database enciclopedico Wikidata, creato allo scopo di supportare Wikipedia, funziona attraverso il\r\ncontrollo di autorit\u00E0 (cfr. Wikidata a). In Wikidata, ogni concetto \u00E8 registrato tramite un numero identificatore\r\n(ID) e descritto nel dettaglio attraverso relazioni gerarchiche. La concezione partecipativa di Wikidata ha\r\npermesso l\u2019inserimento nella piattaforma di un numero considerevole di entit\u00E0 referenziabili. A partire dal\r\n2018, tutti i concetti di VerbaAlpina sono stati connessi ai Q-ID (o Q-item) di Wikidata. Tale connessione\r\nattraverso gli identificatori permette di collegare con altre banche dati esterne le informazioni altrimenti gestite\r\nsolo all\u2019interno del progetto. In questo senso, le risorse interne, come ad esempio il database multimediale,\r\npossono essere collegate in maniera decentralizzata ed essere messe a disposizione di diversi progetti. Questa\r\nconcezione permette ai dati di essere costantemente arricchiti di informazioni aggiuntive. Allo stesso modo, \u00E8\r\npossibile pensare anche alla presentazione dei contenuti in diverse lingue, in quanto Wikidata mette a\r\ndisposizione le traduzioni delle denominazioni relative ai diversi concetti (o ai relativi Q-ID). La\r\ncollaborazione tra Wikidata e VerbaAlpina prevede non solo una connessione attraverso l\u2019applicazione\r\ndell\u2019identificatore, ma anche una partecipazione attiva di quest\u2019ultimo al database enciclopedico. VerbaAlpina\r\ndispone infatti di un account proprio sulla pagina di Wikidata al fine di mappare eventuali concetti ivi mancanti.\r\nAl momento (novembre 2019), 1000 concetti di VerbaAlpina sono muniti di un corrispettivo Q-ID. Una parte\r\nconsistente di concetti contenuti nella banca dati di VerbaAlpina deve essere ancora elaborata e ogni entrata\r\n\r\n78\r\n\r\n\fcorredata del rispettivo Q-ID, un\u2019attivit\u00E0 che \u00E8 in costante aggiornamento. Al momento, \u00E8 stata data priorit\u00E0\r\nall\u2019applicazione degli identificatori ai concetti di VerbaAlpina, in un secondo momento avverr\u00E0 anche la\r\nmappatura di concetti mancanti su Wikidata. L\u2019inserimento dei Q-ID di Wikidata nella banca dati di\r\nVerbaAlpina avviene in maniera del tutto manuale.\r\nLe proposte sopraccitate si riferiscono a un tentativo di standardizzazione del contenuto semantico di\r\nun\u2019ampia quantit\u00E0 di concetti. Dal momento che VerbaAlpina non si occupa solamente del trattamento di\r\nmateriale semantico, ma anche di tipi morfolessicali, la creazione di un controllo di autorit\u00E0 applicabile anche\r\na tali forme linguistiche sarebbe auspicabile per l\u2019identificazione univoca del contenuto lessicale. Per\r\nun\u2019etichettatura di questo genere, la situazione si presenta in maniera diversa: il GND non fornisce ancora un\r\nsistema mirato per la gestione dei dati in questo senso, mentre Wikidata offre la possibilit\u00E0 di creare attestazioni\r\nlessicali contenenti le informazioni legate al lemma stesso, alla lingua e alla categoria lessicale. Ogni\r\nattestazione lessicale \u00E8 correlata a un identificatore L (L-ID) che viene generato automaticamente (cfr.\r\nWikidata b). Le singole voci possono anche essere completate con informazioni quali genere e significato. Per\r\nvariet\u00E0 linguistiche di estensione meno ampia quali ad esempio il ladino dolomitico oppure il friulano,\r\nWikidata richiede non solo l\u2019inserimento del lemma vero e proprio, ma anche di indicare la cosiddetta spelling\r\nvariant, ossia l\u2019ortografia utilizzata per rappresentare un determinato lemma indicata mediante un codice\r\nlinguistico. Ad esempio, se si desidera inserire il lemma paur\u00F9ns (forma ladina per il concetto \u2018siero di latte\r\ndopo la prima separazione della materia solida\u2019), viene richiesto di indicare secondo quale ortografia \u00E8 stato\r\ninserito il lemma stesso (cfr. Wikidata c). Inoltre, \u00E8 possibile anche aggiungere le informazioni relative al\r\nnumero e al caso linguistico e collegarle al rispettivo concetto. Quest\u2019ultimo sistema di referenziazione\r\nconsente di collegare ai singoli lemmi anche le informazioni riguardanti le derivazioni o l\u2019etimologia, una\r\npratica che faciliterebbe, di conseguenza, il lavoro lessicografico. Tale modalit\u00E0 di etichettatura del materiale\r\nlinguistico \u00E8 stata implementata da VerbaAlpina solo recentemente, ma sar\u00E0 portata avanti in maniera\r\nprogressiva con le stesse modalit\u00E0 applicate per i concetti.\r\nLa connessione tra i concetti di VerbaAlpina e quelli di Wikidata attraverso l\u2019applicazione di un\r\nidentificatore non \u00E8 fine a se stessa, ma si inserisce in un\u2019ottica di condivisione pi\u00F9 ampia in grado trovare\r\npunti di aggancio anche con il progetto GeRDI (Generic Research Data Infrastructure). Quest\u2019ultimo nasce\r\nnel 2016 come aggregatore di dati allo scopo di offrire a tutti i progetti di ricerca in Germania la possibilit\u00E0 di\r\narchiviare, condividere e riutilizzare dati. GeRDI impiega Wikidata come base di conoscenza, realizzando cos\u00EC\r\nun sistema di consultazione di dati interdisciplinare e multilingue (cfr. Mutter, 2018).\r\nWikidata rappresenta agli occhi di VerbaAlpina una piattaforma centrale attraverso la quale quest\u2019ultimo\r\npu\u00F2 mettersi in relazione con altri progetti collegati analogamente alla stessa base di conoscenza. Un esempio\r\npotrebbe essere il dizionario enciclopedico BabelNet, anch\u2019esso connesso a Wikidata. La connessione diretta\r\ntra VerbaAlpina e Babelnet sarebbe interessante per quanto riguarda l\u2019identificazione dei lemmi, ma,\r\nquest\u2019ultimo non dispone ancora di numeri univoci per questo tipo di materiale lessicale, n\u00E9 raccoglie dati\r\ndialettali, centrali invece per l\u2019attivit\u00E0 di VerbaAlpina. Ad ogni modo, bench\u00E9 non in maniera diretta,\r\nVerbaAlpina e Babelnet dispongono entrambe di Wikidata come progetto di collaborazione comune.\r\n5\r\n\r\nProspettive e attivit\u00E0 future\r\n\r\nFacendo di nuovo riferimento ai princ\u00ECpi FAIR menzionati all\u2019inizio, organizzare e gestire i dati linguistici nel\r\nquadro dei sistemi di identificazione descritti poc\u2019anzi, rappresenta un passo importante verso il rispetto di\r\nquesti princ\u00ECpi. I dati strutturati si presentano non solo pi\u00F9 accessibili da parte di altri progetti e pi\u00F9 facilmente\r\nreperibili grazie al collegamento in rete, ma la standardizzazione dei riferimenti esatti ne favorisce anche il\r\ntrattamento dal punto di vista dell\u2019interoperabilit\u00E0. VerbaAlpina non \u00E8 leale a questi princ\u00ECpi solamente per\r\nquanto riguarda l\u2019interoperabilit\u00E0 dei dati, ma anche relativamente alla loro rintracciabilit\u00E0 (f) attraverso\r\nl\u2019inserimento del progetto nei cataloghi della biblioteca universitaria dell\u2019universit\u00E0 di Monaco di Baviera.\r\nVerbaAlpina sposa in toto l\u2019idea di libero accesso alla conoscenza come bene comune e utilizza solamente\r\nlicenze Creative-Commons (CC) rinunciando, di conseguenza, al diritto d\u2019autore e permettendo l\u2019utilizzo dei\r\ndati con la sola restrizione dell\u2019obbligo di citazione (cfr. L\u00FCcke, 2016a); il riutilizzo dei dati \u00E8 reso possibile\r\nattraverso la loro esportazione tramite un\u2019interfaccia di programmazione di un\u2019applicazione (eng. Application\r\nProgramming Interface; API), la quale permette l\u2019accesso all\u2019intero dataset di VerbaAlpina.\r\nUna documentazione e spiegazione dettagliata \u00E8 consultabile al seguente indirizzo: https:\/\/\r\nwww.verba-alpina.gwi.uni-muenchen.de\/?page_id=8844&db=191. Attraverso tale API\r\ni dati di VerbaAlpina sono accessibili meccanicamente (machine readable) e possono essere\r\nscaricati, modificati ed elaborati ulteriormente. La connessione dei dati con altri dataset \u00E8 garantita\r\nattraverso lo schema di metadati di DataCite\r\n79\r\n\r\n\f(https:\/\/datacite.org), la quale si trova ancora in fase di sviluppo. Nonostante l\u2019accesso\r\n\r\nmeccanico ai dati dall\u2019esterno sia gi\u00E0 possibile mediante l\u2019API e la connessione dei dati con altri\r\ndataset attraverso i metadati, le procedure per inserire i dati nella nuvola dei Linguistic\r\nLinked Open Data (https:\/\/linguistic-lod.org\/) sono in fase di avvio allo scopo di\r\ncreare un\u2019ulteriore connessione tra progetti e contribuire in questo senso all\u2019idea di web strutturato.\r\nOperare nell\u2019era delle digital humanities significa creare conoscenza interconnessa, condivisibile,\r\naccessibile, una conoscenza pi\u00F9 ampia e coesa. Equivale a creare strumenti e a metterli a disposizione\r\nnon solo della comunit\u00E0 scientifica, ma anche del grande pubblico. Si tratta di un\u2019amplificazione\r\ndell\u2019originale pensiero umanista: creare sapere, renderlo accessibile e diffonderlo affinch\u00E9 l\u2019umanit\u00E0\r\npossa accrescere le proprie conoscenze.\r\nTrattare i dati (lessicografici) nell\u2019era delle digital humanities\r\n\r\nLa ricerca dell\u2019era pre-digitale \u00E8 stata contraddistinta da modalit\u00E0 di concezione progettuale fortemente\r\nindividuali: la raccolta, l\u2019analisi e l\u2019illustrazione dei dati venivano effettuate da persone (o gruppi di persone)\r\nche operavano singolarmente. La comunicazione scientifica si compiva attraverso le pubblicazioni dei libri\r\ncartacei, i quali venivano conservati in luoghi circoscritti come ad esempio le biblioteche, e ciascuno studio\r\nrappresentava un progetto concluso in s\u00E9 (cfr. Krefeld, 2016). Inoltre, molti dati rimanevano nelle mani degli\r\nstessi ricercatori che li avevano raccolti e, in generale, la comunit\u00E0 scientifica non compiva molti sforzi per\r\nmettere a disposizione del grande pubblico i dati provenienti dai diversi progetti scientifici.\r\nOrmai, l\u2019avvento delle nuove tecnologie non \u00E8 pi\u00F9 un fenomeno di recente datazione. Il passaggio dal\r\ncartaceo al digitale \u00E8 avvenuto in maniera sempre pi\u00F9 repentina, contribuendo al mutamento dell\u2019approccio\r\nscientifico nei confronti delle modalit\u00E0 di ricerca. La rivoluzione digitale ha rinsaldato l'idea della condivisione\r\ne, allo stesso tempo, provveduto a fornire gli strumenti necessari per favorire e facilitare l\u2019interscambio di dati\r\ncome pure, pi\u00F9 in generale, l\u2019interazione tra diversi progetti. Ciononostante, la creazione di una rete di progetti\r\ne la condivisione dei relativi dati non sono fornite da una mera digitalizzazione degli strumenti di ricerca. La\r\ncollaborazione rappresenta l\u2019essenza primaria del fare scienza, poich\u00E9 \u00E8 sulla base delle conoscenze gi\u00E0\r\npresenti che si costruisce il progresso. Per operare in maniera collettiva nell\u2019era digitale \u00E8 per\u00F2 necessario che\r\ni dati di ricerca soddisfino alcuni requisiti fondamentali. In primo luogo i dati devono essere strutturati, descritti\r\n\r\n74\r\n\r\n\fed eventualmente etichettati in maniera tale da prestarsi a essere maneggiati in sedi esterne al loro progetto\r\noriginario e devono poter continuare a essere accessibili anche in un momento successivo all\u2019eventuale\r\nchiusura del progetto. In questo contesto si inserisce l\u2019idea di Web Semantico (e successivamente dei Linked\r\nOpen Data), nata con l\u2019obiettivo di rimodellare l\u2019ambiente virtuale di internet. Numerosi sono i progetti che si\r\ncollocano all\u2019interno di questo pensiero: essi formano la cosiddetta Linked Open Data Cloud (cfr. Cyganiak e\r\nJentzsch, 2007 -) e costituiscono, al contempo, una comunit\u00E0 interconnessa attraverso relazioni basate sui loro\r\ninsiemi di dati (cfr. Bizer et al. 2009, 154).\r\nLe esigenze di condivisione e connessione dei dati sul web sono inoltre state formulate in maniera esplicita\r\nnel 2016, quando un grande numero di ricercatori provenienti da diversi Paesi ha pubblicato le linee guida per\r\nla gestione moderna dei dati di ricerca (cfr. Wilkinson, Dumontier et al., 2016). Tali raccomandazioni sono\r\nstate racchiuse nell\u2019acronimo FAIR, una sigla che raccoglie i quattro princ\u00ECpi fondamentali sui quali\r\ndovrebbero essere basate la comunicazione e la cooperazione scientifiche nell\u2019era digitale. Secondo tali\r\nprinc\u00ECpi, i dati della ricerca dovrebbero essere rintracciabili (findable), accessibili (accessible), interoperabili\r\n(interoperable) e riutilizzabili (reusable). Per essere rintracciabili, i progetti ai quali i dati appartengono,\r\ndevono essere reperibili attraverso portali centrali quali, ad esempio, i cataloghi delle biblioteche. I dati di\r\nricerca che non sono soggetti ad alcuna restrizione giuridica (come potrebbero essere ad esempio i dati\r\nstrettamente personali) devono essere messi a disposizione del grande pubblico e rinunciare, di conseguenza,\r\nal diritto d\u2019autore. Al fine di poter essere interoperabili, inoltre, i dati devono essere innanzitutto scissi,\r\nsuccessivamente strutturati ed essere descritti in maniera precisa. Il riutilizzo dei dati si rende infine possibile\r\nattraverso una corretta applicazione dei tre princ\u00ECpi precedenti: si tratta di princ\u00ECpi estensibili che non si lasciano\r\nmettere in contrapposizione l\u2019un l\u2019altro (cfr. Force11, 2011-2017; GoFair, 2011-2017; L\u00FCcke, 2018).\r\nLe possibilit\u00E0 offerte dalla digitalizzazione in termini tecnici hanno altres\u00EC consentito di valutare da un nuovo\r\npunto di vista una delle questioni storiche relative al trattamento dei dati lessicografici. In linea di massima, le\r\nopere lessicografiche possono essere strutturate in maniera semasiologica (le parole di un dato idioma sono\r\nelencate seguite dal loro significato) oppure onomasiologica (si descrivono i significati e vi si collegano le\r\ndiverse parole che ad essi conducono). Nell\u2019era analogica tali opere erano strutturate secondo uno o secondo\r\nl\u2019altro modo: in ambito romanzo, si ricordino, tra gli altri, l\u2019atlante linguistico ed etnografico dell\u2019Italia e della\r\nSvizzera meridionale (AIS), di stampo onomasiologico, oppure, il Dicziunari Rumantsch Grischun (DRG),\r\nstrutturato, invece, in maniera semasiologica. Una concezione in entrambi i sensi non era possibile a causa di\r\nlimiti pratici imposti dalle modalit\u00E0 di pubblicazione del passato, mentre oggigiorno, le possibilit\u00E0 fornite dalla\r\ndigitalizzazione offrono nuovi strumenti e la concezione di un\u2019opera lessicografica che vada in due direzioni\r\n\u00E8 realizzabile. Ciononostante, bench\u00E9 l\u2019aspetto tecnico non rappresenti pi\u00F9 alcun problema alla messa in pratica\r\ndi tale approccio bidirezionale, le sfide si aprono soprattutto dal punto di vista contenutistico, come si evincer\u00E0\r\ndai capitoli che seguiranno.\r\nLa compilazione digitale di opere lessicografiche quali i dizionari sembra essere oggi un\u2019attivit\u00E0\r\nrelativamente consolidata. Non sono rari i dizionari che offrono la possibilit\u00E0 di essere consultati in rete, anche\r\nse, alcuni di essi, non si sono mai realmente distanziati da una concezione cartacea. Tali opere, presentano\r\nancora un grande margine di sviluppo e ampliamento per potersi definire digitalizzate in modo interoperabile\r\nsecondo i gradi definiti in L\u00FCcke (2016). A titolo illustrativo, ma non esaustivo, si pensi alla versione online\r\ndel Romanisches Etymologisches W\u00F6rterbuch (Meyer-L\u00FCbke, 1935): il contenuto dell\u2019opera \u00E8 messo a\r\ndisposizione online in formato PDF, ma, ai sensi della digitalizzazione, in essa potrebbero essere implementate\r\ndiverse funzioni, tra cui, ad esempio, la ricerca di singoli lemmi. Lo scopo di una lessicografia basata sul web\r\nnon si limita alla mera presentazione virtuale di una determinata opera. Lo sviluppo digitale \u00E8 bens\u00EC\r\nrappresentato da un pi\u00F9 ampio tentativo di costituzione di reti lessicali e semantiche messe in relazione tra di\r\nloro. Progetti volti a fornire tali interconnessioni sono stati iniziati gi\u00E0 verso la met\u00E0 degli anni Ottanta. A titolo\r\nesemplificativo si pensi a WordNet, il database lessicale per la lingua inglese, ma anche a EuroWordNet nato\r\nnegli anni Novanta come rete semantica per le lingue europee (cfr. Fellbaum 2006, 665, 669). Tali progetti\r\ncostituiscono un primo tentativo di strutturare il materiale in maniera semantica e non solo lessicale come si \u00E8\r\nsoliti fare nei dizionari cartacei e, soprattutto, si inseriscono nel panorama dei lavori relativi all\u2019elaborazione\r\ndel linguaggio naturale multilingue. In tempi pi\u00F9 recenti si colloca la concezione di\r\nBabelNet (https:\/\/babelnet.org\/), una rete semantica multilingue, automatizzata e di ampia\r\ncopertura, ovvero un dizionario enciclopedico costituito unendo il contenuto lessicale di WordNet al\r\nsapere enciclopedico di Wikipedia attraverso processi automatizzati di integrazione dei contenuti di\r\nambedue i database (cfr. Navigli e Ponzetto, 2010, 216).\r\n\r\n75\r\n\r\n\f2 VerbaAlpina: geolinguistica e lessicografia digitali\r\nMentre la digitalizzazione dei primi dizionari e corpora lessicali si colloca tra gli anni Ottanta e gli anni\r\nNovanta (cfr. Chiari 2012, 98), pi\u00F9 recente risulta invece essere il passaggio dal cartaceo al digitale per quanto\r\nriguarda gli atlanti linguistici. Relativamente all\u2019area alpino-romanza, diversi atlanti sono oggi disponibili in\r\nrete in formati PDF o JPG ma non hanno percorso tutte le tappe del passaggio dal cartaceo al digitale (cfr.\r\nL\u00FCcke, 2016; Knapp, 2017). \u00C8, a titolo esemplificativo, il caso di NavigAIS (Tisato, 2009-2018) all\u2019interno del\r\nquale, nonostante la sua presenza su internet sia lodevole, potrebbero essere implementate diverse funzionalit\u00E0,\r\ntra le quali ad esempio la rintracciabilit\u00E0 di forme attestate oppure una visualizzazione quantificata dei dati, la\r\npossibilit\u00E0 di consultare singoli gruppi di dati linguistici in prospettiva onomasiologica o semasiologica, come\r\npure l\u2019esportazione dei dati.\r\nAlle esigenze della ricerca lessicografica e atlantistica in chiave moderna, cerca di dare una risposta il progetto\r\nVerbaAlpina dell\u2019Universit\u00E0 Ludwig-Maximilian di Monaco di Baviera, nato nel 2014 con l\u2019intento di\r\nindagare lo spazio linguistico delle Alpi nella sua storica unit\u00E0 linguistico-culturale (cfr. Krefeld e L\u00FCcke, 2014\r\n-). Fin dalla sua concezione, completamente digitale e pensata non solo sul web, ma per il web, il progetto\r\nVerbaAlpina ha operato nel pieno rispetto dei princ\u00ECpi FAIR (che sarebbero stati formulati solamente due anni\r\ndopo) e promosso un\u2019idea innovativa di lessicografia e atlantistica linguistica (cfr. Krefeld, 2018). Oltre\r\nall\u2019aspetto linguistico, una parte consistente del progetto \u00E8 specificatamente dedicata alla creazione di\r\nstrumenti per la gestione dei dati di ricerca nei progetti digitali e pensati per il web.\r\n2.1 Concezione e presentazione del progetto\r\nNucleo centrale dell\u2019attivit\u00E0 di VerbaAlpina \u00E8 la raccolta strutturata di una precisa cornice semasiologica e\r\nonomasiologica, costituita dagli ambiti terminologici alpini, alla quale \u00E8 possibile accedere attraverso una\r\ncartina interattiva. La ricerca prende in esame il lessico dialettale degli idiomi alpini, in modo particolare le\r\nparole relative agli ambiti della natura (flora, fauna, formazioni paesaggistiche), della cultura alpina storica\r\n(lavorazione del latte) e di quella corrente (turismo). I dati raccolti e analizzati da VerbaAlpina sono puramente\r\ndialettali, mentre i termini relativi alle lingue standard non sono presi in considerazione. Diversi sono gli scopi\r\nperseguiti da VerbaAlpina: in primo luogo il progetto intende documentare e analizzare in prospettiva\r\nlinguistica e storico-etimologica la regione alpina, uno spazio fortemente frammentato per quanto riguarda le\r\nlingue e i dialetti ivi parlati. I confini dell\u2019area di ricerca sono definiti dalla Convenzione delle Alpi\r\n(http:\/\/www.alpconv.org\/), un trattato tra i Paesi del territorio alpino atto a promuovere e sviluppare questa\r\narea montana in diversi ambiti (cfr. L\u00FCcke 2018a).\r\nI dati sono forniti dagli atlanti linguistici e dai dizionari relativi all\u2019area di ricerca, analogici o digitali,\r\npubblicati nel corso del tempo. In un primo momento, il materiale linguistico georeferenziato proveniente dalle\r\nfonti affronta un percorso di digitalizzazione attraverso un sistema di trascrizione basato esclusivamente sui\r\ncaratteri ASCII (cfr. Krefeld e L\u00FCcke, 2016). In un secondo momento, il materiale trascritto viene sottoposto\r\na una tokenizzazione, un processo che separa in singoli token (parole) il materiale trascritto in un momento\r\nprecedente. L\u2019interesse principale del progetto si esplica nella presentazione dei punti di coesione tra i diversi\r\nidiomi e le diverse famiglie linguistiche presenti sul territorio alpino soprattutto in prospettiva lessicologica.\r\nPer l\u2019adempimento di tale scopo, il materiale linguistico viene raggruppato in tipi di base, ossia secondo la\r\nradice lessicale comune a diverse attestazioni che possono appartenere anche a diverse famiglie linguistiche1\r\ne in tipi morfolessicali, vale a dire in forme di un solo tipo di base, appartenenti a un\u2019unica famiglia linguistica\r\nche presentano caratteristiche grammaticali comuni quali la parte del discorso, il genere e gli elementi di\r\nformazione delle parole (cfr. Krefeld e L\u00FCcke, 2016a). Ad esempio, il tipo di base latino *CASEU(M) \u2018formaggio\u2019\r\n\u00E8 presente sia in area linguistica germanica, sia in area romanza nelle forme deu. K\u00E4se e ita. cacio, i quali, a\r\nloro volta, rappresentano due tipi morfolessicali differenti.\r\nI dati linguistici storici rilevati dagli atlanti e dai dizionari sono completati attraverso una piattaforma\r\ndi crowdsourcing\r\nsviluppata\r\nall\u2019interno\r\ndel\r\nprogetto\r\n(https:\/\/www.verbaalpina.gwi.uni-muenchen.de\/en\/?page_id=1741&db=191). La piattaforma si rivolge\r\ndirettamente ai parlanti dei dialetti delle Alpi al fine di raccogliere materiale linguistico attuale e poter\r\nosservare lo spazio alpino anche in prospettiva diacronica. Una volta aperta la pagina, viene chiesto agli\r\nutenti di scegliere una lingua di navigazione tra quelle proposte (francese, italiano, sloveno, tedesco).\r\nSuccessivamente, vengono mostrate le istruzioni per l\u2019utilizzo\r\n1\r\n\r\nIn molti casi non \u00E8 dato sapere se la radice lessicale comune a diverse parole sia da ricollegare allo stesso sostrato linguistico oppure\r\na un contatto linguistico pi\u00F9 recente. Per questo motivo VerbaAlpina utilizza il termine \u201Ctipo di base\u201D, in quanto \u201Cetimo\u201D si riferisce\r\ngeneralmente allo strato linguistico immediatamente precedente (cfr. Krefeld e L\u00FCcke 2016a).\r\n76\r\n\r\n\fdella piattaforma e gli utenti sono invitati a inserire l\u2019idioma alpino di cui essi sono i parlanti. Nel caso in cui\r\nun idioma non sia presente nella lista, gli utenti hanno la possibilit\u00E0 di segnalarlo direttamente alla redazione\r\nche provveder\u00E0 a inserirlo. Innanzitutto, gli utenti sono chiamati a inserire il nome del comune di cui\r\npadroneggiano l\u2019idioma. Cliccando sull\u2019apposito campo \u201Cconcetto\u201D, appare una lista con tutti i concetti\r\nesistenti nella banca dati di VerbaAlpina. Da qui, gli utenti possono scegliere per quali concetti inviare parole.\r\nI dati raccolti attraverso il crowdsourcing vengono trattati alla pari dei dati provenienti dai dizionari e dagli\r\natlanti, con l\u2019unica differenza che non sono sottoposti al processo di trascrizione. Per questi dati, la\r\ntokenizzazione avviene solamente qualora si tratti di un sintagma costituito da pi\u00F9 elementi. La tipizzazione di\r\nqueste parole avviene alla stregua dei dati raccolti dai dizionari e dagli atlanti linguistici. A livello di database,\r\nle singole attestazioni provenienti dal crowdsourcing ricevono un identificatore e sono collegate ai concetti di\r\ncui rappresentano le diverse forme dialettali. Successivamente al trattamento strutturato, i dati analizzati\r\nda VerbaAlpina possono essere visualizzati sulla cartina interattiva (https:\/\/www.verbaalpina.gwi.uni-muenchen.de\/it\/?page_id=27&db=191). Tramite l\u2019utilizzo di filtri\r\nappropriati, i dati sono accessibili in prospettiva onomasiologica (si rappresentano tutte le attestazioni\r\nlinguistiche collegate a un determinato concetto) oppure semasiologica (si rappresentano i concetti legati a\r\nun preciso tipo morfolessicale). Inoltre, la visualizzazione pu\u00F2 essere impostata in modalit\u00E0 qualitativa,\r\nattraverso la quale \u00E8 possibile evincere la distribuzione generale delle attestazioni linguistiche, oppure\r\nquantitativa, ossia indicante il numero di dati all\u2019interno di una certa area. I dati possono essere visualizzati\r\nin prospettiva geografico-fisica oppure astratta: la prima mostra i dati distribuiti su una cartina fisica,\r\nattraverso la seconda, invece, i dati sono presentati su una mappa a nido d\u2019ape.\r\nParallelamente all\u2019attivit\u00E0 linguistica, il progetto ha profuso un grande impegno nella gestione dei dati\r\ndigitali con l\u2019obiettivo ultimo di promuovere la sostenibilit\u00E0 e la durabilit\u00E0 del progetto anche dopo la sua\r\nchiusura definitiva. La descrizione di una parte delle attivit\u00E0 che sono state intraprese in questo senso avviene\r\nnel corso del presente contributo. VerbaAlpina si impegna a utilizzare strumenti tecnologici adatti al web e\r\napplicabili al pensiero open source, come ad esempio Wordpress per la piattaforma centrale, Leaflet per la\r\ncartina interattiva e le banche dati relazionali MySQL. Dato che VerbaAlpina intende altres\u00EC fungere da creatore\r\ndi nessi tra istituzioni e progetti gi\u00E0 esistenti al fine di interscambiare, integrare e completare i dati linguistici\r\nriguardanti l\u2019area alpina, per i diversi partner sono messe a disposizione banche dati all\u2019interno delle quali i\r\nprogetti cooperanti possono inserire i loro dati e collegarli cos\u00EC a quelli di VerbaAlpina.\r\n2.2 Status quo\r\nL\u2019area di ricerca di VerbaAlpina prende in considerazione tutti gli idiomi parlati nell\u2019arco alpino. Si tratta di\r\nuna superficie di 190.000 km2 comprendente alcune regioni di sei Paesi diversi (Austria, Francia, Germania,\r\nItalia, Slovenia e Svizzera) e due interi stati (Liechtenstein e Montecarlo) per un totale di quasi 6000 comuni,\r\ni quali rappresentano per VerbaAlpina le unit\u00E0 di georeferenziazione. Considerato che in linguistica\r\nl\u2019unanimit\u00E0 di opinioni su una definizione unitaria di dialetto rappresenta ancora una visione remota (e, a dire\r\nil vero, di scarso interesse per la disciplina stessa), non \u00E8 possibile indicare un numero, nemmeno\r\napprossimativo, di variet\u00E0 alpine locali parlate dalla Francia alla Slovenia. Pur concedendo grande importanza\r\nall\u2019aspetto locale delle variet\u00E0, l\u2019analisi linguistica di VerbaAlpina si eleva al livello delle tre famiglie\r\nlinguistiche che occupano il territorio alpino (romanza, germanica e slava). Per questo motivo, attraverso il\r\nprocesso di tipizzazione di cui sopra, il variegato materiale linguistico locale \u00E8 raggruppato in tipi\r\nmorfolessicali etichettati rispettivamente con le sigle ISO 639-5 relative alle famiglie linguistiche: roa. per\r\nromanzo, gem. per germanico e sla. per slavo (cfr. ISO 639-5). La base di conoscenza di VerbaAlpina\r\nracchiude ad oggi (novembre 2019) 55.407 stimoli (si tratta solitamente dei titoli delle carte degli atlanti di\r\nriferimento) ai quali sono collegate 165.521 attestazioni linguistiche, distribuite tra 3.989 concetti e riassunte\r\nin 9.556 tipi morfolessicali. Per quanto riguarda i dati provenienti dal crowdsourcing, si contano 1.065\r\ninformanti diversi e 15.249 parole totali inviate dagli utenti.\r\n\r\n3\r\n\r\nLa lessicografia tradizionale e le sfide per il futuro nell\u2019era delle digital humanities\r\n\r\nCome \u00E8 stato gi\u00E0 accennato, solitamente, i dizionari classici sono strutturati in maniera semasiologica, ovvero\r\nil lessico ivi contenuto viene elencato partendo dall\u2019unit\u00E0 lessicale (parola) alla quale sono collegati i diversi\r\nsignificati. La fortuna di questo modello di dizionario \u00E8 da ricondurre essenzialmente a due ragioni: da un lato\r\ntali opere lessicografiche hanno il compito di raccogliere e illustrare il lessico appartenente a un dato idioma\r\n(e in questo senso fungono da ausili per la documentazione di una lingua); dall\u2019altro lato, concepire un\u2019opera\r\n77\r\n\r\n\flessicografica in prospettiva semasiologica risulta essere un\u2019operazione di pi\u00F9 facile realizzazione. Per\r\nriprodurre una serie ordinata di segni che compongono un\u2019unit\u00E0 lessicale si pu\u00F2 contare su un sistema\r\ncodificato e standardizzato di caratteri: la scrittura stessa. La difficolt\u00E0 di creare un\u2019opera lessicografica\r\npartendo dal contenuto concettuale (prospettiva onomasiologica) \u00E8 invece molto pi\u00F9 estesa. Il contenuto\r\nsemantico dell\u2019unit\u00E0 lessicale non pu\u00F2 essere delineato cos\u00EC facilmente, n\u00E9 tantomeno pu\u00F2 essere\r\nstandardizzato. L\u2019utilizzo di una determinata espressione, non predice nulla sulle caratteristiche intrinseche del\r\nconcetto al quale \u00E8 collegata la parola che si \u00E8 cercato di riprodurre. Fondamentalmente, sia per quanto riguarda\r\nla riproduzione di una singola parola, sia per quanto riguarda la descrizione di un significato, si fa appello alla\r\nscrittura, ovvero, alla lingua. Tuttavia, tale ricorso alla lingua \u00E8 problematico giacch\u00E9 \u00E8 possibile utilizzare\r\nsolamente un determinato idioma alla volta, mentre invece, alla luce di quanto detto poc\u2019anzi e nell\u2019ottica di\r\nuna scienza interconnessa, sarebbe opportuno potersi riferire ai concetti indipendentemente dalla singola\r\nlingua. Il mero ricorso ai codici linguistici ostacola inoltre la condivisione dei dati e la loro connessione ad\r\naltri database, limitando in parte una pi\u00F9 ampia collaborazione tra progetti scientifici.\r\n\r\n4\r\n\r\nL\u2019approccio al problema sull\u2019esempio di VerbaAlpina\r\n\r\nPer l\u2019accorpamento dei contenuti provenienti dai vari atlanti linguistici e dai dizionari, anche il progetto\r\nVerbaAlpina si \u00E8 dovuto misurare con la suddetta questione. All\u2019interno della banca dati relazionale che funge\r\nda base del progetto, \u00E8 stata creata una tabella che racchiude i concetti (si tratta prevalentemente dei contenuti\r\ntematici delle mappe linguistiche). Le singole mappe degli atlanti sono quindi collegate al concetto appropriato\r\ncorrispondente. Per quanto riguarda il contenuto semantico di un concetto, il minimo comune denominatore \u00E8\r\nrappresentato dall\u2019insieme delle informazioni che compongono il dato concetto. Dal momento che\r\nVerbaAlpina tratta dati provenienti da diverse fonti esterne al progetto, la gestione e l\u2019uniformizzazione degli\r\nstessi pu\u00F2 essere concepita solamente attraverso una descrizione accurata dei dati che, nell\u2019insieme, formano\r\nun singolo concetto. Tuttavia, il metodo di gestione appena descritto consente solo la comparabilit\u00E0 all'interno\r\ndel progetto, mentre per collegare tra di loro diversi gruppi di dati, sarebbe auspicabile e necessaria una\r\nsoluzione globale e indipendente dalla lingua.\r\nLa sfida della standardizzazione \u00E8 stata intrapresa da lungo tempo all\u2019interno delle biblioteche, dove\r\nl\u2019esigenza dell\u2019uniformit\u00E0 mira a creare uno standard ad esempio per la gestione dei dati relativi agli autori\r\ndelle diverse pubblicazioni o per la realizzazione di differenti indicizzazioni tematiche. \u00C8 da questa esigenza\r\ndell\u2019ambito bibliotecario che nasce l\u2019idea del cosiddetto authority control, ossia un sistema normato per la\r\ncostituzione di un archivio (authority file) che possa contenere dati organizzati secondo uno stesso modello.\r\nIn Germania, a partire dagli anni Ottanta del secolo scorso sono stati creati diversi sistemi per la\r\nstandardizzazione dei dati relativi a persone (PND: Personennamendatei), enti (GKD: Gemeinsame\r\nK\u00F6rperschaftsdatei) e voci (SWD: Schlagwortdatei). Inoltre, tra il 2009 e il 2013, la Biblioteca Nazionale\r\nTedesca (Deutsche Nationalbibliothek) e altre associazioni bibliotecarie di lingua tedesca hanno intrapreso\r\nun\u2019iniziativa volta a creare il cosiddetto GND (Gemeinsame Normdatei), un sistema di controllo di autorit\u00E0\r\nche riassume tutti gli elenchi sopraccitati in un unico file. Oltre alle tradizionali entit\u00E0 quali organismi o\r\npersone, il GND raccoglie anche concetti.\r\nAnche il database enciclopedico Wikidata, creato allo scopo di supportare Wikipedia, funziona attraverso il\r\ncontrollo di autorit\u00E0 (cfr. Wikidata a). In Wikidata, ogni concetto \u00E8 registrato tramite un numero identificatore\r\n(ID) e descritto nel dettaglio attraverso relazioni gerarchiche. La concezione partecipativa di Wikidata ha\r\npermesso l\u2019inserimento nella piattaforma di un numero considerevole di entit\u00E0 referenziabili. A partire dal\r\n2018, tutti i concetti di VerbaAlpina sono stati connessi ai Q-ID (o Q-item) di Wikidata. Tale connessione\r\nattraverso gli identificatori permette di collegare con altre banche dati esterne le informazioni altrimenti gestite\r\nsolo all\u2019interno del progetto. In questo senso, le risorse interne, come ad esempio il database multimediale,\r\npossono essere collegate in maniera decentralizzata ed essere messe a disposizione di diversi progetti. Questa\r\nconcezione permette ai dati di essere costantemente arricchiti di informazioni aggiuntive. Allo stesso modo, \u00E8\r\npossibile pensare anche alla presentazione dei contenuti in diverse lingue, in quanto Wikidata mette a\r\ndisposizione le traduzioni delle denominazioni relative ai diversi concetti (o ai relativi Q-ID). La\r\ncollaborazione tra Wikidata e VerbaAlpina prevede non solo una connessione attraverso l\u2019applicazione\r\ndell\u2019identificatore, ma anche una partecipazione attiva di quest\u2019ultimo al database enciclopedico. VerbaAlpina\r\ndispone infatti di un account proprio sulla pagina di Wikidata al fine di mappare eventuali concetti ivi mancanti.\r\nAl momento (novembre 2019), 1000 concetti di VerbaAlpina sono muniti di un corrispettivo Q-ID. Una parte\r\nconsistente di concetti contenuti nella banca dati di VerbaAlpina deve essere ancora elaborata e ogni entrata\r\n\r\n78\r\n\r\n\fcorredata del rispettivo Q-ID, un\u2019attivit\u00E0 che \u00E8 in costante aggiornamento. Al momento, \u00E8 stata data priorit\u00E0\r\nall\u2019applicazione degli identificatori ai concetti di VerbaAlpina, in un secondo momento avverr\u00E0 anche la\r\nmappatura di concetti mancanti su Wikidata. L\u2019inserimento dei Q-ID di Wikidata nella banca dati di\r\nVerbaAlpina avviene in maniera del tutto manuale.\r\nLe proposte sopraccitate si riferiscono a un tentativo di standardizzazione del contenuto semantico di\r\nun\u2019ampia quantit\u00E0 di concetti. Dal momento che VerbaAlpina non si occupa solamente del trattamento di\r\nmateriale semantico, ma anche di tipi morfolessicali, la creazione di un controllo di autorit\u00E0 applicabile anche\r\na tali forme linguistiche sarebbe auspicabile per l\u2019identificazione univoca del contenuto lessicale. Per\r\nun\u2019etichettatura di questo genere, la situazione si presenta in maniera diversa: il GND non fornisce ancora un\r\nsistema mirato per la gestione dei dati in questo senso, mentre Wikidata offre la possibilit\u00E0 di creare attestazioni\r\nlessicali contenenti le informazioni legate al lemma stesso, alla lingua e alla categoria lessicale. Ogni\r\nattestazione lessicale \u00E8 correlata a un identificatore L (L-ID) che viene generato automaticamente (cfr.\r\nWikidata b). Le singole voci possono anche essere completate con informazioni quali genere e significato. Per\r\nvariet\u00E0 linguistiche di estensione meno ampia quali ad esempio il ladino dolomitico oppure il friulano,\r\nWikidata richiede non solo l\u2019inserimento del lemma vero e proprio, ma anche di indicare la cosiddetta spelling\r\nvariant, ossia l\u2019ortografia utilizzata per rappresentare un determinato lemma indicata mediante un codice\r\nlinguistico. Ad esempio, se si desidera inserire il lemma paur\u00F9ns (forma ladina per il concetto \u2018siero di latte\r\ndopo la prima separazione della materia solida\u2019), viene richiesto di indicare secondo quale ortografia \u00E8 stato\r\ninserito il lemma stesso (cfr. Wikidata c). Inoltre, \u00E8 possibile anche aggiungere le informazioni relative al\r\nnumero e al caso linguistico e collegarle al rispettivo concetto. Quest\u2019ultimo sistema di referenziazione\r\nconsente di collegare ai singoli lemmi anche le informazioni riguardanti le derivazioni o l\u2019etimologia, una\r\npratica che faciliterebbe, di conseguenza, il lavoro lessicografico. Tale modalit\u00E0 di etichettatura del materiale\r\nlinguistico \u00E8 stata implementata da VerbaAlpina solo recentemente, ma sar\u00E0 portata avanti in maniera\r\nprogressiva con le stesse modalit\u00E0 applicate per i concetti.\r\nLa connessione tra i concetti di VerbaAlpina e quelli di Wikidata attraverso l\u2019applicazione di un\r\nidentificatore non \u00E8 fine a se stessa, ma si inserisce in un\u2019ottica di condivisione pi\u00F9 ampia in grado trovare\r\npunti di aggancio anche con il progetto GeRDI (Generic Research Data Infrastructure). Quest\u2019ultimo nasce\r\nnel 2016 come aggregatore di dati allo scopo di offrire a tutti i progetti di ricerca in Germania la possibilit\u00E0 di\r\narchiviare, condividere e riutilizzare dati. GeRDI impiega Wikidata come base di conoscenza, realizzando cos\u00EC\r\nun sistema di consultazione di dati interdisciplinare e multilingue (cfr. Mutter, 2018).\r\nWikidata rappresenta agli occhi di VerbaAlpina una piattaforma centrale attraverso la quale quest\u2019ultimo\r\npu\u00F2 mettersi in relazione con altri progetti collegati analogamente alla stessa base di conoscenza. Un esempio\r\npotrebbe essere il dizionario enciclopedico BabelNet, anch\u2019esso connesso a Wikidata. La connessione diretta\r\ntra VerbaAlpina e Babelnet sarebbe interessante per quanto riguarda l\u2019identificazione dei lemmi, ma,\r\nquest\u2019ultimo non dispone ancora di numeri univoci per questo tipo di materiale lessicale, n\u00E9 raccoglie dati\r\ndialettali, centrali invece per l\u2019attivit\u00E0 di VerbaAlpina. Ad ogni modo, bench\u00E9 non in maniera diretta,\r\nVerbaAlpina e Babelnet dispongono entrambe di Wikidata come progetto di collaborazione comune.\r\n5\r\n\r\nProspettive e attivit\u00E0 future\r\n\r\nFacendo di nuovo riferimento ai princ\u00ECpi FAIR menzionati all\u2019inizio, organizzare e gestire i dati linguistici nel\r\nquadro dei sistemi di identificazione descritti poc\u2019anzi, rappresenta un passo importante verso il rispetto di\r\nquesti princ\u00ECpi. I dati strutturati si presentano non solo pi\u00F9 accessibili da parte di altri progetti e pi\u00F9 facilmente\r\nreperibili grazie al collegamento in rete, ma la standardizzazione dei riferimenti esatti ne favorisce anche il\r\ntrattamento dal punto di vista dell\u2019interoperabilit\u00E0. VerbaAlpina non \u00E8 leale a questi princ\u00ECpi solamente per\r\nquanto riguarda l\u2019interoperabilit\u00E0 dei dati, ma anche relativamente alla loro rintracciabilit\u00E0 (f) attraverso\r\nl\u2019inserimento del progetto nei cataloghi della biblioteca universitaria dell\u2019universit\u00E0 di Monaco di Baviera.\r\nVerbaAlpina sposa in toto l\u2019idea di libero accesso alla conoscenza come bene comune e utilizza solamente\r\nlicenze Creative-Commons (CC) rinunciando, di conseguenza, al diritto d\u2019autore e permettendo l\u2019utilizzo dei\r\ndati con la sola restrizione dell\u2019obbligo di citazione (cfr. L\u00FCcke, 2016a); il riutilizzo dei dati \u00E8 reso possibile\r\nattraverso la loro esportazione tramite un\u2019interfaccia di programmazione di un\u2019applicazione (eng. Application\r\nProgramming Interface; API), la quale permette l\u2019accesso all\u2019intero dataset di VerbaAlpina.\r\nUna documentazione e spiegazione dettagliata \u00E8 consultabile al seguente indirizzo: https:\/\/\r\nwww.verba-alpina.gwi.uni-muenchen.de\/?page_id=8844&db=191. Attraverso tale API\r\ni dati di VerbaAlpina sono accessibili meccanicamente (machine readable) e possono essere\r\nscaricati, modificati ed elaborati ulteriormente. La connessione dei dati con altri dataset \u00E8 garantita\r\nattraverso lo schema di metadati di DataCite\r\n79\r\n\r\n\f(https:\/\/datacite.org), la quale si trova ancora in fase di sviluppo. Nonostante l\u2019accesso\r\n\r\nmeccanico ai dati dall\u2019esterno sia gi\u00E0 possibile mediante l\u2019API e la connessione dei dati con altri\r\ndataset attraverso i metadati, le procedure per inserire i dati nella nuvola dei Linguistic\r\nLinked Open Data (https:\/\/linguistic-lod.org\/) sono in fase di avvio allo scopo di\r\ncreare un\u2019ulteriore connessione tra progetti e contribuire in questo senso all\u2019idea di web strutturato.\r\nOperare nell\u2019era delle digital humanities significa creare conoscenza interconnessa, condivisibile,\r\naccessibile, una conoscenza pi\u00F9 ampia e coesa. Equivale a creare strumenti e a metterli a disposizione\r\nnon solo della comunit\u00E0 scientifica, ma anche del grande pubblico. Si tratta di un\u2019amplificazione\r\ndell\u2019originale pensiero umanista: creare sapere, renderlo accessibile e diffonderlo affinch\u00E9 l\u2019umanit\u00E0\r\npossa accrescere le proprie conoscenze."
	},
	{
		"id": 14,
		"title": "Digital projects for music research and education from the Center for Music Research and Documentation (CIDoM), Associated Unit of the Spanish National Research Council",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Juan Jos√© Pastor Com√≠n",
			"Francisco Manuel L√≥pez G√≥mez"
		],
		"body": "The Center for Music Research and Documentation of Spain\r\n\r\nThe Centro de Investigaci\u00F3n y Documentaci\u00F3n Musical is an Associated Unit of Centro Superior de\r\nInvestigaciones Cient\u00EDficas (CSIC, Spanish National Research Council). Founded in 2012 and formed by\r\nan interdisciplinary team of PhDs in Musicology, History, History of Art and Hispanic Philology\r\ncoordinated by Professors Paulino Capdep\u00F3n and Juan Jos\u00E9 Pastor, CIDoM has among its objectives to\r\nreplace and restore the musical heritage in one of the most important regions of Spain, Castilla-La\r\nMancha, a large area of 80.000 Km2, that hosts civil and religious centres of a great and historical musical\r\nactivity. Centres as the music chapel of the Toledo and Cuenca Cathedrals attracted a large number of\r\ncomposers. This musical legacy has remained, unfortunately, mostly unknown. In the last years CIDoM\r\nhas developed several national I+D+i Research Projects (Research + Development + innovation) focused\r\nin the Musical Heritage of Castilla-La Mancha and its Critical Analysis, Reception and Digital Edition.\r\n\r\n1\r\n\r\nThis work is inscribed within the context of the projects HAR2017-86039-C2-2-P. \"El patrimonio musical de la Espa\u00F1a moderna\r\n(siglos XVII-XVIII): recuperaci\u00F3n, digitalizaci\u00F3n, an\u00E1lisis, recepci\u00F3n y estructuras ret\u00F3ricas de los discursos musicales\" [The\r\nMusical Heritage of the Modern Spain (17th and 18th centuries): recovery, digitalisation, an\u00E1lisis, reception and rhetorical\r\nstructures of musical discourses] and PTA2016-13106-I, Catalogaci\u00F3n y Digitalizaci\u00F3n del Patrimonio Musical de Castilla-La\r\nMancha [Cataloguing and Digitalisation of the Musical Heritage of Castilla-La Mancha].\r\n\r\n82\r\n\r\n\f2\r\n\r\nThe digital database for Musical Heritage of Castilla-La Mancha\r\n\r\nThe digital database gathers the inventory of the musical sources to be consulted by the scientific\r\ncommunity by means of a page web (beta.cidom.es) in order to analyse a little-known musical heritage:\r\nthe Castilla-La Mancha\u2019s musical legacy during the Renaissance and Baroque period. The main aim is to\r\ncover the enormous gap existing in the region as regards the lack of an institution responsible for research\r\nand musical documentation. In this sense CIDoM is presented as a qualified proposal capable of fulfilling\r\nits responsibilities in the directions proposed by the International Council on Archives (ICA)2, the\r\nInternational Association of Music Libraries, Archives and Documentation Centers (IAML)3, the Spanish\r\nAssociation of Musical Documentation (AEDOM)4, the Spanish Society of Musicology (SEdeM) and the\r\nNational Institute of Performing Arts and Music (INAEM)5, following the standards of quality and criteria\r\nof documentation and research sanctioned by these institutions to which the members of the Center\r\nbelong. This essential task must allow, on the one hand, the analysis and study of the works of the\r\nfundamental composers of our region \u2212recognized by international musicology\u2212 belonging to different\r\nperiods, such as Diego Ortiz, Sebasti\u00E1n de Covarrubias, Alonso Xu\u00E1rez, Torrej\u00F3n and Velasco. On the\r\nother hand, it will allow to order, classify and catalogue the sources of information and musical work\r\nignored and contained in the cathedral and administrative archives of the region, Toledo, Sig\u00FCenza,\r\nTalavera de la Reina, Cuenca, Guadalajara, Pastrana, etc., with the aim to provide the professional\r\ncommunity with the opportunity to publish and record this musical heritage.\r\nThe digital catalogue is managed by two independent databases with which information relating to\r\ncomposers is recorded (including basic data of interest for the user's search, biography, registered works,\r\nbibliography and discography), on the one hand, and musical works, on the other. The process of\r\ncataloguing the latter has been developed from the standards ISAD(G) (1999), ALA (2004), and the\r\nguidelines by Gonz\u00E1lez-Valle (1996), Miliano (1999), Schultz and Shaw (2003), so that the following\r\nfields are considered: composer, title, type of document (manuscript or printed), genre, year of\r\ncomposition (or time bracket, when unknown), date of the source (or time bracket, when unknown), author\r\nof the text, literary source, premiere date, file in which it is kept, signature, coded musical incipit (allows\r\nworks with the same musical beginning to be located, even if they are transported to another mode or\r\ntonality, so that it can automatically detect borrowings between different works)6, literary incipit, vocal\r\nand instrumental template, facsimile (allows the user to download directly a scanned copy of the source\r\nin PDF format), transcription (if available), data on the edition (if applicable), and observations (including\r\ndata such as the diplomatic title of the work, information on the copy and the physical state of the medium,\r\nits measurements, the number of folios or pages, the tone or mode of the work and the time signature)7.\r\n\r\n3\r\n\r\nThe digital database for Cervantes and Music\r\n\r\nEvery musical adaptation of a literary work has to be considered as an exercise of perception and\r\ninterpretation that provides additional information on the hermeneutics of a writer\u2019s works. These\r\nadaptations allow us to understand and explain how the literary work has been recreated and transformed\r\nin each epoch. Once a play or a novel has been put in music, every musical version offers the audience a\r\nsort of critical and at the same time musical thought that reflects a new conception \u2212even misconception\u2212\r\nof the work. It cannot be denied that Cervantes\u2019 works have provided composers excellent material for\r\ntheir musical compositions and this fact has to be taken into account to describe the process of his musical\r\n2\r\n\r\nThey have developed the ISAD(G), General International Standard Archival Description, a guidance for cataloguing activities\r\nthat has been considered for CIDoM\u2019s projects.\r\n3 That includes the MARC 21 standard for the representation and exchange of bibliographic information data.\r\n4 In particular, taking into account the orientations offered in Eudom (2010).\r\n5 Under the conclusions of the symposium La gestion del patrimonio musical (Management of the Musical Heritage), expressed\r\nin \u00C1lvarez Ca\u00F1ibano et. al. (2014).\r\n6 The coding adopted consists in recording the intervallic distances between the musical notes of the incipit. To this purpose, the\r\nquality of each interval (ascending or descending) in number of semitones is recoded. For example: C-E flat-D-G would be noted\r\nas 3a-1d-5a.\r\n7 You can access the catalogue of the historical musical heritage in Castilla-La Mancha, made by the CIDoM, in the following\r\nlink: http:\/\/beta.cidom.es\/patrimonio-musical\/patrimonio-musical-historico\/bases-de-datos-de-compositores-y-obras-de-castillala-mancha\r\n\r\n83\r\n\r\n\freception: how the characters and the episodes of his works have been selected by composers and\r\nperceived by the audience and what kind of musical treatment \u2212genres, musical patterns, etc.\u2212 each\r\ncomposer provides (Pastor, 2007; Pastor, 2009).\r\nAt the same time, there cannot be any doubt that Cervantes\u2019 works reflect faithfully the Spanish\r\nmusical world of 16th and 17th centuries: musical instruments, dances and bailes, romances and songs\r\nare often cited and performed in his pages in order to depict not only a special and picturesque\r\nenvironment in which his characters evolve such as a gypsy\u2019s world in La gitanilla or Muslim\u2019s traditions\r\nin La gran sultana or Los ba\u00F1os de Argel, but in addition assign a particular semantic value to each\r\nmusical element adding a supplementary meaning to the work\u2019s understanding (Pastor, 2005; Pastor,\r\n2006). Our digital project distinguishes between three different aspects considered as a powerful educative\r\ninstrument:\r\n3.1. Musical instruments\r\nThis first point of the project will provide a catalogue of the musical instruments cited by Cervantes in\r\nhis works, explaining their social functions in the texts and offering, from an educative point of view,\r\ndifferent sound files, image files and text files in order to familiarize the users with the musical world\r\naround Don Quixote\u2019s author8. Let's consider some examples. In the First Part of Don Quixote (I, XXVI),\r\nthe mad knight says to his squire:\r\n[\u2026] for know, Sancho, that all or most of the knights-errant of times past were great poets and great musicians; these\r\ntwo accomplishments, or rather graces, being annexed to lovers-errant. True it is, that the couplets of former knights\r\nhave more of passion than elegance in them. (Don Quixote, I, XXVI)\r\n\r\nIn the Second Part, in the adventure in Duke\u2019s Palace, Don Quixote requests a lute to console Altisidora:\r\n\u201CDo me the favour, se\u00F1ora, to let a lute be placed in my chamber to-night; and I will comfort this poor maiden to the\r\nbest of my power; for in the early stages of love a prompt disillusion is an approved remedy;\u201D and with this he retired,\r\nso as not to be remarked by any who might see him there.\r\nHe had scarcely withdrawn when Altisidora, recovering from her swoon, said to her companion, \u201CThe lute must be left,\r\nfor no doubt Don Quixote intends to give us some music; and being his it will not be bad.\u201D\r\nThey went at once to inform the duchess of what was going on, and of the lute Don Quixote asked for, and she, delighted\r\nbeyond measure, plotted with the duke and her two damsels to play him a trick that should be amusing but harmless\r\n(Don Quixote, II, XLVI)\r\n\r\nThere cannot be any doubt that Cervantes\u2019s works faithfully reflect the Spanish musical world of the 16th\r\nand 17th centuries: musical instruments, dances and bailes, romances and songs are often mentioned and\r\nperformed in his books depicting not only the environment in which his characters evolve but they also\r\nadd a particular semantic value to each musical element. Participants in this galaxy of musical\r\nperformance are representatives of all walks of life, from the highest noble to the lowliest peasant, and\r\nthe number of instruments one encounters in Cervantes\u2019s writings is truly extensive. Cervantes groups\r\nthem in pastoral, military, popular and aristocratic and there are fifty different instruments cited in his\r\nworks. Let\u2019s go to see some examples, but I would caution previously that the English translations\r\nconsulted don\u2019t respect exactly the nature of musical instruments.\r\nWe see in Cervantes that harps and lutes are playing together. We have several texts in Cervantes that\r\ndescribe the performance of harps and lutes together:\r\nBut the instant the car was opposite the duke and duchess and Don Quixote the music of the clarions ceased, and then\r\nthat of the lutes and harps on the car, and the figure in the robe rose up, and flinging it apart and removing the veil\r\nfrom its face, disclosed to their eyes the shape of Death itself, fleshless and hideous, at which sight Don Quixote felt\r\n8\r\n\r\nTo access the catalogue of musical instruments in Cervantes, use the following link: http:\/\/beta.cidom.es\/musica-yliteratura\/cervantes-y-la-musica\/instrumentos-musicales-en-cervantes.\r\n\r\n84\r\n\r\n\funeasy, Sancho frightened, and the duke and duchess displayed a certain trepidation. Having risen to its feet, this living\r\ndeath, in a sleepy voice and with a tongue hardly awake, held forth as follows:\r\nI am that Merlin who the legends say\r\nThe devil had for father, and the lie\r\nHath gathered credence with the lapse of time. (Don Quixote, II, XXXV)\r\n\r\nThe harp is used too as an aristocratic instrument for ladies:\r\nCalliope\r\nWith so much peculiarity, with so much sweetness, with such harmony, she touched the harp of the graceful muse. She,\r\nhaving sounded the strings awhile, with a voice sonorous past conception, then gave utterance to these stanzas:\r\nSong of Calliope\r\nTo the sweet sound of my attempered lyre\r\nOh shepherds listen with attentive ear (La Galatea, V)\r\nLucinda\r\nI passed in such employments as are not only allowable but necessary for young girls, those that the needle, embroidery\r\ncushion, and spinning wheel usually afford, and if to refresh my mind I quitted them for a while, I found recreation in\r\nreading some devotional book or playing the harp, for experience taught me that music soothes the troubled mind and\r\nrelieves weariness of spirit. (Don Quixote, I, XXVIII)\r\nAltisidora\r\nHe trembled lest he should fall, and made an inward resolution not to yield; and commending himself with all his might\r\nand soul to his lady Dulcinea he made up his mind to listen to the music; and to let them know he was there he gave a\r\npretended sneeze, at which the damsels were not a little delighted, for all they wanted was that Don Quixote should\r\nhear them. So having tuned the harp, Altisidora, running her hand across the strings, began this ballad:\r\nO thou that art above in bed,\r\nBetween the holland sheets,\r\nA-lying there from night till morn,\r\nWith outstretched legs asleep; (Don Quixote,II, XLIX)\r\n\r\nAll these elements studied and analyzed can be consulted on the digital\r\nhttp:\/\/beta.cidom.es\/musica-y-literatura\/cervantes-y-la-musica\/instrumentos-musicales-encervantes\/instrumentos\/1\/arpa.html\r\n\r\nplatform:\r\n\r\n3.2. Songs, romances, dances and bailes\r\nThis second point deals with the accomplishment of the digital edition of the scores related with\r\nCervantes\u2019 texts (Pastor, 2017). For example, some chapters of the First Part of Don Quixote begin with\r\nthe first verse of a sung poem. This interactive frame will be accompanied by sound files, facsimile\r\neditions, bibliographical information about composers, and different articles explaining the significance\r\nof the relationship between music and poetry in Cervantes\u2019 works9. I would like to underline that some\r\nchapters of the First Part of Don Quixote begin with the first verse of a sung poem. Many chapters of\r\nboth parts begin with one, two or several \u201Caccidental verse-lines\u201D \u00B4\u2013prose lines that may be read and,\r\nconsequently, sung\u2013 as endecasyllables, octosyllables, heptasyllables: there are also so many indeed that\r\nwe must assume they are not there by chance but deliberately. It shouldn\u2019t be overlooked that chapter one\r\nof the First Part of Don Quixote also begins with a ballad-line to identify the place where Don Quixote\r\nlived: \u201CEn un lugar de la Mancha\u201D [In a place in La Mancha]. Although in this last case we haven\u2019t got\r\nany evidence or proof of its musical performance, it\u2019s easy to imagine that Cervantes might have\r\nconceived the beginning of his novel like an epic poem composed to be sung (Pastor, 2005).\r\n\r\nUse the following link to see the database of songs, romances, dances and bailes in Cervantes\u2019 texts:\r\nhttp:\/\/beta.cidom.es\/musica-y-literatura\/cervantes-y-la-musica\/danzas-y-bailes-en-cervantes.\r\n9\r\n\r\n85\r\n\r\n\fThe same thing happens with another romance, \u201CMira Nero de Tarpeya\u201D (\u201CNero fiddled while Rome\r\nburned\u201D), that relates the history of an indolent Nero playing the harp from Tarpeian hill while Rome was\r\nburning. This romance was very famous in Iberian Peninsula and was put in music by Bermudo\r\n(Declaraci\u00F3n de instrumentos musicales, 1555) and Venegas de Henestrosa (Libro de cifra nueva para\r\ntecla, harpa y vihuela, 1557). Cervantes introduces and intersperses in several episodes of Don Quixote\r\nthis musical reference as an echo of the musical romance emphasizing the semantic value of the madness.\r\nFirst time it appears, is after Desperate song of Gris\u00F3stomo:\r\nOr comest thou to triumph in the cruel exploits of thy inhuman disposition, or to behold from that eminence,\r\nlike another pitiless Nero, the flames of burning Rome; or insolently to trample on this unhappy corpse, as\r\ndid the impious daughter on that of her father Tarquin? (Don Quixote, I, XIV)\r\n\r\nSecond occurrence, it appears as parody, when Sancho gets stuffed in Camacho\u2019s Wedding:\r\nSancho beheld all this, and was nothing grieved thereat; but rather, in compliance with the proverb he very\r\nwell knew, When you are at Rome, do as they do at Rome, he demanded of Ricote the bottle, and took his\r\naim, as the others had done, and not with less relish. (Don Quixote, II, LIV)\r\n\r\nThe last occurrence of the romance is part of the fun of Altisidora, who makes mock of Don Quixote,\r\nintegrated in another long romance she sings:\r\nManchegan Nero, look not down\r\nFrom thy Tarpeian Rock\r\nUpon this burning heart, nor add\r\nThe fuel of thy wrath. (Don Quixote, II, XLIV)\r\n\r\nAll these elements studied and analysed can be consulted too on the digital platform:\r\nhttp:\/\/beta.cidom.es\/musica-y-literatura\/cervantes-y-la-musica\/canciones-y-topicos-musicales-encervantes\/canciones\/8\/mira-nero-de-tarpeya.html\r\n3.3. Musical reception of Cervantes\u2019 works\r\nFinally, the development of this project will provide a complete catalogue of musical compositions based\r\nin Cervantes\u2019 texts. Information included will be articulated by genres, countries, and musical\r\nperiodization and it will be the first step to seriously study how the Cervantes\u2019 literary genius has\r\nencouraged the composers\u2019 creative imagination10. Some composers who have put the work of Cervantes\r\ninto music can be consulted on our digital platform: http:\/\/beta.cidom.es\/musica-y-literatura\/cervantes-yla-musica\/la-recepcion-musical-cervantina\/recepcionmusical\/3\/millan-de-las-heras-manuel-1971--.html\r\n\r\n4\r\n\r\nConclusions\r\n\r\nFor the CIDoM, the main objective is the cataloguing and digitalisation of the musical heritage of the\r\nregion of Castilla-La Mancha, as well as facilitating the researcher\u2019s search and relation between data,\r\nand providing access to primary sources. In addition, it is crucial to project the results of our research on\r\nthe area of Music Education and to disseminate this information to the educational community, in order\r\nto create and to implement educational tools \u2212demanded by music teachers\u2212 concerned with music\r\nheritage, thus increasing the quality and the cross-sectional relations of the musical education in the\r\ndifferent educational levels. In this sense, the project about musical reception of Cervantes\u2019 works has a\r\nhigh pedagogical project for us (Pastor, 2016). For this reason, the interdisciplinary vocation with which\r\nthe digital projects presented here are born seeks in the educational field the adequate space to project\r\nuniversity research on the reality of other academic levels.\r\n\r\nYou can access to the database about musical reception of Cervantes\u2019 works in the following link: http:\/\/beta.cidom.es\/musicay-literatura\/cervantes-y-la-musica\/la-recepcion-musical-cervantina.\r\n10\r\n"
	},
	{
		"id": 15,
		"title": "Una proposta di ontologia basata su RDA per il patrimonio culturale di Vincenzo Bellini",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Salvatore Cristofaro",
			"Daria Spampinato"
		],
		"body": "Introduzione\r\n\r\nIl patrimonio culturale conservato al Museo Civico Belliniano di Catania comprende collezioni di oggetti\r\n(o risorse) di natura molto variegata riconducibile ai settori museale, bibliografico e archivistico con la\r\nspecificit\u00E0 del domino musicale. Allo stato attuale, le risorse identificate consistono di (circa):\r\n\u2013 250 oggetti tra dipinti, foto, pianoforti, spille, orologi, mobili, poster, medaglie, tessuti, ecc.;\r\n\u2013 4.500 fogli manoscritti di documenti e lettere;\r\n\u2013 9.300 fogli di spartiti manoscritti;\r\n\u2013 1.900 partiture a stampa;\r\n\u2013 50 opuscoli musicali a stampa;\r\n\u2013 280 libri della biblioteca del museo;\r\n\r\n88\r\n\r\n\f\u2013 60 dischi in vinile di varie composizioni musicali.\r\nNegli ultimi anni, questo ricco patrimonio culturale \u00E8 stato promosso in particolare dal progetto BellinInRete (Del Grosso et al., 2018). Il progetto BellinInRete nasce dalla collaborazione tra il Comune di\r\nCatania, l\u2019Istituto di Scienze e Tecnologie della Cognizione del CNR e il Dipartimento di Scienze Umanistiche dell\u2019Universit\u00E0 degli Studi di Catania. Esso mira a rinnovare e creare un cambiamento duraturo\r\nnella valorizzazione del Museo Civico Belliniano di Catania.\r\nAl fine di rendere il patrimonio culturale del Museo Belliniano interoperabile e riutilizzabile da\r\nstudiosi, operatori culturali ed utenti generici si propone l\u2019organizzazione semantica di questo patrimonio\r\nin un unico contenitore omogeneo, l\u2019ontologia OntoBellini, progettata e sviluppata secondo i paradigmi\r\ndel Linked Open Data e del Semantic Web. La grande variet\u00E0 delle risorse museali coinvolte, non\r\nancora completamente digitalizzate e catalogate, ha condotto all\u2019idea di sperimentare lo standard di\r\nmetadatazione RDA (Resource Description and Access).1 RDA \u00E8 un package di concetti e istruzioni per la\r\ncreazione di metadati di risorse eterogenee di biblioteche, archivi e beni culturali (Bianchini and Guerrini,\r\n2016).\r\nIn questo articolo viene descritto il lavoro in fase di sviluppo per la realizzazione dell\u2019ontologia\r\nOntoBellini.2\r\nL\u2019articolo \u00E8 organizzato come segue. Nella Sezione 2 vengono esaminati brevemente alcuni lavori\r\ncorrelati e nella Sezione 3 viene descritto il lavoro svolto e in corso di realizzazione relativo all\u2019analisi\r\ne alla rappresentazione delle risorse del Museo Belliniano, motivando l\u2019esplorazione e lo sfruttamento\r\ndi RDA per la costruzione dell\u2019ontologia OntoBellini. La Sezione 4 presenta, a titolo di esempio, una\r\ndescrizione tassonomica ad alto livello della parte dell\u2019ontologia OntoBellini che si intende sviluppare\r\nrelativa ad un corpus di lettere di Vincenzo Bellini e, infine, nella Sezione 5 si traggono le conclusioni e\r\nsi discutono suggerimenti per lavori futuri.\r\n\r\n2 Lavori correlati\r\nNegli corso degli anni sono state presentate varie proposte riguardanti l\u2019organizzazione semantica del\r\npatrimonio culturale dei musei. Molte di esse si basano sul CIDOC Conceptual Reference Model (CIDOCCRM),3 che rappresenta lo standard internazionale per lo scambio controllato di informazioni riguardanti\r\ni beni culturali dal 2006. CIDOC-CRM fornisce un\u2019ontologia di base generale che pu\u00F2 essere adottata in\r\ncontesti concernenti il patrimonio culturale per sviluppare sistemi informativi semantici basati sul web\r\ne per migliorare la condivisione delle informazioni. Basandosi su CIDOC-CRM, sono stati sviluppati\r\nvari modelli di organizzazione della conoscenza volti a migliorare l\u2019espressivit\u00E0 semantica nel dominio\r\ndel patrimonio culturale e per affrontare questioni specifiche non completamente contemplate da altri\r\nmodelli esistenti. Questo \u00E8 il caso, ad esempio, delle ontologie entry OA e entry F presentate in (Daquino\r\net al., 2017), che arricchiscono le capacit\u00E0 descrittive di CIDOC-CRM attraverso la definizione di svariate\r\npossibili relazioni tra opere d\u2019arte (entry OA) e fotografia (entry F), seguendo gli standard italiani promossi\r\ndall\u2019ICCD 4 Scheda OA e Scheda F, rispettivamente. In (Moraitou et al., 2019) \u00E8 possibile trovare un\r\nampio elenco di altri progetti e proposte basate su CIDOC-CRM nel settore dei beni culturali.\r\nNel contesto della promozione del patrimonio culturale \u00E8 emerso di recente lo standard RDA. Gli\r\nobiettivi principali di RDA sono l\u2019identificazione e la messa in relazione di entit\u00E0 a livello astratto. Inizialmente, RDA implementava il modello di dati Functional Requirements for Bibliographic Records\r\n(FRBR), classificando le risorse informative in termini di una gerarchia di entit\u00E0 a quattro livelli chiamata WEMI (Work, Expression, Manifestation, Item).5 Successivamente, dal novembre 2016, il comitato\r\ndirettivo di RDA ha concordato l\u2019adozione dell\u2019IFLA Library Reference Model (LRM)6 come modello\r\n1http:\/\/www.rda-rsc.org\/. Tutti gli url citati in questo contributo sono stati visitati il 27 novembre 2019.\r\n2Il presente articolo costituisce una versione aggiornata del contributo ad opera degli stessi autori dal titolo OntoBellini:\r\ntowards an RDA based ontology for Vincenzo Bellini\u2019s cultural heritage presentato al convegno JOWO 2019, 23 settembre 2019.\r\n3http:\/\/www.cidoc-crm.org\/\r\n4ICCD (Istituto Centrale per il Catalogo e la Documentazione) - http:\/\/www.iccd.beniculturali.it\/\r\n5https:\/\/www.ifla.org\/best-practice-for-national-bibliographic-agencies-in-adigital-age\/node\/8915\r\n6https:\/\/www.ifla.org\/publications\/node\/11412\r\n\r\n89\r\n\r\n\fconcettuale per lo sviluppo di RDA,7 sostituendo FRBR.\r\nRDA aspira a fornire uno standard universale per il data-recording, un codice univoco per rappresentare\r\nrisorse eterogenee che si possono trovare in:\r\n(A) biblioteche (manoscritti, libri, musica e film);\r\n(B) archivi (documenti istituzionali, documenti personali e familiari e documentazione commerciale);\r\n(C) musei (opere d\u2019arte, costumi, oggetti e foto).\r\nSi evidenzia che, nel contesto italiano, le risorse relative a biblioteche, archivi e musei sono gestite,\r\nattraverso l\u2019utilizzo di norme solide e riconosciute, dalle rispettive istituzioni ICCU 8, ICAR 9 e ICCD.\r\nMentre l\u2019Associazione Italiana MAB 10 esplora le prospettive di convergenza tra i professionisti e le\r\ncompetenze in materia di musei, archivi e biblioteche.\r\nNegli ultimi anni RDA ha attratto l\u2019interesse di diverse istituzioni culturali pubbliche sia europee\r\nche d\u2019oltre oceano che lo hanno adottato e implementato, sperimentandone applicazioni alla catalogazione e condivisione di risorse bibliotecarie (si vedano, ad esempio, (Ducheva and Pennington,\r\n2017) e (Panchyshyn et al., 2019)).\r\n\r\n3\r\n\r\n(Ri)organizzazione dei dati museali\r\n\r\nNell\u2019ambito del progetto BellinInRete, il patrimonio del Museo Civico Belliniano \u00E8 stato parzialmente\r\nstudiato e analizzato da esperti musicologi e da specialisti con competenze museali, archivistiche e\r\nbibliotecarie, con l\u2019obiettivo di recuperare informazioni sulle risorse del museo. Queste sono state\r\nquindi rappresentate formalmente come record di dati che comprendono diversi campi di informazione\r\n(si veda sotto). La collezione di questi record costituisce la base su cui si fonda la presente proposta di\r\norganizzazione semantica del patrimonio belliniano.11\r\nI record delle risorse museali sono stati creati seguendo gli standard italiani ICCD e ICCU per la\r\ncatalogazione e la documentazione (Scheda OA, Scheda F e schede SBN). Il numero dei campi di\r\nciascuno record e il loro significato dipende dal tipo di risorsa rappresentata dal record stesso. Sono stati\r\nidentificati 14 diversi tipi base di risorse museali, ossia:\r\nManoscritti\r\n\r\nTesti a stampa\r\n\r\nMusica manoscritta\r\n\r\nMusica a stampa\r\n\r\nMateriale grafico\r\n\r\nArredi\r\n\r\nDipinti\r\n\r\nDocumenti\r\n\r\nFoto\r\n\r\nMedaglie\r\n\r\nStatue\r\n\r\nStrumenti musicali\r\n\r\nTessuti\r\n\r\nOggetti generici\r\n\r\nAll\u2019interno di ciascun tipo base, le risorse sono suddivise, a loro volta, in sottotipi pi\u00F9 specializzati.\r\nAd esempio, i Manoscritti comprendono: lettere (originali), bozze e minute di lettere, copie di lettere,\r\ncertificati di battesimo, certificati di morte, certificati di matrimonio, note di spesa, bollettini medici,\r\necc. Il Materiale grafico comprende i poster, mentre gli spartiti rientrano nella Musica manoscritta.\r\nGli Oggetti generici comprendono oggetti personali di Vincenzo Bellini, come orologi e spille, e altri\r\noggetti di vita quotidiana come cucchiai, coltelli, tazze, ecc. In Figura 1 si riporta una selezione di\r\ncampi di record in forma tabellare. Si noti che il blocco di informazioni memorizzato in alcuni campi di\r\nrecord presenta un basso livello di granularit\u00E0 che potrebbe essere ulteriormente raffinato suddividendo\r\nil blocco tra campi aggiuntivi di dati atomici. Questo \u00E8 il caso, ad esempio, del campo formato (si veda\r\nnella Figura 1 la penultima colonna della tabella pi\u00F9 in alto) che viene utilizzato per descrivere alcune\r\ncaratteristiche fisiche di un manoscritto, come dimensioni, numero di pagine, foliazione, direzione della\r\nscrittura, ecc. Si osservi anche che alcuni campi di record sono specifici per il particolare tipo di risorsa\r\n7http:\/\/www.rda-rsc.org\/ImplementationLRMinRDA\r\n8ICCU (Istituto Centrale per il Catalogo Unico) - https:\/\/www.iccu.sbn.it\/it\/\r\n9ICAR (Istituto Centrale per gli Archivi) - www.icar.beniculturali.it\/ 10MAB (Musei Archivi Biblioteche) http:\/\/www.mab-italia.org\/\r\n11Si noti che, attualmente, il numero dei record creati corrisponde a circa il 70% del numero totale delle risorse stimate del\r\nmuseo. Il coinvolgimento delle rimanenti risorse \u00E8 programmato per il prossimo futuro.\r\n\r\n90\r\n\r\n\fFigura 1: Alcuni record corrispondenti alle risorse del Museo Belliniano: le righe verdi contengono i\r\nnomi dei campi dei record; le righe blu indicano i tipi base delle risorse. Si noti che la tabella pi\u00F9 in alto\r\ncoinvolge solo risorse cartacee.\r\nmuseale descritta da questi campi. Ad esempio, il campo lingua (cfr. Figura 1) \u00E8 stato specificamente\r\nutilizzato per rappresentare la(e) lingua(e) delle risorse scritte e non pu\u00F2 certo essere applicato agli\r\noggetti; cos\u00EC come non ha senso parlare (ad esempio) della foliazione di un tavolo o di una sedia (infatti\r\nla foliazione \u00E8 una informazione specifica del campo formato relativo ai manoscritti).\r\nIl patrimonio del Museo Belliniano coinvolge anche alcuni oggetti fisici composti (come contenitori per\r\nmedaglie e cornici fotografiche) che richiedono una struttura gerarchica di record per essere ragionevolmente descritti.12 Inoltre, il Belliniano conserva anche alcuni libretti musicali che non sono stati ancora\r\ncatalogati. Si sottolinea ulteriormente che diversi documenti d\u2019archivio (come i vari certificati), hanno\r\nricevuto ad oggi un\u2019analisi solo approssimativa: all\u2019interno del progetto BellinInRete si prevede di creare\r\nmetadati dettagliati per essi seguendo gli standard adottati dal Sistema Archivistico Nazionale Italiano13\r\ngestito dall\u2019ICAR.\r\nCome emerge dalle considerazioni precedenti, le rappresentazioni delle risorse del Museo Belliniano\r\ncreate presentano, allo stato attuale, un carattere eterogeneo con un basso livello di granularit\u00E0 che rende\r\ndifficile tradurle in una base di conoscenza ontologica espressiva ed efficace.14 (Si noti che ci\u00F2 deriva in\r\n12Allo stato attuale, tali oggetti composti non sono ancora stati disassemblati per motivi di conservazione, e quindi, al\r\nmomento, \u00E8 stato possibile recuperare poche informazioni descrittive per essi.\r\n13http:\/\/san.beniculturali.it\/SAN\r\n14Si osservi comunque che recentemente una parte (ristretta) delle risorse museali del Belliniano, consistente in un corpus di\r\nlettere di Vincenzo Bellini, \u00E8 stata oggetto di studi sistematici approfonditi che hanno evidenziato diversi aspetti semanticamente\r\ninteressanti facilmente formalizzabili in un\u2019ontologia. (Ci\u00F2 verr\u00E0 discusso nella Sezione 4.)\r\n\r\n91\r\n\r\n\flettera\r\nnomina\r\n\r\nhaSupporto\r\n\r\nentit\u00E0\r\n\r\npagina\r\n\r\nhaDimensione\r\n\r\ndimensione\r\n\r\nhaDestinatario\r\nSubclass of\r\nSubclass of\r\nhaAutore\r\n\r\nriferimento\r\n\r\nSubclass of\r\n\r\nhaOpposto\r\nSubclass of\r\n\r\nSubclass of\r\nSubclass of\r\nSubclass of\r\noriginale\r\n\r\nSubclass of\r\n\r\ntermine\r\n\r\nSubclass of\r\nSubclass of\r\n\r\nopera\r\ncopia\r\n\r\npreliminareDi\r\nluogo\r\nminuta\r\n\r\norganizzazione\r\nbozza\r\n\r\npersona\r\n\r\nFigura 2: Schema ontologico del corpus epistolare belliniano.\r\nparte dai particolari criteri di rappresentazione adottati per la creazione dei record di dati corrispondenti\r\nalle risorse del museo.)\r\nAl fine di migliorare tali rappresentazioni sarebbe innanzitutto utile pulire e raffinare i record dei dati\r\nacquisiti, in modo da ottenere una collezione pi\u00F9 uniforme. Quindi, le istruzioni RDA potrebbero poi\r\nessere proficuamente sfruttate per ottenere una (ri)organizzazione pi\u00F9 efficace dei dati. Difatti se si dovesse\r\nrappresentare soltanto la collezione museale (composta da oggetti unici) si potrebbe utilizzare CIDOCCRM che \u00E8 stato progettato principalmente per questa tipologia di risorse. Ma volendo utilizzare un unico\r\nmodello di rappresentazione dei dati per l\u2019intero patrimonio belliniano, RDA, attraverso il meccanismo\r\ndi classificazione WEMI (ereditato da FRBR) si presta meglio alla descrizione delle risorse.15\r\nIn termini molto generali, le principali attivit\u00E0 coinvolte nello sviluppo dell\u2019ontologia OntoBellini,\r\npossono quindi essere schematizzate come segue. Dopo una prima fase di ristrutturazione dei dati, con\r\nl\u2019obiettivo di creare collezioni di record pi\u00F9 omogenee e pi\u00F9 dettagliate (come descritto sopra), si prevede\r\ndi identificare i concetti e le propriet\u00E0 alla base dell\u2019ontologia OntoBellini in conformit\u00E0 con il framework\r\nentit\u00E0-relazioni di RDA, e quindi sviluppare l\u2019ontologia stessa rendendola accessibile via web.\r\n\r\n4 Il caso delle lettere Belliniane\r\nUna parte peculiare del progetto BellinInRete riguarda la rappresentazione, l\u2019organizzazione e la codifica\r\nsecondo lo standard TEI-XML (Text Encoding Initiative),16 di un corpus di lettere della corrispondenza\r\ndi Vincenzo Bellini (corpus epistolare) che forniscono informazioni interessanti per diversi aspetti legati\r\nalla vita sociale e all\u2019attivit\u00E0 artistica in ambito musicale del compositore catanese (si veda (Del Grosso\r\net al., 2018)). In questa sezione viene fornita, a titolo esemplificativo, una descrizione ad alto livello della\r\ntassonomia di base della parte dell\u2019ontologia OntoBellini che si intende sviluppare relativa al solo corpus\r\nepistolare, presentandone il corrispondente schema ontologico (cfr. Figura 2). Questo corpus epistolare,\r\ninfatti, \u00E8 stato recentemente studiato ed analizzato in maniera pi\u00F9 dettagliata ed approfondita rispetto\r\nalle altre risorse museali e, a differenza di queste ultime, le informazioni disponibili ad esso relative\r\nrisultano, allo stato attuale, pi\u00F9 complete e strutturate e permettono di delineare un quadro pi\u00F9 concreto e\r\ndefinito degli item di conoscenza da rappresentare e dedurre attraverso l\u2019ontologia. Pi\u00F9 specificatamente,\r\nl\u2019analisi del corpus epistolare belliniano ha condotto alle seguenti considerazioni. Il corpus epistolare \u00E8\r\n15Si menziona che, basandosi sui modelli IFLA FR, \u00E8 stata sviluppata un\u2019estensione di CIDOC-CRM, ossia FRBRoo\r\n(http: \/\/www.cidoc-crm.org\/frbroo-0), che intende rappresentare la semantica delle informazioni bibliografiche\r\ne facilitare l\u2019integrazione e lo scambio di risorse bibliografiche e museali. Tuttavia IFLA LRM, come modello concettuale\r\nsottostante di RDA, consente un livello di generalit\u00E0 maggiore rispetto a FRBRoo, poich\u00E9 include meno dettagli di quest\u2019ultimo.\r\n16https:\/\/www.tei-c.org\/\r\n\r\n92\r\n\r\n\fcostituito essenzialmente da 4 tipologie di documenti epistolari (o lettere) ossia: bozze (di lettere), minute\r\n(di lettere), copie (di lettere) e originali (di lettere).17 Ogni documento epistolare ha uno o pi\u00F9 autori, \u00E8\r\nindirizzato ad uno o pi\u00F9 destinatari ed \u00E8 fisicamente contenuto (scritto) su un supporto costituito da una o\r\npi\u00F9 facciate di fogli di carta (pagine) aventi differenti dimensioni: uno stesso documento pu\u00F2 essere infatti\r\nframmentato su diverse pagine e (parti di) documenti diversi possono trovarsi su una stessa pagina o su\r\npagine opposte (fronte-retro) di uno stesso foglio di carta. Inoltre, in ogni documento epistolare vengono\r\nnominate (in maniera esplicita o implicita) diverse entit\u00E0 interessanti quali persone, organizzazioni,\r\nluoghi, opere (musicali), termini (musicali e non) e riferimenti (bibliografici); in particolare, il complesso\r\ndelle persone e delle organizzazioni include gli autori e i destinatari delle lettere menzionati sopra.\r\nLe precedenti considerazioni si traducono nello schema ontologico riportato in Figura 2 che rappresenta\r\nla tassonomia di base della parte dell\u2019ontologia OntoBellini relativa al corpus epistolare del Belliniano.18\r\nSi osservi che l\u2019organizzazione semantica proposta e rappresentata in Figura 2 per il corpus epistolare \u00E8\r\nmolto generale e di alto livello. Ai fini dell\u2019espressivit\u00E0 essa pu\u00F2 (e deve) essere specializzata imponendo\r\ndegli opportuni vincoli semantici attraverso l\u2019introduzione di appositi assiomi di classi e propriet\u00E0. Ad\r\nesempio, in riferimento alla rappresentazione in Figura 2, sarebbe ragionevole assumere che le classi\r\nbozza, minuta, copia e originale siano disgiunte (si veda la nota n. 17) e che inoltre le propriet\u00E0\r\nhaDimensione e haOpposto siano entrambe funzionali e con la seconda ulteriormente simmetrica e con\r\ninversa funzionale.19 In aggiunta si potrebbe postulare anche la validit\u00E0 della propriet\u00E0 che gli autori e i\r\ndestinatari delle bozze, delle copie e delle minute coincidano con quelli delle rispettive versioni originali,\r\ne cos\u00EC via.\r\n\r\n5\r\n\r\nConclusioni e lavori futuri\r\n\r\nIn questo lavoro \u00E8 stata proposta l\u2019organizzazione semantica del patrimonio culturale conservato nel Museo\r\nCivico Belliniano di Catania attraverso un\u2019ontologia condivisa \u2013l\u2019ontologia OntoBellini\u2013, basandosi sulla\r\ngrande quantit\u00E0 di dati attualmente acquisiti per le risorse del museo. Il basso livello di granularit\u00E0 e il\r\ncarattere eterogeneo di questi dati richiede tuttavia una riorganizzazione preliminare degli stessi al fine\r\ndi renderli pi\u00F9 omogenei e facilmente codificabili nell\u2019ontologia. A tal fine si prevede di sfruttare le\r\nindicazioni RDA per la creazione di metadati di risorse di biblioteche e beni culturali. A titolo di esempio\r\n\u00E8 stata brevemente descritta una proposta di organizzazione semantica ad alto livello relativa ad un corpus\r\ndi lettere di Vincenzo Bellini custodite nel museo Belliniano, presentandone il corrispondente schema\r\nontologico."
	},
	{
		"id": 16,
		"title": "Biblioteche di conservazione e libera fruizione dei manoscritti digitalizzati: la Veneranda Biblioteca Ambrosiana e la svolta inevitabile grazie a IIIF",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Fabio Cusimano"
		],
		"body": "Le funzioni della biblioteca di conservazione e la digitalizzazione\r\n\r\nAlla prova della tradizione storica, la mission che da sempre caratterizza le biblioteche consiste nel\r\nraccogliere, organizzare, ampliare e diffondere la conoscenza tramite l\u2019accesso alle risorse in esse custodite\r\n(Bottasso 1999; Gorman 2004; Kempf 2013; Fabian 2015, 55-70), alla costante ricerca di un delicato\r\nequilibrio tra fruizione e conservazione.\r\nA tal proposito, come affermano Montecchi e Venuda, \u00ABLe due dimensioni dell\u2019attivit\u00E0 bibliotecaria,\r\nquella orizzontale dell\u2019uso dei libri da parte dei nostri contemporanei e quella verticale della loro\r\nconservazione per i posteri, costituiscono i due poli attorno ai quali si strutturano i servizi di ogni biblioteca:\r\nal prevalere dell\u2019uno o dell\u2019altro avremo \u201Cbiblioteche di conservazione\u201D o \u201Cbiblioteche d\u2019uso\u201D, anche se \u00E8\r\nben difficile incontrare biblioteche finalizzate unicamente ed esclusivamente all\u2019una o all\u2019altro. Non esiste,\r\ninfatti, neppure in sede teorica, una netta opposizione tra questi due parametri, essendo la conservazione\r\nfinalizzata all\u2019uso sia presente che futuro del libro e, sull\u2019altro versante, non potendo l\u2019uso dei libri in\r\nbiblioteca prescindere da forme di tutela e di conservazione che assicurino loro lunga vita tra gli uomini\u00BB\r\n(Montecchi e Venuda 2006, 79).\r\nSebbene ogni epoca abbia vissuto fondamentali momenti di evoluzione e progresso in ogni campo del\r\nsapere e della tecnica \u2012 spesso inavvertiti (Montecchi e Venuda 2006, 23; Eisenstein 1986; Eisenstein 2004;\r\nBarbier 2005; Roncaglia 2010; Febvre 2011; McLuhan 2011; Bertolo 2016; Cursi 2016) dai contemporanei \u2012\r\nla nostra societ\u00E0 appare dotata di tali e tanti strumenti tecnologici potenzialmente utili a diffondere la\r\nconoscenza e la cultura che viene spontaneo chiedersi come sia possibile che tutto questo non abbia\r\ncoinvolto nativamente il mondo delle biblioteche! Al giorno d\u2019oggi, infatti, non siamo ancora riusciti ad\r\naffrancarci da quello che ormai sembra essere divenuto un vero luogo comune, ovvero il rapporto antitetico\r\ntra biblioteca e tecnologia: in un simile contesto, la biblioteca di conservazione viene ancora percepita come\r\nil luogo refrattario per eccellenza alla tecnologia e all\u2019innovazione, destinato per definizione alla sola\r\ntesaurizzazione del proprio prezioso patrimonio.\r\n95\r\n\r\n\f2\r\n\r\nIl XVII secolo e l\u2019innovativit\u00E0 della Veneranda Biblioteca Ambrosiana di Milano\r\n\r\nLa Veneranda Biblioteca Ambrosiana (Rodella 1992, 121-147; Panizza 2012) di Milano viene\r\ntradizionalmente considerata \u2013 sin dalla sua solenne inaugurazione avvenuta l\u20198 dicembre 1609 \u2013 come uno\r\ndei primi e principali esempi di biblioteca pubblica (IFLA\/UNESCO Public Library Manifesto 1994)\r\nnell\u2019accezione di un\u2019istituzione creata con il chiaro intento di fornire accesso ai libri (Natale 1995, 1-2) a una\r\ncomunit\u00E0 di lettori (Galluzzi 2011) quanto pi\u00F9 ampia possibile (Serrai 2005, 7-9; Rovelstad 2000, 540-556).\r\nSi ritiene utile approfondire alcuni tratti caratteristici della fondazione della Biblioteca Ambrosiana che il\r\nsuo ideatore e fondatore \u2013 il cardinale Federico Borromeo (Prodi 1971, 33-42; Ravasi 1992, 1-19; Buzzi e\r\nFerro 2005) \u2013 fortemente volle aperta a tutti. Per farlo sar\u00E0 opportuno approcciarsi al modello di biblioteca\r\ntipico del tempo (Burke 1992, 391-416; Ghilli 2015, 365-376; DeSeta 2016), soprattutto attraverso la\r\ntestimonianza di Gabriel Naud\u00E9 (Rovelstad 2000, 549), autore del celebre Advis pour dresser une\r\nbiblioth\u00E8que (Naud\u00E9 1627). Egli riserva al IX e ultimo capitolo del suo Advis, dal titolo Quel doit estre le but\r\nprincipal de cette Bibliotheque, l\u2019aspetto pi\u00F9 importante legato alla trattazione teorica sull\u2019allestimento di\r\nuna biblioteca: quale debba essere lo scopo principale di una biblioteca ben allestita.\r\nIn questo modo il Naud\u00E9 loda senza riserve i particolari servizi che fanno dell\u2019Ambrosiana una vera\r\nbiblioteca aperta al pubblico, unica nel suo genere:\r\nCar pour ne parler que de l\u2019Ambrosienne de Milan, & monstrer par mesme moyen comme elle\r\nsurpasse tante en grandeur & magnificence que en obligeant le public beaucoup de celles d\u2019entre les\r\nRomains, n\u2019est-ce pas une chose du tout extraordinaire qu\u2019un chacun y puisse entrer \u00E0 toute heure\r\npresque que bon luy semble, y demeurer tant qu\u2019il luy plaist, voir, lire, extraire tel Autheur qu\u2019il aura\r\nagreable, avoir tous les moyens & commoditez de ce faire, soit en public ou en particulier, & ce sans\r\nautre peine que de s\u2019y transporter \u00E9s iours & heures ordinaires, se placer dans des chaires destinees\r\npour cet effect, & demander les livres qu\u2019il voudra fueillerer au Bibliothecaire ou \u00E0 trois de ses\r\nserviteurs, qui sont fort bien stipendiez & entretenus, tant pour servir \u00E0 la Bibliotheque qu\u2019\u00E0 tous ceux\r\nqui viennent tous les iours estudier en icelle (Naud\u00E9 1627, 155-156).\r\nProprio in relazione al precedente passo, il Serrai puntualizza che \u00ABper l\u2019Ambrosiana il riconoscimento di\r\nNaud\u00E9, evidentemente frutto di esperienza diretta, va ancora oltre per sfociare in un\u2019autentica stupefatta\r\nammirazione\u00BB (Serrai 2005, 8).\r\nLa descrizione del Naud\u00E9, infine, si avvia alla conclusione con altri interessanti spunti che vedono ancora\r\nl\u2019Ambrosiana assunta a termine di paragone:\r\n[\u2026] il faudroit premierement observer que toutes les Bibliotheques ne pouvant tousiours estre\r\nouvertes comme l\u2019Ambrosienne, il fust au moins permis \u00E0 tous ceux qui y auroient affaire d\u2019aborder\r\nlibrement le Bibliothecaire pour y estre introduits par iceluy sans aucune dilation ny difficult\u00E9:\r\nsecondement que ceux qui seroient totalement incognus, & tous autres qui n\u2019auroient affaire que de\r\nquelques passages, peussent veoir chercher & extraire de toutes sortes de livres imprimez ce dont ls\r\nauroient besoin: tiercement que l\u2019on permist aux personnes de merite & de cognoissance d\u2019emporter\r\n\u00E0 leurs logis les livres communs & de peu de volumes; [\u2026] (Naud\u00E9 1627, 161-162).\r\nProprio riguardo agli spunti di cui il Naud\u00E9 fa esplicita menzione, non si pu\u00F2 non rimanere stupiti per quanto\r\nessi richiamino concetti e servizi di cui oggi si fa un gran parlare, quali, ad esempio, il servizio di reference e\r\nil prestito dei volumi: tutto questo pu\u00F2 far sovvenire un collegamento tra la prassi descritta dal Naud\u00E9 \u2013 che\r\negli stesso auspica possa diffondersi quale strumento di base per l\u2019utenza presso ogni biblioteca \u2013 e la\r\ndigitalizzazione delle risorse catalografiche\/librarie. Quale migliore risposta agli ideali del cardinale Federico\r\nBorromeo e del Naud\u00E9 stesso, di una biblioteca le cui risorse possano essere sempre liberamente accessibili,\r\nricercabili e consultabili proprio attraverso specifici servizi online quali, appunto, una nuova biblioteca\r\ndigitale ad accesso libero?\r\n\r\n3\r\n\r\nDalla Bibliotheca alla Digital Library: cura delle collezioni e Data Curation\r\n\r\nUn altro documento, stavolta strettamente collegato alla vita della Veneranda Biblioteca Ambrosiana, \u00E8 di\r\nfondamentale importanza a proposito del tratteggio della figura del bibliotecario: si tratta delle Constitutiones\r\nCollegii ac Bibliothecae Ambrosianae (Bentivoglio 1835; Marcora 1986, 155-164; Annoni 1992, 149-184).\r\n96\r\n\r\n\fLe Consitutiones ambrosiane dedicano un intero capitolo alla figura del bibliotecario, alle sue mansioni e\r\nalle tipologie dei cataloghi: si tratta del Caput X, De Bibliothecario et Bibliotheca (Bentivoglio 1835, 32-39).\r\nPresso la Biblioteca Ambrosiana il Bibliothecarius \u00E8 stato affiancato dal Custos catalogi, il custode del\r\ncatalogo (Rodella 2013, 35-36): tale espressione risulta essere etimologicamente molto interessante e, come\r\nvedremo, gioca anche un ruolo importante nell\u2019apertura verso funzioni e attivit\u00E0 caratteristiche dell\u2019era\r\ndigitale, quali il Data Curator e il derivato Data Curation.1 Il Data Curator si ispira ai medesimi principi\r\nche guidavano il Custos catalogi del XVII secolo e opera per prendersi cura dei cataloghi (oggi\r\nprevalentemente OPAC), delle informazioni catalografiche (oggi prevalentemente codificate in formati\r\nstandard come l\u2019ISO2709), dei metadati (descrittivi, amministrativi, gestionali, tecnici, tutti accomunati dai\r\ntag e dai metalinguaggi adottati per la loro compilazione, quali, ad esempio, DublinCore e XML), degli\r\noggetti digitali (cos\u00EC come dei diversi formati, specialmente per quanto concerne le immagini digitali), delle\r\nsvariate procedure tecniche da attivare di volta in volta per avviare la produzione di nuovi oggetti digitali\r\ntramite l\u2019utilizzo di differenti apparecchiature (macchine fotografiche digitali, scanner, ecc.), come anche per\r\ngarantire la conservazione (storage) e il perdurare dell\u2019informazione digitale. Altro fondamentale aspetto \u00E8\r\nquello della progettazione globale degli interventi di digitalizzazione e della messa a punto del necessario\r\nflusso di lavoro (workflow) ad essi collegati.\r\n3.1.\r\n\r\nLa nuova biblioteca digitale della Veneranda Biblioteca Ambrosiana\r\n\r\nNel percorso d\u2019attuazione della nuova fase di digitalizzazione presso la Veneranda Biblioteca Ambrosiana si\r\n\u00E8 cercato di tenere in debito conto quanto gi\u00E0 sperimentato presso altre realt\u00E0 a livello internazionale, avendo\r\ncura di porre le basi per la realizzazione di un progetto necessariamente scalabile e aperto a proficue\r\ncollaborazioni e condivisioni, nazionali e internazionali, sia a livello tecnico che scientifico.\r\nIl nodo del data reuse, per esempio, si \u00E8 subito imposto in maniera molto concreta: con la precedente\r\nattivit\u00E0 di digitalizzazione, infatti, \u00E8 stata prodotta un\u2019ingentissima mole di dati (oltre 1.800.000 immagini in\r\nformato .tif non compresso, colori, 24 bit) che rappresentano ancora oggi un prezioso nucleo composto da\r\noltre 2.700 manoscritti integralmente digitalizzati su cui basare l\u2019avvio di una nuova fase di digitalizzazione.\r\nTale ingente quantit\u00E0 di dati, pari a circa 31 Tb di spazio-disco, necessita di cure costanti e va ad aggiungersi\r\nalla quotidiana produzione di nuove copie digitali di manoscritti, il tutto con il preciso obiettivo di rendere\r\nprogressivamente disponibili online, gratuitamente e pubblicamente, le riproduzioni digitali integrali di parte\r\ndel patrimonio manoscritto ambrosiano.\r\n\r\nFigura 1: rappresentazione schematica dell\u2019infrastruttura di digitalizzazione della Biblioteca Ambrosiana; in basso a\r\ndestra: i loghi di IIIF-International Image Interoperability Framework e del visualizzatore Mirador.\r\n\r\n1\r\n\r\nProprio nel merito delle funzioni del Data Curator emerge evidente il collegamento etimologico al Custos catalogi cui ho fatto\r\nriferimento in precedenza: l\u2019etimologia del termine inglese curator \u00E8 direttamente derivata dal latino, rispettivamente dal verbo curo\r\ne dal sostantivo curator, ed \u00E8 proprio per questo motivo che \u00E8 possibile mettere in relazione tra loro le due figure.\r\n97\r\n\r\n\fTale meritorio obiettivo chiama in causa un aspetto fondamentale al giorno d\u2019oggi per la realizzazione di\r\nuna nuova biblioteca digitale: l\u2019utilizzo di IIIF-International Image Interoperability Framework (IIIF 2018)\r\nper la visualizzazione di contenuti digitali di qualit\u00E0 via Internet (Snydman 2015, 16-21; Brantl 2016, 10-13;\r\nSalarelli 2017, 50-66; Magnuson 2018; Cusimano 2019; Lit, Lambertus Willem Cornelis, van 2020, 160167).\r\nViviamo in un\u2019epoca in cui le tecnologie web based, la connettivit\u00E0, la diffusione di dispositivi mobili\r\nsempre pi\u00F9 performanti rendono possibile ci\u00F2 che solo due lustri fa non era nemmeno immaginabile: ogni\r\nbiblioteca digitale di nuova generazione dovrebbe pertanto essere predisposta cercando di approfittare di tali\r\ncondizioni tecnicamente favorevoli, avendo ben chiaro che essa sar\u00E0 soggetta a diversi livelli di lettura che\r\nriguardano l\u2019istituzione-biblioteca che la predispone e gli utenti che ne fruiranno.\r\nL\u2019ecosistema IIIF, dunque, si configura come la \u201Cscelta inevitabile\u201D (cui non a caso faccio riferimento nel\r\ntitolo del presente contributo) poich\u00E9 consente \u2013 dal punto di vista degli utenti \u2013 una semplice ed efficace\r\nfruizione online dei contenuti digitalizzati; e, dal punto di vista dell\u2019istituzione culturale promotrice (nel caso\r\nspecifico, l\u2019Ambrosiana), garantisce interoperabilit\u00E0, scalabilit\u00E0, personalizzazione e condivisione.2 La nuova\r\nbiblioteca digitale dell\u2019Ambrosiana3 implementa l\u2019utilizzo delle APIs <IIIF Image API\r\n2.1.1> (https:\/\/iiif.io\/api\/image\/2.1\/) e <IIIF Presentation API 2.1.1> (https:\/\/\r\niiif.io\/api\/presentation\/2.1\/); il visualizzatore Mirador e l\u2019image server Cantaloupe; il tutto\r\n\u00E8 interconnesso con l\u2019OPAC dell\u2019Ambrosiana al fine di garantire il collegamento diretto tra il record\r\ncatalografico\/descrittivo del manoscritto ricercato e la relativa risorsa digitale: dalla scheda\r\nbibliografica presente nell\u2019OPAC, infatti, tramite l\u2019apposito link <Visualizza la copia digitale>, si\r\nattiva direttamente all\u2019interno del browser il visualizzatore web Mirador (Mirador 2018) che consente\r\nall\u2019utente online un\u2019ottima esperienza di visualizzazione. Il Manifest .json relativo a ogni risorsa\r\ndigitalizzata \u00E8 pubblicamente reperibile all\u2019interno della scheda informativa contrassegnata dalla \u201Ci\u201D\r\nposta in alto a destra nell\u2019interfaccia del visualizzatore Mirador.\r\n\r\nFigura 2: le risorse digitalizzate sono rese pubblicamente e gratuitamente fruibili grazie all\u2019utilizzo del\r\nvisualizzatore IIIF compliant Mirador, e il punto di partenza della fruizione digitale \u00E8 proprio il catalogo (OPAC) della\r\nbiblioteca.\r\n\r\nLa biblioteca digitale pu\u00F2 essere consultata attraverso la landing page predisposta (in\r\ninglese) all\u2019interno del sito web ufficiale della Veneranda Biblioteca Ambrosiana,\r\nattraverso\r\ndue\r\ndifferenti\r\npercorsi\r\ntematici:\r\n<Scopri>\r\n(italiano:\r\nwww.ambrosiana.it\/scopri\/biblioteca-digitale\/;\r\ninglese:\r\nwww.ambrosiana.it\/en\/discover\/the-digital-library\/)\r\ne\r\n(italiano:\r\nhttps:\/\/www.ambrosiana.it\/studia\/biblioteca-digitale\/;\r\nhttps:\/\/www.ambrosiana.it\/en\/study\/the-digital-library\/).\r\n2\r\n\r\nitaliano e in\r\nraggiungibile\r\nhttps:\/\/\r\nhttps:\/\/\r\n<Studia>\r\ninglese:\r\n\r\n\u00AB[\u2026] One of the nicest things about the IIIF approach to shared content is that it lowers the barriers to building light-weight\r\ndemonstrations like this for teaching and research purposes. The institutions that host the images are on the hook for long-term access\r\nand preservation, so it\u2019s not necessary to host your own copies of the images. [\u2026] There are thousands of manuscripts available now\r\nfrom interoperable repositories that can be used, and \u2013 with more institutions joining IIIF each year \u2013 thousands more in the offing.\r\nAs the tools get easier to use and configure, it will be fascinating to see what becomes possible for medieval\r\nstudies\u00BB. (https:\/\/tinyurl.com\/vnhpn3s).\r\n3\r\nLa Veneranda Biblioteca Ambrosiana \u00E8 stata ufficialmente inserita all\u2019interno della lista delle istituzioni che utilizzano\r\nIIIF (https:\/\/iiif.io\/community\/#participating-institutions) quale unica istituzione culturale italiana.\r\n98\r\n\r\n\fFigura 3: indicazione dei percorsi tematici <Scopri> e <Studia> per la consultazione della biblioteca digitale\r\nall\u2019interno del sito web ufficiale della Veneranda Biblioteca Ambrosiana (https:\/\/www.ambrosiana.it).\r\n\r\nFigura 4: dettaglio del percorso tematico <Scopri> all\u2019interno del sito web ufficiale della Veneranda\r\nBiblioteca Ambrosiana (https:\/\/www.ambrosiana.it).\r\n\r\nFigura 5: dettaglio del percorso tematico <Studia> all\u2019interno del sito web ufficiale della Veneranda\r\nBiblioteca Ambrosiana (https:\/\/www.ambrosiana.it).\r\n\r\nLa biblioteca digitale dell\u2019Ambrosiana si apre al pubblico attraverso la sezione ad essa\r\ndedicata all\u2019interno del proprio OPAC: https:\/\/ambrosiana.comperio.it\/bibliotecadigitale\/. Da qui ogni utente pu\u00F2 accedere alla consultazione pubblica e gratuita delle copie digitali\r\nseguendo due vie principali:\r\n\u2022\r\n\r\n\u2022\r\n\r\nattraverso la consultazione diretta della scheda catalografica del manoscritto di proprio interesse a\r\npartire dalla segnatura dello stesso: in questo modo l\u2019utente, utilizzando il catalogo per cercare\r\ntramite la segnatura il manoscritto cui \u00E8 interessato, potr\u00E0 accedere alla visualizzazione pubblica e\r\ngratuita della copia digitale seguendo il link <Visualizza la copia digitale> appositamente inserito\r\nall\u2019interno della pagina di dettaglio di ciascun record catalografico;\r\nattraverso\r\nla\r\nconsultazione\r\ndella\r\nsuddetta\r\npagina\r\nriepilogativa\r\n(https:\/\/ambrosiana.comperio.it\/biblioteca-digitale\/), sfogliando\r\nidealmente la collezione digitale della Veneranda Biblioteca Ambrosiana tramite la lista dei\r\n99\r\n\r\n\fmanoscritti digitalizzati, peraltro riconoscibili grazie all\u2019icona IIIF.\r\n\r\nFigura 6: la pagina principale dell\u2019OPAC dell\u2019Ambrosiana con il nuovo riquadro di ricerca dedicato ai\r\nmanoscritti digitalizzati (https:\/\/ambrosiana.comperio.it).\r\n\r\nFigura\r\n7:\r\nla\r\npagina\r\ndell\u2019OPAC\r\ndell\u2019Ambrosiana\r\ndedicata\r\nalla\r\ndigitale (https:\/\/ambrosiana.comperio.it\/biblioteca-digitale).\r\n\r\nnuova\r\n\r\nbiblioteca\r\n\r\nSi \u00E8 anche proceduto a testare le potenzialit\u00E0 di Mirador collegando una porzione di testo trascritto in\r\nformato TEI \u2013 tratto da un manoscritto della Biblioteca Ambrosiana \u2013 alla corrispondente immagine digitale\r\nin IIIF dello stesso manoscritto tramite manifest .json, il tutto sfruttando le potenzialit\u00E0 dell\u2019Annotation Tool\r\nintegrato nel visualizzatore Mirador (Monella e Cusimano, 2019).\r\n\r\nFigura 8: visualizzazione della trascrizione in formato TEI di una porzione del manoscritto digitalizzato grazie\r\nall\u2019Annotation Tool di Mirador: ms. Ambr. D 23 sup., f. 13v \u00A9 Veneranda Biblioteca Ambrosiana.\r\n\r\n100\r\n"
	},
	{
		"id": 17,
		"title": "Repertori terminologici plurilingui fra normativit√† e uso nella comunicazione digitale istituzionale e professionale",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Klara Dankova",
			"Silvia Calvi"
		],
		"body": "1 Introduzione\r\nIl presente articolo intende investigare il rapporto tra terminologia e Digital Humanities rispetto\r\nall\u2019utilizzo di strumenti computazionali per la conservazione e la valorizzazione del patrimonio culturale,\r\nveicolato dai termini della comunicazione professionale. La disciplina della terminologia \u00E8 fin dalle sue\r\norigini profondamente legata al trattamento informatico: W\u00FCster, ingegnere elettronico e fondatore di\r\nquesta disciplina (1931, 1979) considerava infatti la terminologia in un\u2019ottica interdisciplinare ai confini\r\ntra linguistica, logica, ontologia e informatica. La presenza di riflessioni di natura informatica negli studi\r\nterminologici si intensific\u00F2 al fine di introdurre nuove metodologie di lavoro che consentissero per la\r\nprima volta l\u2019accesso a una enorme quantit\u00E0 di dati da analizzare, raccolti e categorizzati all\u2019interno di\r\nrisorse digitali. Il trattamento informatico in studi terminologici si arricch\u00EC via via nel corso degli anni\r\n1970 e 1980, sia nella fase dell\u2019estrazione di termini, sia in quella della costituzione di repertori\r\nterminologici, nella maggior parte dei casi basati sul concetto di synset (set of synonyms) (Zanola, 2018:\r\n32). A partire dagli anni 1990, la terminologia inizi\u00F2 ad essere associata all\u2019ontologia, studiando i termini\r\na partire dalla loro dimensione epistemologica e concettuale. Questo percorso di natura onomasiologica\r\ndiede le basi teoriche per lo sviluppo dell'innovativo approccio introdotto da Christophe Roche2, basato\r\nsul concetto di ontoterminologia, ovvero una terminologia il cui sistema concettuale \u00E8 un\u2019ontologia\r\nformale (Roche, 2012: 2626). In questo studio si \u00E8 scelto di adottare questo approccio, ritenuto il pi\u00F9\r\nappropriato per la rappresentazione dei concetti e la schedatura dei termini che li designano nell\u2019ottica\r\ndella divulgazione delle terminologie spontanee e non istituzionalizzate, create dai professionisti sul\r\ncampo o in uso nelle pratiche di gruppi o di precise comunit\u00E0 professionali.\r\n\r\n1\r\n\r\nKlara Dankova ha redatto il \u00A7 4.1. Silvia Calvi ha redatto i \u00A7 1 e 2. I \u00A7 3, 4 e 5 sono frutto di una collaborazione delle due autrici.\r\nLe autrici ringraziano il professore Christophe Roche dall\u2019Universit\u00E9 Savoie Mont-Blanc per la disponibilit\u00E0 ad aver fornito l\u2019accesso\r\nal programma Tedi, utilizzato ai fini del presente studio.\r\n\r\n2\r\n\r\n104\r\n\r\n\f2 Obiettivi di ricerca\r\nNel presente articolo si osserveranno i limiti dei repertori terminologici multilingui istituzionali rispetto\r\nalla descrizione di terminologie non ufficializzate ma in concreto uso in diversi domini. In un\u2019ottica\r\nsocioterminologica, si prester\u00E0 particolare attenzione a come uno stesso concetto possa essere designato\r\nda una ricca variet\u00E0 di termini in funzione del contesto e dell\u2019utente di riferimento.\r\nAlla presentazione delle principali banche dati esistenti, repertori di terminologia ufficializzata \u2013 ovvero\r\nla terminologia ufficialmente riconosciuta in fonti primarie \u2013 seguir\u00E0 la presentazione di alcuni esempi di\r\nterminologie non ufficializzate dei domini delle fibre tessili e dell\u2019arrampicata sportiva. Questi esempi\r\npermetteranno di illustrare la variet\u00E0 terminologica relativa a diversi contesti di utilizzo spesso non\r\nmenzionati in repertori terminologici istituzionalizzati, lacuna che con la presente proposta si vuole cercare\r\ndi colmare. Infine, illustrando un caso tratto dal dominio delle fibre tessili, si proporr\u00E0 una metodologia\r\nper la progettazione di una banca dati, costituita da schede terminologiche multilingui.\r\nObiettivi del presente studio sono quindi:\r\n- dimostrare che la terminologia spontanea deve essere conservata e ufficializzata, in quanto portatrice di\r\nun significativo patrimonio culturale, che consente di avvicinarsi anche al mestiere di riferimento per il\r\nquale la terminologia diviene custode del relativo saper-fare;\r\n- riflettere sulla rappresentazione e sulla divulgazione dei termini individuati, prestando particolare\r\nattenzione alla loro dimensione sociale e interculturale;\r\n- proporre un modello di realizzazione di schede terminologiche multilingui attraverso l\u2019utilizzo di Tedi,\r\nontoTerminology EDItor, software realizzato per progettare ontoterminologie multilingui (Roche,\r\n2007).\r\n\r\n3 Banche dati terminologiche e la comunicazione istituzionale e professionale\r\nLe banche dati terminologiche sono strumenti digitali che raccolgono le informazioni sui termini in una o\r\npi\u00F9 lingue, appartenenti a pi\u00F9 settori e le presentano sotto forma di schede terminologiche redatte in modo\r\nstandardizzato al fine di permettere la maggior condivisione dei dati raccolti. Diverse sono le informazioni\r\nfornite in una scheda terminologica, per esempio il termine, la marcatura morfologica, la definizione,\r\neventuali sinonimi, il contesto di utilizzo e le note enciclopediche. Il beneficio maggiore dell\u2019utilizzo dei\r\ntermini definiti in modo univoco \u00E8 la possibilit\u00E0 di comunicare in modo chiaro e preciso, indipendentemente\r\ndai soggetti coinvolti (Zanola, 2018: 64).\r\nIl bisogno di costruire una banca dati terminologica nasce prima di tutto nei contesti multilingui per\r\nrispondere alle necessit\u00E0 della pubblica amministrazione. Un modello efficiente di catalogazione dei dati\r\nterminologici \u00E8 fornito dalle banche dati canadesi, dal Grand dictionnaire terminologique (GDT) dell\u2019Office\r\nqu\u00E9b\u00E9cois de la langue fran\u00E7aise e da Termium Plus gestito dal Bureau de la traduction del governo\r\ncanadese. L\u2019urgenza di standardizzare l\u2019uso dei termini si manifesta anche nel contesto europeo, in\r\nparticolare nelle istituzioni dell\u2019Unione Europea: la progettazione di IATE (InterActive Terminology for\r\nEurope) ha portato alla costituzione di una banca dati contenente soprattutto i termini usati nei testi\r\nlegislativi e amministrativi, pubblicati dalle varie istituzioni dell\u2019Unione Europea. Inoltre, ci sono anche\r\ndelle banche dati terminologiche che operano esclusivamente a livello nazionale, come per esempio\r\nTermdat, la raccolta terminologica della Confederazione svizzera (Zanola, 2018: 64-67). Va sottolineato\r\nche queste banche dati sono state costruite pensando a un gruppo di utenti ben preciso (i cittadini\r\nquebecchesi, dell\u2019Unione Europea, svizzeri ecc.). I termini sono stati individuati all\u2019interno di un contesto\r\nspecifico, quale il contesto giuridico e amministrativo dell\u2019UE o del Canada, e, di conseguenza, non possono\r\nessere sempre utilizzati nei testi relativi ad altre realt\u00E0 socio-culturali.\r\nTuttavia, accanto ai termini recensiti e definiti in un contesto istituzionale esistono anche terminologie\r\nspontanee utilizzate nella comunicazione professionale, che trovano solo un parziale riscontro nelle banche\r\ndati ufficiali, le quali spesso trascurano le variazioni diastratiche. Consideriamo, per esempio, la variet\u00E0 di\r\ntermini in francese usati da vari gruppi di persone con riferimento alla fibra acrilica: gli ingegneri chimici\r\nuseranno probabilmente una denominazione chimica che rivela la composizione della fibra\r\n(polyacrylonitrile), nella comunicazione tra professionisti in una fiera verr\u00E0 invece pi\u00F9 facilmente utilizzato\r\nil codice (PAN), mentre nell\u2019ambito della moda sar\u00E0 pi\u00F9 frequente un termine pi\u00F9 generico (acrylique o fibre\r\nacrylique). Nell\u2019attuale contesto, il bisogno di disporre di raccolte multilingui di termini a uso dei\r\nprofessionisti di vari settori diventa sempre pi\u00F9 forte, rendendo necessario lo sviluppo di un nuovo modello\r\ndi catalogazione.\r\n\r\n105\r\n\r\n\f3.1. La terminologia non istituzionalizzata: il caso delle fibre tessili e dell\u2019arrampicata sportiva\r\nI termini che designano le fibre tessili sono stati estratti manualmente da un corpus di testi in lingua francese,\r\ncontenente quattro tipi di fonti: dei cataloghi delle fiere (Premi\u00E8re Vision Yarns, 12.02.-14.02 2019,\r\nPremi\u00E8re Vision Fabrics, 12.02.-14.02 2019), un documento istituzionale (DGE\/UBIFRANCE, 2006),\r\nun\u2019opera di divulgazione (Fauque e Bramel, 1999) e un manuale tecnico (Weidmann, 2010). Il corpus\r\nrisultante \u00E8 costituito da 245 termini, di cui 60 sono nomi generici e 185 nomi di marca. Si \u00E8 osservato che\r\nla terminologia delle fibre tessili \u00E8 molto complessa sia per le differenze culturali sia per le sue variazioni\r\ndiastratiche. Quanto alla dimensione culturale, si possono riscontrare delle differenze tra i termini usati in\r\ncontesti diversi. Infatti, esistono alcuni casi, in cui i termini usati per designare un determinato concetto\r\ndifferiscono da un paese all\u2019altro, anche all\u2019interno di una stessa lingua. Si considerino i termini in francese\r\nutilizzati per designare la fibra di elastan: mentre nei paesi dell\u2019UE si usa \u00E9lasthanne, il termine\r\ncorrispondente negli Stati Uniti \u00E8 spandex, in Giappone polyur\u00E9thane e in Cina sono in uso i termini\r\n\u00E9lasthanne o spandex (ISO 2076: 2013). Per quanto riguarda le differenze tra terminologia istituzionale e\r\nprofessionale, si pu\u00F2 notare che alcuni termini esclusi dall\u2019uso nella comunicazione istituzionale di un paese\r\npossono continuare ad essere usati tra gli esperti del settore. Per esempio, i termini fibranne e rayonne, che\r\ndesignano in francese rispettivamente le fibre discontinue e i filamenti continui di viscosa, sono stati\r\nsostituiti nel 1976 nella comunicazione istituzionale francese dal termine viscosa (Browaeys, 2014: 18;\r\nBaum e Boyeldieu, 2018: 256). Questo non impedisce per\u00F2 che vengano occasionalmente utilizzati dagli\r\nesperti del settore, per mettere in evidenza la differenza tra le due forme della fibra di viscosa3. A proposito\r\ndei nomi di marca bisogna mettere in evidenza che, anche se designano lo stesso tipo di fibra (la poliammide\r\n6.6), la loro composizione \u00E8 diversa: mentre Nylon \u00E8 una fibra di poliammide 6.6 convenzionale, Ultron\r\npresenta delle caratteristiche antistatiche e Sylkharesse \u00E8 un materiale prodotto in forma di microfibra.\r\nInfine, nella terminologia delle fibre tessili si riscontra una variet\u00E0 di termini che designano lo stesso\r\nconcetto, ma comunque non possono spesso essere usati nello stesso contesto. Nel caso di Nylon possiamo\r\nindividuare le seguenti tipologie di termini: 1) denominazione chimica (poliesametilenadipamide), 2) nome\r\ngenerico (poliammide 6.6), 3) nome generico istituzionale (nei paesi dell\u2019UE poliammide o nylon), 4) codice\r\nindicato sull\u2019etichetta di composizione (PA), 5) codice usato tra i professionisti (PA 6.6), 6) nome di\r\nlaboratorio (Fibre 6.6), 7) nome di marca (Nylon).\r\nDifferenze culturali e variazioni diastratiche possono essere osservate anche nella terminologia\r\ndell\u2019arrampicata sportiva, sport antico che tuttavia ha solo recentemente ottenuto un riconoscimento ufficiale,\r\nquale l\u2019introduzione tra i nuovi sport olimpici di Tokyo 2020, e che giunge a fissare definitivamente per\r\nquesta ragione i propri usi terminologici. Nello studio condotto per l\u2019arrampicata sportiva sono stati estratti\r\nmanualmente 96 termini a partire da un corpus eterogeno in lingua italiana composto da manuali di\r\narrampicata (Commissione Nazionale Scuole di Alpinismo e Arrampicata Libera della Commissione\r\nCentrale per le pubblicazioni, 2009; Bressa, Denicu, Capretta, 2010; Ponta, 2016), documenti pubblicati da\r\nenti ufficiali quali C.A.I (Club Alpino Italiano) e F.A.S.I. (Federazione Arrampicata Sportiva Italiana),\r\narticoli di riviste specializzate come Montagna 360\u00B0, la rivista ufficiale del C.A.I. Trattandosi di una realt\u00E0\r\ninternazionale in cui il confronto tra esperti in occasione di gare e manifestazioni \u00E8 all\u2019ordine del giorno, \u00E8\r\ninteressante osservare come le differenze culturali tra i termini individuati siano poche e prevalentemente\r\nlegate alle scale utilizzate per misurare i gradi di difficolt\u00E0 e ai prodotti che possono essere messi in\r\ncommercio in forme e dimensioni diverse da paese a paese, per esempio mentre in Italia la magnesite pu\u00F2\r\nessere acquistata in forma granulosa, non \u00E8 stato trovato un equivalente nelle fonti canadesi, in cui tale\r\nprodotto sembra essere acquistato prevalentemente in formati differenti. Quanto alla variazione diastratica si\r\npu\u00F2 osservare come il termine arrampicata su massi non venga spesso utilizzato nelle fonti ufficiali che\r\nprediligono invece mantenere il termine internazionale boulder per agevolare la comunicazione tra\r\nprofessionisti provenienti da realt\u00E0 culturali differenti. Inoltre, si pu\u00F2 constatare che per questa terminologia\r\nla rappresentazione visiva degli oggetti e delle tecniche di arrampicata \u00E8 di grande importanza per la\r\ncomprensione dei concetti e deve perci\u00F2 essere presa in considerazione in fase di stesura delle rispettive\r\nschede terminologiche.\r\n\r\n4\r\n\r\nLa progettazione di ontologie in prodotti terminologici\r\n\r\nLa rappresentazione dei concetti e la schedatura dei termini richiede la comprensione dell\u2019organizzazione\r\nconcettuale del dominio oggetto di studio, attraverso la progettazione di ontologie formali. Diversi sono i\r\n3\r\n\r\nSi veda per es. Daniel Weidmann. 2010. Aide-m\u00E9moire textiles techniques. Dunod, Paris, p. 64.\r\n106\r\n\r\n\fprogrammi attualmente disponibili che permettono questa operazione, tra cui il Lexicon Model for Ontologies\r\n(Lemon) modello sviluppato dalla Ontology Lexicon Community il cui principale obiettivo \u00E8 la presentazione\r\ndi informazioni di natura linguistica all\u2019interno di ontologie (McCrae et al. 2017); Prot\u00E9g\u00E9 programma\r\nrealizzato dallo Stanford Center for Biomedical Informatics Research per supportare il OWL 2 Web Ontology\r\nLanguage (Tudorache et al., 2013); Tedi programma proposto dal Condillac Research Group in Knowledge\r\nEngineering per la progettazione di ontoterminologie multilingui4.\r\nAi fini del presente studio si \u00E8 scelto di utilizzare il programma che meglio rispecchia la natura della\r\ndisciplina terminologica, intesa come ontoterminologia: ovvero Tedi, programma il cui punto di partenza non\r\n\u00E8 la dimensione linguistica del termine, come avviene per il Lexicon Model for Ontologies quanto la sua\r\ndimensione nozionale e concettuale. La decisione di prediligere Tedi rispetto a Prot\u00E9g\u00E9 \u00E8 invece giustificata\r\ndal fatto che nel primo la distinzione termine-concetto \u00E8 pi\u00F9 immediata in particolare in ottica di una\r\nrappresentazione terminologica multilingue.\r\nUn esempio tratto dall\u2019ambito delle fibre tessili consentir\u00E0 di illustrare una proposta di metodologia di\r\nlavoro da adottare per la realizzazione di schede terminologiche basate su un\u2019ontologia formale.\r\n4. 1. Tedi, una proposta per la realizzazione di un\u2019ontologia con schede terminologiche multilingui. La\r\nterminologia delle fibre tessili: il caso del termine polyamide 6\r\nL\u2019editore di ontoterminologie Tedi si basa sulla distinzione della terminologia in due dimensioni: 1) la\r\ndimensione concettuale extralinguistica, condivisa dalle diverse comunit\u00E0 linguistiche 2) la dimensione\r\nlinguistica, composta da diversi sistemi lessicali. Le due dimensioni sono strettamente legate, poich\u00E9 i termini\r\nrappresentano i nomi dei concetti in lingua naturale (Roche, 2019: 5). La distinzione delle due dimensioni,\r\npermettendo una migliore comprensione del dominio, consente anche di effettuare delle ricerche non soltanto\r\nin base alle relazioni linguistiche tra i termini (iperonimia, sinonimia), ma anche in base alle relazioni logiche\r\ntra i concetti (concetto generico, concetto specifico) (Roche et al., 2014: 2).\r\n4.2 Tedi e la dimensione concettuale dell\u2019ontoterminologia\r\nPer ricostruire il sistema concettuale del dominio, Tedi mette a disposizione dell\u2019utente il concept editor.\r\nQuesto editor permette di definire i concetti in un linguaggio formale, che consiste nell\u2019indicazione delle\r\ncaratteristiche essenziali del concetto, dette anche \u201Cdifferenze\u201D, in quanto rappresentano una delle possibilit\u00E0\r\ndi realizzazione di una certa caratteristica, predefinita secondo l\u2019asse dell\u2019analisi corrispondente. A titolo di\r\nesempio, l\u2019asse dell\u2019analisi \u201Corigine della fibra\u201D fornisce due caratteristiche essenziali \u201Cnaturale\u201D e \u201Cchimica\u201D.\r\nNel caso del concetto <poliammide 6> (Fig. 1), l\u2019utente definisce l\u2019origine della fibra scegliendo la\r\ncaratteristica essenziale \u201Cchimica\u201D.\r\n\r\n4\r\n\r\nSi veda: http:\/\/new.condillac.org\/projects\/tedi\r\n107\r\n\r\n\fFigura 1: Il concetto <poliammide 6> definito nel concept editor di Tedi\r\nUna volta identificate le caratteristiche essenziali del concetto, il sistema genera in funzione di esse il\r\nnome del concetto, che permette di comprendere la natura degli oggetti che rientrano sotto il concetto stesso\r\n(es. il concetto <poliammide 6> viene denominato < Fibre textile chimique synth\u00E9tique traditionnelle\r\nliaisons amides r\u00E9currentes caprolactame>). In seguito, l\u2019utente inserisce il concetto nella rete di relazioni\r\ntra i concetti del sistema, indicando il suo concetto generico (il concetto <poliammide>: <Fibre textile\r\nchimique synth\u00E9tique traditionnelle liaisons amides r\u00E9currentes>) e, eventualmente, i suoi concetti specifici\r\n(es. il concetto <Lilion>: <Fibre textile synth\u00E9tique caprolactame traditionnelle chimique liaisons amides\r\nr\u00E9currentes soci\u00E9t\u00E9 Snia Viscosa>). In questo modo, si ricostruisce l\u2019organizzazione concettuale del\r\ndominio, che viene visualizzata nel concept editor di Tedi nell\u2019angolo in alto a sinistra (vedi Fig. 1). Nel\r\ncaso di alcune terminologie, quali quella dell\u2019arrampicata sportiva, un ruolo importante nella comprensione\r\ndel concetto \u00E8 svolto dalla sua rappresentazione visiva. Per venire incontro a questa esigenza, Tedi \u00E8 dotato\r\ndella funzione link-illustration che consente di associare al concetto non solo immagini, ma anche video e\r\ncollegamenti ipertestuali.\r\n4.3 Creazione di una scheda terminologica in Tedi: la dimensione linguistica\r\nI termini vengono definiti nel term editor che propone degli editor indipendenti per una serie di lingue (es.\r\nfrancese, italiano, inglese). La definizione del termine si inserisce nella lingua naturale, con la possibilit\u00E0 di\r\nutilizzare un modello di definizione, elaborato da Tedi in base alla definizione formale del concetto. Oltre\r\nalla definizione del termine, Tedi consente di inserire altri dati relativi al termine, quali la fonte della\r\ndefinizione, l\u2019informazione morfologica, lo status del termine (\u201Cpreferenziale\u201D, \u201Calternativo\u201D, \u201Cobsoleto\u201D),\r\nil suo contesto di utilizzo, le note enciclopediche, le varianti ortografiche e le forme flesse. La sezione note\r\nenciclopediche \u00E8 di particolare interesse per la terminologia non istituzionalizzata in quanto in questa\r\nsezione \u00E8 possibile presentare sia delle note di carattere culturale sia delle indicazioni circa la variazione\r\ndiastratica. Nel caso delle fibre tessili per esempio si pu\u00F2 indicare se il termine oggetto di studio \u00E8 connotato\r\nculturalmente e se esso si riferisca alla denominazione chimica, al nome di laboratorio o al suo codice.\r\nInoltre, nel term editor appaiono in modo automatico anche gli equivalenti, gli iperonimi, gli iponimi e i\r\nsinonimi del termine, recensiti nella banca dati e associati a un concetto definito nel linguaggio formale a\r\nsua volta in relazione con il concetto designato dal termine in questione (Fig. 2).\r\n\r\n108\r\n\r\n\fFigura 2: Il termine polyamide 6 definito nel term editor di lingua francese di Tedi\r\nI termini equivalenti in varie lingue (per esempio polyamide 6 (fr), poliammide 6 (it), polyamide 6 (en))\r\nsono associati a un unico concetto, definito in un linguaggio formale e quindi extralinguistico, il che\r\npermette di creare una banca dati terminologica multilingue, contenente una rappresentazione delle\r\nrelazioni tra i concetti, che pu\u00F2 essere visualizzata esportando i dati presenti nel concept editor in altri\r\nprogrammi, quali Cmap Tools, Prot\u00E9g\u00E9. Inoltre, l\u2019esportazione delle informazioni sui termini in una lingua\r\n(nel nostro caso in francese) in formato HTML consente la progettazione di un dizionario elettronico,\r\ncomposto dalle schede terminologiche che forniscono diverse indicazioni tra cui la definizione del termine\r\nin lingua naturale, gli equivalenti in altre lingue e l\u2019elenco delle caratteristiche essenziali del concetto (Fig.\r\n3).\r\n\r\n109\r\n\r\n\fFigura 3: Il dizionario elettronico: la scheda terminologica di polyamide 6 (fr)\r\n\r\n5\r\n\r\nConclusioni\r\n\r\nDisponendo attualmente delle banche dati terminologiche di carattere istituzionale o nazionale, ci si trova\r\ndi fronte a un nuovo bisogno, ossia quello di progettare un ricco repertorio terminologico digitale, facilmente\r\nutilizzabile nella comunicazione professionale multilingue. \u00C8 necessario tenere in considerazione il fatto\r\nche i bisogni dei professionisti di un settore differiscono spesso da quelli dei principali destinatari delle\r\nbanche dati fornite dalle istituzioni (per esempio traduttori, legislatori, giuristi, redattori di testi). Come\r\ndimostrato dalla proposta illustrata nel presente articolo, l\u2019utilizzo del programma Tedi, basato su un\r\napproccio ontoterminologico, permetter\u00E0 quindi di progettare delle innovative banche dati terminologiche,\r\nattente alla dimensione socio-culturale della terminologia spontanea e in uso nella comunicazione\r\nprofessionale. Questo percorso, applicabile a pi\u00F9 domini, permetter\u00E0 quindi di conservare e ufficializzare\r\nun ricco patrimonio terminologico che, finora, non ha goduto dello stesso trattamento della terminologia\r\nistituzionalizzata."
	},
	{
		"id": 18,
		"title": "The digital Lexicon Translaticium Latinum: Theoretical and methodological issues",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Chiara Fedriani",
			"Irene De Felice",
			"William Michael Short"
		],
		"body": "Introduction\r\n\r\nThe Lexicon Translaticium Graecum et Latinum is a collaborative international project aimed at\r\ndeveloping an on-line, extensible, open-access lexicon of metaphors in the ancient languages \u2013 beginning,\r\nin reverse chronological order, with Latin. Unlike existing electronic dictionaries for Latin, which simply\r\nre-create their printed counterparts in machine-readable form, the Lexicon Translaticium incorporates\r\n\r\n112\r\n\r\n\finsights from up-to-date theories of meaning and, in particular, the view developed in cognitive linguistics\r\nof metaphor as a key structuring device of language and thought. In capturing deeply entrenched\r\nand highly conventionalized metaphoric and metonymic patterns that organize meanings pervasively\r\nthroughout this language and at different orders of linguistic encoding, the Lexicon Translaticium is\r\nmeant as a psychologically realistic model of the conceptual system underpinning Latin. Built on top of\r\nthe ontology provided by the Latin WordNet, the Lexicon will be interoperable with existing electronic\r\ncorpora and thus capable of delivering rich figurative data for integration into natural language processing\r\napplications. The project, directed by William Short and Chiara Fedriani and staffed by an international\r\nfive-member team, is currently on-going. However, its intellectual rationalization is well established and\r\nits technical design and implementation have progressed to the point where preliminary \u2018test\u2019 data is\r\nalready publicly available, with about 20 metaphors presently annotated. We aim to launch the Lexicon\r\nofficially in Spring of 2020 with a fuller dataset consisting of about 100 conceptual metaphors.\r\n\r\n2\r\n\r\nTheoretical background\r\n\r\nRecognizing the all-pervasive character of certain metaphorical patterns in language, George Lakoff\r\nand Mark Johnson (1980) argued that the frequent clustering of metaphorical linguistic expressions around\r\nabstract, intellectual or otherwise intangible concepts in fact reflects the inherently metaphorical workings\r\nof cognition itself. People talk about most abstract concepts metaphorically, that is, because \u2013 it is claimed\r\n\u2013 they actually conceive of them metaphorically in terms of other (usually more concrete) concepts. On\r\nthis view, metaphors are the projections of conceptual structure and content from one domain to another\r\nthat occur as a way of mentally representing and reasoning about experiences not directly grounded in the\r\nphysico-spatial world. Cognitive linguists argue, moreover, that it is the systematic nature of metaphors \u2013\r\nin other words, that metaphors characterize regular mappings between organized domains of knowledge\r\n\u2013 that allows people to think and reason (and therefore also to speak) meaningfully about experiences\r\nthat may be difficult to comprehend in and of themselves.\r\nIn line with this theory, the \u2018entries\u2019 of our metaphor dictionary \u2013 unlike those of a traditional lexicon\r\n\u2013 will therefore consist of large-scale patterns of metaphorical understanding that link together concepts,\r\nrather than the semantic structures of words per se and so structure the meanings of words across the\r\nlexicon and at different levels of linguistic encoding. The kinds of metaphors that constitute the data\r\nof our Lexicon are those that are so conventionalized and so entrenched in the shared linguistic and\r\ncognitive habits of Latin speakers that they seem not to have been perceived as figurative at all \u2013 and\r\nindeed deliver Latin speakers\u2019 entirely regular, \u2018everyday\u2019 ways of conceptualizing certain experiences.\r\nOf course, as \u2018imaginative\u2019 or \u2018creative\u2019 (or more narrowly \u2018literary\u2019) metaphors most often derive in\r\nsome way from more conventionalized metaphors, these kinds will also be represented. In this way, the\r\nLexicon Translaticium Latinum will form a comprehensive catalogue of the range of metaphorical themes\r\nthat structure meaning in Latin.\r\n\r\n3\r\n\r\nTechnical implementation\r\n\r\nTechnically, the Lexicon will be realized as a computerized relational database, whose data model\r\ncombines aspects of the architecture of the MetaNet Project of the International Computer Science\r\nInstitute in Berkeley, California, with the WordNet framework. The Berkeley MetaNet is an electronic\r\nrepository, viewable interactively on the Internet as a Wiki, that contains records for hundreds of attested\r\nconventional and imaginative metaphors in English, including time metaphors, mind metaphors, and\r\nemotion metaphors, as well as metaphors relating to government, disease, and violence. Most importantly\r\nfor our purposes, the MetaNet provides a set of high-level ontologies for annotating and organizing\r\nfigurative language data under the theory of conceptual metaphor in cognitive linguistics. In particular,\r\nthe MetaNet provides a theoretically-grounded formal specification for encoding kinds of conceptual\r\nmetaphors as well as kinds of relations between metaphors. For example, the \u2018type\u2019 of a metaphor can\r\nbe tagged with values such as \u2018primary\u2019, \u2018composed\u2019, or \u2018entailed\u2019, which correspond to well defined\r\ntheoretical categories. A primary metaphor is one that emerges directly from correlations in experience,\r\nas in more is up or purposes are destinations, while complex metaphors are those built up out of at\r\n\r\n113\r\n\r\n\fleast two more basic primary ones. Entailed metaphors are specialized submappings that can be inferred\r\nthrough experiential knowledge from a primary or complex metaphor, and which often form the basis of\r\ncoherence between metaphors.\r\nLikewise, metaphors can be organized into hierarchies through simple relations of super- or subordination, or into more intricate systems according to different kinds of (again theoretically grounded)\r\nrelationships, such as \u2018extension\u2019 (where one mapping takes advantage of conceptual material left unused\r\nby another), \u2018elaboration\u2019 (where one mapping embellishes another with additional conceptual material),\r\n\u2018combination\u2019, or \u2018questioning\u2019. \u2018Reciprocality\u2019 is another common feature of metaphor systems and is\r\navailable to capture \u2018orientational\u2019 metaphors that involve body-based experiential polarities such up vs\r\ndown, left vs right, center vs periphery, in vs out, and so on.\r\nWhereas the MetaNet specification provides the foundation for encoding metaphors (as mappings\r\nbetween concepts) and their relations, the ontologies and data structures of the WordNet deliver the core\r\nrepertoire of concepts that participate in these relations. As a semantic database, the WordNet represents\r\nlexical meaning in terms of synsets, which are uniquely identifiable \u2018definitions\u2019 for hypothetically all the\r\nsenses capable of being expressed in a given language (thus organizing the lexicon into discrete \u2018synonym\r\nsets\u2019). In other words, a WordNet synset \u2013 which pairs a unique identifier, consisting of a part-of-speech\r\ntag and a string of between six and eight integers, with a descriptive gloss and possibly higher-order\r\n\u2018domain\u2019-level tags \u2013 should be seen as representing a distinct concept that may constitute the meaning of\r\na word or words in the language under scrutiny. A WordNet for Latin was developed by Stefano Minozzi\r\nfor the Fondazione Bruno Kessler\u2019s MultiWordNet Project (see Minozzi, 2008), consisting of about 9,000\r\nlemmas tagged with synsets drawn from English and Italian. This is now being expanded through an\r\ninternational collaboration directed by the University of Exeter, to include over 70,000 words covering the\r\narchaic through classical periods of this language, as well as language-specific synsets defining meanings\r\nthat are peculiar to Latin and not represented among the 100,000 or so synsets originally defined for\r\nEnglish.\r\n\r\n4\r\n\r\nInnovations of design\r\n\r\nBecause the Latin WordNet (and indeed the WordNet specification generally) does not presently\r\ndistinguish between literal and figurative sense attributions, it is being re-architected to accommodate\r\nthe encoding of metonymic and metaphoric as well as literal senses of words. Annotation at the level of\r\nthe lemma of specific sense (synset) assignments as being either literal, metonymic, or metaphorical is\r\nin fact one of the major new \u2018layers\u2019 at which figurative information is represented within the Lexicon.\r\nConsider, for example, the database entry for the word baculum, which can be accessible and marked-up\r\nby project participants through our bespoke on-line curation and annotation interface. In classical Latin,\r\nthis word meant \u2018walking stick\u2019 and thus has been tagged with synset n#03585559, \u2018a stick carried in the\r\nhand for support in walking\u2019 as one of its literal senses (and indeed also its prototypical sense). Over\r\ntime, however, and particularly in the early Christian period, the word came to be used more abstractly in\r\nthe sense of any \u2018support\u2019 and in ecclesiastical texts regularly exhibits this meaning. This chronologically\r\ncircumscribed figurative meaning of the word (n#04399253, \u2018something providing immaterial support or\r\nassistance to a person or cause or interest\u2019) is therefore annotated as a metaphorical sense. Differentiating\r\nbetween literal, metonymic, and metaphorical signification introduces an entirely new dimension of\r\nsemantic structure into the WordNet framework, validated by modern linguistic theory.\r\nAlong with annotations at the level of lexical semantic structure distinguishing between a word\u2019s literal,\r\nmetonymic, and metaphorical senses (represented by synsets), conceptual metaphors themselves will be\r\ncoded as a relationship between synsets, understood as discrete concepts. For example, the fear is a\r\nweapon metaphor, known in Latin in expressions such as the one in (1), is represented as a mapping\r\nbetween the synset that means \u2018fear\u2019 (n#05590260) and the one that means \u2018weapon\u2019 (n#03601056).\r\nIn turn, the anxiety is a substance metaphor, again illustrated by the passage in (1), is structured\r\nas a mapping between the synsets meaning \u2018anxiety\u2019 (n#04491326) and \u2018substance\u2019 (n#00010572),\r\nrespectively.\r\n1. ipsius regis non tam subito pavore perculit pectus, quam anxiis inplevit curis (LIV. 1, 56) \u2018As for the\r\n\r\n114\r\n\r\n\fking himself, his heart was not so much struck with sudden terror as filled with anxious forebodings\u2019\r\nAccordingly, any lemma annotated with one of these synsets as a literal, metonymic, or metaphorical\r\nsense is automatically linked (and accessible) via the metaphor by virtue of those sense attributions. In\r\nother words, as the theory posits, the metaphor operates as a supralexical structuring device of meaning in\r\nLatin: it helps determine, and motivate, the specific semantic developments of words and explains why the\r\nvocabulary of \u2018weapons\u2019 (not only the word corresponding to weapon but the whole conceptual domain\r\nrelating to weapons and their use) can be used to talk about fear. Without the conceptual metaphor, there\r\nis no way to explain why weapon concepts are so regularly used to represent fear concepts and these\r\nwould have to remain isolated, and \u2013 worse \u2013 arbitrary \u2013 facts of Latin\u2019s semantics. Crucially, moreover,\r\nthe layer of more global conceptual-metaphorical information is tightly integrated with the more local\r\nlayer of lexical-semantic information. In other words, the two layers of annotation \u2013 1) the conceptual\r\nmetaphor itself, as a mapping between synsets (concepts) and 2) the attribution of synsets to lemmas as\r\nspecifically metaphorical senses \u2013 work hand in hand. When a lemma is tagged as \u2018having\u2019 a synset as\r\none of its literal, metonymic, or metaphorical sense, the annotator is also able to indicate the specific\r\nmetaphor that underpins the given sense.\r\nThis is to recognize within the relational structure of the database \u2013 and thus of the organization\r\nof Latin\u2019s semantic system \u2013 the theoretical claim that metaphors operate supra-lexically and provide\r\nmotivating conceptual frameworks for the figurative extension of word meaning. In other words, rather\r\nthan belonging to the semantic structure of any particular word (or determining, wholesale, the possible\r\nfigurative meaning of a word), metaphors provide the specific pathways of figurative development that\r\nspecific word senses may undergo in the course of a language\u2019s history. For instance, baculum\u2019s metaphorical sense of \u2018something providing immaterial support or aid\u2019, would be tagged with the metaphor an\r\nemotional support is a physical support (or even more generally, the emotional is the physical).\r\nThis metaphor operates independently of this word\u2019s semantic structure \u2013 it very likely also determines\r\nthe metaphorical usage of, e.g., fulcio \u2013 literally, \u2018to prop up\u2019 \u2013 in the sense of \u2018to uphold (emotionally)\u2019,\r\nas in CIC. Rab. 16, 43, veterem amicum suum (. . . ) labentem excepit, fulsit et sustinuit re, fortuna,\r\nfide (\u2018he supported his old friend \u2013 who was slipping downward \u2013 with his goods, his fortune and his\r\nconfidence\u2019) \u2013 and so provides a powerful mechanism of bringing together otherwise disparate aspects\r\nof Latin\u2019s semantic system and discovering relationships that otherwise might remain hidden, obscured\r\nby outmoded principles of lexicographic organization.\r\nFinally, the ability to organize metaphors into highly articulated networks or groupings via different\r\nkinds of mapping relations recognizes that, at a higher level of conceptual structure, metaphors participate\r\nin systems. Besides the relations mentioned above, another \u2018organizing\u2019 mechanism of metaphors is that\r\nof the image schema. In conceptual metaphor theory, an image schema is \u201Ca recurring dynamic pattern\r\nof our perceptual interactions and motor programs that gives coherence and structure to our experience\u201D\r\n(Johnson 1987: xiv). Metaphorical mappings are usually encoded at a quite specific level of semantic\r\ngranularity, and can be seen as detailed instantiations of more superordinate metaphors relying on general\r\nimage schemas (e.g., force, container, object). In turn, mappings can give rise to further subordinate\r\nfigurative patterns, with more semantic details filled in. These hierarchical relationships are all annotated\r\nwithin each metaphor record and give rise to a dense network of interconnected figurative meanings.\r\n\r\n5\r\n\r\nAnnotation procedures and tagging scheme\r\n\r\nAnnotators first identify (a set of) documented metaphor(s) used by Latin writers to express an abstract\r\nconcept, corresponding to a given synset, by analysing all occurrences of a relevant (set of) lemma(s)\r\nincluded in the synset within a selected corpus of literary texts. Encoding of metaphors, conceived\r\nas mappings between two synsets, is manually conducted through an annotation layer which has been\r\ndesigned expressly for this purpose. Very specifically, a metaphor is annotated according to its status\r\n(conventional, literary, or imaginative), type (primary, complex, orientational, ontological, one-shot\r\nimage) and period of documentation. Moreover, it is labelled with a shorthand expression (e.g. \u2018ideas\r\nare food\u2019) and an adjectival descriptor (e.g. \u2018alimentary\u2019) following conventions in cognitive linguistics.\r\nThe mapping itself is represented as a unidirectional relationship between two synsets, identified as the\r\n\r\n115\r\n\r\n\fsource and target. Additional information includes relationships between two or more metaphors at\r\nhigher or lower levels of semantic specificity, namely through superordinate and subordinate mappings.\r\nAnnotators can also catalogue relations between mappings (e.g. extension, elaboration, reciprocity,\r\nderivation, combination, and entailment) that may characterize complex metaphor systems.\r\nTo exemplify this methodology, we present a case study of metaphor annotation pertaining to the\r\nsemantic field of fear. A preliminary step identified synset n#05590260, \u2018an emotion experienced in\r\nanticipation of some specific pain or danger\u2019 (which pertains to five lemmas pointing to the concept\r\nof fear in Latin: formido, metus, pavor, terror, timor), as the primary target domain of the mapping.\r\nWe scrutinized all occurrences of these lemmas (4,995) in the \u2018Antiquitas\u2019 section of the Bibliotheca\r\nTeubneriana Latina (3 BCE to 4 CE), distinguishing between literal (ex. 2) and figurative (ex. 3) usages.\r\nWe counted but discarded literal usages, and further subclassified figurative usages into more fine-grained\r\nmetaphorical subschemas.\r\n2. prae metu ubi sim nescio (PLAUT. Cas. 413) \u2018I don\u2019t know where I am for fear\u2019\r\n3. huic aliquem in pectus iniciam metum (PLAUT. Cas. 589) \u2018I\u2019ll inject some fear into his heart\u2019\r\nThrough careful analysis of the literal wording of the contexts in which these words appear, we identified\r\n23 metaphorical mappings which instantiate three main superordinate image schemas, namely force,\r\ncontainer, and object. An example of a metaphor actualizing the force schema is fear is a military\r\nforce (ex. 4); whereas fear is a substance that fills the experiencer (ex. 5) exemplifies the object\r\nschema.\r\n4. tum vero ingens metus nostros invadit (SALL. Iug. 106, \u00A7 6) \u2018at last a great fear assailed the Romans\u2019\r\n5. vidi hominem XIIII Kal. Febr. plenum formidinis (CIC. Att. 9, 10) \u2018I saw him on January 17,\r\nthoroughly cowed [lit. filled up with]\u2019\r\nOnce the catalogue of subschemas appeared to cover all possible metaphorical expressions involving\r\nthe relevant lexical field, a generalized annotation template was used to record details about each mapping.\r\nFor example, the annotation record for fear is a military force is as follows:\r\nstatus <conventional>\r\ntype <ontological>\r\nperiod Naev.+ <Pun. fr. 57, magnae metus tumultus pectora possidit>\r\nshorthand expression <fear is a military force>\r\nadjectival descriptor \u2018military\u2019\r\nsource <n#06088783 | \u2018an opposing military force\u2019>\r\ntarget <n#05590260 | \u2018an emotion experienced in anticipation of some specific pain or danger\u2019>\r\nderives from <fear is a hostile force>\r\n\r\nAnd it is annotated as follows in the Lexicon interface (Figure 1):\r\n\r\nFigure 1. The annotation layer of the fear is a military force metaphor.\r\n\r\n116\r\n\r\n\fFinally, the metaphor entry is enriched with illustrative examples drawn from literature (ex. 6).\r\n6. olim iam adversus hunc metum emunivit animum (SEN. Con. 3, 17, 10) \u2018but he has long since\r\nfortified his mind against fear of that\u2019\r\nAccording to this annotation procedure, users will be able to search the database using a variety of\r\nquery types. For example, it will be possible to search for a single lemma (like amor), for a specific\r\nfigurative source (like \u2018fire\u2019) or target domain (\u2018love\u2019), for an image schema (counterforce), and thus to\r\nview all the metaphorical concepts built up from any of these elements. This will make it straightforward\r\nto discover certain features of figurative structuration within Latin\u2019s semantic system, such as the set of\r\nsource domains that characterize the understanding of a given concept (what cognitive linguists called the\r\n\u2018range of the target\u2019) or, conversely, the set of target domains that are structured by a concept (the \u2018scope\r\nof the source\u2019). It could also help shed light on the ways in which presumably human-universal aspects\r\nof cognition (sensorimotor gestalts) provide the scaffolding for culture-specific conceptualizations. What\r\nis more, because the metaphorical information of the Lexicon Translaticium Latinum piggybacks on the\r\nontology provided by the WordNet, users will automatically be able to take advantage of the rich lexical and\r\nsemantic knowledge already present in this database, enabling highly complex figuratively-aware queries.\r\nThe Lexicon therefore portends to have significant implications for corpus search, text-processing and\r\nother natural language understanding applications.\r\n\r\n6\r\n\r\nConclusions\r\n\r\nThe theoretical and methodological underpinnings of this project, along with the practical annotation\r\nprocedure it has implemented, suggest that the Lexicon Translaticium Latinum could contribute significantly not only to cognitive and semantic approaches and to metaphor theory, but also to linguistic,\r\nliterary, and cultural research in Classical Studies, especially as part of this field\u2019s wider ecosystem of\r\nnatural language understanding applications. Indeed, we hope to position the Lexicon not merely as a\r\nrepository of figurative usages in Latin, but as an interface to the system of knowledge itself that Latin\r\nspeakers relied upon in thinking and speaking in diverse contexts of symbolic expression, and thus as a\r\nresource for better understanding how members of Roman society \u2018made sense\u2019 in, and of, their world.\r\n"
	},
	{
		"id": 19,
		"title": "Selling autograph manuscripts in 19th c. Paris: Digitising the Revue des Autographes",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Simon Gabay",
			"Lucie Rondeau du Noyer",
			"Mohamed Khemakhem"
		],
		"body": "Introduction\r\n\r\nThe number of projects dealing with French related objects circulating on the private market is increasing:\r\nresearch is currently being carried out in art history (Saint-Raymond, 2018), book history (Montoya,\r\n2018), medieval manuscripts (Wijsman, 2017). . . Following recent trends, the latter project, that is the\r\nmost relevant to our own work, is now sharing its data with other teams (Burrows et al., 2019) in the USA1\r\nand in the UK 2 to trace the history of manuscripts over time and places, across national and linguistic\r\nborders.\r\nUnfortunately, no similar survey has been conducted yet on modern French autographs. If Renaissance\r\nmanuscripts and their history are better known thanks to the Biblissima project (Turcan-Verkerk and\r\nBertrand, 2014), no systematic work has been carried out on 17th, 18th and 19th c. materials. However,\r\nsale catalogues are recognised as being useful, since they are, for instance, regularly used as sources for\r\ncritical editions (S\u00E9vign\u00E9, 1978, p. 158) (Voltaire, 1960, p. 18) (Lamartine, 2001, p. 348).\r\nSuch a hole in our knowledge is due to the fact that it remains extremely tedious to extract information,\r\nbecause this task is either performed manually or with imperfect digital solutions (Cuadra and Michels,\r\n2013; Barman, 2019). In the present paper, we therefore want to propose an (almost) automated workflow\r\nfor the retroconversion of catalogues to transform images into structured information and create a database\r\nof sold items.\r\n\r\n2\r\n\r\nThe corpus\r\n\r\n2.1\r\n\r\nThe manuscript market\r\n\r\nSince the beginning 19th c., rich collectors have been selling manuscripts on the private market (Bodin,\r\n2000). Archives and libraries still keep sales catalogues that have been published on a regular basis (fixed\r\nprice catalogues) or for special sales (auction catalogues) by dealers. Most of the manuscripts sold in\r\nthese catalogues are modern and contemporary autograph manuscripts.\r\nThe information contained in these catalogues is crucial for at least four different reasons.\r\n1https:\/\/sdbm.library.upenn.edu\r\n2http:\/\/mappingmanuscriptmigrations.org\r\n\r\n119\r\n\r\n\f\u2022 It helps assessing the authenticity of autographs: it is unlikely that a document sold repeatedly on\r\nthe private market, and therefore authenticated each time by an expert, is a forgery.\r\n\u2022 It documents the reception of authors, via the history of collections (i.e. who collected what?) and\r\nprices (i.e. who costs how much?).\r\n\u2022 It informs us on the distance between what has been sold and what is available in libraries (i.e. are\r\nthere autographs we do not know about?).\r\n\u2022 It provides us with images of documents which are still in private hands, because catalogues\r\nsometimes offer either facsimiles or pictures of autographs sold.\r\nFor a first test phase, we have concentrated our efforts on the Revue des Autographes, a journal published\r\nsince 1860\u2019s in Paris by Gabriel Charavay.\r\n2.2\r\n\r\nThe RDA collection\r\n\r\nIn the second half of the 19th c., the autograph market is mature and the first generation of dealers\r\nbegins to retire. In 1865, Gabriel Charavay (1818-1879) cease the opportunity of Auguste Laverdet\u2019s\r\n(1809-1867) retirement to take over his business, following the example of his elder brother Jacques\r\n(1809-1867), who opened his own shop in 1830. At that moment, Gabriel abandons his role of editor\r\nfor L\u2019Amateur d\u2019autographes, a journal about the autograph market in Paris created in 1862, which keeps\r\nbeing published by his brother Jacques.\r\nRealising the importance of a publication attached to his activities, Gabriel creates another journal\r\none year after his installation, in 1866: the Revue des autographes, des curiosit\u00E9s de l\u2019histoire et de la\r\nbiographie (RDA). Two journals for such a small market is however too much: in December 1868, after\r\neight months of interruption, the price of the publication is divided by two and part of the content consists\r\nnow of a list based on the autographs for sale in Gabriel\u2019s stock. Over time, the proportion of articles\r\nkeeps diminishing and the RDA becomes first a hybrid publication mixing news and items to be sold, and\r\neventually a fixed-price catalogue with the name of a journal published (almost) monthly until 1936. In\r\nthe meantime, Gabriel\u2019s shop is taken over by Gabriel\u2019s son Eug\u00E8ne (1879-1892), and then by Eug\u00E8ne\u2019s\r\nwidow (1892-1918) and by Eug\u00E8ne\u2019s daughter (1918-1936).\r\nThe transformation of an hybrid journal into a disguised fixed-price catalogue under a journal\u2019s name\r\nis confirmed by a modification of the format: Eug\u00E8ne Charavay opts for a two-columns layout and a\r\nsmaller font (cf. figure 1), harder to read but easier to browse for readers, who are now buyers, looking\r\nfor the autograph of their dreams.\r\n\r\nFigure 1: One-column layout (1873) vs two-columns layout (1893).\r\n\r\n120\r\n\r\n\f3\r\n3.1\r\n\r\nEncoding\r\nEntries\r\n\r\nIt is the images of these catalogues that we want to transform into minable data. Each of them generally\r\ncontains a minimum of c. 200 entries, all of them being extremely dense in information and always\r\nfollowing the same structure:\r\n\r\nFigure 2: RDA, n\u00B067 (March 1881), lot N\u00B055.\r\nWe can clearly see the lot number (in brown), the name of the author (in red), a short biography\r\n(in orange), the material description of the autograph (in blue), the price (in pruple) and an additional\r\ndescription (in green).\r\nTo render the structure of the document, we propose the following encoding in XML-TEI:\r\nSuch an encoding allows a simple disambiguation of the entry and increases the accuracy of the search.\r\nIn our example, we have the names of three major 17th c. French writers: the novelist Madeleine de\r\nScud\u00E9ry (1607-1701), the bishop Daniel Huet (1630-1721), and the satirist and poet Nicolas BoileauDespr\u00E9aux (1636-1711). The three names are enclosed in three different tags (name, desc and note)\r\nreflecting their status in the document (author, addressee, mention): we can therefore easily narrow down\r\nour query to a name depending on its role.\r\n3.2\r\n\r\nWorkflow\r\n\r\nThe presented encoding can be compiled semi-automatically through a simple three steps workflow:\r\n\r\nFigure 3: Workflow.\r\nThe scan is OCRised with Transkribus (Kahle et al., 2017), for which a substantive model of 125,000\r\nwords has been created (CER of 0.59%). The pdf with a text layer is then processed with GROBIDDictionaries (Khemakhem et al., 2018b), a tool relying on text and layout features (cf. Figure 4) to\r\nperform a supervised classification of the parsed text and generate a TEI compliant encoding where the\r\nvarious segmentation levels are associated with an appropriate XML tessellation (Khemakhem et al.,\r\n2017). Preliminary designed for the retroconversion of dictionaries (Bohbot et al., 2018), it is also used\r\nto parse large bibliographical collections (Lindemann et al., 2018) or address directories (Khemakhem\r\net al., 2018a).\r\n\r\n121\r\n\r\n\fFigure 4: Features.\r\nGROBID-dictionaries provides an answer to important issues left open by previous attempts, that do\r\nnot produce standardised data (e.g. in XML-TEI), are not open source (Cuadra and Michels, 2013) or do\r\nnot offer fine-grained encoding (Barman, 2019). On top of this, GROBID is a free, language agnostic,\r\neasily trainable solution compatible with other sub-projects of the GROBID galaxy, which leaves the door\r\nopen to further analysis with complementary tools, such as GROBID-NERD (Named-Entity Recognition\r\nand Disambiguation).\r\nAfter preliminary tests ensuring the compatibility of GROBID-Dictionaries with sale catalogues (Khemakhem et al., 2018c) models have been created for the RDA and many other catalogues (Rondeau du\r\nNoyer et al., 2019) with a new bigram template for the GROBID-Dictionaries models (Rondeau Du Noyer\r\net al., 2019) to reinforce the parsing of the structure of each entry.\r\nWith GROBID-Dictionaries, the document is annotated using a cascading approach: several Conditional Random Fields (CRF) models are applied one after the other, each of them corresponding to a\r\ngranularity level in the final XML hierarchy:\r\nLevels\r\n\r\nTag(s)\r\n\r\nTask\r\n\r\n1\r\n2\r\n3\r\n4\r\n5\r\n\r\nbody\r\nentry\r\nnum, form and sense\r\nname and desc\r\nsubsense and note\r\n\r\nseparates the content from running titles, page numbers, . . .\r\nseparates the entries in the <body>\r\nseparates the lot n\u00B0, information on the author and the MS in <entry>\r\nseparates the name of the author and its biography in <form>\r\nseparates the MS description and the additional note in <sense>\r\n\r\nTable 1: GROBID-Dictionaries Segmentation levels\r\nThe GROBID-Dictionaries output for our example is therefore the following:\r\nGROBID-Dictionaries being developed for lexicographic purposes, its results are encoded in a TEI\r\ncompliant output, but with tags reflecting the content of dictionaries rather than catalogues. Therefore, we\r\nautomatically convert the output into a second TEI document whose tags are dedicated for the described\r\ncatalogue elements. The consistency of the transformation output is controlled with a specific schema,\r\nprior to its final publication via an XML database.\r\n\r\n4\r\n\r\nFuture work\r\n\r\nAs for future work, four tasks will be undertaken. First, we will move towards a fully open source\r\nworkflow and therefore abandon Transkribus for Kraken (Kiessling, 2019). Second, we will increase the\r\nsize of our database by retroconverting the entire RDA collection (c. 500 catalogues), but also another\r\nimportant series of catalogues: the Lettres autographes et documents published by the other branch of\r\nthe Charavay family up to the First World War (c. 500 catalogues). Third, we will improve our modeling\r\nand increase the granularity of our data collection to capture more informations regarding the document\r\n(size, format, length) and named entities (people, places). Fourth, we will use this additional information\r\nto reconcile entries and share our work with similar projects via an RDF export.\r\n\r\n122\r\n\r\n\fIdeally, we should eventually be able to go from the TEI digital edition of sales catalogues to a semantic\r\ndataset, described using controlled vocabularies where authors, places and manuscripts would be referred\r\nto using unique identifiers (ISNI, ISMI. . . ). It would allow federated search with other databases of sold\r\nmanuscripts, but also with catalogues of libraries in France and abroad.\r\n"
	},
	{
		"id": 20,
		"title": "Enriching a Multilingual Terminology Exploiting Parallel Texts: an Experiment on the Italian Translation of the Babylonian Talmud",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Angelo Mario Del Grosso",
			"Emiliano Giovannetti",
			"Simone Marchi"
		],
		"body": "Introduction\r\n\r\nTranslation is the only way of making a text accessible to people that do not understand the language\r\nthe original text is written in. Translation, in other words, allows to build bridges between peoples and\r\ncultures. It is no coincidence that it has been through a translation, contained in the well-known Rosetta\r\nStone, that Egyptian hieroglyphs could be deciphered. The work we here describe is based upon a similar\r\nprinciple: how to exploit the translation in a \"known\" language of a text written in an \"unknown\" language\r\nto derive some linguistic information from the latter. In our case, the \"known\" language is a language for\r\nwhich tools and resources are available to automatically extract information from a text written in that\r\nlanguage. Viceversa, the \"unknown\" language is the one that poses analytical problems, as it typically\r\nhappens in projects involving ancient texts and languages. In particular, as detailed in the following\r\nsection, we wanted to experiment a way of supporting the construction of a multilingual terminology by\r\nexploiting an existing translation.\r\nThe use of parallel texts in support to lexicon construction is a field known as bilingual lexicon\r\nextraction, and it has a wide scientific literature (see for example (Fung, 1998), (Tufi\u015F et al., 2004),\r\n(Gutierrez-Vasques, 2015)). From a more applicative point of view, tools and software libraries have\r\nbeen implemented to assist developers in implementing the word-by-word text alignment necessary to\r\nprocess parallel texts. Giza++1 and the Berkeley aligner2, for example, have been largely adopted for\r\nthese tasks. More in general, and in the context of Digital Humanities, the idea of exploiting parallel\r\ntexts has been adopted in a number of initiatives, among which we point out the Perseus project, where\r\nthe project team, together with the Von Humboldt professorship G. Crane within the Global Philology\r\n1 http:\/\/www.statmt.org\/moses\/giza\/GIZA++.html\r\n2 https:\/\/code.google.com\/archive\/p\/berkeleyaligner\/\r\n\r\n125\r\n\r\n\fproject, analyzed and implemented a collection of technologies and tools to envisage the \"complexities of\r\nworking with a historical record that contains far more languages than any individual could study, much\r\nless master\" (Crane, Gregory et al., 2019).\r\n\r\n2\r\n\r\nObjectives and motivation\r\n\r\nThe experiment we here illustrate, still in progress, was conducted on the Babylonian Talmud Italian\r\ntranslation, in the context of the homonymous project3. The project, in addition to the development of the\r\nsoftware Traduco used to support in the translation of the Talmud (Giovannetti et al., 2016), envisages the\r\nconstruction of a multilingual (Hebrew-Aramaic-Italian) terminological resource to support a number of\r\nactivities, such as boosting the Translation Memory System with terminological information and creating\r\nan ontology of the talmudic domain. As described in Section 3, the Italian portion of the resource was built\r\nwith the aid of a terminology extractor exploiting linguistic analysis tools for Italian. However, no tool or\r\nlinguistic resource was available to automatically process the three main ancient languages appearing in\r\nthe Talmud, namely, mishnaic Hebrew, biblical Hebrew and babylonian Aramaic. To obviate to this issue,\r\nand to the di\uFB03culty of automatically detecting the source terms through standard extraction processes, we\r\nchose to exploit the data produced in the last seven years of project activities, i.e. the available translated\r\ntractates of the Talmud. The results of the experiment suggested more ways of exploiting the obtained\r\nlist of term-pairs in addition to the enrichment of the terminological resource, for example, as it will be\r\ndiscussed in the final version of the paper, to help in the lemmatization of semitic languages.\r\n\r\n3\r\n\r\nMethodology\r\n\r\nBasically, the proposed approach makes use of a word-by-word alignment technique applied to a text\r\nin translation. The overall extraction process, leading to the enrichment of the terminological resource,\r\nfollowed a four step approach: i) encoding of the parallel text in TEI, ii) extraction of the Italian terms\r\nusing a customized term extractor, iii) application of a word-by-word alignment technique to the parallel\r\ntextual segments of the Talmud, iv) manual revision of the obtained alignment for the detection of the\r\nHebrew\/Aramaic terms corresponding to the Italian ones.\r\n3.1 TEI encoding of the parallel text\r\nWe have first modeled and encoded the available parallel text (i.e. the Talmud and its Italian translation)\r\nby means of the best practices dictated by the Text Encoding Initiative (TEI), whose schema is currently\r\nthe de facto standard to encode text-bearing objects (TBO) within the most authoritative scholarly projects\r\ninvolving literary inquiries. Actually, the choice to adhere to TEI environment provides benefits both\r\nto scholars, o\uFB00ering a standard model for the digital representation of critical texts, and to technicians,\r\nconcerning modularity, data management, and, in particular, independence related to specific development\r\nchoices. We have adopted the hierarchical text-group technique in order to encode the basic textual\r\nsegments in three di\uFB00erent modalities: 1) the original talmudic text; 2) the Italian translated text; 3)\r\nthe literal Italian translated text. Moreover, the linkage among the di\uFB00erent textual fragments has been\r\nconducted by means of the linkgroup technique 4. Section 3.3 will illustrate the word-by-word alignment\r\ntask that has been developed.\r\n3.2 Extraction of the target terms\r\nAs mentioned before, given the lack of NLP tools and resources for Ancient Hebrew and Aramaic we\r\ncould carry out the automatic extraction of the terminology only on the italian translation of the Talmud.\r\nFor this purpose we used T2K2 (Dell\u2019Orletta et al., 2014), a platform for linguistic analysis available at\r\nthe Institute of Computational Linguistics (ILC) of the Italian National Research Council (CNR). T2K2\r\nincludes a stochastic module for terminology extraction which appeared adequate for our experimental\r\npurposes. We applied the extractor to four of the already translated and revised tractates of the Talmud,\r\n3https:\/\/www.talmud.it\r\n4Module number 16 of the TEI guidelines - Groups of Links. https:\/\/www.tei-c.org\/release\/doc\/teip5-doc\/en\/html\/SA.html#SAPTLGen\/html\/SA.html#SAPTLG\r\n\r\n126\r\n\r\n\fnamely: Berakh\u00F2t, Rosh haShan\u00E0, Ta\u2019an\u00ECt and Qiddush\u00ECn. The corpus made of textual (plain-text UTF-8)\r\ndocuments was analyzed with T2K2 and the obtained output was furtherly processed in order to remove\r\nerroneous terms deriving from Part-Of-Speech tagging errors, and to sort the extracted terms by means of\r\nthe TF-IDF (Term Frequency-Inverse Document Frequency) statistical measure. An important outcome\r\nof the TF-IDF is to permit to measure the relevance of each term for each tractate in which it appears:\r\na high value of TF-IDF represents a high degree of relevance in the context of a specific tractate. The\r\nTable 1 shows some examples of term relevance by tractate.\r\nBerakh\u00F2t\r\nTerms\r\nBirk\u00E0t haMaz\u00F2n\r\nemissione di seme\r\nShem\u00E0\r\nsogno\r\ngabinetto\r\nfrutto della terra\r\nbenedizione sul vino\r\ntipi di cibi\r\npane dalla terra\r\nbisogni\r\n\r\nQiddushin\r\ntfidf\r\n\r\nfreq\r\n\r\nTerms\r\n\r\n0.0239\r\n0.0209\r\n0.0205\r\n0.0166\r\n0.0076\r\n0.0072\r\n0.0062\r\n0.0060\r\n0.0056\r\n0.0047\r\n\r\n120\r\n105\r\n206\r\n167\r\n38\r\n36\r\n31\r\n30\r\n28\r\n47\r\n\r\ndocumento\r\nperut\u00E0\r\nqiddush\u00ECn\r\nschiava\r\nterra di Israele\r\ndivorzio\r\nrapporto sessuale\r\npadrone\r\nschiava ebrea\r\ntrovatello\r\n\r\ntfidf\r\n\r\nfreq\r\n\r\n0.0159\r\n0.0155\r\n0.0142\r\n0.0119\r\n0.0107\r\n0.0104\r\n0.0101\r\n0.0100\r\n0.0088\r\n0.0080\r\n\r\n123\r\n120\r\n55\r\n46\r\n83\r\n40\r\n78\r\n186\r\n34\r\n31\r\n\r\nTable 1: The first ten Italian terms extracted from two of the four analyzed tractates and ordered by tf-idf.\r\n3.3 Extraction of the source terms via alignment\r\nWord-by-word text alignment is a very useful technique to help understanding cross-lingual properties of\r\nparallel texts while processing only one half of the whole resource (Tiedemann, 2011). In order to add\r\nthe Hebrew and Aramaic terms to the terminological resource we are building up from the Talmud, we\r\nset up the alignment process at token granularity. Specifically, we used an open source library realized by\r\nthe Berkeley University (Liang et al., 2006) to develop a tool for the linking of Hebrew\/Aramaic textual\r\nsegments with the corresponding Italian translations.\r\nItalian terms\r\nbenedizione (2.1)\r\nShem\u00E0 (1.1)\r\npreghiera (2.2)\r\npane (1.9)\r\nanno (2.14)\r\nmese (1.93)\r\ngiorno (1.90)\r\nshof\u00E0r (0.87)\r\nobbligo (2.1)\r\nschiavo (0.82)\r\n\r\nmost likely Hebrew term\r\n\u202B( \u05B0\u05D1\u05BC \u05B8\u05E8\u05DB\u05B8 \u05D4\u202C0.41)\r\n\u202B( \u05B0\u05E7 \u05B4\u05E8\u05D9\u05D0\u05B7 \u05EA\u202C0.53)\r\n\u202B( \u05B0\u05EA\u05BC \u05B4\u05E4\u05DC\u05B8\u05BC \u05D4\u202C0.30)\r\n\u202B( \u05DC\u05B6 \u05D7\u05B6 \u05DD\u202C0.27)\r\n\u202B( \u05E9\u05B8\u05C1 \u05E0\u05B8\u05D4\u202C0.32)\r\n\u202B( \u05D7 \u05B6\u05B9\u05D3\u05E9\u05C1\u202C0.25)\r\n\u202B( \u05D9 \u05DD\u202C0.36)\r\n\u202B( \u05E9\u05C1 \u05E4\u05B8 \u05E8\u202C0.77)\r\n\u202B( \u05D9\u05B8\u05E6\u05B8 \u05D0\u202C0.34)\r\n\u202B( \u05E2\u05B6 \u05D1\u05B6 \u05D3\u202C0.80)\r\n\r\nother candidates Hebrew terms\r\n\u202B( \u05B0\u05DE\u05D1\u05B8 \u05B5\u05E8\u202C0.29), \u202B( \u05B0\u05DE\u05D1\u05B8 \u05B0\u05E8 \u05B4\u05DB\u05D9\u05DF\u202C0.09)\r\n\u202B( \u05B0\u05E9\u05C1\u05DE\u05B7 \u05E2\u202C0.44)\r\n\u202B( \u05B0\u05EA\u05BC \u05B4\u05E4\u05DC\u05B8\u05BC \u05EA\u202C0.13), \u202B( \u05B0\u05EA\u05BC \u05B4\u05E4\u05DC\u05B8\u05BC \u05D4\u202C0.15), \u202B( \u05B0\u05EA\u05BC \u05B4\u05E4\u05DC\u05B7\u05BC \u05EA\u202C0.16)\r\n\u202B( \u05D4\u05B7 \u05E4\u05B7\u05BC \u05EA\u202C0.16), \u202B\u05D9\u05E4\u05EA\u05B8\u05BC \u05D0\u202C\r\n\u05B0 \u202B( \u05B4\u05E8\u202C0.16), \u202B( \u05E4\u05B7\u05BC \u05EA\u202C0.22)\r\n\u202B( \u05D4\u05B7 \u05E9\u05B8\u05BC\u05C1 \u05E0\u05B8\u05D4\u202C0.10), \u202B( \u05D4\u05B7 \u05E9\u05B8\u05BC\u05C1 \u05E0\u05B8\u05D4\u202C0.15), \u202B( \u05E9\u05B8\u05C1 \u05E0\u05B8\u05D4\u202C0.19)\r\n\u202B( \u05DC\u05B7 \u05D7 \u05B6\u05B9\u05D3\u05E9\u05C1\u202C0.21), \u202B( \u05D4\u05B7 \u05D7 \u05B6\u05B9\u05D3\u05E9\u05C1\u202C0.24)\r\n\u202B( \u05D1\u05B7\u05BC \u05D9\u05BC \u05DD\u202C0.09), \u202B( \u05D4\u05B7 \u05D9\u05BC \u05DD\u202C0.14), \u202B( \u05D9 \u05DE\u05B8 \u05D0\u202C0.19)\r\n\u202B( \u05B0\u05D1\u05BC\u05E9\u05C1 \u05E4\u05B8 \u05E8 \u05EA\u202C0.08)\r\n\u202B( \u05D7 \u05D1\u05B8 \u05D4\u202C0.09), \u202B( \u05D7 \u05D1\u05B8 \u05EA\u202C0.22)\r\n\u202B( \u05D5\u05B0 \u05E2\u05B6 \u05D1\u05B6 \u05D3\u202C0.09)\r\n\r\nTable 2: Some examples of Italian-Hebrew\/Aramaic aligned terms. Italian terms with high entropy\r\n(such as \"preghiera\") have been aligned with multiple Hebrew\/Aramaic terms: the confidence that the\r\nterm \"\u202B( \" \u05B0\u05EA\u05BC \u05B4\u05E4\u05DC\u05B8\u05BC \u05D4\u202Cthe one with the highest likelihood) is the actual translation of preghiera is low.\r\nTo carry out the word-by-word alignment, the tool implements generative models that have been\r\nstudied during the last decades by the IBM researchers and by the Machine Translation community\r\n\r\n127\r\n\r\n\f(Brown et al., 1993). In particular, it adopts the IBM Model-1 with the extension of the Hidden Markov\r\nModel paradigm (\u00D6stling and Tiedemann, 2016). The alignment task employed non-supervised machine\r\nlearning algorithms adopting probabilistic models to calculate the likelihood estimation of aligning a\r\nterm in a known language to a term in a foreign language. The expressiveness of these kinds of alignment\r\nmodels is particularly suitable in the literary domain, where translations tend to be more interpretative\r\nand less literal. Eventually, for each Italian term the computed probabilistic alignment model provided\r\na list of Hebrew\/Aramaic candidate words. In Table 2, the numbers reported next to the Italian terms\r\nrepresents the entropy measure, which indicates the confidence of the translated word. The numbers next\r\nto the Hebrew\/Aramaic words indicate the likelihood that word is the translation of the corresponding\r\nItalian word.\r\n3.4 Manual revision\r\nThe aligner developed so far is based on statistical approaches which are, inherently, prone to errors. For\r\nthis reason, the alignment environment requires a tool to validate and manually annotate the obtained\r\noutputs. We are thus developing a Web application able to manage and process the aligned text segments.\r\nAs shown in 1, we have provided the proofreader with the possibility to annotate each word with a number\r\nof language and textual traits, namely lemma, Part-of-Speech, type of text, and language.\r\n\r\nFigure 1: The annotation component of the proofreader.\r\n\r\nThe output of the aligner is formatted as a sequence of strings like 0-0 4-6 2-5 3-4 1-2 1-1 representing the word pairs that have been aligned. The order of the pairs is not significant, while the\r\nnumber within the pair represents the position of the word within the source-target strings; for example,\r\nin the two strings \"\u202B\u05DE\u05D5\u05BC\u05E8\u05D4 \u05D4\u05B8 \u05B4\u05E8\u05D0\u05E9\u05C1 \u05E0\u05B8\u05D4\u202C\r\n\u05B8 \u202B \"\u05E2\u05B7 \u05D3 \u05E1 \u05E3 \u05D4\u05B8 \u05D0\u05B7 \u05B0\u05E9\u05C1\u202Cand \"fino alla fine della prima veglia\" the pair 0-0 would\r\nindicate the word pair \"\u202B\u05E2\u05B7 \u05D3\u202C-fino\" (Hebrew is read from right to left). More details about the proofreader\r\nwill be provided in the final version of the paper. Eventually, the revision process will allow to build a\r\nground truth and\/or a gold training set and consequently put in place a complete validation process of the\r\nalignment results.\r\n\r\n4\r\n\r\nPreliminary results, discussion and next steps\r\n\r\nAs it was shown, a parallel text can be exploited fruitfully via text alignment techniques to help in the\r\nconstruction of a multilingual terminology. Our reference scenario was the Italian translation of the\r\n\r\n128\r\n\r\n\fBabylonian Talmud, carried out in the context of the homonymous project. At the current stage of the\r\nwork, 219.000 tokens have been analyzed, distributed on 42.000 textual segments extracted from the four\r\naforementioned tractates which have been translated so far.\r\nIn addition to their use in populating the terminological resource, the obtained term-pairs may be also\r\nexploited in other ways. The first two applications we are going to investigate are: the boosting of text\r\nsearch, as recently experimented also in (Andonovski et al., 2019), and the support in the automatic\r\nprocessing of the source language.\r\nConcerning the next steps of this research, once a significant number of segments (and, thus, of the\r\nterms appearing in the segments) will have been revised by the expert of the Talmud, a formal evaluation\r\nof the accuracy of the approach will be carried out. Fig. 2 shows an example of revision of the alignment.\r\n\r\nFigure 2: An example of use of the proofreader: the output of the automatic alignment (at the top) and\r\nthe relative revision (at the bottom).\r\n\r\nBesides, we intend to improve the performance of the approach by taking into account the variety of\r\ntexts and languages that coexist inside the Talmud before the application of the aligner. As a matter of\r\nfact, the Babylonian Talmud is constituted by two (macro) texts, i.e. the Mishna and the Gemara, which,\r\nin turn, incorporate portions of other texts, such as, for example, quotes from the Tanakh (the Hebrew\r\nBible). In the particular case of the Talmud, each text is written in a specific language: the Mishna in\r\nMishnaic Hebrew, the Gemara in Babylonian Aramaic and the Tanakh in Biblical Hebrew. The idea is to\r\nautomatically classify each segment of the Talmud on the basis of the text it belongs to and, after that,\r\n\r\n129\r\n\r\n\fto apply the aligner on each textual class composed of linguistically homogeneous segments. By doing\r\nthis, we expect a better accuracy from the aligner and, ideally, no need from the revisor to indicate the\r\nlanguage of each segment.\r\n\r\nAcknowledgement\r\nThis work was conducted in the context of the TALMUD project and the scientific cooperation between\r\nS.ca r.l. PTTB and ILC-CNR."
	},
	{
		"id": 21,
		"title": "Towards a lexical standard for the representation of etymological data",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Fahad Khan",
			"Jack Bowers"
		],
		"body": "Introduction\r\n\r\nIn this submission we will introduce a new standard, currently in an advanced stage of development1, for\r\nthe modelling and publication of etymological data in computational lexical resources2. The standard\r\nin question, which we will refer to it as LMF-Ety, is the third part of a new multi-part revision of the\r\nLexical Markup Framework (LMF), ISO 24613-4, originally published by the International Standards\r\nOrganisation (ISO) as a single standard in 20083. We will motivate the need for LMF-Ety by describing\r\nsome of the main challenges of modelling etymological data in computational lexical resources and\r\nshowing how our new standard meets these challenges as well as how it differs from other previous\r\nmodels. Subsequently, we will describe the core concepts which we have so far established in our model\r\nand illustrate them through the use of an extended example taken from an etymological dictionary. Our\r\nintention is both to summarise the work we have carried out in the development of the LMF-Etymology\r\nstandard as well as to showcase our broader approach to modelling etymologies. This approach entails the\r\nrepresentation of etymologies as formal graphs describing simple narratives relating to a given lexicon\r\nphenomena; it is an approach that takes account of and consolidates previous attempts at modelling\r\netymologies computationally but that also seeks to extend them in various different directions with a view\r\nto obtaining a more robust and expressive model. Additionally both authors are also involved in other\r\ninitiatives for modelling etymologies in two other frameworks\/standards, the Text Encoding Initiative\r\n(TEI) (Bowers and Romary, 2016) and Linked Data (Khan, 2018). We will end the submission by\r\n1At the time of writing, December 2019, the standard is being prepared for submission to an ISO ballot as a Draft International\r\nStandard (DIS). The classes and the approach which we present are now therefore fairly stable.\r\n2The two authors are the joint project leaders of this standard.\r\n3Note that the original version of LMF did not contain specific provision for modelling etymological\/diachronic information.\r\n\r\n131\r\n\r\n\fdescribing how LMF-Etymology may be rendered inter-operable with work being carried out in these\r\ntwo latter frameworks. This will also be relevant for understanding the practical details of how LMFEtymology can actually be used (SPOILER ALERT: Part 4 of the LMF standard is a serialisation of all\r\nthe previous parts in TEI-XML).\r\n\r\n2\r\n\r\nBackground\r\n\r\nThe importance of standards for the publication of scientific and scholarly datasets and resources for\r\nrendering them more findable, accessible, interoperable and reusable is by now well understood across\r\nthe board. There have been a number of initiatives for promoting such standards and best practices in the\r\nfield of language resources. Three of the most notable of these as applied to the case of lexical datasets\r\nare: the original version of the Lexical Markup Framework described below; the Dictionaries chapter\r\nof the Text Encoding Initiative (TEI) guidelines4; and finally the RDF-based Ontolex-Lemon guidelines\r\n(McCrae et al., 2017). These standards not only help to ensure a greater measure of interoperability\r\nbetween different computational lexicons, but they also facilitate the representation of lexical information\r\nin a way that makes it more amenable to advanced kinds of machine processing. Up until recently all three\r\nof these standards have dealt almost exclusively with synchronic lexical data5. This neglect of diachronic\r\ndata is due, in part, to the awkwardness associated with the addition of extra temporal parameters to\r\nstatements in data frameworks such as UML or RDF, and partly due to the (relatively) slow pace of\r\ndevelopment in the three standards overall \u2013 even if this would seem to constitute a missed opportunity,\r\nparticularly in the case of etymological data since, at an abstract level, etymologies traditionally describe\r\ngraph structures. They would therefore be ideally suited for representation in formalisms where this\r\nunderlying structure can be rendered explicit, making such data easier to query and process. Moreover\r\nstandards like LMF and especially the RDF-based Ontolex-Lemon would potentially make it easier to link\r\ntogether and query across different etymological datasets and to therefore create extended etymological\r\nnetworks. LMF-Ety is intended both for the creation of etymological datasets ex novo as well as for\r\nthe conversion of legacy print resources as structured data. In this latter respect it should be noted that\r\nalthough our initial use cases have so far been largely concerned with the conversion of legacy dictionaries\r\ninto structured resources, descriptions of etymological graph structures can be found in, and therefore\r\npotentially extracted from, numerous different kinds of texts. These include both scholarly works in\r\nlinguistics, especially in the sub field of historical linguistics (articles, book chapters, monographs,\r\netc), along with other genres of texts, literary, religious and philosophical6. It is clear then that the\r\ncomputational modelling of etymologies stands firmly at the intersection of computational linguistics,\r\ne-lexicography and the digital humanities. This inter-disciplinarity can also be appreciated in the fact that\r\netymologies for languages with a sufficiently extensive written tradition will often contain attestations to\r\ncoprora of historic texts; these texts can sometimes be reconstructions or have disputed interpretations as\r\nto particular word senses, (bringing to bear issues concerned with textual criticism and philology\/literary\r\ncriticism more generally).\r\n2.1\r\n\r\nThe Lexical Markup Framework\r\n\r\nThe original 2008 version of LMF, ISO 24613: 2008, was intended as a \u201Cstandardized framework for the\r\nconstruction of computational lexicons\u201D (Francopoulo, 2013) an was conceived of as a common model\r\nboth for lexicons for use in NLP applications as well as for computational versions of print or legacy\r\ndictionaries. Regarded as one of the most important standards in the field of lexical resources, LMF\r\nwas enormously influential in the definition of the Ontolex-Lemon model and its predecessorlemon. A\r\nreview of the Lexical Markup Framework undertaken by ISO in 2016 resulted in a decision to revise\r\nthe standard, and to publish it as a multi-part standard (Romary et al., 2019) to render it more modular;\r\nthe decision was also made to broaden the applicability of the new version of LMF to capture more\r\nkinds of lexical information. In consequence it was decided that one of the new parts of LMF should\r\n4https:\/\/www.tei-c.org\/release\/doc\/tei-p5-doc\/en\/html\/DI.html\r\n5The TEI guidelines do permit the representation of etymological data in a structured way but in a relatively shallow way. A\r\nrecent proposal to allow for more salient kinds of etymological annotation can be found in (Bowers and Romary, 2016)\r\n6See (Khan, 2018) which presents an example taken from Hobbes\u2019Leviathan.\r\n\r\n132\r\n\r\n\fbe a specialised module dealing with diachronic lexical information. The different parts of the new\r\nLMF standard are: the Core Model (ISO 24613-1); the Machine Readable Dictionary Module (ISO\r\n24613-2); the Diachrony-Etymology Module (ISO 24613-3); Serialisations in both TEI (ISO 24613-3)\r\nand LBX (ISO 24613-3). Two new modules dealing with syntax and semantics and morphology have\r\nalso been proposed.\r\n2.2\r\n\r\nRelated Work and Standards\r\n\r\nA number of proposals have been made in the past to try and redress the lack of provision for encoding\r\nstructured etymological data in lexical resources. These have included suggestions for extensions of both\r\nfor TEI (Bowers and Romary, 2016) and LMF (Salmon-Alt, 2006), as well as proposals for new Ontolexlemon classes and properties(Chiarcos et al., 2016), (Khan, 2018). The standard described in the current\r\nwork is influenced by the aforementioned works, and in particular it is informed by the approach in taken\r\nin (Khan, 2018) while abstracting away from specific details mentioned in that work and which pertain to\r\nthe Resource Description Framework (RDF). Moreoever, it is the result of an attempt to converge towards\r\na set of high level concepts, abstractions, that are sufficiently expressive to encode the main kinds of\r\nphenomenon and information which tend to be included in etymologies, as well as being simple enough\r\nto be usable by a wide community of potential users. The main concepts which we have determined\r\nupon are described in the next section. As we mentioned above the fourth part of the new standard is a\r\nTEI serialisation of all the other parts (due to be published at the same time as LMF-Etmyology). This\r\nshould ensure that at a high level both TEI and LMF are interoperable and in particular the etymological\r\napprpach taken by LMF-Etymology is compatible with TEI. It also means that LMF-Etymology should\r\nultimately be accessible to digital humanists who are more used to working with TEI.\r\n\r\n3\r\n\r\nLMF Etymology\r\n\r\nThe definition of LMF Ety (ISO Standard 24613-3) is dependent on the two preceeding ISO standards\r\nin the new multi-part LMF standard, i.e., the Core Module, recently published by ISO, and the Machine\r\nReadable Dictionaries Module, due to be published in 2020 (Romary et al., 2019). Both of these standards\r\ncontribute foundational concepts (for modelling lexicons) to LMF Ety such as Lexical Entry, Lemma,\r\nForm, and Sense: all of which keep their (fairly intuitive) meaning from the previous version of LMF\r\nand all of which share the meaning of similarly titled concepts in TEI and Ontolex-Lemon. On the basis\r\nof these foundational concepts then LMF Ety defines a number of additional classes which enable us\r\nto associate temporal\/historical information with lexical data encoded in LMF. The strategy we adopt\r\nis that suggested in (Khan et al., 2014) of modelling linguistic elements such as words, senses, forms,\r\netc as perdurants, that is, as entities associated with a lifespan, which in the present case represents\r\nthe interval of time in which they are considered to have been part of common usage within a given\r\nlinguistic community7. This enables us to situate lexical entries etc in a temporal dimension and also\r\nto relate them together via diacrhonic linguistic processes. Our model then represents etymologies as\r\nsimple narratives, or as rather simple narrative graphs, in which different linguistic phenomena (each\r\nof which can be potentially associated with a lifespan and situated on a timeline) are linked together\r\nusing special etymological link elements, individuals of the class EtyLink, which can represent different\r\nkinds of historical linguistic processes such as inheritance or borrowing or semantic shift. The other new\r\nelements in the standard are the described below:\r\n\u2022 Etymon and Cognate: Two elements modelled as subtypes of Lexical Entry. What differentiates\r\nthem from other lexical entries in a lexicon is their (specialised) role: they are used in describing the\r\netymologies of other lexical entries: Etymons are lexical entries from which a given lexical entry\r\nis derived via some historical process; Cognates are lexical entries which share a common ancestor\r\nwith a given a lexical entry; Additionally Cognate Set represents the reification of a set of cognates.\r\n\u2022 Etymology: An element that represents a single history of a lexical entry or other element. We\r\nassociate Etymology individuals with an ordered series of EtyLink instances; this allows us to\r\n7This approach makes it easier to represent such information in RDF.\r\n\r\n133\r\n\r\n\fFigure 1: Entry for forum.\r\n\r\nFigure 2: Encoding of the entry for forum.\r\ndefine different etymologies featuring shared elements. In addition Etymology instances can be\r\nrecursive, they can also be typed to define the changes undergone according to any number of\r\nlinguistic processes.\r\nAlthough we have had to leave out a number of details in this brief summary, the classes which we have\r\nenumerated above are the fundamental ones for understanding and using the standard. We were able to\r\nestablish these classes over the course of numerous iterative design cycles during which draft proposals\r\nwere reviewed against a large and diverse number of use cases: evaluating them on the basis of their\r\nsalience, expressivity, and understandability. In Figure 1 we present the entry for the Latin word forum\r\nfrom De Vaan\u2019s Etymological dictionary of Latin and the other Italic languages (De Vaan, 2018), and\r\nin Figure 2 a partial encoding of this entry using LMF. Here we have focused chiefly on the information\r\nin the written entry which concerns etymons and cognates8. Note the relationship between the lexical\r\nentry and its two etymons (both of which have been categorized as reconstructed lexemes). We have\r\nalso added two Cognate Set elements which, although we haven\u2019t shown it in the diagram, can be linked\r\nto their associated etymons. Note that these elements are linked to the LMF lexical entry for forum via\r\nan Etymology element which is in reality a container for an ordered set of EtyLink elements. It is\r\nimportant also to note that not all of the information in an etymology can be easily represented in a graph\r\nlike structure and this we can instead represent in additional textual elements.\r\n8There exists provision in LMF-Ety for representing information concerning attestations, references to secondary literature\r\nand for adding textual information as notes attached to entries or etymologies although we haven\u2019t presented it here. We also\r\nhaven\u2019t added explicit temporal important to this example either. Full details will be available in the final version of the standard.\r\n\r\n134\r\n\r\n\f4 Acknowledgements\r\nThe first author was supported by the EU H2020 programme under grant agreements 731015 (ELEXIS European Lexical Infrastructure).\r\n"
	},
	{
		"id": 22,
		"title": "Workflows, digital data management and curation in the RETOPEA project",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Ilenia Eleonor Laudito"
		],
		"body": "Description and aim of the RETOPEA project\r\n\r\nFunded by the European Commission under the program Horizon 2020, RETOPEA (REligious TOleration\r\nand PEAce) aims at creating a modern understanding of religious conflicts and peace-making history among\r\nyoungsters and students throughout Europe. The intention is to teach in a comprehensible and appealing way,\r\ncomplex aspects of the past and present society. The project\u2019s target group are students between 12 and 18\r\nyears old, attending schools as well as non-academic institutions in European countries partnered with the\r\nproject (which are Spain, UK, France, Belgium, Germany, Finland, Estonia and Macedonia).\r\nCharacteristic to this project is its mixture of a historical corpus of peace treaties and agreements \u2013 spanning\r\nfrom settlements prior to the anno domini to the most recent Charter of Fundamental Rights of the European\r\nUnion \u2013 and contemporary political discourses, popular culture among teenagers, new spiritual initiatives and\r\nheritage. The materials selected and processed by the historical research groups (called \u201Cclippings\u201D1) will be\r\ndisclosed on the RETOPEA official website and will serve as primary resources and background information\r\nfor the creation of short documentary films about the different aspects of tolerance and religious coexistence\r\n1\r\nA clipping is a piece of information with a length of ca. 200 \u2013 500 words, about a specific subject, possibly containing different\r\nmedia types and formats.\r\n\r\n136\r\n\r\n\fby the students participating to the project. These short films (called \u201Cdocutubes\u201D) will be published on the\r\nproject\u2019s online platform and can be used in the future for further teachings.\r\nIt is essential to the project\u2019s aim and purpose that all research data is correctly translated in the seven\r\nlanguages corresponding to the above-mentioned partnered European countries. This fundamental requirement\r\nassures that all students, independently from their knowledge of the topics presented and their level of\r\nunderstanding of the English language, are able to comprehend solidly the informations provided by the\r\nresearchers.\r\n\r\n2\r\n\r\nTechnical environment, workflow and data management\r\n\r\nThe Data Management Plan guarantees the storage sustainability and the long-term preservation of all relevant\r\nresearch data produced and processed by the historical research groups. The collected data will be publicly\r\navailable via a Virtual Research Environment (VRE). Additionally, the designed workflow establishes a proper\r\ncoordination between the historical research groups and the technical requirements of the project.\r\n2.1\r\n\r\nTechnical enviroment\r\n\r\nThe technical specifications for the storage, preservation and visual presentation of the collected data consists\r\nof three main components: a collective access database (with an API to link material from other databases and\r\nplatforms), a digital repository (TENEO) and a publishing tool (Omeka) that also serves as the project\u2019s official\r\nwebsite.\r\nCrucial for the publication platform\u2019s selection was a user-friendly environment for students, teachers and\r\nresearchers. Omeka is a web publication platform for sharing digital collections and creating media-rich online\r\nexhibits (Omeka S User Manual, 2019). This tool is mainly used by universities, archives, museums and\r\ngalleries and fits the project\u2019s specific demands, due to the heterogeneity of its data.\r\n2.2\r\n\r\nWorkflow\r\n\r\nThe workflow divides the clipping\u2019s production process in four main stages: creation of the source clippings\r\nin English, automatic translation of the clippings, review of the automatic translations and upload by the\r\nresearch data manager into the VRE.\r\n\r\nFigure 1: The four stages of the workflow.\r\nAll clippings require to be written in English to facilitate the translation of its content afterwards.\r\nThe four stages\r\nof the is\r\nworkflow.\r\nAdditionally, the clippings need toFigure\r\npass a2:readability\r\ntest \u2013 which\r\nbased on the English language \u2013 to match\r\nthe student\u2019s reading and grade level. The selected material differs widely in subject matter, field and case\r\nstudy. According to the analysis made on the clippings reading ease and grade level, RETOPEA safeguards\r\nan appropriate level of understanding of the complex topics selected by the researchers. The pedagogical\r\nimportance of this step underlines once again the project\u2019s priority concerning the didactical value of the data\r\nproduced.\r\n\r\n137\r\n\r\n\fThe second stage concerns the automatic translation of the clippings, whereas the third stage concentrates\r\non the review of the automatically translated content of the clippings. As explained, it is fundamental that all\r\nclippings are grammatically, linguistically and thematically comprehendible by youngsters. The current\r\ntranslation tools available are fallible, thus the need for a reviewing process by native language speakers.\r\nNonetheless, the possibility to accelerate the workflow and lighten the workload of the translation process\r\nthrough automation is a major advantage. At the present state, the tools used for the automation were tested\r\nonly on a small number of clippings, since the research groups must work on its production first. From the\r\nvarious web translators tested, the following tools will be used in the project: DeepL for Dutch,\r\nGerman, Spanish, French and Polish, Google Translate for Macedonian and Finnish and Tilde for Estonian.\r\nThe most arduous languages to translate result to be Finnish and Macedonian, not only for the complexity of\r\nthe language per se, but mostly for the lack of tools available.\r\nIn the final step the research data manager will upload the complete dataset in the VRE and make small\r\nadjustments concerning the clippings\u2019 visual representation based on the resources and formats used (text,\r\nvideo and\/or audio), in concordance with the clipping\u2019s creators.\r\nAlbeit this workflow is efficient and functional to RETOPEA\u2019s work organisation, it ought to be adapted\r\nto fit other requisites and necessities, if used in different projects. There are mainly two things that should be\r\ntaken into consideration: first, the volume of the research data produced; second, the financial aspect of a\r\nreviewing process should not be underestimated and adequately evaluated.\r\nRETOPEA has a relatively small dataset of short text snippets (approximately 400 clippings) which can be\r\nuncomplicatedly handled and humanly reviewed. Due to this factor, the costs and extent of a reviewing process\r\nare limited. If a project has a broader dataset, then the human involvement and financial aspects should be\r\ncarefully scrutinised, especially in the case of minor European languages. For example, as for the Finnish\r\nlanguage review, it may be more convenient in terms of financial resources and time management to directly\r\ntranslate the data from the source language without going through an automatic translation.\r\nThis notwithstanding, one major advantage of this workflow is that it can be easily managed, exploited and\r\nfollowed by researchers, independently from their computer skills and know-how.\r\n2.3\r\n\r\nMetadata structure\r\n\r\nEvery clipping necessarily includes a written part and may contain different media types in various formats\r\nbeyond its written content (e.g. additional textual resources, images, video and\/or audio material, YouTube or\r\nother URLs, etc\u2026). To facilitate both the researchers\u2019 and the research data manager\u2019s work, the researchers\r\nwill fill the clipping\u2019s metadata in two simple spreadsheets, containing the tags selected by the research data\r\nmanager in the column header. The spreadsheets will be pasted and exported to CSV format and uploaded in\r\nthe Omeka environment. Given the dataset\u2019s heterogeneity in terms of composition of resources, the\r\ndevelopment of a project-wide metadata scheme to normalise the metadata records was mandatory.\r\nThe metadata standards and controlled vocabulary used in the project are the Dublin Core Metadata\r\nStandard (DC), the Bibliographic Ontology (BIBO), GeoNames (GN) and the Canadian Writing Research\r\nCollaboratory Ontology (CWRC). Omeka provides an automapping module that maps the metadata terms of\r\nthe spreadsheet\u2019s header to the imported vocabularies (Omeka S User Manual, 2019).2 The module also allows\r\nto associate a media source (e.g. HTML, URL, YouTube link, etc\u2026) to a selected metadata tag. In\r\nRETOPEA\u2019s case, the clippings\u2019 contents are ingested as HTML-code through the Bibliographic Ontology\r\nTerm <Content>. The tag will be hidden afterwards and will not be displayed as metadata, yet the content will\r\nbe visible as media attachment.\r\n\r\n2\r\n\r\nFor example, \u201Cdcterms:title\u201C is automatically mapped to the Dublin Core <Title> property.\r\n138\r\n\r\n\fFigure 2: Draft of a clipping as displayed on the RETOPEA website.\r\nFurther, Omeka allows to aggregate the ingested data in user-made collections. The twelve collections used\r\n3: The\r\nstages\r\nof the workflow.3\r\nin this project aim at grouping theFigure\r\nclippings\r\nintofour\r\nabstract\r\nthematic\r\nclassifications of project-relevant keynotes.\r\nThe collections used in RETOPEA represent generic topical focuses (e.g. \u201CReligious practice\u201D, \u201CGender and\r\nSexuality\u201D, \u201CPropaganda and stereotyping\u201D, etc\u2026), to which clippings can belong independently of their\r\nsubject. Subjects differ from the topical focuses, for the latter have a broader thematic range and may apply to\r\nan indefinite number of clippings, whereas the intent of the subject\u2019s list is to bundle a relative small number\r\nof clippings into strictly defined subjects (e.g. \u201CPeace of Augsburg\u201D, \u201CEdict of Nantes\u201D, \u201CYouTube channels\u201D,\r\n\u201CPolitical speeches\u201D, etc\u2026). The same clipping can appear in more than one collection, depending on how\r\nmany relations the researcher associated to the clipping. This type of arrangement constructs an intricate\r\nentanglement between clippings in order to create vast links and relations between clippings that do not share\r\nthe same subject matter. These relations and clusters belong to and are part of the metadata description, creating\r\nboth a vertical and a horizontal hierarchical structure. The main purpose of this structure is to drive the website\r\nusers and the teenagers using the provided clippings to produce their docutubes to discover as many clippings\r\nas possible, independently of the clipping\u2019s affiliation or subject matter.\r\n\r\n139\r\n\r\n\fBesides the clipping\u2019s title, description, contextual focus and content, the implemented BabelNet API will\r\nautomatically extract and translate all other metadata tags and keywords through an HTTP interface that\r\nreturns JSON (Navigli and Pozzetto, 2012). BabelNet not only functions as an online translator, but also\r\nrecognises synonyms, word sense and (multilingual) semantic relatedness, shaping the possibility to generate\r\nlinked data and semantic networks.\r\nThe described metadata structure was designed specifically for RETOPEA and determined by the intensive\r\n\r\nFigure 3: BabelNet translation of \u201CGood Friday\u201D.\r\ncollaboration, confrontation and discussion with the two historical research groups. Due to the project\u2019s\r\ndistinctive didactical purpose and sundry data, it contains peculiar arrangements that are not always feasible\r\nor desirable in other DH-projects. This notwithstanding, this structure could be readily adopted and accordingly\r\nmodified to fit other requirements.\r\nConsidering RETOPEA\u2019s didactical and pedagogical purpose, the project\u2019s resources will be disclosed\r\nunder the Creative Commons Licenses (i.e. CC BY-NC-SA). Most of the external materials used in the project\r\ncan be likewise used and remixed by third parties. Additionally, future tasks will concern the implementation\r\nof external databases, like the IEGs \u201CMaps\u201D and \u201CEuropean History Online\u201D (EGO) Databases, and the \u201COn\r\nsite, in time\u201D project.\r\nFurther developments in RETOPEA will give a more precise evaluation about the translation tools used\r\nand the metadata structure. Moreover, the controlled vocabularies used leave open the possibility to connect,\r\norganise, retrieve and interlink the project\u2019s resources in Linked Open Data.\r\n\r\n3\r\n\r\nInnovation possibilities and DH importance\r\n\r\nThe methodology and the workflow described in this paper aims at giving a suggestion on how humanistic\r\nprojects can organise and arrange the digital data produced and the immense possibilities that Natural\r\nLanguage Processing tools and approach may offer.\r\nIn the last years the availability and growing amount of data extremely increased, also through the thriving\r\nof Digital Humanities related projects. The need for automatically translated documents, data and metadata\r\nwill increase as more DH-projects arise worldwide. This need does not only apply to major and minor\r\nEuropean languages, but also to Arabic and Asiatic languages as well as dialects.\r\n\r\n140\r\n\r\n\fAcknowledgements\r\nThe research was supported by the Leibniz Institute for European History (IEG) and the Religious\r\nToleration and Peace (RETOPEA) research project. This paper is based upon work supported and funded\r\nby the European Commission under the funding program Horizon 2020, Grant CULT-COOP-05-2017.\r\nSpecial thanks go to Marco B\u00FCchler, who provided insight and expertise and collaborated to the\r\ndevelopment of the workflow. Further gratitude goes to Bram De Ridder, who wrote the clipping shown\r\nin \u201CFigure 2\u201D, for granting and permitting the publication of his draft example."
	},
	{
		"id": 23,
		"title": "Il confronto con Wikipedia come occasione di valorizzazione professionale: il case study di Biblioteca digitale BEIC",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Lisa Longhi"
		],
		"body": "1 Cosmetica del nemico o delle diverse comunit\u00E0\r\nL\u2019attendibilit\u00E0 dei contenuti di Wikipedia continua ad essere oggetto di accesi dibattiti che vedono come\r\nprotagonisti su un fronte i bibliotecari coinvolti nella redazione delle voci dell\u2019enciclopedia, sull\u2019altro i\r\nbibliotecari che ancora se ne tengono a distanza1. Esiste ancora un solco che si pensava colmato. Cosmetica\r\ndel nemico \u00E8 il titolo di un breve romanzo di Am\u00E9lie Nothomb, dove la parola \u2018cosmetica\u2019 va intesa nel suo\r\nsignificato semantico, ovvero \u00ABla morale suprema che determina il mondo\u00BB. Il problema \u00E8 che ogni mondo ha\r\nla sua.\r\nLa prima reazione, una decina di anni fa, all\u2019idea di una collaborazione tra bibliotecari e wikipediani \u00E8 stata\r\ninfatti quella di collocare le due realt\u00E0 agli antipodi: da un lato una professione con tradizioni secolari e\r\nnormative condivise per il trattamento delle risorse bibliografiche; dall\u2019altro una comunit\u00E0 giovane con pochi\r\nprincipi operativi ed etici - i cinque pilastri - che nascono dall\u2019esigenza di un sapere libero2. Negli ultimi anni\r\n\u00E8 apparso sempre pi\u00F9 chiaro il fatto che questa impressione derivasse da un pregiudizio: sebbene infatti uno\r\ndei pilastri reciti che \u00ABWikipedia non ha regole fisse\u00BB, esistono molti punti di contatto fra le tradizioni\r\nbibliotecarie e le prassi di Wikipedia, laddove le intenzioni programmatiche di garantire l\u2019accesso libero alla\r\nconoscenza si riflettono nell\u2019utilizzo di strutture a schema fisso - come template e legami con URI - per\r\norganizzare le informazioni. Inutile sottolineare che si tratta di strumenti assai noti a chi si occupa di\r\ninformation literacy e di cataloghi bibliografici3.\r\n\r\n2 Lo straniero o dei diversi bibliotecari\r\nQuesto contributo prende tuttavia le mosse da un processo di crescita pi\u00F9 lontano. Negli ultimi anni si \u00E8\r\ndiscusso a lungo sui percorsi formativi utili a diventare bibliotecari e sulla necessit\u00E0 di acquisire competenze\r\nche sviluppino capacit\u00E0 storico-umanistiche, ma anche una formazione tecnica e tecnologica. L\u2019obiettivo \u00E8\r\nquello di aderire a un modello di bibliotecario ampio e complesso, per essere riconosciuti come bibliotecari in\r\nItalia e in Europa. A tal fine, si sta cercando di delineare un iter formativo unico e accreditato4.\r\n\r\n1\r\nUno degli ultimi dibattiti \u00E8 apparso a inizio 2019 sul forum di discussione Humanist, poi diffuso dalla lista AIB-CUR: i messaggi\r\nriguardanti Wikipedia sono raccolti nel thread the question on Wikipedia, <https:\/\/dhhumanist.org\/volume\/32\/>.\r\n2\r\n\r\nCinque pilastri, <https:\/\/it.wikipedia.org\/wiki\/Wikipedia:Cinque_pilastri>.\r\nEsiste una nutrita bibliografia che attesta il dialogo e i risultati di una virtuosa collaborazione realizzata e possibile, dimostrando\r\nvicinanza sul piano operativo ma anche affinit\u00E0 metodologica. Questi argomenti sono raccolti per esempio in: L. Catalani-P. Feliciati,\r\nWikipedia, le biblioteche e gli archivi \/ Wikipedia, Libraries and Archives, \u00ABJLIS.it\u00BB, 9, 3 (September 2018), p. I-III,\r\n<https:\/\/www.jlis.it\/issue\/view\/789>.\r\n\r\n3\r\n\r\n4\r\n\r\nL\u2019emanazione della l. 14\/1\/2013, n. 45 ha sancito in Italia l\u2019obbligo di rintracciare dei criteri univoci, relativi alle conoscenze e alle\r\ncompetenze, che ogni professionista intellettuale. Per una disamina della legge e dei suoi effetti in ambito bibliotecario, si veda: R.\r\nDe Magistris, Il riconoscimento delle professioni non regolate e la legge n. 4 del 14 gennaio 2013, \u00ABAIB studi\u00BB, 53, 3 (2013), p.\r\n239-260, <http:\/\/aibstudi.aib.it\/article\/view\/9074>.\r\n142\r\n\r\n\fTuttavia, bench\u00E9 esistano pubblicazioni che descrivono quali siano le tappe di questo percorso5, bench\u00E9\r\nl\u2019Associazione Italiana Biblioteche si sia fatta carico di rilasciare l\u2019attestato di qualit\u00E0 dei servizi presso il\r\nMinistero dello sviluppo economico6, bench\u00E9 l'ICCU abbia intensificato il piano di formazione e\r\naggiornamento professionale7, ci troviamo ancora di fronte a numerosi singoli bibliotecari che mettono a frutto\r\nle loro diverse competenze, risultato di percorsi di studio eterogenei e non sempre pensati nell\u2019ottica della\r\nprofessione bibliotecaria. A questo proposito, \u00E8 significativo il caso dei \u2018bibliotecari-studiosi\u2019, vale a dire di\r\nquei, non pochi, bibliotecari provenienti dal campo della ricerca, che prestano le loro competenze di paleografi,\r\nmusicologi, storici dell\u2019arte, al lavoro di catalogatore, facendosi portavoce all\u2019interno della biblioteca delle\r\nreali necessit\u00E0 degli utenti pi\u00F9 specialisti. L\u2019utente-studioso pu\u00F2 trovare in questo tipo di bibliotecari delle\r\nalleanze: il catalogatore esperto infatti, seppur \u2018straniero di nascita\u2019, a differenza del protagonista di Camus,\r\nche non si integra nella sua stessa vita, sposa la missione del bibliotecario come facilitatore dell\u2019apprendimento\r\ndel lettore. Da ricercatore, conosce i manoscritti, i libri antichi o le partiture musicali e sa quali siano i pochi\r\ndati certi che non devono cambiare mai in un catalogo (titoli uniformi, voci di autorit\u00E0 normalizzate), ma \u00E8\r\nanche in grado di fornire quei dati sensibili che spesso si trovano fuori dai campi di ricerca, rispondendo cos\u00EC\r\na un bisogno informativo specifico8.\r\n\r\n3 I custodi del libro o della collaborazione con altre istituzioni\r\nI custodi del libro di Geraldine Brooks \u00E8 un romanzo che racconta la storia di un libro - un codice ebraico del\r\nXV secolo - ma \u00E8 anche la storia delle persone che hanno confezionato il libro nelle sue parti e soprattutto delle\r\npersone, i bibliotecari (cui il libro \u00E8 dedicato), che l\u2019hanno tutelato e tramandato nei secoli. E\u2019 una storia che\r\ninvita alla collaborazione tra bibliotecari e tra biblioteche, come stimolo alla crescita e arricchimento, nella\r\nsalvaguardia della cultura.\r\nAprirsi al confronto in un panorama di cooperazione nazionale e internazionale \u00E8 infatti una grande\r\noccasione per chi lavora nel settore dei beni librari, poich\u00E9 reca giovamento non solo ai singoli bibliotecari,\r\nattraverso il confronto coi colleghi, ma anche alle istituzioni di appartenenza, che donano maggiore visibilit\u00E0\r\ne fruibilit\u00E0 al loro patrimonio. Per le collezioni digitali poi il confronto e la cooperazione diventano necessari,\r\npoich\u00E9 il rischio di confondersi nella rete e di rendere i dati poco reperibili diviene esponenziale.\r\nI progetti di cooperazione sono molteplici: possiamo citare Internet Culturale9, Europeana10, WorldCat11, il\r\nCERL12. Far parte di queste reti offre ai bibliotecari il vantaggio di favorire la circolazione delle informazioni,\r\nla visibilit\u00E0 dei dati e l\u2019opportunit\u00E0 del confronto e della collaborazione con gli altri professionisti della rete.\r\nTuttavia per favorire dialogo e cooperazione \u00E8 necessario parlare la stessa lingua, ovvero adottare\r\npreventivamente standard aggiornati che permettano l\u2019interoperabilit\u00E0 e il riuso di metadati affidabili in\r\nun\u2019ottica internazionale.\r\n\r\n4 Il filo del rasoio o del confronto con il mondo Wiki\r\n\u201CCamminare sul filo del rasoio \u00E8 difficile\u201D, dice un passo delle Upanishad: in quel contesto - come nel romanzo\r\ndi Somerset Maugham, che ne trae titolo e ispirazione - ci si riferisce allo stare in equilibrio tra senso pratico\r\ne spiritualit\u00E0. Tra due piani diversi, tra due realt\u00E0. Se il bibliotecario-studioso gi\u00E0 deve esercitare il proprio\r\nequilibrio, cimentandosi, da ricercatore, negli scenari pi\u00F9 \u2018tradizionali\u2019 della creazione e dello scambio di\r\n5\r\nAssociazione italiana biblioteche, Il portfolio delle competenze: un nuovo strumento per il professionista dell\u2019informazione, a cura\r\ndell'Osservatorio Formazione (coordinatore Patrizia L\u00F9peri); contributi di Manuela De Noia, Matilde Fontanin, Patrizia L\u00F9peri,\r\nRoma, Associazione italiana biblioteche, 2017.\r\n6\r\nMinistero dello sviluppo economico, Associazioni professionali che rilasciano l\u2019attestato di qualit\u00E0 dei servizi.\r\n2016,<http:\/\/www.sviluppoeconomico.gov.it\/index.php\/it\/cittadino-e-consumatori\/\r\nprofessioni-non-organizzate\/associazioni-che-rilasciano-attestato-di-qualita>.\r\n7\r\nIl piano di formazione e aggiornamento professionale ha previsto, per il 2019, ben due cicli di corsi di aggiornamento organizzati\r\ndall\u2019ICCU indirizzati al personale bibliotecario specializzato, <https:\/\/www.iccu.sbn.it\/it\/attivita-servizi\/\r\nformazione-e-didattica\/Corsi-di-formazione-attivi\/>.\r\n8\r\nChiara Cauzzi [et al.], Conoscersi per riconoscersi: la partecipazione come specchio del bibliotecario, \u00ABAIB studi\u00BB, 56, 2\r\n(maggio\/agosto 2016), p. 219-239, <http:\/\/aibstudi.aib.it\/article\/view\/11462>.\r\n9\r\nInternet Culturale, <http:\/\/www.internetculturale.it\/>.\r\n10\r\nEuropeana Collection, <https:\/\/www.europeana.eu\/portal\/it>.\r\n11\r\nWhat is WorldCat?, <http:\/\/www.worldcat.org\/whatis\/default.jsp>.\r\n12\r\nConsortium of European research libraries, <https:\/\/www.cerl.org\/>. Il Consorzio promuove l'attivit\u00E0 di ricerca\r\nattraverso gruppi di lavoro internazionali come Heritage of the Printed Book Database (<https:\/\/www.cerl.org\/\r\nresources\/hpb\/main>), CERL Thesaurus, <https:\/\/data.cerl.org\/thesaurus\/_search>), Material evidence\r\nin incunabula (<http:\/\/data.cerl.org\/mei\/_search>).\r\n\r\n143\r\n\r\n\fmetadati bibliografici, la difficolt\u00E0 aumenta quando il bibliotecario si trova a lavorare in ambienti\r\napparentemente estranei.\r\nI progetti GLAM (acronimo per Galleries, Libraries, Archives, Museums, analogo internazionale\r\ndell\u2019italiano MAB) sono nati formalmente nel 2010, proprio con l\u2019intenzione di promuovere la collaborazione\r\ntra istituzioni culturali e Wikipedia, integrando bibliotecari, archivisti e curatori museali tra i propri\r\ncontributori, ma alcuni tra questi professionisti del settore culturale avevano gi\u00E0 contribuito - individualmente,\r\ncome utenti - ai contenuti dell'enciclopedia libera fin dalla sua nascita nel 200113. Da quindi una decina di anni\r\nil mondo GLAM collabora con le comunit\u00E0 wikipediane di tutto il mondo per l\u2019arricchimento dell\u2019enciclopedia\r\nlibera sui temi del patrimonio culturale: collaborazioni che vanno dalla semplice \u201Cdonazione\u201D di dati\r\n(scansioni, fotografie d\u2019archivio, dati bibliografici) a partnership pi\u00F9 strutturate che prevedono il\r\n\u201Cwikipediano in residenza\u201D, figura di mediazione che collabora a stretto contatto con l\u2019istituzione\r\nculturale, formando il personale su come contribuire a Wikipedia e agli altri progetti Wiki14.\r\nAbbiamo detto che l\u2019interazione tra le due comunit\u00E0 pu\u00F2 essere utile alla crescita di entrambe, se la si sa\r\nosservare senza pregiudizi: Wikipedia \u00E8 ormai una delle principali fonti di accesso all\u2019informazione per molti\r\nutenti del web, ma l\u2019enciclopedia ha bisogno di colmare le lacune nel settore dei beni culturali, proponendosi\r\nnel contempo di dare risalto al lavoro di archivisti e bibliotecari, altrimenti poco visibile in rete (con materiale\r\ndigitale visibile solo dal sito dell\u2019istituzione, ma non rintracciabile con ricerche sui motori di ricerca): in pratica\r\nsono i bibliotecari stessi, gli archivisti e gli operatori di musei a creare le voci relative ai propri patrimoni e ai\r\nrelativi ambiti disciplinari, oltre a condividere le proprie immagini digitali con licenze libere e aperte (Creative\r\nCommons CC-BY e CC BY-SA) o nel pubblico dominio (CC0). La collaborazione \u00E8 infatti finalizzata a\r\ncondividere contenuti e risorse in un\u2019ottica di promozione della conoscenza libera e le biblioteche digitali\r\npossono contribuire ulteriormente, fornendo ad esempio fonti bibliografiche verificabili alle voci\r\ndell\u2019enciclopedia.\r\n\r\n5 I doni della vita o dei beni comuni digitali\r\nI doni della vita \u00E8 un romanzo di Ir\u00E8ne N\u00E9mirovsky e i doni cui fa riferimento l\u2019autrice sono i solidi valori e le\r\ncertezze di una famiglia che, in un momento di difficolt\u00E0 materiale, si trasformano in forza. Fuori di metafora,\r\noccorre riconoscere l\u2019opportunit\u00E0 di miglioramento offerta dalla contribuzione, bisogna avere la\r\nconsapevolezza del fatto che aggiungere qualit\u00E0 alle voci create o arricchite, e contemporaneamente\r\nbeneficiare di nuove conoscenze tecniche, \u00E8 fonte di aggiornamento professionale e crescita personale. Una\r\nvolta fugato il pregiudizio che porta ad arroccarsi nella contezza della propria professionalit\u00E0 e scegliere di non\r\ncontribuire, pu\u00F2 verificarsi il rischio, ugualmente insidioso, di contribuire, usando la rete come una cattedra.\r\nEssendo Wikipedia uno dei siti d\u2019informazione pi\u00F9 visitati al mondo ed essendo un progetto no profit aperto\r\na tutti, ha inevitabilmente attirato a s\u00E9 coloro che cercano visibilit\u00E0 sul web, trasformando la sua apertura in\r\nvulnerabilit\u00E0. Per arginare il rischio dell\u2019autopromozione da un lato i wikipediani si sono dotati di una\r\nburocrazia \u201Cdifensiva\u201D15, dall\u2019altro occorre che i professionisti dell\u2019informazione - insegnanti, docenti\r\nuniversitari, bibliotecari, archivisti - si premurino di partecipare in un'ottica neutrale perch\u00E9 le informazioni\r\ndi Wikipedia siano il pi\u00F9 accurate possibile. \u00C8 auspicabile quindi una sempre maggiore contribuzione da parte\r\ndegli addetti ai lavori: un bibliotecario, ancor pi\u00F9 un un bibliotecario studioso, dovrebbe dare una mano a\r\nquesto progetto di condivisione nella sua globalit\u00E0, secondo il concetto di filiera del dato open, ben descritto\r\nda Andrea Zanni in un suo recente contributo16, in cui si dimostra quanto \u201Cla partecipazione a un bene comune\r\ndigitale diventa un mezzo per il bibliotecario di adempiere alla sua missione, andando a fornire le informazioni\r\ndirettamente dove gli utenti della rete sono presenti, attraverso le competenze, le collezioni e i valori propri\r\ndella professione bibliotecaria\u201D. Del resto l\u2019utilizzo di protocolli e formati aperti, di licenze e policy aperte\r\ngarantisce l\u2019interoperabilit\u00E0 necessaria fra macchine e macchine, ma anche fra umani e umani.\r\n\r\n13\r\n\r\nGLAM (cultura). In: Wikipedia: l\u2019enciclopedia libera, <https:\/\/it.wikipedia.org\/wiki\/GLAM_(cultura)>; in\r\nparticolare Progettto:GLAM\/biblioteche, <https:\/\/it.wikipedia.org\/wiki\/Progetto:GLAM\/Biblioteche>.\r\n14\r\nProgetto:GLAM\/Wikipediano in residenza, <https:\/\/it.wikipedia.org\/wiki\/Progetto:GLAM\/\r\nWikipediano_in_residenza>.\r\n15\r\nWikipedia:Contenuti promozionali o celebrativi,\r\nhttps:\/\/it.wikipedia.org\/w\/index.php?\r\ntitle=Wikipedia:Contenuti_promozionali_o_celebrativi&oldid=88978752.\r\n16\r\nA. Zanni, Le biblioteche e la filiera dell\u2019open, \u00ABJLIS.it\u00BB, 9, 3 (September 2018), p. 75-91, <https:\/\/www.jlis.it\/\r\n144\r\narticle\/view\/12486>.\r\n\r\n\f6 Le affinit\u00E0 elettive o del metodo di lavoro\r\nLe affinit\u00E0 elettive di cui parliamo sono quelle tra Fondazione BEIC e Wikipedia. Anche Goethe aveva desunto\r\nil titolo del suo romanzo dall'affinit\u00E0 chimica, cio\u00E8 la tendenza di alcuni elementi chimici a legarsi tra loro. Nel\r\ncaso di BEIC e Wikipedia l\u2019affinit\u00E0 nasce da una comunione di intenti: le due istituzioni hanno infatti\r\nl\u2019obiettivo comune di promuovere e condividere contenuti e risorse.\r\nDal 2008 la Fondazione BEIC17 sta realizzando una biblioteca digitale che raccoglie collezioni tematiche,\r\nselettive e multidisciplinari18. Per le digitalizzazioni si \u00E8 affidata a studiosi e a istituzioni di prestigio che,\r\nattingendo a raccolte italiane e straniere, hanno selezionato gli esemplari in base a specifici criteri, legati alla\r\nrilevanza degli autori e delle opere e al carattere internazionale delle fonti. E\u2019 stata dedicata molta attenzione\r\nalla creazione di un catalogo amichevole che ponesse particolare cura nella presentazione dei dati e nella loro\r\nintegrazione con altri cataloghi, cos\u00EC come nella rapida visualizzazione e nella facile manipolazione delle\r\nimmagini19. In questo modo la Biblioteca Digitale BEIC si propone di rendere liberamente accessibile un vasto\r\ncomplesso di opere umanistiche e scientifiche, in un arco temporale che va dal medioevo all\u2019et\u00E0\r\ncontemporanea20. Lo staff di Biblioteca Digitale BEIC, eterogeneo gi\u00E0 al suo interno, ha una routine di lavoro\r\nche prevede il confronto quotidiano con cataloghi nazionali (SBN OPAC, Edit16) e internazionali (Karlsruhe,\r\nGW, ISTC), con database per il controllo delle voci di autorit\u00E0 (VIAF, CERL Thesaurus) e con strumenti\r\nenciclopedici (Dizionario biografico degli italiani, Wikipedia). Questa attivit\u00E0 di consultazione ha portato alla\r\nconsapevolezza di dover contribuire alle reti che si ritengono efficaci: la Biblioteca Digitale BEIC collabora\r\nquindi da tempo ad alcuni progetti, attraverso il riversamento - possibile grazie all\u2019interoperabilit\u00E0 degli\r\nstandard MARC - di registrazioni della biblioteca digitale in WorldCat, Europeana e ISTC.\r\nL\u2019obiettivo \u00E8 quello di valorizzare le proprie collezioni in un contesto internazionale e di permettere il\r\nmassimo riutilizzo dei metadati, per questo rilasciati con licenza CC0 (pubblico dominio). Queste\r\ncaratteristiche rendono la Biblioteca Digitale BEIC particolarmente adatta alla collaborazione con Wikimedia\r\nItalia. Dal 2014 \u00E8 stato cos\u00EC avviato un progetto GLAM21, che, nell\u2019ottica di una condivisione aperta dei\r\ncontenuti e di diffusione della conoscenza libera, ha il fine di disseminare le risorse della Biblioteca Digitale\r\nall\u2019interno di Wikipedia e dei progetti fratelli. Dall\u2019inizio del GLAM a oggi, le attivit\u00E0 svolte dalla BEIC\r\ninsieme al Wikipediano in residenza - inizialmente Federico Leva, poi Marco Chemello - hanno spaziato\r\ndall\u2019arricchimento delle voci dell\u2019enciclopedia, al caricamento di immagini digitalizzate in Wikimedia\r\nCommons, alla collaborazione con i progetti di Wikisource22. Negli ultimi anni, tuttavia, \u00E8 cresciuta la\r\nconsapevolezza che uno in particolare fra i progetti sarebbe stato di grande interesse dal punto di vista\r\nbibliotecario: Wikidata23. Wikidata \u00E8 un database libero che si basa su principi storici e fondamentali della\r\nteoria della catalogazione, come il controllo di autorit\u00E0, e li affianca alla pi\u00F9 recente ottica Linked Open Data\r\n(LOD). L\u2019affinit\u00E0 di Wikidata con i modelli per la costruzione di registrazioni di autorit\u00E0 nei cataloghi \u00E8\r\nconfermata dal fatto che le entit\u00E0 descritte in Wikidata sono arricchite dagli identificativi persistenti provenienti\r\nda dataset bibliografici autorevoli, come quello della Biblioth\u00E8que nationale de France o del CERL Thesaurus.\r\nNel 2017, dunque, si \u00E8 cominciato a procedere al riversamento in Wikidata dei metadati sottoposti a\r\ncontrollo di autorit\u00E0 della Biblioteca digitale BEIC, a partire dagli autori persona che ricorressero come\r\nintestazioni principali dei record bibliografici (circa 5000 nomi), seguiti poi dagli autori persona con\r\n17\r\nBiblioteca europea di informazione e cultura, Biblioteca digitale, <http:\/\/www.BEIC.it\/it\/\r\narticoli\/biblioteca-digitale>. Per le tappe del progetto: La Biblioteca europea di Milano (BEIC): vicende e\r\ntraguardi di un progetto, a cura di Antonio Padoa-Schioppa, Milano, Skira, 2014.\r\n18\r\nBiblioteca europea di informazione e cultura, Le collezioni, <http:\/\/www.BEIC.it\/it\/pagina\/\r\nle-collezioni>.\r\n19\r\nIl protocollo catalografico BEIC ha come primo punto di riferimento le Regole italiane di catalogazione: REICAT, integrate con\r\nnorme per il trattamento di risorse particolari (ad esempio, per la musica, ci si attiene a: Istituto centrale per il catalogo unico,\r\nTitolo uniforme musicale: norme per la redazione, Roma, ICCU, 2014, allo standard MARC21 e alle indicazioni di RDA.\r\n20\r\nAttualmente la Biblioteca Digitale BEIC conta circa 39.569 oggetti digitali per un totale di oltre 99.936 registrazioni, che\r\nspaziano per tipologia dagli incunaboli alle opere d\u2019arte, dai libri antichi ai periodici, dai manoscritti alle risorse audio-video.\r\n21\r\nProgetto:GLAM\/BEIC, <https:\/\/it.wikipedia.org\/wiki\/Progetto:GLAM\/BEIC>.\r\n22\r\nSono state caricate in Wikimedia Commons le immagini di migliaia di opere presenti nella Biblioteca Digitale BEIC in formato\r\nTIFF a 300 o 400 dpi; le immagini sono state inserite in migliaia di voci esistenti di Wikipedia in italiano e di altre 200 versioni\r\nlinguistiche; sono stati aggiunti riferimenti bibliografici a oltre 1600 voci in italiano, aggiungendo i link agli oggetti digitali\r\nconsultabili nella Biblioteca Digitale BEIC; sono state create oltre 650 nuove voci in italiano e inglese di autori; infine, sono stati\r\ncreati in Wikisource gli indici di numerose opere in italiano, provenienti da Internet Archive o dalla stessa Biblioteca Digitale\r\nBEIC. Si veda: C. Consonni-F. Leva, Progetto GLAM\/BEIC, \u00ABBiblioteche Oggi\u00BB, 33 (marzo 2015), p. 47-50,\r\n<http:\/\/www.bibliotecheoggi.it\/rivista\/article\/view\/24\/265> ; F. Leva-M. Chemello, The effectiveness\r\nof a Wikimedian in permanent residence: the BEIC case study, \u00ABJLIS.it\u00BB, 9, 3 (September 2018), p. 141-147, <https:\/\/\r\nwww.jlis.it\/article\/view\/12481>.\r\n23\r\n<https:\/\/www.wikidata.org\/wiki\/Wikidata:Main_Page>\r\n\r\n145\r\n\r\n\fintestazione secondaria (quasi 15000 nomi)24. La fase preliminare al riversamento dei dati ha comportato la\r\npreparazione dei metadati di partenza: la Biblioteca Digitale BEIC utilizza lo standard MARC21, pertanto tutte\r\nle informazioni relative agli autori di tipo persona sono state rintracciate all\u2019interno dei tag 100 (per le\r\nintestazioni principali) e 700 (per le intestazioni secondarie). Il fatto che il protocollo catalografico BEIC\r\npreveda in ogni caso l\u2019inserimento delle date nel record di autorit\u00E0 (e non soltanto per disambiguare gli\r\nomonimi) ha permesso un\u2019identificazione pi\u00F9 certa nel corso del riversamento in Wikidata e ha assicurato la\r\npresenza di un dato stabile e di tipo enciclopedico in tutte le registrazioni. I metadati associati agli autori\r\npersona sono stati esportati dal catalogo, rielaborati e riversati in Mix\u2019n\u2019match25, un tool wiki che permette di\r\nconfrontare i record del catalogo con gli elementi esistenti in Wikidata e abbinarli nel caso si riferiscano alle\r\nstesse entit\u00E0. Gli elementi abbinati, cio\u00E8 i casi in cui i nomi del catalogo BEIC corrispondevano a un elemento\r\ngi\u00E0 esistente in Wikidata, sono stati arricchiti di una nuova propriet\u00E0 che evidenziasse la corrispondenza con\r\nle singole entit\u00E0 del Catalogo BEIC (propriet\u00E0 \u2018Descritto nella fonte\u2019); per gli elementi unmatched, ovvero i\r\nnomi presenti nel catalogo BEIC che non avevano alcun abbinamento con entit\u00E0 di Wikidata (circa il 30% del\r\ntotale), si \u00E8 resa necessaria la creazione da zero di elementi Wikidata. In entrambi i casi, vista la mole di dati e\r\nla difficolt\u00E0 di procedere manualmente alle modifiche e alle creazioni, si \u00E8 deciso di utilizzare\r\nQuickStatements, un altro tool che permette di intervenire in Wikidata in maniera semiautomatica: partendo\r\nda un elenco di elementi Wikidata, questo strumento \u00E8 in grado di inserire lo stesso tipo di informazione in\r\ntutti con un\u2019unica operazione26. Le nuove entit\u00E0 sono state poi arricchite di nuove informazioni, come il genere\r\no l\u2019occupazione, attingendo le informazioni da fonti diverse. Nel caso dell\u2019occupazione, il dato talvolta era gi\u00E0\r\nstato inserito nel record di autorit\u00E0 (per esempio, la qualifica di santo, papa, imperatore o elementi\r\ndisambiguanti per le forme omonime)27, talaltra, il dato si \u00E8 basato sull\u2019analisi dei relator code associati ai tag\r\n100 e 700 del record bibliografico28.\r\nIl risultato di questo lavoro vede per ora l\u2019entit\u00E0 \u201CBiblioteca digitale BEIC\u201D associata a quasi 15000 elementi\r\nWikidata e fotografa lo stato attuale del progetto che, come si \u00E8 detto, \u00E8 ancora molto ampio. Una volta concluso\r\nil progetto sugli autori di tipo persona, l\u2019obiettivo \u00E8 quello di estendere la prassi descritta anche agli altri\r\nmetadati sottoposti al controllo di autorit\u00E0 (autori ente, editori, luoghi e titoli): si sta gi\u00E0 lavorando sulle forme\r\nnormalizzate degli editori antichi e moderni \u2013 includendo anche quelli presenti nel catalogo dell\u2019Archivio della\r\nProduzione Editoriale Lombarda, sempre allestito dalla BEIC \u2013 e sui titoli. Una volta raggiunto l\u2019obiettivo di\r\nesportare i dati di autorit\u00E0 dal catalogo a Wikidata, la prospettiva sar\u00E0 quella di integrare le informazioni\r\npresenti nelle entit\u00E0 di Wikidata all\u2019interno dei record di autorit\u00E0 del catalogo, a partire dagli identificativi\r\npersistenti e dalle forme varianti del nome. Un\u2019ulteriore prospettiva sar\u00E0 quella di migliorare la struttura dei\r\nrecord di autorit\u00E0 del catalogo BEIC, creando per ciascuno di essi un identificativo persistente, che diventer\u00E0\r\nil riferimento stabile al di fuori del catalogo, in primo luogo negli elementi Wikidata, nel rispetto di un\u2019ottica\r\nLinked Open Data.\r\nL\u2019incontro (e talvolta scontro) tra catalogo BEIC e Wikidata ha dato risultati molto positivi: l\u2019esposizione\r\ndei dati di autorit\u00E0 della Biblioteca digitale BEIC al di fuori del contesto del catalogo ha permesso il loro\r\narricchimento, ha messo alla prova la loro struttura profonda in termini di interoperabilit\u00E0 e possibilit\u00E0 di riuso.\r\nIn generale, la contribuzione ai contenuti di tutti i progetti Wikimedia ha portato notevole visibilit\u00E0 agli oggetti\r\ndigitali, sia nei siti Wikimedia (con 23 milioni di visualizzazioni mese) sia nel sito di provenienza: a novembre\r\n2019 il 75% delle visite al portale BEIC proveniva dai progetti Wikimedia. Quindi il progetto GLAM-wiki,\r\ncome CERL, Europeana, ISTC, continua a essere per la Biblioteca Digitale BEIC un modo per guardare oltre\r\ni propri confini e per amplificare le competenze delle persone che lavorano al suo interno.\r\n24\r\n\r\nL\u2019eterogeneit\u00E0 delle collezioni della Biblioteca Digitale BEIC si riflette sull\u2019insieme di autori che le rappresentano: autori classici\r\ngreci e latini, sconosciuti commentatori medievali, incisori e disegnatori Seicenteschi, fotografi del Novecento, direttori d\u2019orchestra e\r\nmusicisti contemporanei.\r\n25\r\nCreato da Magnus Manske, uno dei membri della comunit\u00E0 wiki internazionale.\r\n26\r\nI dati sono stati impostati secondo una struttura che accoppia la propriet\u00E0 \u201CDescritto nella fonte\u201D (P1343) al valore \u201CBiblioteca\r\ndigitale BEIC\u201D (Q51955019), a sua volta arricchito da due riferimenti: la forma del nome cos\u00EC come ricorre nel Catalogo BEIC e\r\nl\u2019URL che lancia una ricerca corrispondente nel Discovery tool della biblioteca digitale. Nel secondo caso, gli elementi da creare non\r\navrebbero dovuto contenere solo la coppia propriet\u00E0-valore relativa alla presenza nella Biblioteca digitale BEIC, ma anche alcune\r\ninformazioni canoniche usate per descrivere le persone: nome, cognome, date di nascita e morte. Queste informazioni sono state\r\ndesunte con facilit\u00E0 dai metadati bibliografici presenti nel catalogo e sono state successivamente normalizzate in una struttura adatta\r\nall\u2019immissione in Wikidata.\r\n27\r\nEmblematico il caso di \u201CFrancesco Rossi\u201D, nome presente nel Catalogo BEIC che si riferisce a quattro diverse persone, distinte\r\nproprio in base alla loro professione: chirurgo, egittologo, filologo, giurista.\r\n28\r\nIl relator code \u00E8 un codice standard che permette di associare al nome dell\u2019autore il ruolo che tale autore ha rivestito nell\u2019ambito\r\ndella risorsa descritta, ad esempio curatore, editore, illustratore, regista.\r\n146\r\n\r\n\f7 L\u2019opera struggente di un formidabile genio o della crescita del bibliotecario\r\nIn conclusione, ho scomodato uno dei titoli pi\u00F9 evocativi della letteratura contemporanea, perch\u00E9 il\r\nprotagonista cerca di capire le regole delle cose, di smontare i pezzi e di metterli in ordine, per mostrarli in un\r\nmodo diverso e pi\u00F9 semplice. E lo fa in una logica classificatoria, corredata di indici tematici, semplificati in\r\nun elenco di parole-chiave. E questo mi sembra un buon modo per parlare la lingua dei bibliotecari.\r\nQuello che mi stupisce della discussione apparsa in rete, e di cui abbiamo parlato in apertura, \u00E8 la posizione\r\ndi chi, tra i bibliotecari, vede a priori poca qualit\u00E0 in uno strumento di pubblica utilit\u00E0, un\u2019enciclopedia che pu\u00F2\r\nessere scritta da tutti, dimostrando una certa nescienza su cosa sia realmente Wikipedia. La parola enciclopedia\r\nviene dal greco \u1F10\u03B3\u03BA\u03CD\u03BA\u03BB\u03B9\u03BF\u03C2 \u03C0\u03B1\u03B9\u03B4\u03B5\u03AF\u03B1, istruzione circolare, insieme di dottrine che formano un\u2019educazione\r\ncompleta. In questo senso Wikipedia ha lo stesso obiettivo che le biblioteche (soprattutto quelle pubbliche) si\r\nsono sempre poste: il servizio alla comunit\u00E0, la circolarit\u00E0 della conoscenza. Nel caso di Wikipedia, una\r\nconoscenza a portata di tutti e perfezionabile da tutti. Circolante, quindi democratica.\r\nL\u2019enciclopedia libera si basa sulle competenze che ciascun utente pu\u00F2 mettere a disposizione e sul confronto\r\nche ne nasce per arrivare a una versione condivisa. Pu\u00F2 sembrare una sfida, ma questo \u00E8 l\u2019atteggiamento che i\r\nbibliotecari dovrebbero tenere: non si contribuisce alla comunit\u00E0 semplicemente aggiungendo link ai propri\r\nfondi o a pagine dedicate alle istituzioni di appartenenza, occorre comprenderne la filosofia e adeguarsi alle\r\nregole. Il case study di Biblioteca Digitale BEIC dimostra quanto un approccio collaborativo, aperto al\r\nconfronto e all\u2019arricchimento, possa allargare le prospettive di tutta la comunit\u00E0, in armonia - nel senso pi\u00F9\r\nolistico del termine - con la quinta legge di Ranganathan, laddove la biblioteca cresce insieme ai propri\r\ntempi, alla tecnologia, alle esigenze dei propri lettori."
	},
	{
		"id": 24,
		"title": "Making a Digital Edition: The Petrarchive Project",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Isabella Magni"
		],
		"body": "The Rerum vulgarium fragmenta is an icon of the Italian and Western literary tradition. Unlike other\r\ncanonical works, we still possess authorial drafts of several poems and a partial holograph \u2013 MS Vaticano\r\nLatino 3195 \u2013 transcribed over roughly a decade (1366ca.-1374) in part by Giovanni Malpaghini1, a young\r\ncopyist from Ravenna working under Petrarch\u2019s strict supervision (Dotti 1987). After completing the first four\r\nquaternions, part of the fifth, the seventh and part of the last quires, around 1368, for unknown reasons\r\nMalpaghini decided to leave Petrarch\u2019s employment and protection. After his departure, while transcribing\r\nthe remaining poems, Petrarch began a long, difficult and often interrupted process of transcription and\r\nrevision of the entire songbook. From its intended status as a fair copy, Vaticano Latino 3195 soon became a\r\nservice copy in which the poet experimented his visual poetics as the basis for a potential but never realized\r\nfinal fair copy. The basis of any textual research on the Fragmenta is accepting that it is not a perfect work\r\nand that it was unbound and unfinished when Petrarch died in 1374. Copies of the Fragmenta often partial\r\nand unauthorized \u2013 at times even corrupted \u2013 were already circulating during the poet\u2019s lifetime. Petrarch\r\nhimself often lamented in his letters that his youthful vernacular poems were disseminated without his consent\r\namong the \u201Cmultitude\u201D2. Centuries of textual transmission and cultural mediation have progressively altered\r\nIn a 2015 publication (\u201CMalpaghini copista di Petrarca?\u201D in Cultura neolatina LXXV: 2015 205-16) Monica Bert\u00E9 proposed to\r\nseparate the historical figure of young Giovanni Malpaghini from that of Petrarch\u2019s scribe.\r\n2 In a letter to Giovanni Boccaccio Petrarch writes: \u201Cthose brief and scattered vernacular works of my youth are no longer mine, as I\r\nhave said, but have become the multitude\u2019s, I shall see to it that they do not butcher my major ones\u201D (Sen. V 2, transl. Bernardo). And\r\n1\r\n\r\n148\r\n\r\n\fthe way we visualize, read and ultimately interpret Petrarch\u2019s poems, and the way we reconstruct the history\r\nof the work, often more conjectural than factual3. The starting point of our work on the Petrarchive \u2013 and that\r\nof material and digital philology \u2013 is therefore to go back to the material sources and to re-examine original\r\ndocuments in order to dig below the surface of a \u201Cmodernized Petrarch\u201D and to re-construct the forms and\r\ncontexts in which the work was produced.\r\nThe Petrarchive does not reproduce in OCR other editions of the Fragmenta but offers in XML-TEI and\r\nJohn Walsh\u2019s TEIBoilerplate, high-definition images of each charta of manuscript Vaticano Latino 3195, new\r\ndiplomatic transcriptions and edited forms of the entire songbook, and its discoverable palimpsests. Through\r\ncarefully structured text encoding, the site aims at re-constructing Petrarch\u2019s texts maintaining their most\r\noverlooked aspect: their visual and material forms4. The basic authorial principles that characterize Petrarch\u2019s\r\n366 texts and his carefully and authorially constructed visual poetics in MS Vaticano Latino 3195 are:\r\n1. 31-line per charta organized in two columns;\r\n2. thematic and visual integrity of the charta, in which the poems are often not simply juxtaposed but\r\ncarefully selected to form groupings of pomes deeply linked by meaning, thematic unity and contrast;\r\n3. contrasting visual structures to distinguish the five different poetic genres of which the Fragmenta is\r\ncomposed: two-column horizontal reading strategy for sonnets (transcribed over 7 lines-two verses\r\nper line), madrigals, ballata and canzone, as opposed to the two-column vertical reading strategy of\r\nsestina;\r\n4. use of space as organizational device, signaling, for example, the subdivision of the collection in two\r\nparts (cc.49-52v), or a new trajectory of the macro-text with the transcription on c.22v of the canzone\r\nMai non vo\u2019 pi\u00F9 cantar com\u2019io soleva anticipated by blank space (eight transcriptional lines) at the\r\nbottom of c. 22r.\r\nThe Petrarchive\u2019s first task is therefore to re-visualize these basic authorial principles while maintaining a\r\nsimple interface and ease of use. The encoding of charta 1v, which presents four sonnets, serves as an example\r\nof how the digital code translates textual and prosodic features together with visual aspects of the fa\u00E7ade of\r\nthe charta:\r\n<pb n=\u201Dcharta 1 verso\u201D facs=\u201D..\/images\/vat-lat3195-f\/vat-lat3195-f-001v.jpg\u201D \/>\r\n<lg xml :id=\u201Drvf005\u201D type=\u201Dsonnet\u201D n=\u201D5\u201D>\r\n<lg type=\u201Doctave\u201D>\r\n<lg type=\u201Ddblvrs\u201D corresp=\u201D#canvasline\u201D>\r\n<l n=\u201D1\u201D><hi rendition=\u201D#red #fs24pt\u201D>Q<\/hi><hi rendition=\u201D#small-caps\u201D>u<\/\r\nhi>ando io <choice><orig>mouo<\/orig><reg>movo<\/reg><\/choice> i sospiri a chiamar\r\n<choice><orig>uoi<\/orig><reg>voi<\/reg><\/choice><supplied>,<\/supplied><\/l>\r\n<l n=\u201D2\u201D><choice><orig>El<\/orig><reg>E \u2019l<\/reg><\/choice> nome che nel cor mi scrisse\r\n<choice><orig>amore<\/orig><reg>Amore<\/reg><\/choice>&v2c ;<\/l>\r\n<\/lg> [\u2026]5\r\nPetrarch\u2019s visual poetics is maintained in the digital code: every pair of verses is translated in the strip of\r\nencoding as a <lg> (line group) of two verses (type=\u201Cdblvrs\u201D) corresponding to one canvas line\r\n(corresp=\u201C#canvasline\u201D). The result of the transformation of the encoding onto the web page is a new\r\n\r\nin a letter to Pandolfo Malatesta, responding to the request by the signore of Rimini to receive a copy of his letters, Petrarch writes:\r\n\u201CNow they have all circulated among the multitude, and are being read more willingly than what I later wrote seriously for sounder\r\nminds. How could I deny you, as great a man and so kind to me and pressing for them with such eagerness, what the multitude has\r\nand mangles against my wishes?\u201D (Seniles XIII, 11. Transl. Bernardo).\r\n3 For a critique of the still widely accepted theory of the \u201Cforms\u201D of the canzoniere see, among others, Dario Del Puppo and H. Wayne\r\nStorey\u2019s \u201CWilkins nella formazione del Rvf di Petrarca.\u201D (2003); Teodolonda Barolini\u2019s \u201CPetrarch at the Crossroads of Hermeneutics\r\nand Philology. Editorial Lapses, Narrative Impositions, and Wilkins\u2019 Doctrine of the Nine Forms of the Rerum Vulgarium Fragmenta.\u201D\r\n(2007); and Carlo Punsoni\u2019s \u201CIl metodo di lavoro di Wilkins e la tradizione manoscritta dei Rerum vulgarium fragmenta.\u201D (2009).\r\n4 See among others H. Wayne Storey\u2019s Transcription and Visual Poetics in the Early Italian Lyric (1993) and Furio Brugnolo\u2019s Libro\r\nd\u2019autore e forma-canzoniere: Implicazioni grafico-visive nell\u2019originale dei Rerum vulgarium fragmenta (1991).\r\n5 The tag <pb> indicates a page break including the facsimile image (facs=\u201D..\/images\/vat-lat3195-f\/vat-lat3195-f-001v.jpg\u201D) of charta\r\n1v (n=\u201Dcharta 1 verso\u201D) and is followed by the markup of the first line group (<lg>): sonnet Rvf 5 (type=\u201Dsonnet\u201D n=\u201D5\u201D). Every tag\r\nof the alphanumeric strip of encoding refers to one specific textual, prosodic or visual component of the manuscript. This fourteenverses line group is then subdivided into two subsequent <lg>: octave (lg type=\u201Coctave\u201D), the first four verses organized over four\r\ncanvas lines (lg type=\u201Cdblvrs\u201D correps=\u201C#canvasline\u201D); and sestet (lg type=\u201Csestet\u201D), the remaining six verses transcribed over two\r\ncanvas lines (lg type=\u201Cdblvrs\u201D correps=\u201C#canvasline\u201D). For more on the Petrarchive encoding see my essay I codici paralleli dei\r\nFragmenta (2015).\r\n149\r\n\r\n\frepresentation of Petrarch\u2019s visual poetics and editorial principles for which he worked restlessly for over a\r\ndecade:\r\n\r\nFigure 1. The Petrarchive, c.1v: diplomatic transcription and manuscript image, displayed side by side.\r\nURL: http:\/\/dcl.slis.indiana.edu\/petrarchive\/content\/c001v.xml#c001v?facs=active\r\nOther than being a research tool6, text encoding also represents a close reading of the texts: it \u2018forces\u2019\r\nencoders to \u2018break down\u2019 each poem and to \u2018label\u2019 its single components using specific tags. It can therefore\r\nalso serve as an alternative and highly stimulating teaching tool: while being asked to encode Petrarch\u2019s\r\ntexts, students necessarily need to re-think the very basic words, linguistic and prosodic structures of each\r\npoem to be able to digitally translate them into tags.\r\nAdvanced uses of text encoding also offer new ways of representing and analyzing erasures,\r\nrenumbering, palimpsests, while maintaining a clean and simple digital interface. The most notable\r\nexample is the palimpsest Donna mi vene spesso ne la mente on c.26r, erased and overwritten by Or vedi\r\namor che giovenetta donna (Rvf 121) by the poet himself:\r\n\r\nFigure 2 and 3. The Petrarchive, c.26r: diplomatic transcription of Rvf.121 Or uedi amor (left);\r\ndiplomatic transcription of palimpsest Donna mi uene. URL: http:\/\/petrarchive.org\/content\/\r\nc026r.xml\r\n\r\nFor more on digital editing and TEI encoding see, among others, the MLA White paper Considering the scholarly edition in the\r\ndigital age: a white paper of the modern language association\u2019s committee on scholarly editions (2015 and 2016); Elena\r\nPierazzo. Digital scholarly editing: Theories, models and methods (2015); Elena Pierazzo and Driscoll (eds.). Digital Scholarly\r\nEditing: Theories and. Cambridge (2016); Kenneth Price, and Ray Siemens. Literary Studies in the Digital Age: An Evolving\r\nAnthology. Modern Language Association, 2013; and Siemens, Ray and Susan Schreibman, A Companion to Digital Literary\r\nStudies (2008).\r\n6\r\n\r\n150\r\n\r\n\fThrough a combination of common Web design techniques and text encoding, by clicking on the manicula7\r\nin the right margin, the user can easily move from one version of Rvf 121 to the other as diplomatic and edited\r\nversions of both poems are available. To encode the simultaneous presence of erased and overwritten poems,\r\nwe employ the following TEI elements: <del> (deletion): to contain the erased \u201CDonna mi vene\u201D; <add>\r\n(addition): to contain the added \u201COr vedi amor\u201D; <subst> (substitution): to wrap the related <del> and <add>\r\nand assert that the <add> is substituted for the <del>; and <ab type=\u201DblockSubst\u201D> (anonymous block)8.\r\nPetrarch\u2019s visual poetics is so authorially designed and so deeply part of the collection that a reader can\r\nrecognize the genre and sometimes function of poems even before reading them. The SGV images created to\r\ndigitally reconstruct the fa\u00E7ade of the manuscript page are simple graphic representations of the Fragmenta\u2019s\r\nmaterial structures. On charta 1 verso and charta 3 verso, for example, users can easily identify the most\r\nfrequent four-sonnets \u2018canvas\u2019 (c.1v) and distinguish the shift in directionality between the sonnet, horizontal,\r\nand the sestina, vertical (c.3v):\r\n\r\nFigure 4 and 5. A visualization of c.1v and c.3v created for the Petrarchive project.\r\nThe description of Petrarch\u2019s visual poetics embedded in the text encoding is therefore also represented in the\r\nPetrarchive visual indexes9: the graphic image files are in fact XML files in the Scalable Vector Graphics\r\nformat (SGV) which contain the information necessary for the Web browser to reproduce the image. The\r\ngraphic information in the SVG files may also be derived from the codes embedded in the TEI\/XML file,\r\nproving the visual and representational capabilities of the encoded document. From a brief look at the visual\r\nindex of the entire songbook, also developed for the Petrarchive, the nine pairings sonnet-sestina10 present in\r\nthe collection are immediately recognizable: once again, even before accessing the textual contents of the\r\npoems, the Medieval reader \u2013 and now the digital user \u2013 can collect a series of information regarding the\r\npoetic genres, their material and visual treatments:\r\n\r\n7 The manicula is not present in Vaticano Latino 3195: it is an interface element introduced by the Petrarchive, mimicking those\r\nPetrarch and other medieval and early modern readers used to draw attention to specific passages in manuscripts.\r\n8 For more information about the encoding developed to digitally reconstruct the palimpsest see Isabella Magni and John A. Walsh\u2019s\r\nDigital Representations and the Pivotal Instability of Donna mi vene spesso ne la mente in the Study of the Fragmenta (2016).\r\n9 In the examples in Figure and 5: arrows signaling the directionality of the text distribution (on c.3v the user can distinguish the\r\nshift in directionality between the sonnet, horizontal, and the sestina, vertical); paragraph markers in the sestina as internal visual\r\nindexicality indicating the vertical disposition of the text over the two columns; initials on the right of the text indentation signaling\r\nto the medieval reader, and now to the contemporary user, the beginning of new poems, marking the passage from fair- to work- and\r\nservice copy (red and blue the rubricated fair-copy initials, in blank ink the remaining ones) and functioning as a textual index; blank\r\nspace serving as additional punctuation device (visible in light blue color in the graphic SGV images).\r\n10All of the Fragmenta\u2019s sestine \u2014 except for the double sestina of part II (Rvf 332) \u2014 are always presented in contrast to a sonnet\r\non the same charta (see Petrarchive Glossary \u201CSestina\u201D. URL: http:\/\/dcl.slis.indiana.edu\/petrarchive\/\r\ncontent\/glossary.xml#sestina).\r\n\r\n151\r\n\r\n\fFigure 6. The Petrarchive: visual index of MS Vaticano Latino 3195. URL:\r\nhttp:\/\/dcl.slis.indiana.edu\/petrarchive\/images\/\r\nPetrarchive__Visual_Index_to_Vat__lat__3195.jpg\r\n\r\nThe representational values established by the Petrarchive visual indices also offer a unique insight into the\r\npreparation of the manuscript, still in the form of loose gathering at the time of Petrarch\u2019s death: from the\r\noriginal project revealed by the rubricated chartae transcribed by Malpaghini (in brown) and set aside in\r\n136811, to Petrarch\u2019s addenda in his own hand12, and to the last service-copy transcriptions for the poet only13\r\n(both in dark blue). A newly developed visual index arranged by fascicles, also allows users to navigate into\r\nPetrarch\u2019s material construction of his fascicles, including the two final binions (cc. 63-66 and 67-70) that the\r\npoet inserted last into an already existing binion (cc.61-62, 71-72):\r\n\r\nFigure 7. The Petrarchive: visual index by fascicles. URL:\r\nhttp:\/\/dcl.slis.indiana.edu\/petrarchive\/\r\nvisindex_fascicles.php\r\n\r\nFive quaternions (cc.1r-40v) in Part I and two fascicles (cc.53r-60v) (cc.61r-62v and 71r-72v) in Part II.\r\nAnother quaternion (cc.41r-48) in Part I and four more chartae (c.59r-62v) in Part II.\r\n13 This last section includes four chartae at the end of Part I (cc.49r-52v) and the last binion added towards the end (cc.69r-70v) with\r\nthe transcription of canzone Quel\u2019 antiquo mio dolce empio signore.\r\n11\r\n12\r\n\r\n152\r\n\r\n\fDigital tools allow us to start from what we possess, the material evidence, and to dig below the surface\r\nto re-discover the original contexts in which the text was produced. Through carefully studied TEI encoding\r\nand the virtual representation on the web page of the different aspects of Petrarch\u2019s Fragmenta - its textual\r\nand graphic, temporal and spatial components \u2013 the Petrarchive aims at re-building the structural and visual\r\nprinciples implemented by the poet himself at the level of single charta, fascicles and macro-structures and\r\ntherefore to re-propose Petrarch\u2019s editorial choices, diminishing the distances between the experience of\r\ncontemporary users and that of manuscript readers in the medieval context and providing innovative ways of\r\nteaching and conducting philological and literary research."
	},
	{
		"id": 25,
		"title": "Extending the DSE: LOD support and TEI/IIIF integration in EVT",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Paolo Monella",
			"Roberto Rosselli Del Turco"
		],
		"body": "1 Introduction\r\nIn the currently available DSEs there is a considerable lack of homogeneity since, besides a small number of\r\n(necessarily) common features, some are still rather traditional in the way of modelling the data, of\r\nconceiving the visualization of the edited text and in the kind of tools which they make available to the\r\nscholar, while others explore the enrichment of the edition\u2019s contents and the design of research tools in a\r\nhighly innovative way. As W. Gibson (1990) said \u201CThe future is already here \u2013 it\u2019s just not very evenly\r\ndistributed\u201D.\r\nAs a matter of fact, the first, pioneering DSEs were quite conservative in their approach to User Interface\r\n(UI) layout. This is understandable, since the first electronic editions were considered an equivalent of\r\ntraditional editions in a different medium: the \u201Cprinted page paradigm\u201D (Sahle, 2016) inspired a mimetic\r\ndesign which would take into account very few advantages of the new publication framework besides its\r\nubiquity and the apparently endless space it grants. The \u201Cremediation\u201D (Bolter & Grusin, 2000) of the\r\nscholarly edition in the new media had not yet taken place.\r\nWhen the methodology advances possible thanks to a Web-based digital edition started to be evident, a\r\nnew research field was born: digital philology can be traced back to the desire to explore the potential of a\r\ntruly Digital Scholarly Edition. This impulse has led to a lively but somewhat chaotic research activity, with\r\nan apparent paradox:\r\n\u25CF\r\n\r\nonly projects which can rely on generous resources may afford to explore new approaches to a DSE\r\ndesign, but usually they are focused on the specific task at hand, while the decision makers aren\u2019t\r\ninterested in broader reflections on the general methodology;\r\n\r\n1\r\n\r\nFor the purposes of the Italian academy, R. Rosselli Del Turco is responsible for sections 1-4 and P. Monella for sections 5 and\r\n6. The initial abstract was jointly written by the two authors, who also planned and revised the article together.\r\n154\r\n\r\n\f\u25CF\r\n\r\ninterested scholars, on the other hand, may be hampered not only by the lack of resources to\r\nexperiment, but also by the fact that the DH-related IT world is moving very fast, so fast that\r\nsometimes new technologies are introduced, enhanced, exploited and set aside within a few years.\r\n\r\nThis article aims at presenting some of the latest methods which can be applied to a DSE with the purpose of\r\nmaking it an even better (more flexible, modular, distributed, interconnected) research tool, and will also\r\nconsider the software design and implementation issues that they imply.\r\n\r\n2 No DSE is an island\r\nOf the many limits artificially imposed to the DSE by the printed page paradigm, the first to fall was that of\r\nthe book as a monolithic product, isolated from other books and unalterable, if not with rather high costs,\r\nafter its publication. A DSE, in fact, can be changed both occasionally, f.i. to correct errors, and\r\nsystematically, to add new texts, commentaries, bibliographic items, etc., so that a publication date by no\r\nmeans implies the end of the editing process, rather just the beginning of a new phase. Furthermore, a DSE is\r\na dynamic, not a static object, a research tool which assists the scholar in data interpretation and analysis;\r\nand it is not a \u201Cclosed box\u201D, but it can engage in dialogue and interaction with other Internet-based resources\r\nthanks to the global linking framework upon which the Web itself is built (Bodard & Garc\u00E9s, 2009; van\r\nZundert & Boot, 2011).\r\nAs a consequence, the greatest advantage of Web-based publication is not only that it makes scholarly\r\ncontent dissemination much easier and cheaper, but that the DSE can access other resources (and be\r\naccessed), and it can rely on external assets and services for specific functionalities. Examples include:\r\n\u25CF\r\n\u25CF\r\n\u25CF\r\n\r\ntaking advantage of semantic web technologies and Linked Open Data to enrich the edition content;\r\ntext\/image linking, pointing to digital collections of images of manuscripts maintained by external\r\nrepositories (IIIF framework);\r\nmodelling intertextual relationships through canonical text services (CTS and DTS protocols).\r\n\r\nWhile this deep interconnecting and sharing of resources may look like a \u201Cquantitative\u201D only advantage\r\nwhen compared to traditional scholarly editions (i.e., you can modify your edition when you want and\r\nintegrate all the available content that you may see fit), there are important methodological consequences:\r\n\u25CF\r\n\u25CF\r\n\u25CF\r\n\u25CF\r\n\u25CF\r\n\u25CF\r\n\u25CF\r\n\r\nthe definitive rejection of the concept of an edition as an isolated and immutable entity;\r\nthe acknowledgement of the fact that the use of external materials to integrate a DSE is highly\r\ndesirable as it results in an enrichment of the DSE itself;\r\nan impulse to the collaboration between different edition projects, possibly adopting principles\r\ntypical of the social edition (Siemens et al. 2012);\r\nas a consequence, an incentive to adopt a modular and distributed approach in the design of digital\r\neditions, making them more flexible;\r\nthe possibility of virtually re-assemble dismembered manuscripts scattered across several preserving\r\ninstitutions (see f.i. the Fragmentarium project);\r\na simplification of the problem of copyright management for digital reproductions of MS images by\r\nlibraries, since images are functionally integrated in the DSE, but remain resident in their repository;\r\nthe DSE itself may become a resource for other editions if the data on which it is based is published\r\nin such a way as to make it available to third parties.\r\n\r\nThis approach, however, poses a number of problems:\r\n\u25CF\r\n\u25CF\r\n\u25CF\r\n\r\ngreater technical complexity: so far only the editions defined as \u201Chaute couture\u201D by E. Pierazzo\r\n(2019) can afford to adopt this approach because its digital implementation is certainly more\r\ncomplex than a simple reproduction of static images and texts in a Web-based edition;\r\nqualitative homogeneity of the different components of a DSE: if part of the content is entrusted to\r\nexternal resources, it is of critical importance that these resources meet the same academic standards\r\nas the original materials;\r\nlong term sustainability: the interdependence between the \u201Cinternal\u201D and \u201Cexternal\u201D components of a\r\n155\r\n\r\n\fDSE makes it possible to modify the existing connections and to add new materials as soon as they\r\nbecome available, but it also implies a strict and continued control on the actual availability and\r\ncompatibility of these materials in the long run.\r\nWhat is needed on the methodological level:\r\n\u25CF\r\n\u25CF\r\n\u25CF\r\n\r\nopen protocols, for data-sharing infrastructures, and open licenses, for resources to be shared;\r\nan open and ongoing scholarly discussion on the systematization of fundamental concepts to create a\r\nshared conceptual framework: we need shared terminologies and open ontologies as a necessary\r\nmethodological condition for the creation of such shared resources;\r\nan integration of those methodological experiments in common editorial practice.\r\n\r\n3 EVT 2: the linked DSE\r\nEVT 2 is the second version, currently based on the AngularJS framework, of EVT (Edition Visualization\r\nTechnology), a software tool for publishing editions based on the TEI\/XML format, which has been\r\ndeveloped in such a way as to go beyond the printed page paradigm \u2013 especially with regard to the User\r\nInterface design (see Di Pietro & Rosselli Del Turco 2018). The way in which it manages data, however,\r\nmakes it mostly suitable for self-contained editions, created on the basis of local resources. Furthermore, at\r\nthe present moment EVT is based on the client-only architecture, which presents many advantages (it is easy\r\nto install, little to no maintenance is required, it has indefinite durability), but also quite a few limits\r\nconcerning important functionality (such as server functionality, f.i. for textual searches or to serve images).\r\nFor these reasons, it is an important goal of the development team to add support for protocols such as\r\nLOD for semantic Web resources, IIIF for images and CTS\/DTS for intertextual relationships, so that\r\nscholars can count on a pr\u00EAt-\u00E0-porter tool for their work. In the long term this goal may be achieved by\r\nadding RESTFul services, to add server functionality without encumbering the software too much, but it is\r\nalready possible for projects based on EVT 2 to include LOD and IIIF resources. This will also have the\r\nbeneficial effect of allowing more widespread knowledge of these protocols.\r\n\r\n4 EVT and LOD\r\nAlthough LOD resources are extremely diversified in terms of type of content, semantic-based operation,\r\netc., it is undeniable that this is a very promising area which has seen constant growth in recent years.\r\nEVT is already able to access resources on the Web thanks to URIs specified in the TEI markup, in fact\r\nsome of the existing EVT-based editions (e.g. the Codice Pelavicino Digitale) use resources such as external\r\nrepositories to provide additional information about the named entities identified in the text. The\r\nmanagement of named entities can be significantly improved through the use of LOD resources such as\r\nFOAF and DBpedia.\r\nIn the Codice Pelavicino Digitale, person names are already linked to the Dizionario Biografico degli\r\nItaliani when possible, for instance:\r\n<!--Code snippet 1-->\r\n<note>Guido da Velate, arcivescovo di Milano dal 1045 al 1068, morto nel 1071 (si\r\nveda <ref target=\"http:\/\/www.treccani.it\/enciclopedia\/guido-da-velate_\r\n%28Dizionario-Biografico%29\" type=\"biblio\">Guido da Velate nel Dizionario\r\nBiografico degli Italiani<\/ref>).<\/note>\r\n\r\nThis information can be supplemented or replaced by linking to a DBpedia entry:\r\n<!--Code snippet 2-->\r\n<ref target=\"http:\/\/dbpedia.org\/page\/Guido_da_Velate\">Guido da Velate<\/ref>\r\n\r\nso that it is possible to take advantage of the wealth of connections and of the sophisticated ontologies that\r\nLOD resources allow. Furthermore, as hinted above the DSE itself can make (part of) its material available\r\nas LOD, so that other editions can build upon it. The concept of \u201Cdistributed edition\u201D, therefore, is coming\r\ncloser to reality, in fact this is the goal of a new research project aiming at disseminating a DSE on\r\nsustainable and public resources such as Zenodo and GitHub (see O\u2019Donnell et al., 2018; note that EVT\r\nalready runs on GitHub).\r\n\r\n156\r\n\r\n\f5 EVT and IIIF\r\n5.1 The rise of IIIF\r\nOne notable example of an open protocol for online resource integration is IIIF \u2013 International Image\r\nInteroperability Framework. IIIF is rapidly emerging as a technology to exchange and integrate image-based\r\nresources in Web-based systems. One interesting use-case is a DSE in which portions of a TEI-encoded text\r\nbased on a primary source such as a manuscript or a printed book are linked to the images of that source,\r\nstored and described via the IIIF framework. For instance, the transcription of a page of a manuscript can be\r\nlinked to its facsimile, and the transcription of a line can be linked to the region of that facsimile\r\ncorresponding to the line.\r\n5.2 IIIF Image and Presentation APIs\r\nThe IIIF protocol defines different APIs, two of which will be briefly discussed here:\r\n1. The IIIF Image API \u201Cspecifies a web service that returns an image in response to a standard HTTP or\r\nHTTPS request\u201D. Simply put, this API returns one image or a portion of it. The description on the\r\nimage are stored in a JSON file named info.json.\r\n2. The IIIF Presentation API instead \u201Cdescribes how the structure and layout of a complex image-based\r\nobject can be made available in a standard manner\u201D. Such a complex object can be a collection of\r\ndigital images of a manuscript, accompanied by the relevant metadata, stored in a JSON file named\r\nmanifest.json.\r\n5.3 TEI and IIIF: a marriage made in heaven?\r\nThe TEI approach has always been text-centric, and only more recently the TEI editors have included a\r\ndocument-based approach in which the digital images of a textual source have equal dignity as its textual\r\nrepresentation, via the <facsimile> \/ <surface> \/ <zone> encoding approach. On the other hand, IIIF is\r\novertly and intentionally image-based. The TEI\/IIIF integration thus looks very promising and productive for\r\nDSEs aiming to combine textual representation and digital images. However, at this point this is very much\r\nan open field of experimentation.\r\n5.4 Two directions for a TEI\/IIIF integration\r\nTwo approaches are theoretically possible for this integration:\r\n1. Linking from IIIF to TEI\r\n\u25CF How: according to the IIIF Presentation API, the node of a IIIF manifest identifying a specific\r\n(portion of an) image can point to an annotationList (a separate JSON file) including an annotation\r\npointing to an external TEI XML file with the relevant textual representation (transcription).\r\n\u25CF Why this might not be a good idea: the institution curating the IIIF collection (for textual sources,\r\nmost probably a library or an archive) should create, curate and update the annotations linking to the\r\nTEI XML files. If those files belong to external DSEs incorporating the IIIF images, the DSE URIs\r\nmight change and require constant update from the library\u2019s side \u2013 which is clearly not sustainable.\r\nOn the other side, the library could create those TEI XML files itself to store and expose\r\ntranscriptions of its own manuscripts, but in the current division of labour, libraries focus on digital\r\nimaging and metadata rather than on full transcriptions and textual criticism.\r\n2. Linking from TEI to IIIF\r\n\u25CF How: within a TEI XML file, f.i. in a <pb\/> (page beginning) or <lb\/> (line beginning) element, a\r\n@facs attribute points to a IIIF URI, either directly or indirectly.\r\n\u25CB 2A - Directly: @facs takes the relevant IIIF URI as value (identifying, f.i., a whole\r\nmanuscript page or a rectangle of that page including a line);\r\n\u25CB 2B - Indirectly: @facs points to a <surface> or <zone> element within the <facsimile>\r\nsection of the TEI file, and the <surface> \/ <zone> element points to the relevant IIIF URI.\r\n157\r\n\r\n\f\u25CF\r\n\r\nThe indirect strategy adds a layer of complexity, but also increases flexibility.\r\nWhy this might be a good idea: this approach keeps resources separated (TEI XML files for textcentered digital philology and a IIIF infrastructure for digital image collections), with modularity\r\nand interoperability in mind. Many DSEs can link to the same IIIF image collections. Shortly said,\r\nphilologists work on text with TEI, librarians work on document digitization with IIIF.\r\n\r\n5.5 IIIF implementation in EVT\r\nEVT 2 now features IIIF integration thanks to OpenSeadragon, its embedded image viewer. It implements the\r\nsecond approach described in the previous paragraph (\u201CLinking from TEI to IIIF\u201D) and uses the IIIF\r\nImage API. Digital philologists can thus integrate external images of a textual source, hosted by a third-party\r\nIIIF image server, in the DSE, with an arbitrary level of alignment granularity. IIIF-compliant servers\r\ninclude e-codices, the Veneranda Biblioteca Ambrosiana in Milan (Cusimano, 2019) or the Biblioteca\r\nApostolica Vaticana in Rome.\r\nMore precisely, EVT currently implements encoding strategy 2A described in paragraph 5.4 above:\r\n1. The TEI XML source code has an element pointing to an image exposed by a IIIF server (typically a\r\nfacsimile of a page) or to a portion of that image (typically a rectangle including a line or an other\r\ntextual division). In the following code sample, we are pointing to a whole image (representing a\r\nmanuscript page) from the IIIF server e-codices - Virtual Manuscript Library of Switzerland. The\r\nvalue of @facs is a URI following the IIIF Image API:\r\n<!--Code snippet 3-->\r\n<pb facs=\"https:\/\/www.e-codices.unifr.ch\/loris\/csg\/csg-0730\/csg-0730_002.jp2\/\r\nfull\/full\/0\/default\/jpg\"\/>\r\n\r\n2. The image viewer integrated in EVT, OpenSeadragon, dereferences the URI from the @facs\r\nattribute, fetches the image from the external IIIF server and shows the whole image of the\r\nmanuscript page alongside its TEI-based transcription.\r\nPlease note that the version of OpenSeadragon currently embedded in EVT (2.4.1) also allows to align a\r\n<pb\/> element with a specific portion of an image, defined as a rectangle as per the IIIF Image API. Thus\r\ncode snippet 3 above can be edited to:\r\n<!--Code snippet 4-->\r\n<pb facs=\"https:\/\/www.e-codices.unifr.ch\/loris\/csg\/csg-0730\/csg-0730_002.jp2\/\r\n1800,600,3000,5000\/full\/0\/default\/jpg\"\/>\r\n\r\nto crop out the manuscript page margins. Coordinates \u201C1800,600\u201D (in pixels) define the top left corner of the\r\nrectangle, \u201C3000\u201D defines the rectangle\u2019s base, \u201C5000\u201D its height. The encoding strategies described so far\r\nfulfill the common need of editors to pair <pb\/> elements with a manuscript or book page (as well with the\r\nsurface of an inscription, a tablet or any other support) and to display it aside the transcription.\r\nThe following code sample, instead, exemplifies the TEI XML encoding strategy currently supported by\r\nEVT to link elements such as <lb\/>, <p> or <div> to smaller portions of the surface image:\r\n<!--Code snippet 5-->\r\n<surface>\r\n<zone lrx=\"1052\" lry=\"211\" rend=\"visible\" rendition=\"Line\" ulx=\"261\" uly=\"156\"\r\nxml:id=\"zone-line-2-1\"\/>\r\n<\/surface>\r\n[...]\r\n<pb facs=\"https:\/\/www.e-codices.unifr.ch\/loris\/csg\/csg-0730\/csg-0730_002.jp2\/\r\nfull\/full\/0\/default\/jpg\"\/>\r\n<lb facs=\"#zone-line-2-1\"\/>\r\n\r\nIn code snippet 5 (which incorporates snippet 3 above), the <pb\/> element references the whole manuscript\r\npage via a IIIF URI; <lb\/> points to a <zone> element in the <facsimile> section that defines a rectangle\r\nwithin the IIIF image through the internal TEI XML encoding strategy: @lrx and @lry define the\r\ncoordinates of the lower right corner of the rectangle, @ulx and @uly those of the and upper left corner (see\r\nattribute class att.coordinated in the P5 TEI Guidelines). Please note that this is different than strategy 2B\r\nfrom paragraph 5.4 because it still is the <pb\/> element, not <zone>, that is retrieving the IIIF image.\r\n\r\n158\r\n\r\n\f5.6 Future development\r\nSupport for encoding strategy 2B (<pb\/> or <lb\/>\u2019s attribute @facs points to <surface> or <zone>, and the\r\nlatter points to an image in an IIIF server) is not yet available in EVT, but the development team aims at\r\nincluding it in future releases. These are examples of TEI XML code that will be managed by EVT:\r\n<!--Code snippet 6-->\r\n<facsimile>\r\n<surface xml:id=\"image-p2\">\r\n<graphic url=\"https:\/\/www.e-codices.unifr.ch\/loris\/csg\/csg-0730\/csg0730_002.jp2\/\r\nfull\/full\/0\/default\/jpg\"\/>\r\n<!--'zone' elements may be included here-->\r\n<\/surface>\r\n<\/facsimile>\r\n[...]\r\n<pb facs=\"#image-p2\"\/>\r\n\r\nOr, with a more compact encoding:\r\n<!--Code snippet 7-->\r\n<facsimile>\r\n<surface xml:id=\"image-p2\" facs=\"https:\/\/www.e-codices.unifr.ch\/loris\/csg\/\r\ncsg-0730\/csg-0730_002.jp2\/full\/full\/0\/default\/jpg\">\r\n<!--'zone' elements may be included here-->\r\n<\/surface>\r\n<\/facsimile>\r\n[...]\r\n<pb facs=\"#image-p2\"\/>\r\n\r\nAnother feature that may be implemented in the future, should any project collaborating with the EVT\r\nteam express this need, is the definition of a rectangle within an image (e.g. for a manuscript line encoded\r\nwith <lb\/>) not through the TEI XML strategy (attributes @lrx, @lry, @ulx and @uly in <zone>), but\r\ndirectly through the IIIF image API, such as in the following code:\r\n<!--Code snippet 8-->\r\n<lb facs=\"https:\/\/www.e-codices.unifr.ch\/loris\/csg\/csg-0730\/csg-0730_002.jp2\/\r\n1900,930,2580,140\/full\/0\/default\/jpg\">\r\n\r\nFinally, an experimental version of EVT based on the Angular 8 framework, not yet available for\r\ndownload, already supports the IIIF Presentation API and loads the full manifest.json document\r\ndescription including metadata on all manuscript images, while the image API currently supported by EVT\r\nprovides access to one manuscript page image at a time. To load a whole manifest.json file, it will be\r\nenough to specify its URL in the EVT config.json configuration file, e.g.\r\n<!--Code snippet 9-->\r\n{ \"title\": \"My Digital Edition\",\r\n\"manifestURL\": \"https:\/\/www.e-codices.unifr.ch\/metadata\/iiif\/csg0730\/manifest.json\" }\r\n\r\nBesides allowing a quick publication of all pages of a manuscript, this is a first step towards exploiting\r\nthe full potential and flexibility of the IIIF framework.\r\n\r\n6. Conclusions\r\nThe EVT development team is committed to supporting the current trend towards the distributed DSE,\r\nintegrating resources such as entity or relationship definitions (LOD) and images (IIIF), as well as text\r\nfragments (CTS\/DTS) in future versions. Internal and external objects alike can be better modelled by the\r\nnew modular and object-oriented implementation adopted in EVT 2. Further development aims at supporting\r\nalternative encoding strategies for linking to external resources while keeping the whole edition (XML, other\r\ninternal and external data, software) sustainable, durable and compliant with the FAIR principles, according\r\nto which \u201Call research objects should be Findable, Accessible, Interoperable and Reusable (FAIR) both for\r\nmachines and for people\u201D (Wilkinson et al., 2016)."
	},
	{
		"id": 26,
		"title": "Mapping as a contemporary instrument for orientation in conferences",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Chloe Ye-Eun Moon",
			"Dario Rodighiero"
		],
		"body": "Introduction\r\n\r\nMaps are formidable instruments for abstracting territory and travels. Centuries of cartographic mapping\r\nmarked the evolution of world history, and today\u2019s technological innovation has opened a new paradigm\r\nof mapmaking (Dodge et al., 2011; L\u00E9vy, 2016). Maps are no longer static; instead, they can change dynamically given external inputs. For instance, users can zoom into the map to obtain detailed information,\r\nand they can choose to filter selected information. Due to the big data technology, maps are now able to\r\nrepresent larger and larger datasets (Kitchin, 2014). Moreover, maps now have a new level of abstraction\r\nnot only for the territory but also for individuals; unlike before, visualizing social relationships became a\r\ncommon practice since Jacob Moreno\u2019s work in social relationships (1934).\r\nThis article presents a case study of human mapping, specifically of scholars and their scientific productions, focusing on the Digital Humanities Conference 2019 that took place in Utrecht, the Netherlands\r\nin July 2019. Such a case study develops the previous work of Rodighiero (Rodighiero, 2015, 2018;\r\nRodighiero et al., 2018), demonstrating how a map can be a powerful instrument of reduction not only\r\nfor territory but also for individuals. The cartography of DH20191 is intended to be a generic tool for\r\nmapping scientific communities, in the form of an open-source project2. The article will present the result\r\nof such cartography by discussing the following four sections: 1) Documentality as a way human activity\r\nis regulated through textual inscriptions 2) Lexical Analysis as the way documents can be automatically\r\nanalyzed under human supervision 3) Graphic Design as a visual translation that makes a conference\r\n1The cartography of DH2019 is accessible at https:\/\/rodighiero.github.io\/DH2019\/\r\n2The open-source project is available at https:\/\/github.com\/rodighiero\/DH2019\/\r\n\r\n162\r\n\r\n\fattended by a thousand authors wholly graspable 4) Reading as a form of user interaction through which\r\nthe map reader acquires information from the visual media.\r\n\r\n2\r\n\r\nDocumentality\r\n\r\nA series of social rules govern humans behaviors (Kaplan, 2012), and scholars are no exception. Their\r\nactivities follow specific rules when they attend a conference; submitting articles, waiting for reviews,\r\nattending the conference, and presenting their work are the steps they are required to complete in order\r\nto be a part of an academic field. Such behaviors are auto-regulated by the scientific community itself,\r\nwhich gives a structure to the research domain.\r\nArticles play a significant role in this process as they are an extension of their authors. They convey\r\ndifferent types of information, such as collaboration, affiliation, scientific interests, and the writing\r\nproficiency of the authors. Therefore, articles are valuable as they portray the authors\u2019 values and social\r\nrelevance. We hypothesize that documents embody textual information, which can be used to measure the\r\nproximity between scholars most effectively. As individuals express themselves through their language,\r\nauthors can be described through their writing. Terms are not a barrier, nor they are private. Everyone\r\ncan choose and use preferred words and speech styles according to their taste, and the choice is profoundly\r\naffected by social and cultural environments, such as their education level and location. Nonetheless,\r\nthere is complete freedom in the selection of language, and this freedom goes beyond any collaboration\r\nor citation. Just like how Pierre Bourdieu used personal interviews to classify individuals (Blasius and\r\nSchmitz, 2014; Romele and Rodighiero, 2019), the goal here is to map scholars using scientific articles.\r\n\r\n3\r\n\r\nLexical Analysis\r\n\r\nNatural Language Processing (NLP) is a branch of artificial intelligence that aims to process and analyze\r\nlarge amounts of text (Manning and Schuetze, 1999). Since humans\u2019 natural language has no structured\r\nrules, computers can understand, NLP is especially challenging; therefore, the techniques in NLP are\r\nsignificant as they derive meaningful insights from texts written in such a language. In order to profile the\r\nauthors who attended the Digital Humanities 2019 conference, we performed a lexical analysis to map\r\nthe distance from one author to another.\r\nFirst, the XML data of DH2019 were cleaned for a more accurate analysis utilizing JavaScript programming language and the Cheerio library3. From each article are extracted authors, title, and text body.\r\nSince multiple authors can co-author a paper, the text body is grouped by authors, allowing multiple\r\noccurrences of the same publication if a paper is co-authored. Each text is then tokenized using a lexical\r\nanalyzer provided by the Natural library4 for NLP. Successively, tokens are singularized and filtered\r\nby a list of stopwords in various languages, including Brasilian, English, French, German, Italian, and\r\nPortuguese.\r\nThe arrays of tokens associated with authors are then computed via the Term Frequency - Inverse\r\nDocument Frequency algorithm, also known as TF-IDF (Luhn, 1957; Sparck Jones, 1972). TF-IDF\r\nextracts the most relevant terms for each author by counting the frequency of each term with respect to\r\nthe inverse frequency of the entire collection of words. The list of terms for each other is then shortened\r\nto the fifteen most relevant terms in order to simplify the visual computation. Table 1 shows a sample\r\nconcerning the scholar Fr\u00E9d\u00E9ric Kaplan.\r\n\r\n3Cheerio is an open-source library available at https:\/\/github.com\/cheeriojs\/cheerio\/\r\n4Natural is an open-source library available on GitHub at https:\/\/github.com\/NaturalNode\/\r\n\r\n163\r\n\r\n\fToken\r\nParcel\r\nAmsterdam\r\nStreet\r\nCity\r\nCadastre\r\nRue\r\nNeighbourhood\r\nUrban\r\nCentury\r\nBottin\r\nTranscription\r\nGeometrical\r\nExtraction\r\nGeographical\r\nGeometry\r\nWine\r\nActivity\r\nDialect\r\nCinema\r\nTime\r\n\r\nValue\r\n156.29240966203554\r\n147.92161777735498\r\n97.3716554233416\r\n78.98538009996346\r\n68.97376382900369\r\n64.52753719852456\r\n62.45323622544596\r\n61.733372168535126\r\n52.240930125777\r\n51.62202975881964\r\n51.58659792981266\r\n44.74034261035416\r\n43.895897928544414\r\n41.91500209834842\r\n39.821449253041536\r\n38.71652231911473\r\n37.66779857289015\r\n37.53334200758336\r\n34.18896461357028\r\n33.67020614879688\r\n\r\nTable 1: An excerpt from the JSON file that describes the profile of Prof. Fr\u00E9d\u00E9ric Kaplan. Among the\r\nmetadata are his name, the number of articles, and a list of fifteen tokens weighted through the TF-IDF\r\nalgorithm. As his research is mainly focused on the European Time Machine, which is focused on the\r\ncomputational analysis of ancient maps, the result can be considered adequate.\r\n\r\n4\r\n\r\nGraphic Design\r\n\r\nData analysis is followed by the creation of a network, in which each author forms a node, and the shared\r\ntokens are transformed into weighted edges. The resulting visual rendering does not recall a classic\r\nnetwork visualization, such as Gephi\u2019s (Bastian et al., 2009), but rather a cartographic projection. It is a\r\nhybrid form that combines the characteristics of networks and maps.\r\nAuthors are placed on the map using the Simulation function5 from the d3.js library (Bostock et al.,\r\n2011). Then, between each pair of authors, is displayed the most relevant token whose size corresponds to\r\nthe TF-IDF value; a high TF-IDF value corresponds to a high degree of relevance in the whole collection.\r\nThe elevation contours (Monmonier, 1991) displayed at the end of the simulation due to computation\r\nlimits make the density of documents visible; as a result, an author who authored many articles is placed\r\nat a peak. The result is an elevation map that shows the most relevant tokens, such as languages, music,\r\nnewspapers, and films. When zoomed in, the map can be enlarged to display the details, and the user will\r\nnotice the tokens changing. The reason is that the tokens are selected according to the zoom level; when\r\nthe map is utterly visible, the user can see the most relevant tokens with high-frequency values, and by\r\nzooming he will see the most generic ones. This choice makes the view more specific and less generic,\r\nwhile a zoom-in allows viewing the complete gradient of tokens (See Figures 1, 2, and 3).\r\n\r\n5Simulation runs to place the nodes in a proper way, more information is available at https:\/\/github.com\/d3\/\r\nd3-force\/\r\n\r\n164\r\n\r\n\f5\r\n\r\nReading\r\n\r\nIn cartography, the reader is the individual who interacts with the map (Dodge et al., 2011; L\u00E9vy, 2016).\r\nReaders interpret the cartographic visualization differently depending on their knowledge and culture.\r\nIn front of the DH2019 visualization, the reader identifies a configuration of scholars who attended the\r\nconference with an article. When authors share the same text because they co-authored a paper, they\r\nappear to be close on the map, and the most relevant token between them appears (Figure 1). When\r\nthere is a continuity of tokens, it indicates an area of particular interest (Figure 2). When the density of\r\nindividuals is visible, there is a high chance that it represents a collective work (Figure 3).\r\nThe map offers a novel point of view on the conference, which is a different approach from the\r\nproceedings or the website. It makes all the authors and their work graspable at the same time, while also\r\nallowing the readers to navigate through individual authors.\r\nWhen a reader starts to explore a specific part of the map by zooming in, the interaction becomes\r\npersonal and unique. Therefore, readers play an active role while putting an interpretation on the map;\r\ntheir pathway influences what they see. Furthermore, if a reader who participated in the conference\r\nrecognizes her identity in the map, the reading validates the reader\u2019s representation by evaluating the\r\ncorrectness of the map and the neighborhood where the reader is placed (Rodighiero and Cellard, 2019).\r\n\r\n6\r\n\r\nConclusion\r\n\r\nLanguage is not only a means to convey one\u2019s ideas, but also to express interest and background. If a\r\nconference forms a scientific community by attendance, the articles presented at the conference shape\r\nthe specific language of such a community with a delicate balance of different voices (Von Glasersfeld,\r\n1992).\r\nLexical analysis of terminologies is an effective means to study the community. Thanks to the current\r\ntechnology and visualization techniques, we are now able to create a dynamic, interactive map of the\r\ncommunity, which is dense and rich in information. From this new form of data visualization, readers\r\ncan interpret the lexical proximity of all the authors at a glance, both the distance and placement.\r\nNow the question is, why don\u2019t we use the map as an instrument during the conference? It would undoubtedly be a much more contextually rich and visually intriguing way of understanding the conference,\r\ninstead of merely using statistics to summarize the event.\r\n\r\nAcknowledgement\r\nWe want to thank Kurt Fendt for his constant support and supervision, and the MIT Literature section for\r\nhosting us, in particular Shankar Raman, Diana Henderson, and Alicia Mackin. Thanks also to Jeffrey\r\nSchnapp and the laboratory members of Harvard MetaLab.\r\nAcknowledgement also goes to Stephan Risi for developing the search function and Philippe Rivi\u00E8re\r\nfor his priceless collaboration, which was fundamental for recent projects. A special mention to Daniele\r\nGuido, an inseparable friend and colleague whose design capacities are stimulus for doing better; the\r\ngrapefruit color palette is his merit.\r\nThis article is part of the grant Early Postdoc.Mobility P2ELP1_181930 Worldwide Map of Research\r\nfunded by the Swiss National Science Foundation.\r\n\r\n165\r\n\r\n\fFigure 1: The cartography of DH2019. At this level of zoom, the most specific terms suggest some entry\r\npoints to explore. For instance, in the middle, the terms \u201Cnews\u201D and \u201Cnewspaper\u201D invite to zoom in these\r\nareas. The reader is also free to explore empty spaces or search a scholar by using the search box at the\r\ntop right of the interface.\r\n\r\n166\r\n\r\n\fFigure 2: This image illustrates a specific area of the map in which the term \u2018dariah\u2019 is recurrent. The\r\nterm reassembles the people working within the Dariah community. By zooming, the terms change\r\naccording to the scale; the more the reader zooms in, the more generic the terms are.\r\n\r\nFigure 3: Co-authoring is an easily recognizable phenomenon, especially in areas with low density. At\r\nthe center, it is visible a group of scholars that share the term \u2018interdisciplinarity.\u2019 Terms, like in this\r\ncase, can be used to spot small communities within the conference."
	},
	{
		"id": 27,
		"title": "Argumentation mapping for the History of philosophical and scientific ideas: The TheSu annotation scheme and its application to Plutarch‚Äôs Aquane an ignis",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Daniele Morrone"
		],
		"body": "1 Introduction\r\nThe field of \u201Ccomputational history of philosophy\u201D (Betti et al., 2019) is rather new but promising, as it can\r\nprovide historians with powerful research tools to work with large amounts of data in an organized fashion,\r\ngiving them the possibility of finding patterns, similarities and links. History of philosophy and History of\r\nscience can be regarded as subfields of History of ideas \u2013 meant in the broadest possible sense \u2013 and although\r\ndigital methods seem to have only recently been introduced in this latter (Betti and van den Berg, 2016)1,\r\nHistory of science has been benefiting from them for a long time already, under the influence of Computational\r\nlinguistics (Dibattista, 2009). By presenting the novel XML annotation scheme TheSu, this paper aims to\r\ncontribute to the general trend of digitalizing the research methods in these fields, focusing on \u201Cideas\u201D in the\r\nsense of judgements about states of things and giving relevance to the way these judgements are presented\r\nand promoted by their authors.\r\nPlutarch\u2019s short conference (D\u2019Ippolito and Nuzzo, 2012, pp. 180\u2013191) Aquane an ignis utilior sit (Aq.) \u2014\r\n\u201CWhether fire or water is more useful\u201D\u2014 has been annotated according to the TheSu scheme to give some\r\nexamples of this latter\u2019s possible applications and capabilities. The digital XML\/TEI edition (TEI Consortium,\r\n2019) of the original Greek text chosen as a base for the annotation has been downloaded from\r\nPerseusDL\/canonical-greekLit (Cerrato et al., 2019), and corresponds to Bernardakis\u2019s critical edition of the\r\nwork (1895, pp. 1\u201310).\r\n2 TheSu and related work in Argumentation Mining\r\n\r\n1\r\n\r\nBetti and Van den Berg do not seem to consider the activity of the ILIESI (Istituto per il Lessico Intellettuale Europeo e Storia\r\ndelle Idee) in Rome, which has long been working on History of ideas in the frame of Digital Humanities. See http:\/\/\r\nwww.iliesi.cnr.it\/.\r\n169\r\n\r\n\fThe aim of the TheSu (Thesis-Support) annotation scheme is to provide the possibility of easily navigating\r\nthrough enunciates (Theses) contained in written texts and all their linked explanations, justifications and\r\nrefutations (Supports), each indexed as a node in an abstract network defined as \u201CArgumentative-Expository\r\nSystem\u201D (AE System), which is stored in a database. Focusing on argumentative relations of whatever\r\nrhetorical nature, TheSu can be likened to the various annotation schemes that are being proposed in the field\r\nof Argumentation Mining (Lippi and Torroni, 2016; Stede and Schneider, 2019), even if it doesn\u2019t share their\r\ncommon objective of digitally automatizing argument extraction from texts. TheSu, although similar to these\r\napproaches, is different from them for two main reasons:\r\n(1) It builds its system on theses abstracted from the texts by human interpreters, which can then be linked\r\nto their possible textual supports (if there are any). Argumentation mining approaches influenced by Toulmin\r\n(2003 [1958]) and Walton (1998; Id. et al., 2008) tend to directly search the texts for premise-conclusion\r\nenunciative pairs to tag them under schemes such as Walton\u2019s \u201Cargumentation schemes\u201D (see e.g. Lauscher et\r\nal., 2018; Mochales Palau and Moens, 2009; Rocha et al., 2016; Green, 2018a); approaches based on\r\nRhetorical Structure Theory (RST) instead (see Mann and Thompson, 1987; Taboada and Mann, 2006, secs.\r\n2.4, A.2) select their elements through objective textual markers (see the definitions of EDUs \u2014Elementary\r\nDiscourse Units\u2014 in e.g. Carlson et al., 2001; Marcu et al., 1999), and as a consequence segment the text into\r\ndiscrete \u2014albeit interconnected\u2014 non-overlapping units (on the undesirable aspects of these approaches see\r\nGreen, 2018b; Peldszus and Stede, 2013, pp. 15\u201319). In contrast, TheSu focuses first on the indexing of\r\nindividual theses, i.e. treating every single declarative sentence as a \u201Cclaim\u201D, and then on their connection\r\nwith supportive spans of text: the latter can be contiguous to their targeted theses or very far away in the text,\r\nas well as in other works from the same author or from different authors too (as will become clearer below).\r\n(2) While Argumentation Mining methods are generally concerned with textual cohesion and natural\r\nargumentation patterns, TheSu is interested in the coherence and justification of an author\u2019s ideas in her\r\nthought, inasmuch as it is exhibited in her textual production. This also differentiates TheSu from annotation\r\n\r\nFigure 1. Fragment of a concept visualization of a TheSu map: Argumentative-Expository contexts linked\r\nto the theses in Aquane an ignis utilior sit instantiating the proposition \u2018Fire is better than water\u2019. Every\r\nTHESIS and SUPPORT that doesn\u2019t receive justifications or explanations is highlighted by \u201Cnone \u2013targets\u2192\u201D:\r\nit\u2019s desirable to be able to notice them at a glance because it can be proof that their speaker considered them\r\nclear and non-controversial enough not to spend more (supportive) words on their presentation, thus being\r\nthe ideological \u2018building-blocks\u2019 of the whole argumentative discourse.\r\nschemes in Argumentation Mining that seem to be more independent from Walton\u2019s and RST\u2019s influence (e.g.\r\nPeldszus and Stede, 2013). An intellectual historian, while researching on an author\u2019s thought, usually tries to\r\nreach a comprehensive view of it in order to identify trends and elements of cohesion, incompatibility, and\r\nevolution. When the historian extends the scope of her research to include texts from different authors, her\r\n170\r\n\r\n\faim is usually to be able to discover traces of historical influences or innovations based on independent\r\nreasoning. Sometimes she tries to elucidate the author\u2019s texts by putting them in relation to others pertaining\r\nto the same culture or current of thought: when certain ideas are presented synthetically and without\r\nexplanation, she can always look at works from different authors \u2014culturally and philosophically close to the\r\nfirst\u2014 to find their plausible sense and justifications (on current research practices in History of ideas cf. e.g.\r\nvan den Berg et al., 2014, sec. 3). TheSu is intended as a tool to help the historian reach these aims, by\r\nproviding databases for generating maps of the networks of ideas conveyed by texts, and arrange and filter\r\nthem according to her interests (see Figure 1).\r\nTheSu is thus distinguished from the other annotation schemes in a way that can be summarized as follows:\r\nalthough it always starts from a text containing natural argumentation, it only uses it as a proof for the\r\nexistence of a scientific discourse that the text\u2019s author intends to convey. The \u201Cdiscourse\u201D is composed of\r\nboth explicitly stated enunciates and their implicit assumptions and alluded consequences, as well as all the\r\nexplicit and implicit argumentative links between them. These are only \u201Cscientific\u201D in the sense that they are\r\nto be \u2018taken seriously\u2019 by the interpreter, who must always start by assuming the hypothesis that the author\r\nhas legitimate reasons to believe in and present all of them: to test her hypothesis, the interpreter must thus do\r\nher best to find in the text all the supports that might qualify the claims as well founded and adopted critically\r\nby the author, and so \u201Cscientifically\u201D legit (in the context of their existence). In so doing, the interpreter cannot\r\nbut be guided by a strong principle of charity2, and in this way detach the scientific discourse from the text up\r\nabove a certain degree of \u2018charitable\u2019 arbitrariness. The structure of the scientific discourse, then, can not\r\nalways correspond to the structure of the text, and the latter is only used as grounding for the reconstruction\r\nof the former.\r\nTheSu annotations, in addition, can serve the purpose of gathering organized data as a basis for logical and\r\nepistemological evaluations of an author\u2019s style of reasoning. To make these further analyses possible, the\r\ninterpreter must be as non-judgemental as possible in the annotation phase: weird and weak as they may seem,\r\nevery extra-logical \u201Cargumentation\u201D practice deserves the same space as the actual \u201Cdemonstrations\u201D\u2014\r\nadopting Perleman and Olbrecht-Tyteca\u2019s distinction (2013)\u2014 in the network of ideas. This also distinguishes\r\nTheSu from more \u2018normative\u2019, logically rigid, approaches in Argumentation Mining (e.g. Green, 2018a), and\r\nfrom the CRMinf Argumentation Model, an extension to the CIDOC CRM that complements CRMsci, a\r\nmodel for the structuring of metadata about contents and practices of current empirical sciences (Stead et al.,\r\n2019). In CRMinf, the epistemological evaluation of the arguments is embedded in the annotation itself (e.g.\r\nits class \u201CI3 Inference Logic\u201D can only include \u00ABanything that is scientifically or academically acceptable as a\r\nmethod for drawing conclusions\u00BB, ib. p. 11), and much of the discourses\u2019 rhetorical contexts is thus ignored.\r\nPlutarch\u2019s Aq. has been chosen as a case study because of its short length and its elaborate, though very\r\nclear, argumentative structure. It is a rhetorical exercise where both the superiority of water and the superiority\r\nof fire are argued for in persuasive speeches that are symmetrical in extension as well as in cogency, and\r\nwherein no final solution is provided to the controversy. It contains way more \u201Cargumentation\u201D than\r\n\u201Cdemonstration\u201D, and its interesting rhetorical features have already been analysed by Milazzo (1991),\r\nalthough with a different approach. In this paper, its theses will only be quoted by their annotated paraphrases\r\nin English, which is the standard language for the TheSu sheets: considering that all the theses have been\r\nextracted from the original Greek text, in this case every paraphrasis is also a translation, original to this\r\nannotation and sometimes diverging from the previous ones \u2014including Helmbold\u2019s in Cherniss and\r\nHelmbold (1957)\u2014 to improve on clarity and faithfulness. The original (pre-annotated) text will be quoted in\r\ntranslation as well.\r\n3 Encoding the Argumentative-Expository Systems\r\nEvery TheSu XML sheet corresponds to at least one work to be annotated. Considering the general need for\r\nhistorians to keep track of the textual locus of every passage that they analyse and quote, it\u2019s better for the\r\nannotator to work on already-existing XML\/TEI editions of the texts, if suitably provided with milestone\r\nelements with IDs corresponding to the desired reference system. This has been the case with the adopted\r\ndigital edition of Aq. Often TheSu elements need to include non-contiguous spans of text. These, in turn, can\r\noften be interpreted as composing multiple theses or supports (explicit or implicit) cumulatively, sometimes\r\nleading to the problem of overlapping hierarchies. For these two reasons stand-off markup has been chosen\r\nas the annotation method for TheSu: each of its elements has to refer to a span of text in another document,\r\nlinked through xLink and xPointer.\r\n2\r\n\r\nSee Davidson\u2019s \u201CPrinciple of Coherence\u201D (Davidson, 1991).\r\n171\r\n\r\n\fEvery TheSu sheet contains an Argumentative-Espository System (AE System), that is theoretically defined\r\nas a set containing theses, their argumentative and expository supports, and the functional relations between\r\nthe two. As will be shown below, this also needs to include a few more elements in its digital implementation.\r\nA \u201Cthesis\u201D is an instantiation of a declarative proposition at a certain point of the text representing the stance\r\nof its speaker. It can be explicit in the form of an enunciate (e.g. \u2018Putrefaction is the decay of liquids in the\r\nflesh\u2019, Aq. 957e) or implicit, e.g. in the form of a rhetorical question (e.g. \u2018[ Water is more useful to humans\r\nthan fire ]\u2019 in \u00ABhow, then, should water not be more useful\u2026 ?\u00BB, 957b).\r\nA \u201Csupport\u201D is a segment of text that is presented by its speaker in function of a part of the scientific\r\ndiscourse conveyed by the same text. A \u201Csupport\u201D can:\r\n[1] provide justifications for the acceptance or refusal of a thesis or of another support (argumentative\r\nsupport): e.g. \u00ABIn most cases, it\u2019s not possible to use water without fire: in fact, it\u2019s more useful when it\u2019s\r\nheated, otherwise it\u2019s harmful\u00BB, 958a.\r\n[2] explain more clearly, stylistically, or in depth the meaning of another segment of text containing theses\r\nand\/or supports (expository support): e.g. \u00ABIsn\u2019t it more helpful what we always and continuously stand in\r\nneed of, like a tool and an instrument, \u2026?\u00BB, 955f;\r\n[3] expand on an information conveyed by a thesis, favouring a more complete knowledge and\r\nunderstanding of it (expansive support or excursus): \u00AB\u2026 and (don\u2019t you see) that every sense partakes of fire,\r\nas it fabricates the vital principle, and especially sight, which is the keenest of the bodily senses, being an\r\nignition of fire\u2026 ?\u00BB, 958e;\r\n[4] contextualize the interpretation and reception of another segment of text containing theses and\/or\r\nsupports (contextualizing support): \u00ABIn fact, (about) the saying that sometimes humans exist without fire:\r\nhumans can\u2019t at all exist (without it)\u00BB, 958b.\r\nThe reader here may notice that in TheSu\u2019s annotation scheme the \u201Csupport\u201D elements, having four distinct\r\nfunctions, include rhetorical uses that do not correspond directly to argumentative and expository aims. One\r\ncan still speak of \u201CArgumentative-Expository Systems\u201D, though, because careful consideration of both the\r\nexpansive and contextualizing supports is needed for a complete understanding of the argumentative and\r\nexpository roles of the theses surrounding them, and of their linked segments of text.\r\n\u201CTheses\u201D and \u201Csupports\u201D are encoded as THESIS and SUPPORT XML elements, both children of an AEsystem,\r\nwhich is in turn child of a work. Aq.\u2019s AE System, in its current version, contains 259 manually annotated\r\nTHESIS elements (corresponding to 334 theses, 56 of which are implicit) and 216 SUPPORT elements (121\r\nimplicit). These numbers are striking if the very short nature of the text is considered (1627 words in total).\r\nIt\u2019s clear that a high amount of information on an author\u2019s thought and on her cultural context can always be\r\nextracted from even relatively small bits of text: mapping it in detail can be crucial to avoiding\r\nmisinterpretations and misattributions.\r\nEvery THESIS and SUPPORT must have its own ID, so that each can be targeted by SUPPORT elements through\r\nxPointer. THESIS elements\u2019 IDs are also necessary for the most original feature of the TheSu annotation\r\nscheme. Absent, to the best of my knowledge, from current Argumentation Mining techniques is the\r\npossibility of linking together unrelated argumentative-expository chains when converging towards the same\r\nidea. It is a need for the historian, when studying the thought of a certain author, to have a clear view of how\r\nthe same theses are presented and argued for in different contexts, even when unrelated. For example, if the\r\nauthor does not provide supports for a judgement in a certain work or paragraph, it does not necessarily mean\r\nthat she does not argue for it, or better explains it, elsewhere. To have a map where all its occurrences in\r\ndifferent loci, with all their corresponding argumentative-expository apparatuses, are linked together, would\r\nnaturally be helpful to the researcher. This is made possible, in TheSu, through the creation of a \u201Cpropositions\u201D\r\nsheet containing only PROPOSITION elements (a modified version of THESIS for the annotation of non-textual\r\ndeclarative sentences), and by linking to their IDs all the textual THESIS elements instantiating them. In Aq.,\r\nthe proposition e.g. \u2018{ Water is more useful than fire }\u2019 is repeatedly argued for in different manners, and\r\nimplicitly conveyed by the words in [a] 955f-956a, [b] 956c and [c] 957b. The thesis at [a] is the target of 5\r\nsupports, the one at [b] of 5 more, and the one at [c] of only 2. It is undesirable to keep these 12 supports\r\nfragmented in their respective rhetorical chains, as they all converge towards the same idea. Indeed, it is\r\ninteresting to see how this proposition is argued for in all of its enunciative occurrences. Accordingly, it is\r\npreferable to connect each of the textual theses to their common abstract proposition within the same network.\r\nThe usefulness of such a connection becomes even clearer if one imagines its extension to the whole textual\r\nproduction of an author, as well as to works from different authors.\r\nWhat follows is a non-exhaustive presentation of some of the required or optional attributes and subelements of the [i] THESIS and [ii] SUPPORT elements.\r\n172\r\n\r\n\f[i] Every THESIS has an @id, a @value (affirmative or negative) and a @quantity. It can sometimes be\r\n@implicit (boolean), as has been explained above. Every non-propositional THESIS can have one or more child\r\nelements instanceOf, each with a @propRef pointing to the corresponding PROPOSITION. A required child\r\nelement is the speakersGroup, containing at least one speaker, corresponding to the person, group or entity\r\nthe thesis is interpreted to be \u2018pronounced\u2019 by, with a @ref pointing to its name in an authority sheet. The\r\nTHESIS\u2019s child element assent is used to specify whether the thesis is shared, unaccepted or actively attacked\r\nby its speaker (sub-element assentSpeaker with its @assentValue), or by the author of the work\r\n(assentAuthor). The child element thesisType mainly serves indexing purposes, as it classifies the THESIS\r\nthrough its sub-elements: value (epistemic \u2014 to specify with @valueTag whether the thesis is offered as the\r\nspeaker\u2019s real stance, as a hypothesis, or fictitiously), macroThemesGroup (to specify the \u2018macroscopic\u2019\r\ntheme(s) of the thesis, e.g. \u201Cphysical\u201D, \u201Chistorical\u201D, \u201Caxiological\u201D), microThemesGroup (for the \u2018microscopic\u2019\r\ntheme(s) of the thesis, e.g. \u201Cphysiology\u201D, \u201Ccosmology\u201D, \u201Cdialectic\u201D), and keywordsGroup (to point through\r\nkeywordRef elements to the textual or implicit keyword(s) corresponding to the object(s) of the thesis).\r\nNote that each keywordRef\u2019s @ref links to the ID of a keyword that is a child of AEsystem. Separating the\r\nkeywords from the theses becomes necessary due to the possibility of different theses including the same\r\nkeywords: in 957c (\u00ABbut, in general, water (\u03C4\u1F78 \u1F54\u03B4\u03C9\u03C1) is so far away from being self-sufficient for selfpreservation or the bringing-forth of other things that lack of fire, for it, is even destruction\u00BB) the theses\r\n\u2018not(Water is self-sufficient for self-preservation)\u2019, \u2018not(Water is self-sufficient for the bringing-forth of other\r\nthings)\u2019 and \u2018Without fire, water is destroyed\u2019 all share the textual keyword \u03C4\u1F78 \u1F55\u03B4\u03C9\u03C1. Each keyword can point\r\nto a segment of the annotated text or be \u2018implicit\u2019, and must always be tagged semantically through an attribute\r\n@namely, pointing to a class in a vocabulary sheet (e.g. \u201Cwater\u201D). Although the choice of the controlled\r\nvocabulary can be left to the interpreter, all new exhaustive TheSu annotations should consider the keyword\r\nclasses already used in the previous ones, to facilitate the linking of the novel theses to all the corresponding\r\nprevious propositions. It is better not to refer to an ontology of real-world entities, both to free the classification\r\nfrom the need of specifying vague or untranslatable terms, and to avoid projecting alien categories of thought\r\nto different cultural and scientific contexts. More freedom can be granted in the choice of the classes for the\r\n\u201Cmacro-\u201D and \u201Cmicro-themes\u201D, as coherent keywords give sufficient help for the discovery and aggregation\r\nof (quasi-)equivalent theses. Each of the microTheme and keywordRef elements also has an attribute @focus to\r\nspecify, by order of rank, their relative prominence in the thesis: the one just quoted, \u2018Without fire, water is\r\ndestroyed\u2019, is about \u201Cwater\u201D and \u201Cfire\u201D and includes both as its keywords, but it\u2019s more relevant to an\r\nunderstanding of Plutarch\u2019s ideas on water than those on fire. The keywordRef linked to it has thus been given\r\n@focus = 1, and the other @focus = 2. keywordRef can be used as grounding for visualizable analyses such as\r\nthe one in Figure 2, where fire- and water-related keywords are assigned a score (\u201CEpistemic relevance\u201D)\r\nbased on the quantity of THESIS elements containing them at different points of the text, weighted on the basis\r\nof their @focus. One can learn from such a graph that a comparative style is maintained (almost) throughout\r\nthe text, instead of it featuring two \u2018separate\u2019 speeches on the individual excellence of each element: such an\r\nanalysis can lead to interesting findings if compared to similar analyses of other works of the same genre.\r\nOther child elements of THESIS are recap and text. The former contains a short paraphrase in English of the\r\nthesis as interpreted and annotated: no logical formalization is required, as the annotation process must remain\r\naccessible to interpreters untrained in logic. The same goes for the PROPOSITION elements\u2019 recap: avoiding a\r\nstrict logical formalization of the propositions allows the interpreter to consider as their instances theses that\r\nare not quite logically equivalent, but that can count as synonymous enough for the History of ideas, as is the\r\nUse in\r\nSUPPORTS\r\n\r\nForm of\r\nSUPPORT\r\nas premises\r\n\r\nEmployed in\r\njustifications\r\n\r\nTHESES\r\n\r\nEmployed in explanations\r\nEmployed in\r\njstf.\/expl.?\r\nUnused\r\nAll THESES\r\n\r\nFigure 2. Relevance of fire- and water-related\r\nkeywords to the theses conveyed by different\r\ncontiguous spans of Aquane an ignis\u2019s text.\r\n\r\nas\r\nillustrations\r\nin other\r\nforms\r\n\r\nas\r\nexamples\r\n\r\nJustified?\r\nyes\r\nno\r\nyes\r\nno\r\nyes\r\nno\r\nyes\r\nno\r\nyes\r\nno\r\nyes\r\nno\r\nyes\r\nno\r\n\r\nTHESIS\r\nquantity\r\n36\r\n38\r\n65\r\n75\r\n9\r\n41\r\n9\r\n21\r\n0\r\n1\r\n19\r\n64\r\n208\r\n126\r\n\r\nTotal\r\n\r\nJustified\r\n\/ Total\r\n\r\n74\r\n\r\n49%\r\n\r\n140\r\n\r\n46%\r\n\r\n50\r\n\r\n18%\r\n\r\n30\r\n\r\n30%\r\n\r\n1\r\n\r\n0%\r\n\r\n83\r\n\r\n23%\r\n\r\n334\r\n\r\n62%\r\n\r\nTable 1. Theses in Aquane an ignis in relation to\r\nsupports: by how many and in which forms they are\r\nemployed, and by how many they are targeted.\r\n173\r\n\r\n\fcase with the thesis in the bottom-right corner of Figure 1 (quoting \u2018Fire is first, in relation to water\u2019) in respect\r\nto \u2018Fire is better than water\u2019. Finally, the text points through its sub-element textRef (containing at least one\r\nsegment with @from and @to) to the textual proof of the existence of the thesis at a certain point of the discourse.\r\n[ii] SUPPORT elements share with THESIS the attributes @id and @implicit. The sub-elements speakersGroup,\r\nassent, recap and text are present here as well. The first unique child element of the SUPPORT is targetsGroup,\r\ncontaining at least one target pointing through @ref to the ID of a supported element. Very useful is\r\nempolyedTheses, including one or more thesisRef (with @ref) to link to the theses in the SUPPORT\u2019s textual\r\nspan that are actually presented to support the targeted element(s), discriminating between them and other\r\nnon-relevant theses possibly annotated in the same text, thus solving ambiguities.\r\nFor mainly indexing purposes, as with thesisType, each SUPPORT element contains a supportType, also\r\nnecessary for the analysis of the reasoning styles of the discourses they are part of. While their child element\r\nvalue is identical to the one in thesisType, they also include their own function and form. The function\u2019s\r\nsub-elements are justification, explanation, expansion and contextualization, each with a @rank (default\r\n= 4) representing their relative centrality to the support (most central = 1). The idea is that every support, as\r\neverything else in a cohesive discourse, is always at the same time justifying, expository, expansive and\r\ncontextualizing of its surroundings to a certain degree (cf. Perelman, Olbrechts-Tyteca, 2013 [1958], p. 203),\r\nand that its speaker, in order to achieve different rhetorical effects, simply choses to make one or another of\r\nthese functions more prominent than the others. The possibility of ranking the functions solves the problems\r\nthat would come from having to choose only one of them even in cases where there is enough ambiguity to\r\nmake it seem impossible. For the annotation of whether the support, when \u201Cjustifying\u201D, serves the purpose of\r\narguing for or against its target(s), justification has been given the attribute @for ( = \u201Cacceptance\u201D,\r\n\u201Crefutation\u201D or \u201Cmix\u201D). Finally, using the element form the interpreter can classify the support by its rhetorical\r\ntype, referring through @formTag to any class in a typology contained in an authority sheet. The TheSu standard\r\ntypology of supportive forms is meant to be very simple and intuitive for intellectual historians: among the\r\n\u201Cjustifying\u201D forms, the \u201Clogical premise\u201D is a sentence from which the supported target can be inferred by\r\ndeduction, the \u201Cillustration\u201D is a particular case from which the conclusion can be derived by induction, the\r\n\u201Cauthority\u201D is an appeal to an authoritative figure that adheres to the targeted idea, etc. Table 1 illustrates a\r\nquantitative analysis strictly dependent on the elements SUPPORT, function and form: it is not surprising that\r\nin a rhetorical work such as Aquane an ignis a very high amount of theses are given argumentative support\r\n(62%), but it is not necessarily expected that \u201Cillustrative\u201D supports are twice the deductive \u201Cpremises\u201D (140\r\nto 74), characterizing the speech as scarcely \u201Clogical\u201D in tone and much more \u201Cexemplary\u201D. It is also\r\ninteresting that theses employed in supports tend here to attract further argumentation, especially the\r\n\u201Cpremises\u201D (49% justified) and \u201Cillustrations\u201D (46%), in contrast with the theses not used in supports (23%).\r\nThis breakdown is only a small tile of the mosaic that is Plutarch\u2019s personal argumentation style, waiting for\r\nfurther analyses to be combined with and compared to.\r\nConclusion\r\nThe previous sections have described the essential features of the TheSu annotation scheme, its theoretical\r\nframework, and some of the potential uses of a TheSu sheet. This exposition has focused on the\r\nmethodological usefulness of this kind of argumentation and exposition mapping for an historian working on\r\na text, but TheSu can also be helpful for an optimal, transparent and reusable, exposition of the basis and\r\nresults of her research: a historian\u2019s \u2018secondary\u2019 interpretation of a certain text \u2014e.g. its ideas\u2019 dependency\r\nfrom the ones in a contemporary philosophical current, or their ideological or popular nature\u2014 always depend\r\non a \u2018primary\u2019 interpretation of the argumentative and expository chains it is composed of. Storing these\r\nprimary interpretations in easily-accessible TheSu databases would help with the evaluation of the secondary\r\ninterpretations proposed by the historian, and would facilitate the work of future researchers who wish to build\r\nupon her research and generate new interpretations from the argumentative-expository material. This is only\r\npossible thanks to digital interfaces and database interrogation techniques, and would otherwise be too\r\ndifficult and\/or time consuming using traditional, non-digital methods.\r\nAcknowledgements\r\nThis publication is part of the research project Alchemy in the Making: From Ancient Babylonia via GraecoRoman Egypt into the Byzantine, Syriac, and Arabic Traditions, acronym AlchemEast. The AlchemEast\r\n\r\n174\r\n\r\n\fproject has received funding from the European Research Council (ERC) under the European Union\u2019s Horizon\r\n2020 research and innovation programme (G.A. 724914)3."
	},
	{
		"id": 28,
		"title": "Leitwort Detection, Quantification and Discernment",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Racheli Moskowitz",
			"Moriyah Schick",
			"Joshua Waxman"
		],
		"body": "1 Introduction\r\n1.1 The Leitwort in Literature\r\nIn music, a Leitmotif is a recurring musical phrase (a \u201Cmotif\u201D) which is used to \u201Clead\u201D or guide the listener to\r\nrecall something or make a connection. In literature, the Leitwort (\u201Cleading word\u201D) plays a similar function. A\r\ncertain word, or word root, is unusually repeated several times in a passage, in a way that jumps out at the\r\nreader, in order to establish a theme. Optionally, once established, it might then be echoed in a later passage\r\nto recall that theme. Here is one of the many Leitworte which Pinault (1986) identifies in his analysis of stylistic\r\nfeatures in The Arabian Nights. In the story \u201CThe City of Brass\u201D, over the course of a few consecutive pages,\r\nin poetry and prose, there is repetition of words with the Arabic root \u202B \uFEE3\uFEEE\u062A\u202C\/ mawt, meaning death.\r\n\u202B \u0623\uFE91\uFE8E\u062F\u06BE\uFEE2 \u0627\uFEDF\uFEE4\uFEEE\u062A\u202C\/ abadahum mawt, \"Death destroyed them\".\r\n\u202B \uFEDB\uFE84\u0633 \u0627\uFEDF\uFEE4\uFEEE\u062A\u202C\/ ka`s al-mawt, \"the cup of doom\".\r\n\u202B \u0625\u0646 \uFEDB\uFE8E\u0646 \uFEE3\uFEEE\uFE97\uFEF2 \uFEBB\uFEFC\u0629 \uFEB3\uFEA0\uFE8E\u0644\u202C\/ in kana mawti mahtuman cala cajal, \"when my death was decreed all at once\".\r\n\u202B \uFE91\uFE88\uFEB3\uFEE2 \u0627\uFEDF\uFEA4\uFEF2 \u0627\uFEDF\uFEAC\u064A \uFEFB \uFBFE\uFEE4\uFEEE\u062A\u202C\/ bi-ism al-hayy alladhi la yamut, \"in the name of the Living, who dies not\".\r\n\u202B \u064F\uFBFE\uFEB0\uFEA7\uFEAE\uFED3\uFBAD\uFE8E \u0627\uFEDF\uFEB8\uFBFF\uFEC4\uFE8E\u0646 \uFEDF\uFEFA\uFEE7\uFEB4\uFE8E\u0646 \u0625\uFEDF\uFEF0 \u0627\uFEDF\uFEE4\uFEE4\uFE8E\u062A\u202C\/ yuzakhrifuha al-shaytan lil-insan ila al-mawt, \"Satan adorns it for man to\r\nlead him to death\". (MacNaghten edition)\r\n177\r\n\r\n\fThis literary technique has been discussed by scholars as it is found in sacred texts such as the Hebrew Bible\r\n(Alter, 1981) and the Koran (Wansbrough, 1978). It is similarly employed in the works of Goethe, Nietzsche,\r\nHeidegger and others. Then, one task of those who analyze these texts is to discern the use of a Leitwort and\r\nexplain its purpose.\r\n1.2 Leitworte and the Hebrew Bible\r\nVarious interpreters of the Hebrew Bible within the past century have taken note of the use of the Leitwort,\r\nthough they differ as to its parameters and purpose. In Buber (1927), and in Buber and Rosenzweig (1936),\r\nthe purpose of a thematic repetition in a passage is to reveal or clarify a meaning in the text, or to emphasize\r\nthat meaning. They select Leitworte based on their subjective estimation of the word\u2019s significance, rarity, and\r\nthe degree of repetition. These are all factors which contribute to a word capturing the attention of the reader.\r\nThe repetitions can occur densely in a single passage, or can be distributed throughout the text. They draw\r\nconnections between passages in which the same Leitwort occurs, as indicating an allusion or thematic echo.\r\nIn one famous example from Buber, the seven scenes of revelation that compose the Abraham story arc are all\r\ntied together by the use of the term \u202B \u05E8\u05D0\u05D4\u202C\/ ra\u2019ah \/ \u201Csee\u201D in each passage, and unlike other translations which\r\nobscure this connection, Buber and Rosenzweig\u2019s German translation preserves the Leitwort by translating it\r\nconsistently throughout.\r\nAt around the same time, Umberto Cassuto also took an interest in Leitworte in the Hebrew Bible. Cassuto\r\nwas first the chief rabbi of Florence and subsequently a professor at University of Florence and at the\r\nUniversity of Rome La Sapienza. Like Buber and Rosenzweig, Cassuto (1961, published posthumously)\r\nconsidered that Leitworte in the Bible served the purpose of establishing the theme of a passage or emphasizing\r\na point. However, he was more selective in what types of repetitions he would consider to be bona fide\r\nLeitworte. In Cassuto\u2019s view, repetitions are only interpretable if they occur within a coherent passage and\r\noccur in a multiple of 3, 7, or 10. As illustrated below, he discusses the threefold repetition as a way of\r\nemphasizing a point within or between passages. The numbers seven and ten are chosen because they are\r\nparticularly significant to an ancient Israelite author from a ritual and spiritual perspective. For instance, the\r\nnumber seven is echoed from the Creation narrative to a weekly cycle of seven days, a seven-year cycle before\r\neach Sabbatical year appears, and seven Sabbatical years leading up to the Jubilee. Similarly, the number ten\r\nbrings the Ten Commandments immediately to mind.\r\nWe present an extended example to illustrate the position of Leitworte in Cassuto\u2019s textual analysis. In\r\nExodus 2:2-6, in the background of Egyptian governmental decrees to drown all firstborn Hebrew boys,\r\nMoses is born. This story, Passage A, contains the word-root \u202B \u05E8\u05D0\u05D4\u202C\/ ra\u2019ah \/ \u201Csaw\u201D, repeated three times:\r\nMoses\u2019 mother \u201Csaw [\u202B \u05B7\u05D5\u05B5\u05EA\u05B6\u05BC\u05E8\u05D0\u202C\/ vateire`] him that he was a goodly child\u201D, hid him for as long as she could,\r\nand then placed him in an ark on the riverside. When Pharaoh\u2019s daughter visited the river, \u201Cshe saw [\u202B \u05B7\u05D5\u05B5\u05EA\u05B6\u05BC\u05E8\u05D0\u202C\/\r\nvateire`] the ark among the flags, and sent her handmaid to fetch it. (6) And she opened it, and saw it [ \u202B\u05B7\u05D5\u05B4\u05EA\u05B0\u05BC\u05E8\u05B5\u05D0\u05D4\u05D5\u05BC\u202C\r\n\/ \u25CC\u05B7vatir`eihu], even the child; and behold a boy that wept. And she had compassion on him, and said: 'This\r\nis one of the Hebrews' children.' \u201D (Jewish Publication Society Translation)\r\nA few verses later (Exodus 2:11-15), Moses, after being raised as a prince in Pharaoh\u2019s house with his\r\nbiological mother as his nursemaid, decides to check on his enslaved brethren, sees their suffering, and reacts.\r\nOnce again, the thrice-repeated index root is ra\u2019ah, though there are others marked. We label this Passage B.\r\n\u202B \u05B7\u05D5\u05B7\u05D9\u05B0\u05BC\u05E8\u05D0\u202C,\u202B \u05D9\u05D1 \u05B7\u05D5\u05D9\u05B4\u05B6\u05BC\u05E4\u05DF \u05DB\u05BC \u05B9\u05D4 \u05B8\u05D5\u05DB \u05B9\u05D4\u202C.\u202B\u05B4\u05E2\u05B0\u05D1\u05B4\u05E8\u05D9 \u05B5\u05DE\u05B6\u05D0\u05B8\u05D7\u05D9\u05D5\u202C-\u202B \u05B7\u05DE\u05B6\u05DB\u05BC\u05D4 \u05B4\u05D0\u05D9\u05E9\u05C1\u202C,\u202B \u05B0\u05D1\u05B4\u05BC\u05E1\u05B0\u05D1\u05B9\u05DC\u05B8\u05EA\u05DD; \u05B7\u05D5\u05B7\u05D9\u05B0\u05BC\u05E8\u05D0 \u05B4\u05D0\u05D9\u05E9\u05C1 \u05B4\u05DE\u05B0\u05E6\u05B4\u05E8\u05D9\u202C,\u202B \u05B7\u05D5\u05B7\u05D9\u05B0\u05BC\u05E8\u05D0\u202C,\u202B\u05B6\u05D0\u05B8\u05D7\u05D9\u05D5\u202C-\u202B\u05E9\u05C1\u05D4 \u05B7\u05D5\u05D9\u05B5\u05B5\u05BC\u05E6\u05D0 \u05B6\u05D0\u05DC\u202C\r\n\u05B6 \u05B9 \u202B \u05B7\u05D5\u05D9\u05B0\u05B4\u05BC\u05D2\u05B7\u05D3\u05BC\u05DC \u05DE\u202C,\u202B\u05D9\u05D0 \u05B7\u05D5\u05D9\u05B0\u05B4\u05D4\u05D9 \u05B7\u05D1\u05BC\u05D9\u05B4\u05B8\u05BC\u05DE\u05D9\u05DD \u05B8\u05D4\u05B5\u05D4\u05DD\u202C\r\n.\u202B \u05B5\u05E8\u05B6\u05E2\u05B8\u05DA\u202C,\u202B \u05B8\u05DC\u05B8\u05DE\u05BC\u05D4 \u05B7\u05EA\u05B6\u05DB\u05BC\u05D4\u202C,\u202B\u05E9\u05C1\u05E2\u202C\r\n\u05B8 \u202B \u05B8\u05DC\u05B8\u05E8\u202C,\u202B\u05E9\u05C1\u05D9\u05DD \u05B4\u05E2\u05B0\u05D1\u05B4\u05E8\u05D9\u05DD \u05B4\u05E0\u05B4\u05E6\u05BC\u05D9\u05DD; \u05B7\u05D5\u05D9\u05BC \u05B9\u05D0\u05B6\u05DE\u05E8\u202C\r\n\u05B4 \u202B\u05B2\u05D0\u05B8\u05E0\u202C-\u202B\u05E9\u05B5\u05C1\u05E0\u05D9\u202C\r\n\u05B0 \u202B \u05B0\u05D5\u05B4\u05D4\u05E0\u05B5\u05BC\u05D4\u202C,\u202B\u05E9\u05B4\u05BC\u05C1\u05E0\u05D9\u202C\r\n\u05B5 \u202B \u05D9\u05D2 \u05B7\u05D5\u05D9\u05B5\u05B5\u05BC\u05E6\u05D0 \u05B7\u05D1\u05BC\u05D9\u05BC\u05D5\u05B9\u05DD \u05B7\u05D4\u202C.\u202B \u05B7\u05D1\u05BC\u05D7\u05D5\u05B9\u05DC\u202C,\u202B \u05B7\u05D5\u05D9\u05B0\u05B4\u05BC\u05D8\u05B0\u05DE\u05E0\u05B5\u05D4\u05D5\u05BC\u202C,\u202B\u05B7\u05D4\u05B4\u05DE\u05B0\u05BC\u05E6\u05B4\u05E8\u05D9\u202C-\u202B \u05B6\u05D0\u05EA\u202C,\u202B\u05B4\u05DB\u05BC\u05D9 \u05B5\u05D0\u05D9\u05DF \u05B4\u05D0\u05D9\u05E9\u05C1; \u05B7\u05D5\u05B7\u05D9\u05B0\u05BC\u05DA\u202C\r\n\u202B \u05D8\u05D5\u202C.\u202B \u05D0\u05B5\u05B8\u05DB\u05DF \u05E0\u05D5\u05B7\u05B9\u05D3\u05E2 \u05B7\u05D4\u05B8\u05D3\u05B8\u05BC\u05D1\u05E8\u202C,\u202B\u05E9\u05C1\u05D4 \u05B7\u05D5\u05D9\u05BC \u05B9\u05D0\u05B7\u05DE\u05E8\u202C\r\n\u05B6 \u05B9 \u202B\u05B7\u05D4\u05B4\u05DE\u05B0\u05BC\u05E6\u05B4\u05E8\u05D9; \u05B7\u05D5\u05D9\u05B4\u05BC\u05D9\u05B8\u05E8\u05D0 \u05DE\u202C-\u202B\u05E9\u05C1\u05E8 \u05B8\u05D4\u05B7\u05E8\u05D2\u05B0\u05B8\u05EA\u05BC \u05B6\u05D0\u05EA\u202C\r\n\u05B6 \u202B \u05B7\u05DB\u05B2\u05BC\u05D0\u202C,\u202B\u05B7\u05D4\u05B0\u05DC\u05B8\u05D4\u05B0\u05E8\u05D2\u05B5\u05E0\u05B4\u05D9 \u05B7\u05D0\u05B8\u05EA\u05BC\u05D4 \u05D0 \u05B5\u05B9\u05DE\u05E8\u202C--\u202B \u05B8\u05E2\u05B5\u05DC\u05D9\u05E0\u05D5\u05BC\u202C,\u202B\u05E9\u05C2\u05E8 \u05B0\u05D5\u05E9\u05C1 \u05B5\u05B9\u05E4\u05D8\u202C\r\n\u05B7 \u202B\u05E9\u05B0\u05C2\u05DE\u05B8\u05DA \u05B0\u05DC\u05B4\u05D0\u05D9\u05E9\u05C1\u202C\r\n\u05B8 \u202B\u05D9\u05D3 \u05B7\u05D5\u05D9\u05BC \u05B9\u05D0\u05B6\u05DE\u05E8 \u05B4\u05DE\u05D9\u202C\r\n.\u202B\u05B7\u05D4\u05B0\u05D1\u05B5\u05BC\u05D0\u05E8\u202C-\u202B\u05E9\u05C1\u05D1 \u05B7\u05E2\u05DC\u202C\r\n\u05B6 \u05B5\u202B\u05B4\u05DE\u05B0\u05D3\u05D9\u05B8\u05DF \u05B7\u05D5\u05D9\u05BC\u202C-\u202B\u05E9\u05C1\u05D1 \u05B0\u05D1\u05B6\u05BC\u05D0\u05B6\u05E8\u05E5\u202C\r\n\u05B6 \u05B5\u202B \u05B7\u05D5\u05D9\u05BC\u202C,\u202B\u05E9\u05C1\u05D4 \u05B4\u05DE\u05B0\u05E4\u05B5\u05BC\u05E0\u05D9 \u05B7\u05E4\u05B0\u05E8\u05E2 \u05B9\u05D4\u202C\r\n\u05B6 \u05B9 \u202B\u05E9\u05C1\u05D4; \u05B7\u05D5\u05D9\u05B0\u05B4\u05BC\u05D1\u05B7\u05E8\u05D7 \u05DE\u202C\r\n\u05B6 \u05B9 \u202B\u05DE\u202C-\u202B \u05B7\u05D5\u05D9\u05B0\u05B7\u05D1\u05B5\u05E7\u05BC\u05E9\u05C1 \u05B7\u05DC\u05B2\u05D4\u05E8 \u05B9\u05D2 \u05B6\u05D0\u05EA\u202C,\u202B\u05B7\u05D4\u05B8\u05D3\u05B8\u05BC\u05D1\u05E8 \u05B7\u05D4\u05D6\u05B6\u05BC\u05D4\u202C-\u202B\u05E9\u05B7\u05C1\u05DE\u05E2 \u05B7\u05E4\u05B0\u05BC\u05E8\u05E2 \u05B9\u05D4 \u05B6\u05D0\u05EA\u202C\r\n\u05B0 \u05B4\u202B\u05B7\u05D5\u05D9\u05BC\u202C\r\n(11) And it came to pass in those days, when Moses was grown up, that he went out unto his brethren, and\r\nlooked [vayar`] on their burdens; and he saw [vayar`] an Egyptian smiting [makeh] a Hebrew, one of his\r\nbrethren. (12) And he looked this way and that way, and when he saw [vayar`] that there was no man, he\r\nsmote [vayakh] the Egyptian, and hid him in the sand. (13) And he went out the second day, and, behold,\r\ntwo men of the Hebrews were striving together; and he said to him that did the wrong: 'Wherefore smitest\r\n[takeh] thou thy fellow?' (14) And he said: 'Who made thee a ruler and a judge over us? thinkest thou to\r\nkill me [haleharegeni], as thou didst kill [haragta] the Egyptian?' And Moses feared, and said: 'Surely\r\nthe thing is known.' (15) Now when Pharaoh heard this thing, he sought to slay [laharog] Moses. But\r\nMoses fled from the face of Pharaoh, and dwelt in the land of Midian; and he sat down by a well. (Jewish\r\nPublication Society Translation)\r\n178\r\n\r\n\fBefore we turn to Cassuto\u2019s analysis, a brief discussion of Passage B can help us understand the general\r\nphenomenon of Leitwort. Note that while \u202B \u05E8\u05D0\u05D4\u202C\/ ra`ah, \u201Csaw\u201D, is repeated three times, this is evident only when\r\nreading the passage in the original Hebrew, rather than in the English translation above. It appears twice in\r\nverse 11; the first time it is rendered as \u201Clooked\u201D and the second time as \u201Csaw\u201D. Meanwhile, in verse 12, the\r\nword \u201Clooked\u201D appears translating a different word-root, \u202B \u05E4\u05E0\u05D4\u202C\/ panah, while the word \u201Csaw\u201D translates \u202B \u05E8\u05D0\u05D4\u202C\/\r\nra\u2019ah.\r\nThese threefold repetitions seem deliberate. Firstly, an author can include or omit details while still\r\nadvancing the narrative, so many times, a repeated word didn\u2019t truly need to appear. Secondly, Hebrew has\r\nsynonyms. An author can select from an inventory of terms. For instance, in the same passage, the roots nakhah\r\n(\u201Csmite\u201D, \u201Cstrike\u201D) and harag (\u201Ckill\u201D) are used to refer to Moses\u2019 killing of the Egyptian taskmaster.\r\nIn analyzing Passage B, Cassuto notes the threefold repetition of the root ra\u2019ah. He says it is for emphasis.\r\nIt matches the threefold repetition of the same root in passage A. The parallel is not accidental, but it is to\r\nstress that just as Moses\u2019 mother, and Pharaoh\u2019s daughter saw and had mercy on him, so did Moses take pity\r\nand have mercy on his brethren. Cassuto also notes the threefold repetitions of smiting and killing.\r\n1.3 The Problem of Leitwort Operationalization\r\nDespite the value of Leitworte as a literary technique that unifies a text and enriches the experience of the\r\nreader, any attempt to accurately identify Leitworte is somewhat problematic, particularly in a large and varied\r\ncorpus such as the Hebrew Bible. The problem, essentially, is that language is repetitive by nature. Zipf (1945)\r\ndiscusses the relative frequencies of all words in a corpus, repetitions within clusters, as well as intervals\r\nbetween clusters, and these phenomena are observed in the absence of deliberate stylistic repetition.\r\nSubject matter or narrative concerns can require a certain word to appear more than once in a story. For\r\ninstance, the first chapter of Carlo Collodi\u2019s Le Avventure di Pinocchio contains repetitions of the Italian word\r\nlegno, \u201Cwood\u201D and pezzo, \u201Cpiece\u201D. This is because it is where we first encounter a talking, weeping, and\r\nlaughing piece of wood. Even if Collodi had no intention of drawing the reader\u2019s attention to these words, the\r\nnarrative would be senseless without them. Luhn (1958) demonstrates that this occurs in non-narrative texts\r\nas well, and establishes the content of a text based on the non-deliberate repetition of words or word roots as\r\nan author advances his arguments or elaborates on an aspect of a subject. Additionally, certain words, such as\r\nfunction words (e.g., the articles \u201Ca\u201D and \u201Cthe\u201D), are extremely common in language because they assist\r\ncommunication. These words will necessarily occur numerous times throughout a text, and will be totally\r\nunrelated to any theme that the writer wishes to emphasize. In fact, Luhn suggests that words that occur above\r\na certain frequency threshold can generally be considered insignificant.\r\nIn the midst of a sea of non-significant word repetitions, identification of meaningful Leitworte for further\r\nstudy poses a challenge which can be addressed in one of two ways. The first solution is to maximize\r\nsubjectivity, simply relying on a human interpreter to name which words should be considered Leitworte. This\r\ntraditional approach has the benefit of embracing the nuances of human insight and the finely-honed skills of\r\nexpert analysis, but can also be considered arbitrary and subjective by its very nature. An alternative solution,\r\nwhich we implement in this paper, is to introduce objective measurement tools in an attempt to quantify\r\nsignificance of repeated words in a given passage. Of course, our algorithm cannot truly appreciate a text, and\r\nstandardized rules do not take intuitive understanding into account. However, our program has the advantage\r\nof working systematically to find every candidate Leitwort, in contrast to human experts who are subject to\r\nthe limitations of their scanning and matching capability and who may be biased by selective interest in certain\r\nterms. This novel approach allows us to move towards greater objectivity in analysis, and is a great tool for\r\nscholars who want to consider every potential Leitwort in the text.\r\nIn the next section, we detail our programmatic approach. Our goals were to devise a quantitative measure\r\nof repetition significance, and to compare output of our program to the list of Leitworte identified by Cassuto,\r\na traditional expert with a relatively systematic approach. To that end, our operational definitions are modeled\r\non Cassuto\u2019s definitions but strip out the subjective components of his expert analysis.\r\n\r\n2 Our Approach\r\nFor clear and consistent Leitwort identification, several practical questions must be answered. First, what\r\nconstitutes a \u201Cword\u201D? Second, what is a qualifying number of repetitions? Third, how closely spaced must\r\nthe repetitions be? Fourth, how is significance of candidate words defined? Here we explain how we\r\naddressed each of these questions in designing our program.\r\n179\r\n\r\n\f2.1 Reduction to Lexemes or Roots\r\nSince Semitic languages such as Hebrew are inflected, and the typical Leitwort is based on repetition of the\r\nword\u2019s root, we first reduce the words in the corpus to their lexemes. For the Hebrew Bible, we use the ETCBC\r\ndataset described in Roorda (2015), which contains manual marking of rich linguistic features by human\r\nexperts. One such feature is the lexeme, which is a close approximation to the root. In Hebrew, most words\r\ncontain a triliteral root which conveys a core meaning. For instance, \u202B \u05D0\u05D5\u05E8\u202C\/ `or has the meaning of \u201Clight\u201D. In the\r\nETCB dataset, words with this root are divided into two separate lexemes, \u202B \u05D0\u05D5\u05E8\u202C\/ `or (\u201Clight\u201D) and \u202B \u05DE\u05D0\u05D5\u05E8\u202C\/ ma`or\r\n(\u201Cluminary\u201D, thing which gives light). The lexemes are stemmed versions of the full word, stripping out\r\ndefiniteness, gender, number and person. Thus, the full word \u202B \u05B4\u05DC\u05B0\u05DE\u05D0\u05D5\u05B9\u05E8 \u05B9\u05EA\u202C\/ lim\u2019orot \/ \u201Cas luminaries\u201D in Genesis\r\n1:15 is marked with the lexeme \u202B \u05DE\u05D0\u05D5\u05E8\u202C\/ ma`or while the word \u202B \u05B0\u05DC\u05B8\u05D4\u05B4\u05D0\u05D9\u05E8\u202C\/ leha`ir \/ \u201Cto give light\u201D in the same verse\r\nis marked with the lexeme \u202B \u05D0\u05D5\u05E8\u202C\/ `or. The ETCB dataset does not have a root feature.\r\nWe differ here from Cassuto, who primarily considers repetitions of roots. However, it is noteworthy that\r\nmany of these lexemes are also the simple root (such as the \u202B \u05D0\u05D5\u05E8\u202Cexample above). Of the 788 sevenfold lexemebased Leitwort candidates our algorithm discovered across the Pentateuch, 81% consisted of triliteral roots.\r\nMany of the non-root lexeme candidates are names of nations or places.\r\n2.2 Counting Repetitions\r\nFollowing Cassuto, our candidate Leitworte must occur a multiple of 3, 7, or 10 times in a passage. We\r\ngravitate towards Cassuto\u2019s definition of Leitwort for a few reasons. Many modern Biblical interpreters (such\r\nas Elchanan Samet of Yeshivat Har Etzion) employ both Buber and Cassuto-type Leitworte in their analyses,\r\nbut consider the more rigorously defined Cassuto-type Leitworte as especially significant (Grossman, 2011).\r\nFurther, as discussed above, Cassuto shows that these are meaningful numbers for an ancient Israelite author,\r\nand he consistently demonstrates that thematic words are repeated this precise number of times, or a multiple\r\nthereof. Indeed, we are treating this threefold and sevenfold repetition as a mark of authorial deliberateness \u2013\r\nthat the author has set out to employ the Leitwort style. If a word were repeated by chance, simply because it\r\nis the topic of a passage (see the legno and pezzo examples above) or because it is a commonly occurring word\r\n(such as \u201Csaid\u201D), then it mostly would not occur specifically as a sevenfold repetition.\r\n2.3 Scanning for Repetitions\r\nWe scan for repetitions in the text, in a moving window. For face validity, we require a certain minimum\r\ndensity of repetition. Buber did not require close proximity; the seven Abraham scenes that he connects with\r\nthe root \u202B \u05E8\u05D0\u05D4\u202C\/ ra\u2019ah \/ \u201Csee\u201D span 178 verses over 8 chapters. Cassuto only identified Leitwort occurring within\r\nself-contained passages, but personally determined section and paragraph boundaries based on his own close\r\nreading analysis. Neither of these approaches is appropriate to our method, as both rely on subjective expert\r\njudgement to decide whether a given set of repetitions occurs within an acceptable space.\r\nTo define objective limits, we turned to the historical Jewish segmentation scheme of the sidra: the entire\r\nPentateuch is chanted by a reader in synagogues over the course of a lunar year, one portion each week, on the\r\nJewish Sabbath, though there are modifications due to holidays. The Pentateuch was divided into 54 such\r\nportions, or sidrot. While the calendar influenced the number of portions, scholars segmented the text at\r\nappropriate positions, such that there is often a consistency in the narrative or legal codes within the text. To\r\nmake use of this narrative consistency, we only count repetitions within a sidra. Another Biblical segmentation\r\nscheme, of Christian origin, is the well-known series of chapter divisions (e.g. Genesis 1, Genesis 2), which\r\nbreaks up the full text into chapters of about 30 verses each. We further require our repetitions to occur within\r\na maximum window of 60 verses (approximately 2 chapters). Thus, if a word randomly occurs 4 times at the\r\nstart of a sidra and much later has a sevenfold repetition, for a total of 11 occurrences, the sevenfold repetition\r\nwill still remain a candidate.\r\nOnce a qualifying repetition has been found, we continue scanning along two pathways: one in which the\r\npassage stops with the most recent verse (and can thus be far smaller than 60 verses in length), and one in\r\nwhich it continues and allows for higher multiples to be identified.\r\nBecause our sections have flexible starting and ending points, a separate method must ensure that word\r\nappearances from other sections are kept distinct. In line with Cassuto\u2019s numerical definition of Leitworte, an\r\neighth appearance of a candidate word in the same passage should disqualify the word. However, we would\r\nnot wish to incorrectly disqualify a candidate merely because it occurs in an unrelated passage later in the\r\n180\r\n\r\n\fsidra. Using his idiosyncratic paragraph divisions, Cassuto would find a sevenfold repetition within a\r\nparagraph and ignore an unassociated occurrence one or two paragraphs earlier. Lacking such boundary lines,\r\nwe create a buffer zone around each of our identified Leitwort passages. This zone is defined as \u00BC the number\r\nof sentences of the passage span, and we require a total absence of the candidate word within that zone. Thus,\r\na word will qualify as a Leitwort candidate if it occurs 7 times within a 16-verse span but does not appear at\r\nall in the 4 preceding and 4 subsequent verses. By requiring the word to appear in this \u201Cisland,\u201D we create de\r\nfacto passage boundaries in a flexible way.\r\n2.4 Filtering for Significance\r\nFinally, we rate the candidate words for significance. In making estimation of significance the last step in our\r\nprocess, we diverge from the traditional expert-reader model. Scholars such as Cassuto and Buber would start\r\nwith an impression that a word was significant, in the sense of meaningful and important. To Buber, if such a\r\nword was relatively rare (an undefined term) and also repeated within a story, it was a Leitwort. Cassuto\r\nrequired a precise number of repetitions and did not restrict based on rarity, but only examined words that he\r\ndeemed especially significant rather than identifying every threefold or sevenfold repetition. Indeed, it would\r\nbe simplistic to say that all of them are significant; these numbers can occur by chance just like any other.\r\nTherefore, after systematically compiling a list of all sevenfold repetitions within our corpus, we employ a\r\ntf-idf measure to weed out the most clearly insignificant of them. The term frequency (tf) is the number of\r\ntimes a word appears in a given document, while the inverse document frequency (idf) is the log of the total\r\nnumber of documents N divided by the number of documents that contain the word. If a word is frequent in\r\nthe current document and infrequent elsewhere, then the product of the tf and the idf will be high. The\r\n\u201Cdocuments\u201D we use for this computation are the 54 aforementioned sidrot, since the text in each such division\r\nwill typically be of a consistent genre (e.g. genealogy, legal code, narrative) and topic (e.g. trials in the\r\nwilderness).\r\nWe stress that the purpose of this tf-idf ranking is not to discover the emphatic and thematic words. The\r\nspecific numerical repetition establishes that. Rather, our aim was to filter out common yet highly\r\ninsignificant words, which will occur in sevenfold repetition (along with eightfold repetition, ninefold\r\nrepletion, etc.) purely by chance. For this reason, we set our tf-idf threshold very low, at 0.07. After\r\nexamining a small portion of unfiltered candidates, we chose this value because it could retain words that\r\nappeared thematically relevant, while excluding common words with high frequency throughout the corpus\r\nbut no discernable relevance to the specific passage. We did not use a simple stoplist of frequent words since\r\na common word might be extremely significant in a given context. For instance, in Genesis 1-2, in which\r\nGod creates the Universe in a sequence of speech acts, the lexeme \u202B \u05D0\u05DE\u05E8\u202C\/ amar \/ \u201Csaid\u201D occurs 28 times (a\r\nmultiple of 7) and has a tf-idf score of 0.13, above our threshold of significance. It also occurs seven times in\r\nDeuteronomy 5-7 with a non-significant tf-idf score of 0.04.\r\n\r\n3 Results\r\nIn the five books of the Hebrew Bible, we discovered a total of 788 potential Leitwort candidates that appeared\r\na multiple of seven times in an island of text. Of these, 332 (or 42%) exceeded our tf-idf threshold and were\r\ncounted as significant. Passage span ranged from 5 to 60 verses; and candidate lexemes were repeated within\r\nthese passages 7, 14, 21, 28, 35, 42, or 49 times. As would be expected, threefold repetitions had shorter\r\npassage spans on average, and many fewer of them were deemed significant.\r\nWe compiled a comprehensive list of Cassuto\u2019s Leitworte and compared them against the output of the\r\nprogram. Cassuto wrote commentary on the first 13 chapters (out of 50) of Genesis and on the entire (40\r\nchapter) book of Exodus, identifying 164 Leitworte, of which 142 were of simple word or root repetitions. Of\r\nthese root repetitions, 59 represented a sevenfold recurrence. For the same group of chapters, we found 207\r\npotential candidates appearing a multiple of seven times, of which 102 (49%) exceeded our tf-idf threshold.\r\nTable 1 cross-tabulates our results with Cassuto\u2019s. Twenty words were deemed significant by our program\r\nand also discussed by Cassuto, 82 are marked at Leitworte by our program only, 39 by Cassuto only, and 105\r\nwords that do not appear in Cassuto\u2019s work were originally flagged by our program but fell below our\r\nsignificance threshold.\r\n\r\n181\r\n\r\n\fAlgorithm\r\n\r\nCassuto\r\n\r\nYes\r\n\r\nNo\r\n\r\nYes\r\n\r\nTotal: 20\r\nGenesis: 8\r\nExodus: 12\r\n\r\nTotal: 39\r\nGenesis: 18\r\nExodus: 20\r\n\r\nNo\r\n\r\nTotal: 82\r\nGenesis: 17\r\nExodus: 65\r\n\r\nTotal: 105\r\nGenesis: 23\r\nExodus: 82\r\n\r\nTable 1: Cross-tabulation of the results of Cassuto and our algorithm. \u201CYes\u201D means that it appears in the list\r\n(and, for the algorithm, deemed significant). Cells representing agreement of the two sources are italicized.\r\nA few facts are apparent from these results. We see that our algorithm identified many more potential\r\nLeitworte overall than Cassuto. Also, Cassuto and the algorithm agreed about 50% of the time, and were much\r\nmore likely to agree that a word was non-significant than that it was significant. Cassuto discussed many\r\nLeitworte that were not accepted by the algorithm, and vice versa. Finally, it is noteworthy that the results\r\ndiffer substantially based on specific text. Cassuto described almost as many Leitworte in the first 13 chapters\r\nof Genesis as in the 40-chapter Exodus. Meanwhile, the algorithm flagged repetitions with similar density\r\nacross the two books and consistently identified about half of them (52% in Genesis, 48% in Exodus) as\r\npotentially significant. Cassuto and the algorithm therefore find about the same number of Genesis Leitworte,\r\nwith few of Cassuto\u2019s appearing in the computer-generated list, whereas the algorithm finds more than twice\r\nas many Exodus Leitworte as Cassuto does.\r\nIf Cassuto\u2019s work is held up as the gold standard, one can say that the algorithm achieved 19.6% precision\r\n(32.0% in Genesis, 15.6% in Exodus) and 33.9% recall (30.8% in Genesis, 37.5% in Exodus). This suggests\r\nthat it is able to catch about a third of the Leitworte discerned by an expert, and introduces a high number of\r\nspurious candidates, particularly in Exodus. Valid Leitworte can be missed by the program either because they\r\nare never identified or because they are rejected as insignificant. Close inspection of the data reveals that only\r\n5 of Cassuto\u2019s Leitworte that were flagged by our algorithm fell below our tf-idf significance threshold. Most\r\ndid not meet the algorithm\u2019s criteria for being a sevenfold repetition. This may be because we were restricted\r\nto using lexemes while Cassuto primarily used roots or was more flexible about linguistic features, or because\r\nwe lacked his sharp boundaries of paragraph and story. Therefore, we may have inadvertently cut off our\r\n\u201Cpassages\u201D before the end of a scene, or disqualified a true Leitwort because it re-appeared in an unrelated\r\ncontext within our buffer zone. Further work can address some of these issues.\r\nIf, on the other hand, the objective algorithmic approach is considered the ideal, one can say that Cassuto\r\nobtained 33.9% precision (30.8% in Genesis, 37.5% in Exodus) and 19.6% recall (32.0% in Genesis, 15.6%\r\nin Exodus). This suggests he found about a fifth of possible Leitworte in his chosen text, and that about a third\r\nof his self-defined Leitworte are valid. Due to the limits of human attentional capacity, it would be practically\r\nimpossible for a person to manually identify all existing Leitworte in such a complex text. Humans can be\r\nbiased by their own interests to overlook many details, which can lead both to false positive and false negative\r\ndetection errors. Notably, Cassuto\u2019s list for Genesis, the text in which he first perceived Leitworte and which\r\nevoked tremendous enthusiasm for the task, has the highest recall and lowest precision compared against the\r\nalgorithm.\r\nThe truth probably lies somewhere between these extremes. Dismissing 80% of the algorithm\u2019s suggestions\r\nas invalid merely because Cassuto did not talk about them ascribes omniscience to the human expert, which is\r\nabsurd. Similarly, it is ridiculous to say that Cassuto\u2019s analysis is only meaningful if its tf-idf score falls above\r\nour program\u2019s cut-off. The low overlap between Cassuto\u2019s results and the algorithm\u2019s is evidence that the two\r\nmethodologies bring different perspectives and different strengths. Only a human being can explain the\r\nmeaning of a Leitwort in context and weave it into a consistent tapestry with other methods of literary analysis.\r\nHowever, the ability to systematically evaluate every instance, and to apply objective criteria undiluted by\r\npersonal bias, are core benefits of computerized Leitwort detection. The best use of such digital tools will be\r\nto allow merging of these two approaches by using algorithms before or after the human eye. Modern scholars\r\nof Biblical literature might use our program to systematically generate a list of repetitions to consider in their\r\nanalyses, or consult its quantitative information (e.g. tf-idf scores) to consider whether their initial impressions\r\nmight be distorted and in need of further scrutiny. Thus, objective methodology can become a thread woven\r\ninto the subjective tapestry.\r\n182"
	},
	{
		"id": 29,
		"title": "From copies to an original: the contribution of statistical methods",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Amanda Murphy",
			"Raffaella Zardoni",
			"Felicita Mornata"
		],
		"body": "1 Introduction\r\n1.1. The missing original veronica\r\nAccording to tradition, the veronica, the medieval relic conserved in St Peter\u2019s, was the sudarium St Veronica\r\noffered Christ, on which his face was imprinted. Despite vast documentation from 1200 to 1500,1 and the\r\nwitness of pilgrims (such as Dante and Petrarch) visiting the relic in Rome, and countless extant copies, the\r\nexact aspect of the veronica is unknown.2 There is great variance among its copies: e.g. Christ\u2019s face can be\r\ntransfigured, or show signs of suffering, be with\/without the crown of thorns, with open\/closed eyes.\r\nThe first systematic study of copies of the relic in order to find the original was the work of Karl Pearson\r\nin 1887.3 He compared literary and liturgical texts, and lined up images in chronological order, seeking reasons\r\nfor the lack of continuity between representations. Scholars from literature, history, history of art, and theology\r\nhave continued this research and the year 2000 saw a growth in interest in the topic;4 there is, however, still\r\nno definitive answer to the question of what the original medieval veronica looked like.\r\n1.2. Veronica Route project\r\nThe Veronica Route project5 (VR) joined this field of research in 2010, with the creation of an open, expanding,\r\ninterdisciplinary database of artistic and literary citations, ordering \u2018the infinite copies\u2019 of the image in an\r\nonline catalogue. VR holds 4500+ tagged objects, particularly from the Middle Ages.6 The classifications\r\nwere carried out manually, following a traditional methodological approach.7 The collected data was rendered\r\n1\r\n\r\nIn 1289 the Veronica was declared the most important relic in St Peter\u2019s (Coll. Bull. SS. Eccl. Vat., I, 214).\r\nThe image kept in St Peter\u2019s Basilica shows indistinct markings and has not yet been studied. We can only be certain of the size of\r\nthe medieval veronica (40 x 37 cm) thanks to the 14th century frame kept in the Vatican (Sturgis, 2000:75).\r\n3\r\nIt is a curious coincidence that Pearson worked out the statistical index of correlation, a starting point for several statistical\r\nmethods.\r\n4\r\nBelting (1990); G. Morello (1997); Hamburger (1998); Kessler, Wolf (1998); D'Onofrio (1999); Frugoni (1999); Morello, Wolf\r\n(2000); Di Blasio (2000); Burgio (2001); Di Fruscia (2013).\r\n5\r\nVeronica Route was presented at the conference \u2018The European Fortune of the Roman Veronica\u2019, Magdalene College, University of\r\nCambridge, April 2016.\r\n6\r\nThe works are signalled and sent in by volunteers together with the information found in loco: the sources are considered\r\ntrustworthy, unless an error of attribution or dating is easily demonstrable. Many works recorded in Veronica Route do not yet have\r\ncaptions as complete as those in museum catalogues. Although the VR database covers all centuries, the richest and most relevant\r\nperiod for the present purpose is pre-1600.\r\n7\r\nWith time, it will be interesting to be able to adopt automatic face analysis tools, which are not yet appropriate for the recognition of\r\niconographic characteristics, although they already work well in estimations of eye, nose and mouth positions, the degrees of different\r\nemotional expressions, etc. as shown in the Selfiecity project, classified as one of the most significant examples of \u201Cdistant viewing\u201D.\r\n2\r\n\r\n184\r\n\r\n\favailable through tags marking iconographic characteristics, the dimensions of time (dating) and space\r\n(geographical positioning), with a visualisation function of the results allowing maps of veronicas to be\r\nmanipulated.8 Thus classified according to >50 features, the images were turned into information. Data mining\r\nwith appropriate statistic methodologies on a statistically significant number of images has yielded the\r\ndefinition of significant models and new interpretative hypotheses\/research paths about the relic which \u2018for\r\nthree centuries, exerted such a great influence on the literary and artistic artefacts of our ancestors\u2019.9\r\n\r\n2 The variants of the iconographic subject and their geographical spread\r\nTo be able to identify the prototypes of the veronica, we used two different statistical tools, the index of\r\ntransversality and multivariate analysis which juxtapose and aggregate the various tagged features.\r\n2.1 Index of transversality\r\nThe recurrent iconographic characteristics, the subjects (St. Veronica, angels, sudarium, etc.), the various\r\ntransversal themes (Roman relic, Strozzi, St. Spirit, etc.) and the supports (painting, miniature, sculpture),\r\navailable for each veronica in the Veronica Route database, are all dichotomic variables (0-1\r\npresence\/absence).\r\nThe frequencies of the dichotomic variables to be compared through the centuries have to be normalised in\r\norder to make the data homogeneous. For this reason we identified the index of transversality in time, which\r\nwas calculated as follows:\r\n\uD835\uDC3C\"# =\r\n\r\n%\uD835\uDC39\uD835\uDC5F\uD835\uDC52\uD835\uDC5E\"# \u2044\uD835\uDC39\uD835\uDC5F\uD835\uDC52\uD835\uDC5E\" \u2211 # ,\r\n%\uD835\uDC39\uD835\uDC5F\uD835\uDC52\uD835\uDC5E# \u2044\uD835\uDC39\uD835\uDC5F\uD835\uDC52\uD835\uDC5E\u2211 # ,\r\n\r\n\uD835\uDC3E = \uD835\uDC50\u210E\uD835\uDC4E\uD835\uDC5F\uD835\uDC4E\uD835\uDC50\uD835\uDC61\uD835\uDC52\uD835\uDC5F\uD835\uDC56\uD835\uDC60\uD835\uDC61\uD835\uDC56\uD835\uDC50,\r\n\r\n\uD835\uDC46 = \uD835\uDC50\uD835\uDC52\uD835\uDC5B\uD835\uDC61\uD835\uDC62\uD835\uDC5F\uD835\uDC66\r\n\r\nFigure 1 shows that the graphic representation of some indices clearly show the strong coupling of DOUBLEPOINTED BEARD-OPEN EYES and the late appearance of the feature CLOSED EYES.\r\nclosed eyes\r\n\r\ndouble-pointed beard\r\n\r\nopen eyes\r\n\r\n350\r\n300\r\n250\r\n\r\ni\r\nn\r\nd\r\ne\r\nx\r\n\r\n200\r\n150\r\n100\r\n50\r\n0\r\n1200\r\n\r\n1300\r\n\r\n1400\r\n\r\n1500\r\n\r\n1600\r\n\r\ncentury\r\n\r\n1700\r\n\r\n1800\r\n\r\n1900\r\n\r\n2000\r\n\r\nFigure 1. Index of transversality\r\n\r\n8\r\n\r\nSoftware developed by Giacomo Aletti, professor of Probability and Statistical Mathematics, Dipartimento di Scienze e Politiche\r\nAmbientali, Universit\u00E0 degli Studi di Milano.\r\n9\r\n\u2018In this age when scholars attempt to trace the journey of a saga from India to Iceland, I hope that a description of the origin and\r\ndevelopment of a legend which has exercised such a huge influence on the literary and figurative works of our forebears over the\r\ncenturies will not be considered superfluous.\u2019 (Karl Pearson, Die Fronica, p. 94, our translation)\r\n185\r\n\r\n\f2.2 Multivariate analysis: K-Means Cluster Analysis\r\nFrom 1300 on, the considerable number of veronicas makes multivariate analyses on the available data\r\nmeaningful. Another fruitful approach to investigating the correlations between the features is to aggregate\r\nveronicas from the same time frame into homogeneous groups, using the methodology K-means Cluster.10\r\nThis algorithm considers each veronica like a point in a space of N dimensions (N = the available fields for\r\neach record). The value of each field is interpreted as distance from the origin along the corresponding spatial\r\naxis. In the K-means methods the original choice of a value for K determines the number of clusters that will\r\nbe found. The analyst experiments with different values of K and each set of clusters is then evaluated: the one\r\nwhich shows the clearest interpretation of the data is chosen. It is an iterative process, which begins by\r\nidentifying K points as seeds, and continues by aggregating all the other records, assigning each record to the\r\nclosest centroid cluster. After this process the new cluster centroids are calculated, and, according to the\r\nproximity rule, the cluster to which each point belongs is recalculated. This iterative process ends when the\r\ncluster boundaries stabilize. Once the clusters have been defined, the results can be interpreted.\r\nIn order to describe the elements shared by the veronicas belonging to the same cluster, the matrix of the\r\n\"final centroids\" must be analysed, which, being dichotomous variables, quantifies the influence of each\r\nvariable within the cluster. Therefore, Final Cluster Center \u00E81 corresponds to the predominance of that\r\nvariable in that particular cluster, and vice versa.\r\n2.3 The fourteenth century\r\n\r\n10\r\n\r\n1\r\n\r\n2\r\n\r\n3\r\n\r\nwithout crown\r\n\r\n0,937\r\n\r\n0,922\r\n\r\n0,897\r\n\r\nopen eyes\r\n\r\n0,984\r\n\r\n0,882\r\n\r\n0,759\r\n\r\ntransfigured\r\n\r\n0,921\r\n\r\n0,745\r\n\r\n0,655\r\n\r\nSt.Veronica\r\n\r\n0,000\r\n\r\n0,725\r\n\r\n0,000\r\n\r\ncruciform halo\r\n\r\n0,556\r\n\r\n0,667\r\n\r\n0,379\r\n\r\ndouble-pointed beard\r\n\r\n0,444\r\n\r\n0,353\r\n\r\n0,448\r\n\r\nhead of Christ\r\n\r\n0,032\r\n\r\n0,196\r\n\r\n0,000\r\n\r\ncut out\r\n\r\n0,492\r\n\r\n0,039\r\n\r\n0,138\r\n\r\nsudarium\r\n\r\n0,508\r\n\r\n0,020\r\n\r\n0,000\r\n\r\ndark face\r\n\r\n0,048\r\n\r\n0,020\r\n\r\n0,000\r\n\r\nfleury cross\r\n\r\n0,032\r\n\r\n0,020\r\n\r\n0,034\r\n\r\narma Christi\r\n\r\n0,032\r\n\r\n0,020\r\n\r\n0,000\r\n\r\nsuffering\r\n\r\n0,000\r\n\r\n0,020\r\n\r\n0,069\r\n\r\ntransparent veil\r\n\r\n0,000\r\n\r\n0,020\r\n\r\n0,000\r\n\r\nascent to Calvary\/Calvary\r\n\r\n0,000\r\n\r\n0,020\r\n\r\n0,000\r\n\r\nopen mouth\/visible teeth\r\n\r\n0,063\r\n\r\n0,000\r\n\r\n0,069\r\n\r\ndark veil\r\n\r\n0,048\r\n\r\n0,000\r\n\r\n0,000\r\n\r\nSts.Peter and Paul\r\n\r\n0,048\r\n\r\n0,000\r\n\r\n0,000\r\n\r\nimago pietatis\r\n\r\n0,048\r\n\r\n0,000\r\n\r\n0,000\r\n\r\ngreen crown\r\n\r\n0,016\r\n\r\n0,000\r\n\r\n0,000\r\n\r\ncrown of thorns\r\n\r\n0,000\r\n\r\n0,000\r\n\r\n0,069\r\n\r\nblank veil\r\n\r\n0,000\r\n\r\n0,000\r\n\r\n0,000\r\n\r\ntriple veil\r\n\r\n0,000\r\n\r\n0,000\r\n\r\n0,000\r\n\r\nfolds\r\n\r\n0,000\r\n\r\n0,000\r\n\r\n0,000\r\n\r\nmonochrome\r\n\r\n0,000\r\n\r\n0,000\r\n\r\n0,000\r\n\r\nangel\/s\r\n\r\n0,000\r\n\r\n0,000\r\n\r\n0,862\r\n\r\nMass of Saint Gregory\r\n\r\n0,000\r\n\r\n0,000\r\n\r\n0,000\r\n\r\nImplemented in the software SPSS Statistics (Analyze \/ Classify \/ K-Means Cluster procedure).\r\n186\r\n\r\n\fway of the Cross\r\nFrequency of veronicas (143)\r\n\r\n0,000\r\n63\r\n\r\n0,000\r\n51\r\n\r\n0,000\r\n29\r\n\r\nFigure 2. Clusters in the fourteenth century\r\nThe works in the 1300s tagged with ROMAN VERONICA11 can be analysed in three clusters (excluding those with\r\nthe tags BADGE and TEXT). In Figure 2, the predominant characteristics are in yellow, the absent ones in green.\r\nIn this period all the veronicas are characterised, homogeneously, by the serene face of Christ, without a trace\r\nof suffering. Differences are found in the subjects showing the veronica: cluster 1 includes almost all the cases\r\nof the sudarium alone (with the tags CUT OUT and DARK FACE); cluster 2 aggregates the figure of St Veronica\r\n(particularly in Lombardy where we can find one of the first pictorial representations of the saint);12 cluster 3\r\naggregates angels holding up the sudarium, positioned throughout eastern and central Europe.\r\n2.4 The Fifteenth century\r\nThe 1400s are the most popular century for the image13 (with 1122 works compared to 264 in the 1300s); in\r\nthe second half of the century, veronicas appear with Christ\u2019s face bearing signs of suffering and drops of\r\nblood, and with the iconographic subject of the ascent to Calvary14. The considerable number of works and\r\ntheir variations in this century determined the choice to analyse the data in four clusters, in the interest of the\r\nbest interpretation of the dominant characteristics in the variants. The 709 works tagged with ROMAN\r\nVERONICA, (excluding those tagged BADGE and TEXT) were thus aggregated: in Cluster 1 (234 veronicas,\r\ncoloured in light blue in Figure 3) we find the transfigured face of Christ; in Cluster 2 (139 veronicas, dark\r\nblue) St Veronica; in Cluster 3 (166 veronicas, red) the new features of the Passion; and Cluster 4 (170\r\nveronicas, orange) the sudarium.\r\nFigure 3 displays the distribution of the works across Europe: we see a dominance of the figure of St\r\nVeronica with the transfigured face of Christ in France, where St Veronica is considered the evangeliser of the\r\nregion,15 and in Flanders (where she is the patron saint of linen and cloth merchants); the first veronicas with\r\nsigns of suffering appear in all European countries, and it becomes slightly prevalent as a characteristic in\r\nGermany, while in England there is a prevalence of the sudarium as the iconographic subject.\r\n\r\n11\r\n\r\nThe tag ROMAN VERONICA is used when the veronica is the only subject or the main subject, and not when the veronica is part of\r\nanother work, such as Arma Christi, Madonna of the Seven Sorrows, and such like.\r\n12\r\nStefano Candiani, The iconography of Veronica in the Lombardy region, late XIII-early XV centuries, in A. Murphy et al. Ed., The\r\nEuropean Fortune of the Roman Veronica in the Middle Ages, Convivium Supplementum 2017, Turnhout: Brepols. p. 264.\r\n13\r\n\u201CFrom the 14th century, wherever the Roman Church went, the veronica would go with it\u201D (Neil MacGregor and Erika\r\nLangmuir, 2000, p.92)\r\n14\r\nThe moment in which the veil was imprinted was not initially linked to the ascent to Calvary, but to Jesus\u2019 public life.\r\n15\r\nHer tomb is still preserved at Saint Seurin (Bordeaux), on the pilgrims\u2019 way to Santiago di Compostela.\r\n187\r\n\r\n\fFigure 3. 1400s - distribution of the clusters\r\n2.5 The Sixteenth century\r\nThere are 1075 works from the 1500s in Veronica Route. In a 4-cluster analysis of the 948 works, we find the\r\nSUDARIUM in cluster 1 (165 veronicas, including those characterised by DARK FACE and CUT OUT); cluster 2,\r\nSAINT VERONICA (always dominant in France), 295 veronicas; cluster 3, 281, SIGNS OF SUFFERING (a feature\r\nwhich becomes dominant in Italy); and in cluster 4, with 207 veronicas, the ASCENT TO CALVARY and BLANK\r\nVEIL. This feature, meaning that the face of Christ imprinted on the cloth is no longer visible, seems to shift\r\n\r\nattention away from the relic kept in Rome and onto the woman\u2019s pious gesture. In actual fact, the Protestant\r\nReform and the Sack of Rome of 1527 interrupted \u2013 for various reasons \u2013 the history of the Roman relic.\r\n\r\n3 Research on the Roman relic: validation\r\nThe last investigation concerns the relic kept in St Peter\u2019s, of which there are no photographic reproductions\r\nand which has never been an object of study.\r\nIn Veronica Route the tag ROMAN RELIC is assigned when the historical sources of the work refer directly\r\nto the relic. This feature is present only in 3.4% of the 1081 veronicas that are catalogued up to the end of the\r\n1400s (excluding the veronicas with the tags TEXT and BADGE). These are decidedly small numbers for making\r\na predictive analysis of the characteristics of the ROMAN RELIC.\r\nThe investigation therefore proceeded by evaluating the concentrations of the tag ROMAN RELIC in the\r\nother features and ordering them according to decreasing values. In the graph, the average concentration of\r\n2% - index 100 \u2013 is indicated by the blue line and the characteristics with a higher index are those that\r\ncorrespond most to the ROMAN RELIC; the clear emergence of the features CUT OUT and DARK FACE can be\r\nseen, whereas the features DOUBLE-POINTED BEARD - CROWN OF THORNS - SUFFERING, positioned underneath\r\nthe blue line are clearly separate from the Roman relic.\r\n\r\n188\r\n\r\n\f500\r\n\r\n<=1400\r\n\r\n400\r\n\r\ni\r\nn\r\nd\r\ne\r\nx\r\n\r\n300\r\n200\r\n100\r\n\r\nin\r\ng\r\nfe\r\nr\r\nsu\r\nf\r\n\r\nth\r\nor\r\nof\r\nn\r\now\r\ncr\r\n\r\ndo\r\nub\r\nle\r\n-\r\n\r\npo\r\nin\r\nte\r\nd\r\n\r\nbe\r\na\r\n\r\ney\r\nop\r\nen\r\n\r\nns\r\n\r\nrd\r\n\r\nes\r\n\r\nn\r\now\r\ncr\r\nith\r\nou\r\nt\r\nw\r\n\r\nns\r\nfig\r\nur\r\ned\r\n\r\nrm\r\nuc\r\nifo\r\ncr\r\n\r\ntra\r\n\r\nha\r\n\r\nlo\r\n\r\nce\r\nfa\r\nrk\r\nda\r\n\r\ncu\r\n\r\nto\r\n\r\nut\r\n\r\n0\r\n\r\nFigure 4. Concentration of the ROMAN RELIC\r\nCUT OUT and DARK FACE are the features characterising the Mandylion in the Vatican (Figure 5),16 a\r\nwork likened to the medieval relic.17 Identifying the Vatican Mandylion with the medieval Veronica would\r\nnot be contradicted by the data, but the proportion of works tagged as CUT OUT and DARK FACE compared to\r\nthe total works in the database (5 in 1200, 3 in 1300, 53 in 1400) and the late spread of the iconography,\r\nsuggest the need for further research on this.\r\n\r\nFigure 5. Left, the Vatican Mandylion, Lipsanoteca of the Pontifical Palaces, Vatican, next to works tagged\r\nCUT OUT and DARK FACE: Veronica d\u2019oro, 1368 ca. Prague, Cathedral Treasury; Santa Veronica col velo tra\r\ni SS. Pietro e Paolo, 1430, altar Santa Maria del Monastero, Manta.\r\n\r\nConclusions\r\nFirstly, the Veronica Route project intends to continue investigating the origins of the medieval relic\u2019s\r\niconography. Secondly, we intend to break up the temporal arches (linked so far to centuries) so as to align\r\nthem better with historical events, such as the Holy Years, in order to investigate the origins and development\r\nof the relic variations more precisely. Lastly, an exploration of the features which do not seem to derive from\r\nthe Roman relic, but which have nevertheless become highly famous, would be an interesting new direction.\r\n\r\n16\r\n\r\nThe Mandylion was once considered the most ancient reproduction of the Face of Christ, even though it is documented in Rome\r\nonly from 1517. Until 1870 it was kept in the Church of St Sylvester the First (San Silvestro in Capite), and is now kept in the\r\nVatican.\r\n17\r\nG. Morello (2012) p. 78.\r\n189"
	},
	{
		"id": 30,
		"title": "FORMAL. Mapping Fountains over time and place. Mappare il movimento delle fontane monumentali nel tempo e nello spazio attraverso la geovisualizzazione",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Pamela Palomba",
			"Emanuele Garzia",
			"Roberto Montanari"
		],
		"body": "1 Introduzione\r\nL\u2019esistenza umana si organizza e pensa se stessa anche in termini spaziali, \u00E8 nel suo rapporto con lo\r\nspazio che l\u2019individuo struttura la sua esperienza, in maniera situata (Merleau-Ponty, 1965), tanto che si pu\u00F2\r\na buon diritto affermare che l\u2019esistenza \u00E8 spaziale e lo spazio \u00E8 esistenziale. Lo spazio in tal modo, inteso\r\nin senso antropologico, cessa di essere solo un\u2019astrazione geometrica e definisce il campo di studi oggetto\r\ndelle spatial humanities. L\u2019interesse crescente per lo spazio negli studi storiografici \u00E8 uno degli aspetti\r\ndella svolta culturalista (Torre, 2008) dell\u2019ultimo quarto del XX sec., che, con l\u2019abbandono del concetto di\r\nspazio assoluto del sistema cartesiano in favore di un concetto di spazio relativo, ha comportato una svolta\r\nche viene definita spatial turn ossia il passaggio a un sistema tolemaico che distingue tra spazio assoluto e\r\nrelativo, tra geografia e corografia (Cosgrove, 2003). La prima, con le sue capacit\u00E0 descrittive basate sulla\r\nmatematica e sul rilievo scientifico, e la seconda, imperniata su un\u2019impostazione di tipo pi\u00F9 visuale e\r\nletterario, ma in grado di connettere per\u00F2 la dimensione storica a quella geografica. In sostanza si\r\ntratta di una svolta verso un\u2019interpretazione simbolica del paesaggio che tenga conto della sua doppia\r\nanima di spazio naturale e culturale: quella che si riferisce a processi naturali e sociali, e quella che\r\ncorrisponde alle conseguenze delle azioni umane che lo trasformano. Lo spazio in questo senso da entit\u00E0\r\ngeometrica astratta si trasforma in luogo, ossia l\u2019espressione peculiare di uno spazio geografico.\r\n\r\n191\r\n\r\n\fLa \u2018peculiare\u2019 natura del luogo lo definisce come l\u2019ordine secondo il quale diversi elementi vengono\r\ndistribuiti entro rapporti di coesistenza, gli uni affianco agli altri; un luogo \u00E8 dunque una configurazione di\r\nposizioni, una indicazione di stabilit\u00E0. Al contrario lo spazio \u00E8 un incrocio di entit\u00E0 mobili, \u00E8 in qualche modo\r\nil prodotto dell\u2019insieme dei movimenti che si verificano al suo interno e lo animano, orientandolo, rendendolo\r\ncontingente tanto da tramutarlo in unit\u00E0 polivalente di programmi conflittuali o di prossimit\u00E0 contrattuali (de\r\nCerteau, 2001). Lo spazio non \u00E8 il semplice scenario dell\u2019azione storica, ma un prodotto significativo e\r\ndeterminante di cambiamento. Ne deriva che un movimento produce uno spazio e ne traccia la storia. Ogni\r\nnarrazione che si rispetti si struttura lungo una sequenza di eventi producendo quella che pu\u00F2 essere chiamata\r\n\u2018la forma del tempo\u2019 e privilegiando cos\u00EC la componente cronologica, ma tutte le narrazioni implicano un\r\nmondo di estensione spaziale. Alcuni teorici infatti riconoscono un pi\u00F9 che evidente collegamento tra spazio e\r\ntempo, come si evince dalla concezione di cronotopo di Mikhail Bakhtin (1981) intesa come \u00ABl\u2019intrinseca\r\nconnessione di relazioni spaziali e temporali\u00BB, con il tempo che fornisce la quarta dimensione dello spazio. In\r\nquesta prospettiva la narrazione \u00E8 immaginabile come \u00ABla rappresentazione del movimento all\u2019interno delle\r\ncoordinate di spazio e tempo\u00BB, con gli eventi marcati dall\u2019intersezione di assi orizzontali e verticali in un\r\nintreccio dinamico tra superficie e profondit\u00E0 (Bakhtin, 1981).\r\nSulla base di queste premesse, con il presente lavoro di ricerca si \u00E8 inteso rilevare nella catalogazione e\r\nvisualizzazione della distribuzione e del movimento delle fontane storiche di Napoli le condizioni\r\nnarratologiche per produrre una narrazione spaziale attraverso la geo-visualizzazione, un ramo della Gis\r\nScience che sviluppa tecniche e strumenti disegnati per rendere visuali dei fenomeni spaziali (Craine and\r\nAitken, 2009).\r\nL\u2019operazione di traduzione dei concetti di spazio e luogo in quelli rispettivamente di mappa e itinerario,\r\nintesi come linguaggi simbolici e antropologici dello spazio e come i due poli dell\u2019esperienza (de Certeau,\r\n2001), ha condotto alla messa in scena della loro interazione, attraverso l\u2019impiego di una piattaforma di\r\ndatabase management system come nodegoat1, in grado di produrre uno scenario in cui i luoghi figurano come\r\n\u2018OGGETTI\u2019 della mappa, e gli itinerari come tracciati degli spostamenti delle fontane nel tempo e nello\r\nspazio.\r\nL\u2019operazione effettuata consente cos\u00EC di arrivare a una pi\u00F9 profonda comprensione dei contesti spaziali in\r\ncui i beni culturali sono inseriti come testimonianze vive e multilivello, capaci di raccontare una storia\r\nstratificata sia in senso cronologico che spaziale, in cui possano essere rilevabili quindi come parti in relazione\r\ncomplessa con i luoghi ai quali conferiscono valore di civilt\u00E0.\r\nPer comprendere e diffondere queste informazioni con contenuti multidisciplinari il data storytelling si\r\nrivela uno strumento efficace nella divulgazione di fenomeni complessi. Per trasformare questi dati in\r\ninformazioni utili alla conoscenza \u00E8 necessaria la visualizzazione cos\u00EC come il racconto attraverso una storia.\r\nAlcuni strumenti come penne, mappe e calcolatrici possono essere considerati artefatti cognitivi che\r\nmigliorano la nostra conoscenza, amplificando i processi cognitivi coinvolti nell\u2019interazione con le\r\nrappresentazioni tipiche del mondo esterno. Nell\u2019Information Visualisation un ruolo fondamentale \u00E8 dato dal\r\ndata storytelling. Gli esseri umani hanno sempre utilizzato le storie per trasmettere informazioni, valori\r\nculturali ed esperienze attraverso mezzi tecnologici che si sono evoluti nel tempo (si pensi alla scrittura, alla\r\nstampa e oggi ai computer). Una buona narrazione trasmette una mole di informazioni in un formato facilmente\r\nassimilabile dal fruitore o dallo spettatore (Segel and Heer, 2010). La visualizzazione delle informazioni\r\nattraverso i dati, combinata con un adeguato storytelling, permette di produrre rappresentazioni visive\r\nmolteplici. Ogni visualizzazione pu\u00F2 essere utilizzata per raccontare una storia e le diverse modalit\u00E0 sono\r\nfunzionali ai differenti tipi di storia.\r\nProprio per queste loro caratteristiche, le tecniche di visualizzazione dimostrano di essere un valido\r\nstrumento nelle mani dello studioso interessato a valorizzare il patrimonio informativo collegato ai rapporti fra\r\nla citt\u00E0, la sua evoluzione e i beni culturali.\r\n\r\n1\r\n\r\nPim van Bree and Geert Kessels. 2013. nodegoat: a web-based data management, network analysis & visualisation\r\n\r\nenvironment, http:\/\/nodegoat.net from LAB1100, http:\/\/lab1100.com\r\n192\r\n\r\n\f2 Caso d\u2019uso: FORMAL, Mapping fountains over time and place.\r\nIl nome del progetto, Formal, oltre ad essere un acronimo delle parole che lo specificano (Mapping\r\nFountains OveR tiMe And pLace), si ispira al termine \u2018formale\u2019 che indica uno dei segmenti di cui si\r\ncompone la rete degli acquedotti ed \u00E8 dunque anche usato per definire un canale principale di alimentazione\r\ndelle fontane pubbliche. Testimoni di eccezione delle trasformazioni politiche, sociali e urbanistiche della\r\ncitt\u00E0, le fontane pubbliche di Napoli hanno una storia complessa, fatta di traslazioni, mutilazioni, cambiamenti\r\nche si \u00E8 tentato di restituire in forma semplessa grazie all\u2019impiego di nodegoat, un DBMS web-based in grado\r\ndi processare, analizzare e visualizzare dataset complessi in modalit\u00E0 relazionale, diacronica e spaziale.\r\nLa prima fase del progetto \u00E8 consistita nel reperimento di tutti i dati della ricerca, di tipo bibliografico,\r\ncartografico e iconografico, necessari alla compilazione del database. Successivamente si \u00E8 passati alla\r\ncostruzione del modello dei dati (data modelling) , dapprima a livello concettuale attraverso la creazione del\r\ndataset e sulla base delle esigenze della domanda di ricerca e, in seguito, a livello logico tramite l\u2019inserimento\r\ndei dati nelle schede di struttura approntate. La piattaforma nodegoat ha consentito la creazione di un database\r\ncompletamente personalizzato da parte di un utente non esperto e perfettamente rispondente alle esigenze di\r\nricerca. Il data modelling nell'ambito umanistico \u00E8 largamente percepito come un processo epistemologico,\r\npiuttosto che come un processo ontologico. L'interfaccia dell'applicazione del database pu\u00F2 far nascere infatti\r\nnuove opportunit\u00E0 o creare sfide ulteriori.\r\nLa prima operazione necessaria \u00E8 stata la definizione dei type principali di consultazione; il type principale\r\nconsente di leggere nella scheda di ciascun oggetto (object) a esso riferito tutte le informazioni anagrafiche,\r\ngeografiche e storiche (Figura 1). Sono state inserite 28 fontane storiche pubbliche, che coprono un arco\r\ncronologico che va dal XVI sec. al XXI sec., 14 di esse hanno subito almeno uno spostamento, con il caso\r\neclatante della fontana del Nettuno che ha avuto ben otto trasferimenti (cfr. Figura 3).\r\nFigura 1. Scheda dell'object Fontana della Sirena\r\n\r\nGli obiettivi sono stati essenzialmente due: da un lato catalogare e ordinare il materiale di studio raccolto\r\ne sistematizzarlo attraverso la produzione di schede anagrafiche e multimediali che descrivessero le\r\ncaratteristiche di ciascuna fontana (object); dall\u2019altro ottenere per ciascun object la visualizzazione geografica\r\ndella sua posizione nello spazio e nel tempo con il tracciamento dello spostamento da un luogo all\u2019altro nelle\r\n\r\n193\r\n\r\n\fdiverse epoche storiche (sub-object). Quest\u2019ultima operazione \u00E8 consistita in particolare nell\u2019approntare\r\nl\u2019apparato di coordinate leggibili attraverso la \u2018Geographical Visualisation\u2019 (Figura 2).\r\n\r\nFigura 2. Geographical visualisation degli spostamenti della Fontana Medina\r\nPer quanto riguarda la collocazione si \u00E8 scelto di visualizzare sulla mappa, attraverso l'uso di punti e linee,\r\nla posizione e il percorso compiuto dalla fontana in caso di diverse collocazioni, usando il sistema di coordinate\r\ngeografiche sia della collocazione originaria che delle collocazioni successive per georeferenziare il punto di\r\ninteresse (point of interest). Nel caso di luogo non pi\u00F9 esistente si \u00E8 fatto ricorso alla cartografia storica per\r\nidentificare il punto corrispondente da georeferenziare. Le coordinate geografiche del punto di interesse, cos\u00EC\r\ncome le corrispondenti datazioni di collocazione e\/o spostamento, sono inserite usando la sezione sub-object\r\ndella relativa scheda dell\u2019object di ciascuna fontana (Figura 3).\r\n\r\nFigura 3. Sezione sub-object con gli spostamenti della Fontana del Nettuno\r\n\r\nCiascun type \u00E8 collegato in maniera incrociata (cross-referencing) agli altri type rilevanti, consentendo\r\ncos\u00EC l\u2019esplorazione di una serie di relazioni (network analysis) tra i dati che ha rivelato la sua efficacia dal\r\npunto di vista della ricerca, come da quello del possibile impiego dei dati stessi per la costruzione di narrazioni.\r\nNel primo caso infatti \u00E8 stato possibile analizzare il dataset in base alle diverse domande di ricerca, ad esempio\r\nattraverso il filtro del personaggio storico (committente, artista), piuttosto che per cronologia, per elemento\r\ndecorativo ricorrente o per toponimo.\r\nRelativamente invece alla potenzialit\u00E0 narratologica \u00E8 stato previsto il type \u2018Story\u2019 che contiene una\r\nselezione di brani tratti dalla letteratura periegetica del XVII sec., nello specifico le Notitie del bello, dell\u2019antico\r\ne del curioso della citt\u00E0 di Napoli per i signori forastieri date dal canonico Carlo Celano napoletano, divise\r\nin dieci giornate di Carlo Celano, che ci informano sulla conformazione urbanistica della citt\u00E0 nonch\u00E9 sulla\r\ndescrizione delle fontane pubbliche della Napoli del 1692. Impiegando la network analysis per indagare il\r\ncampo Story \u00E8 possibile avere un quadro chiaro e immediato di tutte le fontane esaminate dal cronista nel\r\n194\r\n\r\n\fcorso della specifica giornata e trarne considerazioni spazialmente orientate in merito alla fonte impiegata. A\r\nmero titolo di esempio si pu\u00F2 rilevare che grazie alla network analysis \u00E8 stato possibile ragionare sugli\r\nspostamenti programmati dal cronista per la stesura del testo letterario della singola giornata: attraverso la\r\nvisualizzazione dei dati collegati in maniera relazionale appare chiaro il percorso spaziale che struttura la\r\nnarrazione e nello stesso tempo pianifica e suggerisce itinerari al destinatario (quante e quali fontane sono\r\ndescritte da Celano nella Giornata V con schema delle relazioni topografiche e topologiche che le legano, cfr.\r\nFigura 4).\r\n\r\nFigura 4. Network analysis del campo Story\r\n\r\nAppare chiaro che un tale uso delle fonti spazialmente strutturato, relazionale e incrociato rende in questo\r\ncaso le fontane un possibile espediente, dal punto di vista narrativo storicamente delineato, per costruire storie\r\nche si avvalgano dei documenti, digitali e non, per generare contesti narrativi scientificamente validi.\r\nNell\u2019ottica del riuso creativo delle fonti digitalizzate e open access si \u00E8 fatto ricorso ad esempio all\u2019edizione\r\ndigitale curata dalla Fondazione Memofonte alla quale si rimanda, all\u2019interno della singola scheda, in maniera\r\ncontestuale al luogo del testo. Allo stesso modo si \u00E8 fatto ricorso allo stradario ufficiale del Comune di Napoli\r\nintegrando nel DBMS il dataset con licenza IODL (Italian Open Data License) per effettuare il collegamento\r\ngeoreferenziato con l\u2019attuale mappa cittadina.\r\nInfine si \u00E8 prestata particolare attenzione alla scelta delle opzioni di visualizzazione dello \u2018Scenario\u2019 di\r\ngeovisualizzazione allo scopo di creare un data storytelling efficace dal punto di vista della fruizione per la\r\nfutura pubblicazione su web. Sulla base della modellizzazione proposta da Segel e Heer (Segel and Heer, 2010)\r\nsi \u00E8 optato per un approccio che prevede una posizione di controllo dei contenuti erogati (Author-driven\r\napproach), ideale per lo storytelling e la comunicazione di contenuti educativi, temperandolo con uno di tipo\r\npi\u00F9 interattivo (Reader-driven approach) che consentir\u00E0 all'utente di esplorare lo scenario, interrogandolo in\r\nbase a diverse chiavi di ricerca con la possibilit\u00E0 di produrre forme pi\u00F9 complesse e personalizzate di analisi.\r\n\r\n3 Sviluppi futuri\r\nLa fase successiva del progetto prevede l\u2019implementazione di un\u2019interfaccia pubblica per la fruizione web\r\ncon la produzione di scenari dedicati in base alla fascia di utenza e l\u2019approfondimento dell\u2019indagine tramite la\r\nnetwork analysis e il data storytelling per la scelta di una narrazione da visualizzare, valorizzare e divulgare.\r\nSar\u00E0 inoltre analizzato e geovisualizzato un secondo dataset relativo alle fontane scomparse e alla distribuzione\r\ndelle acque affioranti cittadine, che in parte le alimentavano, rintracciate attraverso le fonti storiche.\r\n\r\n4 Conclusioni\r\nCi\u00F2 che di interessante emerge dall\u2019interazione tra la mappa e l\u2019itinerario \u00E8 una dimensione spaziale\r\nnarrativa in cui la mappa giustifica l\u2019itinerario e l\u2019itinerario definisce la mappa come spazio geografico e\r\nculturale insieme, non dunque solo una carta geografica, ma anche un libro di storia. Abbiamo infatti da un\r\nlato la mappa, che ha una funzione topica ossia di definitore di luoghi e, dall\u2019altro, un racconto fatto di\r\n195\r\n\r\n\fspostamenti ossia topologico, relativo alla deformazione delle figure. La metodologia usata aiuta a preservare\r\ne presentare le complessit\u00E0 che sono insite nelle fonti storiche e nel loro impiego incrociato attraverso il deep\r\nmapping. La visualizzazione e il data storytelling invece si configura come un\u2019interfaccia interattiva utile per\r\nla ricerca nelle digital humanities, in particolare nel cultural heritage per la storia della citt\u00E0 e delle sue\r\ntrasformazioni, con risvolti interessanti anche per un impiego a servizio della fruizione dei beni culturali e\r\ndelle imprese creative.\r\nLo strumento scelto ci consente di esplorare contemporaneamente le due dimensioni, topica e topologica, in\r\nsenso sincronico (es. analizzare e visualizzare quali e quante fontane nello stesso secolo) e diacronico (es.\r\ntappe del loro percorso all\u2019interno della citt\u00E0 nel corso del tempo). Tra queste due determinazioni vi sono dei\r\npassaggi che portano alla conclusione che i racconti spaziali effettuano un lavoro che trasforma i luoghi in\r\nspazi o gli spazi in luoghi, con un\u2019azione creativa e performativa, delimitando con le loro attivit\u00E0 su di essi una\r\nscena da narrare che ancora vive."
	},
	{
		"id": 31,
		"title": "Paul is dead? Differences and similarities before and after Paul McCartney‚Äôs supposed death. Stylometric analysis of transcribed interviews",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Antonio Pascucci",
			"Raffaele Manna",
			"Vincenzo Masucci",
			"Johanna Monti"
		],
		"body": "Introduction\r\n\r\nPaul McCartney\u2019s supposed death (dated 9th November 1966 because of a car accident) represents a\r\nlegend which does not belong to the music business only but embraces other worlds, both for Paul\r\nMcCartney\u2019s fame given by Beatles\u2019 everlasting success, and because of many stories born around this\r\nepisode. Paul is dead (PID) theory represents one of the most controversial legends in the history of\r\nmusic, enough to be still debated after more than half a century, during which numerous stories are\r\nborn, some feeding and some other damping its truth. In this paper, we show the results of a stylometric\r\nanalysis conducted on Paul McCartney\u2019s interview transcriptions using three different approaches in order\r\nto detect differences and similarities in his speeches before and after 9th November 1966. Our research\r\nis based on:\r\n\u2022 the Beatles Interviews Database (http:\/\/www.beatlesinterviews.org\/), a collection\r\nof one-hundred sixty-three interviews from 1962 to 1984;\r\n\u2022 YouTube subtitles, which we manually corrected, if necessary, listening to the audio.\r\nTo the best of our knowledge, this research represents one of the very first stylometric analyses on\r\ninterview transcriptions. In this paper, we also present the Let IT Corpus (Paul McCartney\u2019s Interview\r\nTranscriptions), composed of fifty-two documents concerning interviews before 9th November 1966\r\nand fifty-two documents concerning interviews after 9th November 1966. Let IT Corpus is still in its\r\nembryonic stage: we foresee to expand it with further texts so that it can be used for more accurate\r\nanalyses in the near future.\r\n\r\n197\r\n\r\n\fThe strongest supporters of PID theory claim that immediately after his death, Paul McCartney has\r\nbeen replaced by a lookalike. There are several theories that even today sustain the veracity of PID,\r\nmany of which spread by the Beatles themselves, who sometimes enjoyed including subliminal messages\r\nin their songs. For example the celebrated John Lennon\u2019s whisper in the song I\u2019m so tired, if listened\r\nbackwards, seems to say Paul is dead, man: miss him! miss him! miss him!. The Abbey Road\u2019s album\r\ncover also shows at least ten references to Paul McCartney\u2019s death. On the other hand, some theories\r\nremove all doubts, claiming that Paul McCartney never died and all PID hypotheses are nothing but a\r\nbusiness choice, which has contributed to add extra charm to Beatles\u2019 success. In October 1969, the\r\nBeatles\u2019 press office categorically denied PID rumours, labelling them as a load of old rubbish. PID\r\ntheory has been investigated in literature (Cartocci, 2005) and in automatic-recognition (Holland et al.,\r\n2014). The present contribution is organized as follows: in Section 2 we show Related Work. The Let\r\nIT Corpus is described in Section 3, in Section 4 we describe three different approaches adopted in the\r\nanalysis and their results. In Section 5 the stylistic differences and similarities detected by the linguistic\r\nanalysis are thoroughly discussed. Conclusions are in Section 6. In Section 7 we introduce Future Work.\r\n\r\n2\r\n\r\nRelated Work\r\n\r\nCS is the statistical analysis of writing style (Zheng et al., 2006) and it is used to identify or profile the\r\nauthor of a text. The main assumption of Authorship Attribution (AA) is that each author operates choices\r\nwhich are influenced by sociological (age, gender and education level) and psychological (personality,\r\nmental health and being a native speaker or not) factors (Daelemans, 2013) which determine a unique\r\nwriting style. In AA some studies are being conducted on speech transcriptions. In 2014 (Herz and\r\nBellaachia, 2014) investigated the authorship of Barack Obama\u2019s speechwriters on a corpus composed\r\nby thirty-seven speech transcriptions. They based their research on the supposition that Barack Obama\r\nhas four principal speechwriters and deal with the AA of Barack Obama\u2019s speeches with four different\r\napproaches, that reached different results, but still showing that CS can be used to differentiate authors\r\nwho write in a similar style. (Airoldi et al., 2006) conducted a similar research on Ronald Regan\u2019s\r\nradio speeches. The corpus they used for their investigation is composed of a thousand thirty-two radio\r\naddresses delivered by Ronald Reagan between 1975 and 1979. The scholars focused the experiment on\r\nthree-hundred twelve radio addresses for which no direct AA evidence is available, and they concluded\r\nthat in 1975, Ronald Reagan drafted seventy-seven speeches and his collaborators drafted seventy-one,\r\nwhereas over the years 1976-1979, Ronald Reagan drafted ninety speeches and his collaborator Peter\r\nHannaford drafted seventy-four speeches. The study of (Herz and Bellaachia, 2014) and that of (Airoldi\r\net al., 2006) share a problem: it is not possible to know the accuracy of the AA results of their study.\r\nCS is also useful in studying changes in the style of an author over time. As argued by (Rybicki, 2015)\r\ntime is one of the most significant factors for the evolution of the literary lexicon. With this in mind,\r\nsome researches are conducted on stylochronometry (for a survey, see (Stamou, 2007)), namely the study\r\nof the change of style correlated to the passing of time. (Forsyth, 1999) differentiates the style of the poet\r\nWilliam Butler Yeats between younger Yeats and older Yeats, devising along the way a measurement he\r\ncalls a youthful Yeatsian Index. (Van Hulle and Kestemont, 2016) use sylometry to periodize Samuel\r\nBeckett\u2019s works, finding stylistically innovative change in his late style. Lastly, the findings of (Evans,\r\n2018) show that the dramatic style of Aphra Behn over the course of her 20-year career, can be divided\r\nin three different phases. Obviously we must keep in mind that our analysis is based on transcription of\r\nspeeches, and therefore not on written texts. Until now, to the best of our knowledge, no stylistic research\r\nanalysis has been carried out to detect differences and similarities in interview transcriptions before and\r\nafter Paul McCartney\u2019s supposed death.\r\n\r\n3\r\n\r\nLet IT Corpus\r\n\r\nFor our research we investigated the Beatles Interviews Database1, a collection of one-hundred sixtythree transcription of Beatles\u2019 interviews from 1962 to 1984 created in 1997 by Jay Spangler and now\r\nmanaged by Jude Southerland Kessler and Suzie Duchateau. The website also contains a songwriting and\r\n1http:\/\/www.beatlesinterviews.org\/\r\n\r\n198\r\n\r\n\frecording database, a collection of Beatles\u2019 movies, quotes and pictures. We also investigated thirty-five\r\nBeatles\u2019 interviews available on YouTube: in this case we analyzed the automatic captions generated by\r\nspeech recognition, and we corrected texts if necessary. In each interview, we isolated Paul McCartney\u2019s\r\nspeeches and we created a document for each interview. The Let IT Corpus is a very small balanced\r\ncorpus composed of one-hundred four documents belonging to two different classes: I) before (composed\r\nby fifty-two documents concerning interviews before 9th November 1966) and II) after (composed by\r\nfifty-two documents concerning interviews after 9th November 1966). A few texts belonging to the after\r\nclass found on YouTube date after 2000. The majority of texts of the Let IT Corpus are from the Beatles\r\nInterviews Database (32 before texts and 25 after texts, including a few chunks). The corpus contains\r\nalso texts from the Beatles Interviews Database concerning interviews involving the whole Beatles group,\r\nfrom which we isolated Paul McCartney\u2019s speeches. The remaining part of Let IT Corpus consists of\r\nBeatles\u2019 interviews freely available on YouTube. Let IT Corpus is still in its embryonic stage, since it is\r\ncomposed of approximatively one-hundred texts and it represents the first step in this field. Further work\r\nwill be carried out as soon as Let IT Corpus will be expanded.\r\n\r\n4\r\n\r\nOur three approaches to stylometric analysis\r\n\r\nWe investigated this AA issue with three different approaches, in order to compare the results. For all the\r\nexperiments we removed punctuation and symbols, and we lowercased all characters.\r\n4.1\r\n\r\nHybrid approach\r\n\r\nIn this section we describe the first approach to stylometric analysis, namely a hybrid approach based on\r\nCS, Linguistic Rules and Machine Learning (ML). Thanks to the analysis of approximatively five thousand\r\nEnglish documents from a variety of sources (newspapers, social media and books) we identified several\r\nstylistic features that we used to write linguistic rules for English. Here we report a short list of stylistic\r\nfeatures: sentence length (Argamon et al., 2003), vocabulary richness (De Vel et al., 2001), word length\r\ndistributions (Zheng et al., 2006), punctuation (Baayen et al., 1996), use of a specific class of verbs or\r\nadjectives, use of first\/third person. The hybrid approach of CS, Linguistic Rules and ML consists in the\r\nfollowing steps: I) Linguistic Definition of Stylometric Features: starting from the assumption that each\r\nauthor operates different grammatical choices when writing a text (Daelemans, 2013), we organized the\r\ngrammatical characteristics of the case-study language (in this case, English) in a taxonomy. The work\r\nwas carried out thanks to COGITO\u00AE by Expert System Corp., a semantic analysis software based on\r\nArtificial Intelligence algorithms. In each limb of the taxonomy it is possible to write linguistic rules\r\nconcerning the language of the case study in order to recognize the grammatical characteristics of the\r\nanalyzed texts (i.e. to detect modal verbs, we create the limb \"modal verbs\" and we associate to it linguistic\r\nrules that allow to find modal verbs in the texts); II) Semantic Engine Development: Expert System\u2019s\r\nsemantic engine is trained in order to extract the aforementioned features from texts and is implemented\r\nthanks to COGITO\u00AE \u2019s semantic network (called Sensigrafo); III) Features Extraction: texts are analyzed\r\nand all features (based on the grammatical characteristics of the texts) are extracted; IV) Supervised ML\r\nProcess: the features extracted are used to train the model in order to detect the features in the untagged\r\ntexts. For ML process we exploit WEKA (Hall et al., 2009), a software with ML tools and algorithms for\r\ndata analysis.\r\nThe hybrid approach is evaluated through the 10-folds Cross Validation method. We tested two different\r\nalgorithms, Random Forest (RF) and Tree J48 (J48). During previous AA investigations RF resulted to\r\nbe the most performing algorithm for a binary classification. The results we obtained for 10-folds Cross\r\nValidation test confirm this result and Table 1 presents the performances in terms of Precision, Recall\r\nand F-Measure for both algorithms (namely, RF and J48). In order to evaluate the performances of the\r\nclassifier, after this process, we tested both RF and J48, as well as 10-folds Cross-Validation (Table 1).\r\nCompared to the results obtained for the 10-folds Cross Validation (see Tables 2 and 3), J48 performances\r\n(Table 3) are better than RF performances (Table 2).\r\n\r\n199\r\n\r\n\f10-folds Cross Validation (RF)\r\n10-folds Cross Validation (J48)\r\n\r\nPrecision\r\n\r\nRecall\r\n\r\nF-Measure\r\n\r\n0.815\r\n\r\n0.824\r\n\r\n0.808\r\n\r\nPrecision\r\n\r\nRecall\r\n\r\nF-Measure\r\n\r\n0.779\r\n\r\n0.784\r\n\r\n0.781\r\n\r\nTable 1: 10-folds Cross Validation on the whole corpus with RF and J48\r\nTest Set 80-20 (RF)\r\n\r\nPrecision\r\n\r\nRecall\r\n\r\nF-Measure\r\n\r\n0.781\r\n\r\n0.750\r\n\r\n0.764\r\n\r\nTable 2: Performances in terms of Precision, Recall and F-Measure with 80% of the corpus as Training\r\nset and the remaining 20% as Test set randomly selected with the support of RF algorithm\r\nTest Set 80-20 (J48)\r\n\r\nPrecision\r\n\r\nRecall\r\n\r\nF-Measure\r\n\r\n0.853\r\n\r\n0.800\r\n\r\n0.819\r\n\r\nTable 3: Performances in terms of Precision, Recall and F-Measure with 80% of the corpus as Training\r\nset and the remaining 20% as Test set randomly selected with the support of J48 algorithm\r\n4.2\r\n\r\nSupport Vector Machine (SVM)\r\n\r\nFor our second approach we exploited SVM with a Bag-of-Words (BoW) features set created using\r\nTF-IDF vectorization. As stated by (Diederich et al., 2003) SVM is capable to process thousands of\r\ninputs, which allows to use all the words of a text directly as features. SVM involves building a decision\r\nboundary to separate the data into classes (in our case, before and after), which may be non-linear if the\r\nkernel trick is used to transform our existing data into a higher dimensional space. As such, the right\r\nchoice to take when fitting an SVM classifier is kernel in addition to others hyperparameters specific to\r\nthat kernel. In applying SVM to AA, (Schwartz et al., 2013) used a linear kernel, while (Diederich et al.,\r\n2003) examined a range of different kernels. Since our AA is a binary classification problem we used\r\nthe linear kernel for our model and considered C values in the set {1, 10, 100}. The optimal value of C\r\nwas determined using GridSearchCV function with a default 3-fold Cross-Validation and accuracy used\r\nas the scoring metric. The optimal C value was determined to be C = 1. Results are in Tables 4 and 5.\r\nSVM-BoW\r\n\r\nPrecision\r\n\r\nRecall\r\n\r\nF-Measure\r\n\r\n0.785\r\n\r\n0.761\r\n\r\n0.773\r\n\r\nTable 4: 10-folds cross validation SVM - BoW features set.\r\nSVM-BoW\r\n\r\nPrecision\r\n\r\nRecall\r\n\r\nF-Measure\r\n\r\n0.885\r\n\r\n0.809\r\n\r\n0.818\r\n\r\nTable 5: Performances in terms of Precision, Recall and F-Measure with 80% of the corpus as Training\r\nset and the remaining 20% as Test set randomly selected.\r\n4.3\r\n\r\nConvolutional Neural Network (CNN)\r\n\r\nTo deal with the problem of AA of speech transcriptions, our third approach consists in a two-class\r\ntext classification based on a deep CNN. We built a neural network that exploits the morpho-syntactic\r\n\r\n200\r\n\r\n\finformation to improve the classification and correctly identify the given samples. The input data are\r\npreprocessed and tagged with linguistic information using the Part-of-Speech (PoS) tagger provided by\r\nthe free NLP open-source library Spacy. Given the importance of function words (Kestemont, 2014),\r\nconjunctions, prepositions, interjections, adverbs and auxiliary verbs were taqken into account for this\r\nanalysis. In fact, as proved by (Mosteller and Wallace, 1963) and confirmed by (Koppel et al., 2006),\r\nfunction words are discriminators of authorship, since the usage variations of such words are a strong\r\nreflection of stylistic choices. Our proposed architecture receives a sequence of tagged texts as input and\r\nthen is transformed into padded sequences of fixed length. The sequences are then processed by four\r\nmodules: an embedding module, a convolutional module and two max pooling layers to consolidate the\r\noutput of the convolutional layer. The output of the three modules are processed by one Dense layer and\r\nan output layer. Results are shown in Tables 6 and 7.\r\nCNN-PoS\r\n\r\nPrecision\r\n\r\nRecall\r\n\r\nF-Measure\r\n\r\n0.681\r\n\r\n0.734\r\n\r\n0.706\r\n\r\nTable 6: 10-folds cross validation CNN + PoS.\r\nCNN-PoS\r\n\r\nPrecision\r\n\r\nRecall\r\n\r\nF-Measure\r\n\r\n0.692\r\n\r\n0.818\r\n\r\n0.750\r\n\r\nTable 7: Performances in terms of Precision, Recall and F-Measure with 80% of the corpus as Training\r\nset and the remaining 20% as Test set randomly selected.\r\n\r\n5\r\n\r\nDifferences and Similarities before and after 9th November 1966\r\n\r\nThanks to the linguistic analysis of the texts belonging to the two different classes, we detected stylistic\r\ndifferences and similarities in the speech transcriptions before and after 9th November 1966. We started\r\nby dividing Paul McCartney\u2019s interviews into separate sentences. A number of stylistic features are\r\nextracted from these sentences and then all features are used for K-Means clustering. Here we report a\r\nlist of some features extracted: number of function words, number of verbs and a number of interjections.\r\nFor clustering, the average of each feature is calculated. Further, a SVM classifier is trained on 70% of\r\nthe interviews and tested on the remaining 30%. Performing this process means to see whether a link is\r\npresent and consistent over time through Paul McCartney\u2019s style. Accuracy is shown in Table 8.\r\nAccuracy on test set\r\n0.561\r\nTable 8: SVM to test stylometric similarity\r\nHere we report some examples of interview transcriptions before and after 9th November 1966 and we\r\nhighlight the most noticeable differences and similarities. It is very important to consider that all the\r\ninterviews collected are from different sources (TV, radio, newspaper), that means that speeches can differ\r\nfrom source to source as well as according to the historical moment in which they were done.\r\n\u2022 You know like, number one records, Sunday Night At The Palladium, Ed Sullivan Show, go to America,\r\nyou know. All kinds of ambitions like that. (Carnegie Hall - New York, 1964 February 12th);\r\n\u2022 The only thing is that we\u2019ve gotta do a lot from London, \u2019cuz a lot of the TV shows are down in\r\nLondon, you know. And so, we\u2019re forced to do a lot down in London. I mean, it\u2019s like someone\r\nsaid the other day Why doesn\u2019t Harry Secombe go to Cardiff? You know, he never does. But no\r\n\r\n201\r\n\r\n\fone ever moans about Harry never going...You know what I mean? (BBC-TV by Gerald Harrison\r\n- Liverpool, 1964 July 10th);\r\n\u2022 Personal differences, business differences, musical differences, but most of all because I have a better\r\ntime with my family. Temporary or permanent? I don\u2019t really know. (Break-up - 1970 April 10th);\r\n\u2022 It was like a gesture to Russia because normally records are released first in America and England\r\nin Europe and then Russia gets them last and because Gorbachev and Reagan were talking about\r\nglasnost and we\u2019re talking about arms reduction. I think a lot of us in Europe were very happy to\r\nhear this so I had the opportunity to release this record so I wrote a little note on the record saying\r\nthis is the peace gesture the hand of friendship from the west to the east and I just felt it might just\r\nhelp a bit of glasnost it\u2019s my little bit of glasnost. (Flemish Public Television Interview - 1989)\r\nAs we can see, slang expressions and fillers such as \u2019cuz, I mean, You know? and You know what I mean?\r\ncompletely disappear in interviews after 9th November 1966. The use of slang disappears also in other\r\ninterviews after this date, in which we can find a different Paul McCartney, who seems to be more serious\r\nand not only because of an older age. Changes can be brought about by the different topics addressed in\r\nthe interviews, but we also believe that speech preserves some characteristics (such as slang) in different\r\ncontexts. In texts belonging to the after class, sentences are longer compared to those of the before class.\r\nWe noticed also that in texts belonging to the after class style changes occur continously not allowing\r\nfor the identification of a specific style. For these reasons we also report the date and the source. Our\r\nresearch highlights some similarities in before and after texts: the overuse of expressions such as We\r\nare gonna do and a lot\/a lot of is confirmed in both periods. These represent the most used expressions\r\nby Paul McCartney in his speeches. In the interviews in the Let IT Corpus we also noticed that Paul\r\nMcCartney is inclined to rely on lists both in before and in after periods.\r\n\r\n6\r\n\r\nConclusions\r\n\r\nIn this paper we have presented the Let IT Corpus, namely a corpus of one-hundred four transcriptions\r\nfrom speech to text of Paul McCartney\u2019s interviews collected from the Beatles Interviews Database and\r\nYouTube. The aim of this research is to detect possible differences and similarities in Paul McCartney\u2019s\r\nspeeches before and after 9th November 1966 (date of his supposed death). For this reason texts have\r\nbeen organised in two classes: I) before and II) after. We investigated three different text classification\r\napproaches and we detected that all methods achieved high percentage of accuracy classifying texts in\r\ntwo different classes referring to two different periods. To reinforce these results and on the basis of the\r\nanalysis of the stylistic features set out above, it is clear that the way of modulating the words of Paul\r\nMcCartney is quite distinguishable between the two periods examined.\r\n\r\n7\r\n\r\nFuture Work\r\n\r\nThe corpus is in its embryonic stage, since it is composed of approximatively a hundred texts. Future\r\nwork therefore concerns the expansion of the Let IT Corpus, so to allow a more thorough investigation.\r\nTo corroborate our hypothesis it might be interesting to see if the differences we detected between the two\r\nclasses represent a pure coincidence. A possible experiment in this respect can be carried out considering\r\na different temporal division of the texts.\r\n\r\nAcknowledgements\r\nThis research has been partly supported by the PON Ricerca e Innovazione 2014-20 and the POR\r\nCampania FSE 2014-2020 funds. Authorship contribution is as follows: Antonio Pascucci is author\r\nof Sections 1, 2, 3, 4.1 and 7 and Raffaele Manna is author of Sections 4.2 and 4.3. Section 5 is in\r\ncommon. This research has been developed in the framework of two Innovative Industrial PhD projects\r\nin Computational Stylometry (CS) by \u201CL\u2019Orientale\u201D University of Naples in cooperation with Expert\r\nSystem Corp. We are grateful to Vincenzo Masucci and Expert System Corp. for providing COGITO\u00AE\r\nfor research and to Prof. Johanna Monti for supervising the research.\r\n"
	},
	{
		"id": 32,
		"title": "Prospects for Computational Hermeneutics",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Michael Piotrowski",
			"Markus Neuwirth"
		],
		"body": "Introduction: The Theoretical DH\r\n\r\nThe interpretation of human artifacts in order to understand their \u201Cmeaning\u201D is the central concern of the\r\nhumanities. They are therefore often characterized as being \u201Cqualitative-hermeneutical,\u201D in contrast to the\r\nnatural sciences and to computer science, which are supposedly \u201Cempirical,\u201D \u201Cquantitative,\u201D and much\r\nless dependent on interpretation. However, as Piotrowski (2018) argues, disciplines are not defined by\r\ntheir methods alone but rather by a \u201Cunique combination\u201D of a research object and a research objective;\r\nresearch methods, he notes, are \u201Csecondary in that they are contingent on the research object and the\r\nresearch objective.\u201D In addition, technical and scientific progress not only enables methods to evolve, but\r\nalso requires them to adapt, while research objects and objectives largely remain stable. Even though\r\nparticular methods may be \u201Ctypical\u201D for a particular discipline, all disciplines can, in principle, use any\r\nmethods, including computational ones, as long as they fit their research objectives. As Orlandi puts it,\r\n\u201Cun po\u2019 di aritmetica ha sempre fatto parte delle discipline umanistiche\u201D (Orlandi, 1990, 114). Conversely,\r\nother fields also use methods commonly associated with the humanities. For example, Frodeman (1995)\r\nhas argued that geology is really a \u201Chistorical and interpretive science,\u201D rather than a \u201Cderivative science,\r\nrelying on the logical techniques exemplified by physics\u201D (see also Comet, 1996). Similarly, artificial\r\nintelligence (e.g., Winograd, 1981) and computer science more generally (e.g., West, 1997) attempted to\r\nformalize and apply the key concept of hermeneutics, namely, understanding.\r\nThe research carried out under the heading of \u201CDigital Humanities\u201D (DH) currently tends to focus on\r\nquantitative analyses, which have long been difficult or even impossible in the humanities and which\r\n\r\n204\r\n\r\n\fyield important new insights complementing traditional (\u201Cmanual\u201D) qualitative analyses. However, if we\r\nunderstand DH as the construction of formal models in the humanities (Piotrowski, 2018; McCarty, 2014;\r\nOrlandi, 1990), we must not neglect the qualitative-hermeneutic dimension. If the humanities want to\r\nsucceed in answering their research questions\u2014which are primarily qualitative in nature\u2014they cannot\r\nrely on quantitative methods alone. Instead, a multilayered research process is required, one in which\r\nquantitative and qualitative analyses continuously alternate.\r\nOne of the main challenges for the theoretical digital humanities (Piotrowski, 2018) remains to find\r\nways to integrate hermeneutic methods and insights into formal models, rather than keeping interpretation\r\ndetached, as a kind of afterthought to automatic analyses (or vice versa). In this regard, there are noteworthy\r\ninitiatives to exploit the computer as a \u201Cmodeling machine\u201D (McCarty, 2014, 256) while continuing the\r\nlong philosophical tradition of hermeneutics (e.g., Dilthey, Heidegger, Gadamer, Ric\u0153ur, Iser, Jauss etc.).\r\nHowever, there does not seem to be a transfer back in the other direction. The goal of this paper is thus\r\nto outline the prospects for a novel approach that might be called computational hermeneutics and to\r\nstimulate a wide discussion on the possibility of a unified science bridging the gap between the humanities\r\nand the sciences.\r\n\r\n2\r\n\r\nHermeneutics and Understanding\r\n\r\nThe main goal of any hermeneutic approach is to achieve what Dilthey called Verstehen, an \u201Cunderstanding\u201D\r\nof human artifacts in order to answer questions about the \u201CWhy?\u201D and \u201CHow?\u201D and to uncover underlying\r\npatterns (Bod, 2015). But what exactly is understanding, and the understanding of what? One may argue\r\nthat hermeneutic interpretation aims at uncovering the meaning of a given text by reference to the author\u2019s\r\nintention (whether empirical or idealized), the envisaged reader, and the dense web of meanings invoked\r\nby the text. Understanding thus involves a reconstruction of (a) the reasons why a given author (or group\r\nof authors) produced a particular text (text understood in a broad sense), (b) the overt or hidden layers\r\nof meaning of a given text, (c) the type of recipient envisaged by the author and\/or the text, and (d) the\r\npotentially infinite number of contexts in which the reconstruction of meaning can take place (Ric\u0153ur\u2019s\r\n\u201Cconflit des interpr\u00E9tations\u201D).\r\nWhenever we interpret language, we need to rely on some pre-understanding that provides the basis on\r\nwhich to build an interpretation. It is thus a recursive process, in which (pre-)understanding is necessary\r\nfor interpretation, which in turn produces understanding, and so forth\u2014hence the term hermeneutic circle\r\n(for an illuminating discussion see G\u00F6ttner (1973)). Since this process leads to a progressive approximation\r\nto an (ideally exhaustive) understanding of a given text, Bolten (1985) has proposed the more apt metaphor\r\nof a hermeneutic spiral.\r\nDespite the supposed \u201Cdeath of the author\u201D (as famously heralded by Roland Barthes), authorial intention\r\nremains an important component of pre-understanding: the interpretation of texts cannot be successful\r\nwhen one just relies on lexical meanings and sentence semantics alone, both of which may not even be\r\navailable when we understand texts in a broad sense. For an interpretation to be sound, one has to make\r\ncomplex inferences that rely on vast knowledge about the world and on the attribution of mental states\r\n(especially intentions) to the author. Here Grice\u2019s conversational maxims play a particularly important\r\nrole in guiding the inferential process. It is characteristic of the ways in which hermeneutics is commonly\r\nconstrued that these inferential processes about the other mind, however foreign, are rarely reflected in\r\ndepth (see Winograd\u2019s model of the speaker as part of the reader\u2019s model of the world (Winograd, 1981)).\r\nNonetheless, despite the importance of authorial intention, it is necessary to draw a distinction between\r\nthe meaning of a text and the meaning as intended by the (empirical rather than ideal) author. In other\r\nwords, the meaning of a text cannot be reduced to the intended meaning either.\r\n\r\n3\r\n\r\nHermeneutics and Digital Humanities\r\n\r\nThere are essentially two \u201Cnative\u201D strands of interpretation in DH, which both now have long traditions\r\ngoing back to the beginnings of computing in the humanities.\r\n\r\n205\r\n\r\n\fContext of\r\ninterpretation\r\n\r\nWork\r\n\r\nFigure 1 \u2013 Hermeneutics considers a work in a particular context of interpretation, peculiar to a reader,\r\nhere modeled as a network of concepts. Links between concepts can be of various types; they can be\r\nthought of as mental associations.\r\n\r\nOne strand is that of annotation, exemplified by the Text Encoding Initiative (TEI, 1987).1 It goes\r\nback to an even longer editorial tradition in philology, focusing primarily (though not exclusively) on a\r\nsingle text and the textual phenomena therein. It is thus a relatively \u201Cweak\u201D form of interpretation, in\r\nthe sense that it makes only limited connections to the extra-textual (which for Ric\u0153ur (2017, 103) is a\r\ndefining feature of interpretation)\u2014but intentionally so: editions are generally used as a basis for a later\r\ninterpretation of the text.\r\nThe other strand, which Rockwell and Sinclair (2016) call \u201Ccomputer-assisted interpretation,\u201D builds\r\non an equally long-standing tradition in literary studies, in particular concordancing, stylometry, and\r\nother quantitative analyses, and belongs to the first applications of computers in the humanities (see, e.g.,\r\nKroeber, 1967). The modern evolution of this strand can be exemplified by Rockwell and Sinclair (2016)\r\nand their work on Voyant.2 This strand is oriented towards tools and automatic analyses informing human\r\ninterpretation. Rockwell and Sinclair\u2019s notion of the \u201Chybrid essay, an interpretive work embedded with\r\nhermeneutical toys\u201D (Rockwell and Sinclair, 2016, 17) illustrates well the idea of the computer providing\r\nscholars with new evidence.\r\nBoth strands are not limited to philology and literary studies; they can also be found in other humanities\r\ndisciplines, and images or other artifacts may replace texts as research objects. Outside of DH, computerassisted interpretation (in the above sense) remains controversial (see, e.g., the debates following the\r\npublication of Da, 2019); critics typically question the legitimacy of quantitative methods in general.\r\nHowever, both annotation and computer-assisted interpretation have an inherent limitation in common,\r\nwhich is rarely, if ever, discussed: human interpretation remains outside of the formal framework. In the\r\ncase of annotation, the (formal) annotation is the result of a preceding human interpretation that motivates\r\na particular annotation (say, that tagging of some text as \u201Cdeleted\u201D), but only the result (in the form of a\r\n<del> tag) is formally documented, the reasoning for this choice generally remains inaccessible, at least to\r\nthe computer. Furthermore, it is usually difficult, or even (practically) impossible, to record alternative\r\ninterpretations.\r\n\r\n4\r\n\r\nProposal\r\n\r\nHow, then, could we link hermeneutics to formal models, so that human interpretations can be taken into\r\naccount as well and different types of methods can be combined to truly complement each other? The idea\r\nof mixed methods, which originated in the social sciences (Kuckartz, 2014), certainly cannot be transferred\r\nto the domain of the humanities without modification. It is important to stress that the goal cannot be to\r\n\u201Cautomate\u201D interpretation; the bedrock of Verstehen is a shared understanding of the conditio humana.\r\n1https:\/\/tei-c.org\r\n2https:\/\/voyant-tools.org\r\n\r\n206\r\n\r\n\fThe goal must rather be to support the scholar by making it possible, for example, to process qualitative\r\nhuman interpretations alongside the results of automatic quantitative analyses.\r\nThe basic idea of our proposal is to model the context of interpretation\u2014i.e., a reader\u2019s knowledge of\r\ncultural concepts and the associations between them\u2014as a semantic network or knowledge graph (see\r\nFig. 1), and interpretation as the linking of features of the interpreted object to nodes of this network,\r\ni.e., the construction of a new network, as illustrated in Fig. 2. Understanding can thus be defined as the\r\nintegration of the object\u2019s properties into a preexisting network.\r\nComputationally, this model can be represented using Semantic Web and Linked Data technologies,\r\nwhich has the advantage that existing tools and methods can be leveraged. In particular, we propose\r\nto use nanopublications, a knowledge representation approach originally developed in bioinformatics\r\n(Groth et al., 2010), although the conceptual model is neutral with respect to a particular implementation.\r\nNanopublications were developed as a common framework for describing scientific statements together\r\nwith contexts (e.g., original publication, authors, organisms involved) in a machine-readable fashion, so\r\nthat scientific results are easier to discover, unambiguously referenced and connected to particular scholars,\r\nand can be automatically aggregated and analyzed.\r\nContext of\r\ninterpretation\r\nContext of\r\ninterpretation 2\r\n\r\nContext of\r\ninterpretation\r\n\r\nP1\r\nWork\r\nP2\r\n\r\nP1\r\n\r\nP4\r\nWork\r\nP2\r\n\r\nP3\r\n\r\nP3\r\n\r\n(a)\r\n\r\n(b)\r\n\r\nFigure 2 \u2013 (a) Interpretation links features of the work (here: passages P1\u20133) to concepts in the reader\u2019s\r\ncontext of interpretations. (b) The contexts of interpretation of different readers may partly overlap (and\r\nthus share associations) but may also have different relations and thus come to different interpretations of\r\nthe same work.\r\n\r\n5\r\n\r\nCase Study\r\n\r\nIs it possible to model the temporal (and geographical) dynamics of the horizon of expectations? Let us\r\nconsider an example from music history to demonstrate our approach to computational hermeneutics.\r\nIn his review of a symphony by Robert Volkmann (which is little known today), Selmar Bagge wrote:\r\n\u201CVolkmann\u2019s Dmoll-Symphonie ist eine durchaus pathetische Production\u201D (AmZ 48, 1863, col. 806).\r\nSuppose this sentence originated from a present-day source. In this case, a translation such as the following\r\nwould be perfectly possible: \u201CVolkmann\u2019s symphony in D minor is a quite emotive work\u201D. However, since\r\na model of understanding contains assumptions about the author and the time of his or her writing, such\r\na translation would ignore that the German word pathetisch has undergone a significant semantic shift.\r\nToday, pathetisch has a rather negative connotation and would thus have to be translated as \u2018melodramatic\u2019\r\nor \u2018pompous.\u2019 To reveal the (historical) meaning likely to be intended by Bagge, we need to explore and\r\nmodel the contexts in which pathetisch has been used. These contexts have to be distinguished according\r\nto their distance to the target object of interpretation. Generally, an interpretation is more likely if it is\r\nsupported by sources that show proximity in terms of time and space. In other words, sources that have\r\nbeen written around the same time and in the geographical vicinity of the source under investigation are\r\nto be preferred over sources that show greater temporal and geographical distance. Both temporal and\r\ngeographical distances can best be modeled using network approaches (see above).\r\n\r\n207\r\n\r\n\fWhen consulting one central source, the approximately contemporaneous Deutsches W\u00F6rterbuch by\r\nthe Brothers Grimm, we find pathetisch glossed as \u2018powerful,\u2019 \u2018dignified,\u2019 or \u2018solemn.\u2019 In addition, the\r\nword is linked to both the passionate and Schiller\u2019s concept of the \u201Cpathetic-sublime\u201D (1793). Both of\r\nthese usages are confirmed by much earlier sources: In Johann Georg Sulzer\u2019s Allgemeine Theorie der\r\nSch\u00F6nen K\u00FCnste (1793), the \u201Cpathetic\u201D is considered a synonym of the \u201Cpassionate.\u201D In Heinrich Christoph\r\nKoch\u2019s Musikalisches Lexikon from 1802, the reader interested in the meaning of \u201Cpatetico, pathetisch\u201D\r\nis directed to the entry on the \u201Csublime\u201D (Koch, 1802), thus suggesting that pathetisch and \u201Csublime\u201D\r\nare synonyms. More distant 18th century sources even suggest an association of the sublime with the\r\n\u201C(delightful) horror.\u201D Given the historical distance, this connotation is less likely to be conveyed in Bagge\u2019s\r\nstatement. Considering this complex semantic history, the modeling task consists in (1) linking related\r\nsemantic concepts, (2) qualifying these links (e.g., as synonym, as super- and subcategory, or as semantic\r\noverlap), and (3) weighing links according to temporal proximity.\r\nThe model reader that Bagge had in mind when making his statement about Volkmann\u2019s symphony as\r\nbeing pathetic is somebody who had a certain prior knowledge of that concept (as reconstructed from the\r\nsources just mentioned). In addition to the semantic history of words, further contexts that need to be\r\nconsidered concern a dense web of musical works. The prototypes of a pathetic work, as invoked by Bagge,\r\nare Beethoven\u2019s 5th and 9th symphonies. Readers of the time likely understood this to be the primary\r\ncontext of Volkmann\u2019s symphony without which a proper understanding could not be achieved. Further\r\nworks featuring \u201Cpathetic\u201D in their titles are Beethoven\u2019s piano sonata op. 13 and, much later, Tchaikovsky\u2019s\r\n6th symphony, the distance between these two works being roughly a hundred years. However, despite\r\nthe lack of a title, many earlier symphonies (by other composers) from the late 18th century on have\r\nbeen referred to as invoking the \u201Csublime,\u201D and hence are \u201Cpathetic\u201D in Koch\u2019s sense. The reason for\r\nBagge\u2019s aesthetic judgment thus lies in the shared musical properties of all the works contained in the set\r\nof pathetic or sublime symphonies: the minor mode, the orchestral setting, a particular tempo, etc. As a\r\nresult, a hermeneutic reconstruction must consider both the semantic tradition (and change) of the word\r\n\u201Cpathetic\u201D and the corresponding musical production.\r\n\r\n6\r\n\r\nConclusion: Implications and Prospects\r\n\r\nAs outlined at the outset of our paper, the humanities and the sciences are widely assumed to be separated\r\nfrom each other by their respective methods, objects, and objectives. However, as suggested above, the\r\nhumanities and the sciences face a common challenge: both have to address explicitly the issues of\r\ninterpretation and decision-making under uncertainty. In particular, they need to formalize and model\r\nthe contexts of interpretation and the inferential processes under uncertainty, seeking to exploit the\r\nrich potential of the computer as modeling machine (Piotrowski, 2019). The development of suitable\r\nprobabilistic tools (Pearl, 2000) for modeling network-like relationships between objects is a crucial task\r\nfor the whole scientific community, one that brings us closer to the ideal of a truly unified science.\r\nThe use of formalization and modeling is often met with a certain hostility in the humanities. Many\r\nhumanities scholars subscribe to the notion that interpretation can in principle never come to a conclusion,\r\nand indeed the fascination of hermeneutics seems to lie in its inherent incompleteness. In addition, it\r\nis assumed that multiple interpretations can exist alongside each other without the need (or even the\r\npossibility) to prefer one over the other; this is in keeping with the cherished notion of plurality and\r\nmultiplicity of perspectives in the humanities. Yet exactly in this respect a computational approach may\r\noffer obvious advantages, as the possibilities of formally representing interpretations, their contexts, and\r\nthe inference procedures allow scholars to better compare different interpretations and assign different\r\nprobability values to them (for applying a Bayesian approach to historiography and the problems of\r\nassigning prior probabilities (see Tucker, 2004; Carrier, 2012). More generally, this approach can give\r\nrise to the idea of progress in the humanities (something that is notoriously rejected by many humanities\r\nscholars). Thus the essential challenge of the theoretical digital humanities is to come up with a convincing\r\napproach to a \u201Chermeneutic computer science\u201D (West, 1997), whose tasks involves modeling interpretation\r\ncontexts, inferential processes, and uncertainty."
	},
	{
		"id": 33,
		"title": "EModSar: A corpus of Early Modern Sardinian Texts",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Nicoletta Puddu",
			"Luigi Talamo"
		],
		"body": "1 Introduction\r\nIn this paper1 we present the Early Modern Sardinian Corpus (EModSar: http:\/\/corpora.unica.\r\nit\/TEITOK\/emodsar)2, a historical corpus developed within a more general project whose aim is to\r\ndescribe the linguistic repertoire of Sardinia in the Modern Era3 (see section 2.1). Our main research\r\nquestion addresses the impact of language contact on Sardinian, and, in order to answer this question, we\r\ndecided to build a pos tagged and lemmatized corpus covering texts from the 16th to the 17th century\r\nwhich also contains extralinguistic information about the chosen texts (see section 2.2). Moreover, given\r\nthat our texts are written in Sardinian, but also contain sections written in Catalan and Latin, we wanted\r\nto both preserve multilingualism, and also ensure that our corpus tools focused on the linguistic analysis\r\nof Sardinian. As Pahta et al. 2018:10 point out, multilingual historical corpora are rarer than monolingual\r\nones, and have not been used extensively in historical linguistics. However \"embracing a multilingual\r\napproach to language history leads the researcher to look beyond the main language of a text and consider\r\nwhat a holistic overview of all the languages in it reveals about the \u2019grammar\u2019 of non-monolingual writing\r\non the one hand or individual identity or social practice on the other\" (Pahta et al. 2018:5). Consequently,\r\nwe decided to adopt the TEI-P5 guidelines to code our documents in order to accomplish Lass 2004\u2019s\r\nthree desiderata for a proper historical corpus (i.e. \"maximal information preservation\u201D, \"no irreversible\r\neditorial intervention\u201D, and \"maximal flexibility\u201D). On the one hand, the use of TEI-P5 for our corpus\r\nallowed editorial choices to be preserved in the text at the philological level, while, all the relevant\r\ninformation could be inserted in the header. The use of a TEI-P5 encoding is not a common standard in\r\nhistorical corpora. As Jenset and McGillivray 2017:125 note, \"TEI is not very widely used for historical\r\ncorpora, where there is a stronger emphasis on linguistic annotation rather than on paleographic and\r\nhistorical markup. However, in the case of historical texts, the information contained in these tags can\r\n1For Italian academic purposes only, Nicoletta Puddu was responsible for Sections 1 and 2 and Luigi Talamo for Sections 3\r\nand 4.\r\n2The corpus is currently composed of nine manuscripts, for a total of 6495 tokens.\r\n3EModSar has been developed under the project System for developing and annotating a corpus of ancient Sardinian texts,\r\nfunded by the Regione Autonoma della Sardegna (Capitale Umano ad alta qualificazione, L.R. 7\/07, year 2015).\r\n\r\n210\r\n\r\n\fbe crucial to the interpretation of the text and should be considered by the language processing tools.\r\n[. . . ].\u201D A convenient solution is the use of softwares such as TEITOK (Janssen 2016), a tool which\r\ncan handle both textual mark-up and linguistic annotation. Since our texts have been annotated at three\r\ndifferent levels (at the document-level, at the section-level and at the token-level (see section 3.2), queries\r\nin EModSar can combine different levels in order to connect linguistic information with extralinguistic\r\ninformation.\r\n\r\n2\r\n2.1\r\n\r\nLanguage and texts\r\nSardinian in the Modern Era\r\n\r\nThe linguistic repertoire of Sardinia in the Modern Era is largely understudied, but it is extremely\r\ninteresting since it sees the presence of many different languages within the same period. From 1324\r\nonwards, the kingdom of Aragon gradually took possession of the Island, and as a consequence Catalan\r\nbecame the official language. After the unification of the Kingdom of Aragon with the kingdom of\r\nCastile, Castilian began to spread, but Catalan actually remained in use for juridical and administrative\r\npurposes, while Castilian became the language of Universities and of the Church (Virdis 2017). Thus,\r\nbetween 1324 and 1720, when the Island was conceded to the House of Savoy and started its process\r\nof Italianization, Sardinia was under Iberian domination. However, Sardinian continued to be used in\r\njuridical documents of both a public and, especially, private nature particularly in the countryside. Both\r\nCatalan and Castilian deeply influenced Sardinian during the Iberian domination.\r\nThe Sardinian language of this period is documented through two typologies of documents: literary\r\nsources and juridical sources. The Sardinian literati in the Modern Era usually wrote in the dominant\r\nlanguages (mainly Castilian). However, some of them (like Antonio Lo Frasso) inserted some Sardinian\r\nsections in their works or even wrote entire compositions in Sardinian (like Girolamo Araolla). What is\r\nclear, however, is that all the literati living in Sardinian were highly plurilingual (Marci 2006).\r\nAs for juridical documents, Sardinian was used during trial courts, not only for testimonies, but also\r\nfor other stages of the trial. In private documents Sardinian appears in notary deeds mainly containing\r\nsales, donations, debit notes, last wills and testaments (Cadeddu 2013). While we have critical editions\r\nof literary texts from the Modern Era, juridical documents are mainly kept in a number of archives in\r\nSardinia. Only a small part of these documents have been published, mainly in historical studies: there\r\nare very few critical editions and no systematic linguistic studies.\r\n2.2\r\n\r\nThe choice of texts\r\n\r\nIn our project, we wanted to study Sardinian of the Modern era, in the perspective of historical sociolinguistics in Romaine 1992\u2019s terms. In order to do so, we decided to create the Early Modern Sardinian\r\nCorpus by encoding and annotating juridical documents of the Modern Era, annotated by POS and\r\nlemma, and accompanied by contextual information. To date, we have encoded nine documents written\r\nin Sardinian dating from the 16th to the 17th century retrieved from the Archivio storico del Comune\r\ndi Cagliari and the Archivio di Stato di Cagliari. Most of the retrieved documents come from villages\r\nin the Northern Sardinian area and from the towns of Sassari and Bosa. However, we know for certain\r\nthat documents written in Sardinian datable to those centuries also exist in southern Sardinia. We do not\r\nexpect to find any documents in Sardinian for the city of Cagliari where Catalan was widespread in all\r\nthe written domains.\r\nOur documents have presented many problematic aspects typical of historical corpora which we will\r\nexemplify by discussing document Osp250 which contains the last will of Canonigu Montixi, the priest\r\nof the diocese of Arborea, who, in 1569, leaves a \u201Cfellowship\u201D to one of his relatives so that he can study\r\ngrammar, philosophy and theology.\r\nFirst of all, our documents are characterized by a high level of orthographic variation, both between\r\ndifferent documents and within the same document. For instance, in Osp250 the preposition \u2019in\u2019 can have\r\ndifferent orthographic realizations (in, jn, en). Moreover, we have many cases of univerbation, such as\r\ninsu \u2019in the\u2019, inpodere \u2019in power\u2019, etinsu \u2018and in the\u2019.\r\n\r\n211\r\n\r\n\fSecondly, our documents are multilingual and we can have code-mixing both at the intersentential\r\nlevel and at the intrasentential level (on different levels of code-switching in historical texts see Kopaczyk\r\n2018). Different codes often correlate with different sections of the document. If we adopt the traditional\r\nsubdivision in the formulae which make up the document, we can see that the datatio and the dispositio\r\n(the core of the document) in Osp250 are written in Sardinian, while the roboratio testes and the completio\r\nare in Catalan. However, we also have intrasentential code-mixing. First of all, as could be expected\r\nin juridical documents, we have Latin expressions, such as ut supra, qui supra fidem facio. But, even\r\nmore interestingly, we have Catalan and Sardinian code mixing. The datatio in Osp250 is in Sardinian,\r\nbut we find the form en for the preposition \u2018in\u2019 , and the name of the month \u2018June\u2019 in the Catalan form\r\njunny. By contrast, in the completio, written in Catalan, the name of the month \u2018July\u2019 is in the Sardinian\r\nform treulas. Given the close affinity between the different languages present in the document, it is\r\nworth noting that, it is not always simple to identify the instances of code-switching, nor to distinguish\r\ncode-mixing from borrowing.\r\nFinally, our documents are \u2018stratified\u2019, since they have come to us via several passages. Osp250 contains\r\nthe last will of Canonigu Montixi, but the codicil was redacted by another scribe-priest, Antiogo Molarja.\r\nMoreover, the document we have was actually copied by the scribe Sebasti\u00E0 Polla in 1648 at the request\r\nof another citizen from Villanovafranca. The document finally arrived in the Archives of the Hospital\r\nof Sant\u2019Antonio, since Canonigu Montixi had decided that, were the chain of heirs to die out, his house\r\nwould have gone to the hospital.\r\n\r\n3\r\n\r\nCorpus building and annotations\r\n\r\n3.1\r\n\r\nCorpus building\r\n\r\nDue to the mixed nature of our corpus, we needed a software that was able to combine philological aspects\r\ni.e., faithful rendering of the manuscripts, bibliographic and historical information with the standard tools\r\nused in corpus linguistics i.e., a powerful and flexible query engine. Our choice fell on TEITOK4 (Janssen\r\n2016), a software developed by Marteen Janssen at the CELTA-ILTEC institute (University of Coimbra,\r\nPortugal); in a nutshell, TEITOK is organized in two main components: (i) a web-based application that\r\nrenders XML files annotated according to the TEI-P5 guidelines and (ii) a suite of executable binaries\r\nthat convert XML files into the Open Corpus WorkBench (CWB: Evert and Hardie 2011) file format.\r\nThe first component of Teitok fits our philological needs, as we were able to reproduce our manuscripts\r\nwith the original page and line breaks, ligatures and graphic variants of linguistic forms (words), while\r\nthe second component allows us to search our corpus using the Corpus Query Processor (CQP), either\r\nfrom the standard command line facility or using the web application.\r\nAlthough Teitok is also a powerful XML editor, we employed external XML editors such as oXygen in\r\norder to deal with the TEI encoding and annotation processes. Once annotated according to the TEI-P5\r\nguidelines5, TEI-XML files are uploaded to the web application where they are automatically split into\r\ntokens by the Teitok tokenizer. As for the linguistic annotations, Teitok contains some in-development\r\npos-tagging and lemmatization facilities, which have been proven to perform well on historical varieties\r\nof languages (Janssen et al. 2017); however, the parts of speech tagging and lemmatization processes,\r\nas well as the difficult process of the annotation of graphic variants are all performed manually: at the\r\nmoment the creation of annotation tools for Sardinian is work in progress (Puddu and Stein 2018) and no\r\nannotated corpus is available even for contemporary Sardinian.\r\nSumming up, our corpus building process can be summarized as follows:\r\n1. creation of the XML files: encoding of manuscripts;\r\n2. XML files become TEI-XML files: text annotation according to the TEI-P5 guidelines (TEI header\r\nand text elements);\r\n3. automatic tokenization of the TEI-XML files, which are stored in the web application (Teitok);\r\n4http:\/\/www.teitok.org\r\n5The EModSar corpus complies with the latest version of the TEI-P5 guidelines, 3.6.0 released on 16\/07\/2019. Whenever\r\nrelevant, we have indicated the URL for the online documentation in the footnotes.\r\n\r\n212\r\n\r\n\f4. manual pos-tagging, lemmatization and annotation of graphic variants.\r\n3.2\r\n\r\nAnnotations\r\n\r\nThe annotations featured in EModSar can be conveniently divided into three types: (i) document-level\r\nannotation, (ii) section-level annotation and (iii) token-level annotation.\r\nThe first type of annotation corresponds to the TEI element known as \u2018header\u2019 and contains bibliographic and, to a lesser extent, linguistic and sociolinguistic information; out of the five principal\r\ncomponents described by the TEI-P5 guideline6, we have compiled the \u2018file description\u2019, the \u2018text profile\u2019 and the \u2018revision history\u2019 components. The \u2018file description\u2019 component7 contains bibliographic\r\ninformation such as the repository, collection and archival reference of the manuscript, a brief history\r\nof the manuscript tradition and the name(s) of the author and copyist. In the \u2018text profile\u2019 component8,\r\nwe have gathered information about the place and redaction of the manuscript, the language(s) employed\r\nand a summary of the content. As we have pointed out in the previous section, this kind of information\r\nis of paramount importance for historical corpora. Finally, the \u2018revision history\u2019 component9, as the\r\nname suggests, works as a change log displaying the date when the TEI-XML file was last changed; the\r\ncomponent is most useful during the process of corpus building, which is usually characterized by many\r\nversions of the same TEI-XML file, often shared between several collaborators.\r\nAnnotations at the section-level are performed within the TEI element known as \u2018text\u2019, which in turn\r\nis divided into different sections, marked up by the <div> tag. Note that this text arrangement does not\r\nreproduce any formal elements of the original manuscript, but was carried out by the archivist during\r\nthe encoding process. As mentioned earlier, we decided to mark this structure since it appears to be\r\nrelated to code switching. The <div> tag contains two attributes: the section attribute, describing one\r\nof the formulae in which a notary document is customarily arranged and the language attribute, giving\r\nthe language used in the section. For instance, the following text snippet represents the section-level\r\nannotation of Osp250, whose formulae were mentioned in Sect. 2.2:\r\n...\r\n<div\r\n<div\r\n<div\r\n<div\r\n<div\r\n<div\r\n<div\r\n<div\r\n<div\r\n<div\r\n<div\r\n...\r\n\r\nn=\"1\" type=\"datatio\" lang=\"srd\" id=\"div-1\"> ... <\/div>\r\nn=\"2\" type=\"dispositio\" lang=\"srd\" id=\"div-2\">...<\/div>\r\nn=\"3\" type=\"notitia testium\" lang=\"cat\" id=\"div-3\">...<\/div>\r\nn=\"4\" type=\"subscriptiones\" lang=\"cat\" id=\"div-4\">...<\/div>\r\nn=\"5\" type=\"completio\" lang=\"cat\" id=\"div-5\">...<\/div>\r\nn=\"6\" type=\"completio\" lang=\"cat\" id=\"div-6\">...<\/div>\r\nn=\"7\" type=\"dispositio\" lang=\"srd\" id=\"div-7\">...<\/div>\r\nn=\"8\" type=\"completio\" lang=\"cat\" id=\"div-8\">...<\/div>\r\nn=\"9\" type=\"dispositio\" lang=\"cat\" id=\"div-9\">...<\/div>\r\nn=\"10\" subtype=\"dorsale\" lang=\"ita\" id=\"div-10\">...<\/div>\r\nn=\"11\" subtype=\"dorsale\" lang=\"cat\" id=\"div-11\">...<\/div>\r\n\r\nThe third type of annotation takes place at the token level and, just like the previous section-level\r\nannotation, is implemented through the attributes of the <tok> tag; the tag is not described in the TEI-P5\r\nguidelines and is added by Teitok during the automatic process of tokenization. Each token is annotated\r\nfor graphic variants and for linguistic information, for a total of five different attributes; as for the\r\ngraphic variants, we have distinguished between (i) \u2018written form\u2019, corresponding to the graphic variant\r\nas found in the manuscript, (ii) \u2018extended form\u2019, which is a written form with expanded abbreviations\r\nand (iii) \u2018normalized form\u2019, showing a tentative normalization of the graphic variant. For example, the\r\nannotation of the three different orthographic realizations of the preposition \u2018in\u2019, which we have discussed\r\nin Section 2.2 is given as follows:\r\n6https:\/\/tei-c.org\/release\/doc\/tei-p5-doc\/en\/html\/HD.html#HD1 Last accessed on 23\/11\/2019.\r\n7https:\/\/tei-c.org\/release\/doc\/tei-p5-doc\/en\/html\/HD.html#HD2 Last accessed on 23\/11\/2019.\r\n8https:\/\/tei-c.org\/release\/doc\/tei-p5-doc\/en\/html\/HD.html#HD4 Last accessed on 23\/11\/2019.\r\n9https:\/\/tei-c.org\/release\/doc\/tei-p5-doc\/en\/html\/HD.html#HD6 Last accessed on 23\/11\/2019.\r\n\r\n213\r\n\r\n\f<tok id=\"w-10\" form=\"en\" fform=\"en\" nform=\"in\" pos=\"PRE\" lemma=\"in\">en<\/tok>\r\n<tok id=\"w-100\" form=\"jn\" fform=\"jn\" nform=\"in\" pos=\"PRE\" lemma=\"in\">jn<\/tok>\r\n<tok id=\"w-513\" form=\"in\" fform=\"in\" nform=\"in\" pos=\"PRE\" lemma=\"in\">in<\/tok>\r\nAs for the linguistic information, we provide annotations for (iv) parts of speech and (v) lemma;\r\nthe parts-of-speech tagset is an adaptation of the tagset used in the Medieval Sardinian Corpus, which\r\ncontains texts written in an earlier stage of Sardinian (Puddu 2015, Puddu and Stein 2018), and features\r\n25 tags, some of which are specified for morpho-syntactic properties such as verbal mode and nominal\r\ndefiniteness.\r\nFinally, let us just briefly mention how we handled linguistic expressions - mostly, noun and prepositional phrases - written without spaces between words in the manuscripts. In order to faithfully reproduce\r\nthe manuscripts, these linguistic expressions are encoded without spaces in the written form of EModSar,\r\nwith a correspondence between a linguistic expression and a single token; at the same time and for the\r\npurpose of linguistic queries, the linguistic expression is split into tokens in the normalized form of\r\nour corpus by means of another non-standard TEI tag, <dtok>, which is introduced by Teitok (Janssen\r\n2016:4038) and nested into the <tok> tag. Take for instance the prepositional phrase in podere, which\r\nwas originally written as a single word in one of the manuscripts:\r\n<tok id=\"w-280\" form=\"inpodere\" fform=\"in podere\" nform=\"in podere\">\r\ninpodere\r\n<dtok id=\"d-280-1\" form=\"in\" fform=\"in\" nform=\"in\" pos=\"PRE\" lemma=\"in\"\/>\r\n<dtok id=\"d-280-2\" form=\"podere\" fform=\"podere\" nform=\"podere\" pos=\"NOUN\"\r\nlemma=\"podere\"\/><\/tok>\r\n\r\n4\r\n\r\nFurther developments\r\n\r\nIn building the Early modern Sardinian Corpus we have already achieved several objectives, summarized\r\nas follows:\r\n\u2022 we established an annotation schema for Early Modern Sardinian notary deeds which allows all the\r\nrelevant external information to be preserved;\r\n\u2022 we have inserted our documents into Teitok which, not onlymakes it easy to use for different kinds\r\nof users, but also permits linguistic searches to be performed with standard corpus tools;\r\n\u2022 since the documents will be freely downloadable, they can be re-used for other searches (for instance,\r\npersonalized queries through XPath, or through other platforms like TXM).\r\nThe first studies on the languages used in the documents show the importance of being able to\r\ncombine linguistic information and extralinguistic information and of considering texts in a multilingual\r\nperspective. For instance, we were able to confirm our idea that, some sections in our documents,such as\r\nthe completio and the subscriptiones, are generally in Catalan while in others, like the datatio, Sardinian\r\nalternates with Latin. The use of Catalan and Latin thus seems to be correlated to more \"formal\" discourse\r\nmoves and is used to add authority to the document. Moreover, since we also collected extralinguistic\r\ninformation, we were able to correlate linguistic phenomena with different levels of linguistic variation.\r\nFor example, some of our documents show variants that mantain the original Latin consonant cluster -pl\/-bl- (as complimentu and obligare) while others have the innovative form in -pr-\/-br- (like comprimentu\r\nand obrigare). Our corpus allowed us to see that the forms in pr\/br tend to appear in documents which\r\nalso show some other \"lower\" phenomena like the methathesis of -r- (as in frimadu for firmadu) and it\r\ncan consequently be hypothesized that both correlate with diastratic variation.\r\nFuture work will focus on two points:\r\n\u2022 at a more general level we need to develop the structural coding of more complex documents such\r\nas court trials, which arrived in the form of a summary report containing different documents such\r\nas letters, trial witness statements, and attestations relative to the delivery of convocations;\r\n\r\n214\r\n\r\n\f\u2022 some issues on normalization and lemmatization are still to be discussed, especially if we want to\r\nplace our corpus in a diachronic and ambitious perspective as one of the steps for the construction\r\nof a diachronic corpus of Sardinian.\r\nIt goes without saying that, only by increasing the size of our corpus, can we confirm the already\r\nnoticed tendencies and give a more detailed picture of the multilingual practices in Modern Sardinia.\r\n\r\nAcknowledgements\r\nThe authors wish to thank Maarten Janssen for his wonderful support on Teitok."
	},
	{
		"id": 34,
		"title": "Shared emotions in reading Pirandello. An experiment with Sentiment Analysis",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Simone Rebora"
		],
		"body": "Introduction\r\n\r\nSentiment Analysis (SA) has recently grown in relevance in Digital Humanities. This computational\r\ntechnique, originally developed with the goal of analyzing \"people\u2019s opinions, sentiments, appraisals,\r\nattitudes, and emotions towards entities and their attributes\" (Liu, 2015), has found multiple applications\r\nin literary studies. From the widely discussed \"shapes of stories\" by Jockers (2014) and Reagan et al.\r\n(2016), to the study of fairy tales (Mohammad, 2012; Rotari, 2018), literary criticism (Rebora, 2017;\r\nMellmann and Du, 2018), genre (Kim et al., 2017; Henny-Krahmer, 2018), and narrative structure (Zehe\r\net al., 2016), SA has become one of the key methodologies in computational literary studies. For an\r\nextensive survey, see (Kim and Klinger, 2018). However, criticisms abound, both in theoretical (Ciotti,\r\n2017) and practical (Sprugnoli et al., 2016) terms.\r\nWith this paper, I will report on an experiment aimed at verifying the efficiency of the approach in the\r\nstudy of two related phenomena: the narratological structure of a story and its associated reader response.\r\n\r\n2\r\n\r\nThe Experiment: Bringing Research and Didactics Together\r\n\r\nThe experiment was conducted during the Digital Humanities course (Informatica per gli studi umanistici)\r\nheld at the University of Verona in the academic year 2018\/2019. Students were asked to read the short\r\nstory (novella) \"Ci\u00E0ula scopre la luna\" (1907) by Luigi Pirandello and were provided with an XML file\r\nwith the following structure:\r\n\u2022 the <novella> root tag;\r\n\u2022 the child <frase>, containing one paragraph from the short story;\r\n\r\n216\r\n\r\n\f\u2022 the child <sentiment>, which the students were asked to fill with a numeric evaluation of the\r\nsentiment of the paragraph, ranging between -5 and +5;\r\n\u2022 the child <commento>, where the students could write a free comment on the effects produced by\r\nreading the passage.\r\nFull text of the novella was downloaded from LiberLiber and based on the 1986 Mondadori edition\r\n(Pirandello, 1986). Sentences were automatically split using the SA software Syuzhet, that was adopted\r\nas a groundwork for the entire experiment1.\r\nAt the end of the annotation process, a total of 51 students wrote at least one comment or sentiment\r\nevaluation, for a total of 1,884 comments (36.94 per student) and 1,401 sentiment evaluations (27.47 per\r\nstudent). The 51 XML documents were then anonymized and merged into a single file, available for\r\nconsultation (together with the R scripts for its analysis) on Github.\r\nThe experiment had the didactic purpose of letting students familiarize with the XML markup language\r\nand with SA computational techniques (both, in a very simplified form). In terms of research purposes,\r\ntheir annotations proved precious for a verification of the efficiency of SA approaches.\r\n\r\n3\r\n\r\nAnalysis\r\n\r\n3.1\r\n\r\nAgreement on Sentiment Annotations\r\n\r\nFigure 1 shows all sentiment annotations by the students. As evident, annotations are widely spread\r\nthroughout the 111 paragraphs of \"Ci\u00E0ula\", with a dominance of the central levels of emotionality and a\r\ndeviation towards the most positive levels only at the end of the novella.\r\n\r\nFigure 1: Sentiment annotations on \"Ci\u00E0ula scopre la Luna\" (51 annotators: -5\/+5)\r\nFor a more detailed understanding of the level of agreement, Krippendorff\u2019s Alpha (Krippendorff, 2018)\r\nwas adopted. To maximize the possible agreement, annotations were reduced to a binary selection:\r\n\u2022 annotation value < 0: \"negative\" tag;\r\n\u2022 annotation value = 0: no tag;\r\n\u2022 annotation value > 0: \"positive\" tag.\r\n1Note that Syuzhet was designed to work on a sentence level (in fact, repeated words do not count towards the total sentiment).\r\nThis is why annotation was performed on a sentence (and not paragraph) level.\r\n\r\n217\r\n\r\n\fHowever, Krippendorff\u2019s Alpha was still substantially low (0.19), thus confirming the result of Sprugnoli\r\net al. (2016), who showed how inter-annotator agreement advises against the application of SA to historical\r\n(or literary) texts.\r\nGiven this acknowledgment, still, some interesting outcomes can be derived from the experiment.\r\n\r\nFigure 2: Krippendorff\u2019s Alpha for sentiment annotation (51 annotators: POS\/NEG; 11-paragraph\r\nmoving window)\r\nFigure 2 shows the evolution of inter-annotator agreement through a moving window procedure: Krippendorff\u2019s Alpha is calculated on just 10% of the text (corresponding to 11 paragraphs), moving from its\r\nbeginning to its end. The most striking result is in the peak of inter-annotator agreement, that comes not\r\nat the very end of the novella (marked by a dominance of positive emotions), but a few paragraphs before.\r\nPrecisely, it happens around paragraph 98 (\"Dapprima, quantunque gli paresse strano, pens\u00F2 che fossero\r\ngli estremi barlumi del giorno\"), that signals the beginning of the emotional shift (the \"plot twist\") in\r\nthe novella: Ci\u00E0ula, still fearing the blank darkness of the night, gradually discovers the presence of the\r\nmoon. This result confirms how an actual sharing of emotions happens not at their climax but with their\r\nmodification, and transformation\u2014as already noted by Oatley (2012)\u2014is the driving force of narratives.\r\n3.2\r\n\r\nCorrelation in Sentiment Analysis\r\n\r\nSA of text and comments was performed using the simplest method (wordcount) implemented by the\r\nSyuzhet package. Being Syuzhet designed for the analysis of English language and given the much more\r\ninflected nature of Italian language, analysis was performed on lemmatized texts, that were prepared\r\nthrough the UDpipe software. Two Italian sentiment dictionaries were prepared and uploaded in Syuzhet:\r\n\u2022 Sentix, where sentiment values were calculated as the product of polarity and intensity;\r\n\u2022 OpeNER (Russo et al., 2016), where sentiment values were calculated as the product of sentiment\r\nand confidence.\r\nTo keep a direct connection between text and comments, for each paragraph of the novella:\r\n\u2022 a single sentiment value was calculated for the text;\r\n\u2022 the mean of all sentiment values was calculated for the comments.\r\nFigure 3 shows a comparison between the analyses of text and comments with the two sentiment dictionaries. A reference point (the black, dashed line) was set by calculating the means (per paragraph) of\r\n\r\n218\r\n\r\n\fFigure 3: Sentiment analysis of \"Ci\u00E0ula\": text (left) and comments (right)\r\nFocus\r\n\r\nSentix\r\n\r\nOpeNER\r\n\r\nparagraph\r\ncomments\r\n\r\n0.135\r\n0.454*\r\n\r\n0.266*\r\n0.659*\r\n\r\nTable 1: Pearson correlations between SA results and mean values of manual annotation. Asterisks\r\nindicate significant correlations (p-value < 0.05)\r\nthe sentiment values annotated by the students. A mathematical evaluation of the similarity between the\r\nplots was provided by Pearson correlation tests. See Table 1 for an overview of the results.\r\nAt least two phenomena call for attention. First, OpeNER seems to achieve better results than Sentix.\r\nSecond, and most importantly, analyses of comments show much higher correlations than analyses of the\r\ncommented text. This may be considered as a confirmation of the fact that SA is much more effective\r\nwhen studying reader response, than when analyzing narrative structure, as already shown by Rebora and\r\nPianzola (2018). These results become even more striking when applying the \"rolling mean\" procedure,\r\nimplemented in Syuzhet to harmonize plots (see Figure 4): here the similarity can be noticed with the\r\nnaked eye.\r\n\r\nFigure 4: Sentiment analysis of \"Ci\u00E0ula\": text (left) and comments (right), normalized with rolling mean\r\n\r\n4\r\n\r\nConclusion\r\n\r\nThe small dimensions of the analyzed corpus call for caution when trying to generalize such results.\r\nHowever, they are in line with evidence already presented in previous studies and they call for new research\r\non the topic. In particular, the high correlation in the SA of comments suggests how, notwithstanding the\r\nlow agreement between readers when trying to evaluate the sentiment of a text, SA is still able to catch\r\n\r\n219\r\n\r\n\fgeneral trends in reader response. At this point, two main lines of enquiry seem advisable: one, that\r\nfocuses on improving the methodologies further2; another, that tries to tighten the connection between\r\ncomputational methods and literary theory.\r\nIn conclusion, while being still a very problematic and disputable technique, SA offers multiple stimuli for\r\ntheoretical and methodological reflection, revealing how, through a direct confrontation with its limitations\r\nand imperfections, research in Digital Humanities can still progress towards unexplored grounds.\r\n\r\n5 Acknowledgments\r\nI thank Tiziana Mancinelli for allowing me to perform this experiment with her students."
	},
	{
		"id": 35,
		"title": "DH as an ideal educational environment: the Ethnographic Museum of La Spezia",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Letizia Ricci",
			"Francesco Melighetti",
			"Federico Boschetti",
			"Angelo Mario Del Grosso",
			"Enrica Salvatori"
		],
		"body": "Introduction\r\n\r\nThe authors present the outcomes of an educational experimentation that took place in the academic year\r\n2018-2019 at the degree course in Informatica Umanistica at the University of Pisa. The experimentation\r\ninvolved the courses of Digital Public History, Digital Text Encoding as well as Digital Philology, and at\r\nthe beginning concerned the digitization of a corpus of postcards from the period of the First World War\r\nowned by the ethnographic Museum of La Spezia \u201CG. Podenzana\u201D.\r\nThe postcards have been historically contextualized, digitized, placed on a collaborative web platform\r\nand distributed to the students in order to be recorded, transcribed and encoded in XML-TEI. Students\r\nhave been involved in the development of the web platform by tracking the usability issues as beta-testers.\r\nStudents contributed also to the requirement analysis and the definition of the specifications necessary to\r\nextend the platform to a broader audience of users without specific skills in Digital Humanities.\r\nIndeed, the project aims were not only the historical study of the corpus, but also the organization\r\nof a public history project with the Museum, its targeted audience and the High School students of La\r\nSpezia. Arriving almost at the end of this educational experiment, we propose now to discuss the current\r\nachievements, based on the common educational statement of \u201Clearning by doing\u201D and announced months\r\nago at AIUCD 2019.\r\n\r\n222\r\n\r\n\f2\r\n\r\nBackground\r\n\r\nWithin the previous annual conference of the Italian Digital Humanities Association held in Udine\r\n(AIUCD2019), a preliminary work towards a profitable collaboration between students and teachers of\r\ndifferent DH classes at the University of Pisa was presented. The context of the project (Booth, 1996;\r\nCati, 2006; Cole, 2016; Delle Cave, 2013) gave the actors the opportunity to collaborate with each other\r\nand with the \u201CG. Podenzana\u201D museum of La Spezia outside the formal classroom constraints, involving\r\nalso some activities carried out by non academic communities (Salvatori, 2017). In Salvatori et al.\r\n(2019), the authors discussed the objectives and the outcomes that have been achieved during the bootstrap\r\nphases of the project. In particular, within that paper, they pointed out the main problems and the added\r\nvalues of the initiative introduced above.\r\nTo date, the collaboration has been getting wider and a few internships have been activated to improve\r\nthe design and development skills of the interested students in order to enhance the tools already developed\r\nwithin the Euporia platform (Mugelli et al., 2016). These new activities facilitate students and general\r\nusers in digital encoding historical documents, which have an inherently complex nature. This objective\r\nhas been possible by adopting a formal but \"common\" rules for annotating and for processing textual data\r\n(Fowler, 2010). Moreover, as far as the actual TEI-XML encoding work (Burnard, 2014; Pierazzo, 2015)\r\nwhich concerns the digitization, the recording and transcription of the postcards provided by the involved\r\ncultural institutions, the main problems have been overtaken by putting in place a chain of document\r\nprocessing tasks. This process has been developed by using XSLT technology and by implementing a\r\nWeb environment to publish the encoded documents (Del Turco and Di Pietro, 2016).\r\n\r\n3\r\n\r\nMethods\r\n\r\nThe project involves students, teachers and representatives of the Ethnographic Museum of La Spezia,\r\nwhich collaborate by sharing information, resources and tools. The Museum has made available two\r\nlarge corpora of postcards dating back to the Great War period, which have been digitized, uploaded onto\r\nthe Euporia platform and encoded in XML-TEI by the students under the supervision of the teachers,\r\naccording to a custom subset of tags declared in the CartolineXML schema. Furthermore, the Museum\r\nplayed the role of an interdisciplinary meeting center among High School students of La Spezia, students\r\nand teachers of the University of Pisa and the Museum managers, in order to share ideas about the project\r\nfrom different perspectives.\r\nTwo students of the aforementioned courses and co-authors of this contribution, made an internship\r\nat the CNR-ILC to improve their skills in text encoding. They focused on the simplification of the\r\nannotation process, in order to involve High School students and volunteers that could actively collaborate\r\nin transcribing and annotating postcards, even if they have not specific skills in XML-TEI encoding.\r\nTherefore, they have defined a Domain Specific Language (DSL), CartolineDSL, with the same\r\nexpressivity of its counterpart in XML-TEI but much less verbose. A DSL is a formal language with a\r\nsimple, understandable and suitable syntax for the domain of interest we are dealing with and based on\r\na limited and controlled vocabulary. A DSL must be defined by a Context-Free Grammar (CFG), that is\r\na set of recursive rewriting rules used to generate string patterns. Therefore CartolineDSL is a language\r\nsuited to the domain of postcards characterized by a series of \"*attribute: value\" fields that users can\r\neasily fill in.\r\n\r\n223\r\n\r\n\fFigure 1: Formal grammar code snippet for the postcard corpus within the Euporia digital environment\r\n\r\nFig. 1 illustrates some rewriting rules in the CFG of CartolineDSL, whereas Fig. 2 shows an example of\r\ndata and metadata encoded in CartolineDSL.\r\n\r\nFigure 2: CartolineDSL snippet\r\n\r\nThe conversion of CartolineDSL to XML-TEI is performed in two steps. In the first phase, the annotations\r\nencoded in CartolineDSL are parsed by the ANTLR compiler compiler (Parr, 2013) and converted in\r\nXML. The proprietary CartolineML schema allows the serialization in XML of the Abstract Syntactic\r\nTree (AST) parsed by ANTLR. In the second phase, the proprietary XML document is converted to\r\nXML-TEI by an XSLT transformation. The XSLT style-sheet has been created by the students on the\r\nlatest part of their internship at the ILC-CNR.\r\n\r\n224\r\n\r\n\fAs usual, other XSLT style-sheets are necessary to transform XML-TEI in HTML for visualization\r\npurposes. Fig. 3 shows the designed interface.\r\n\r\nFigure 3: Mockup sketch of the ongoing web-app aimed at publishing the archive\r\n\r\n4\r\n\r\nResults\r\n\r\nThe main achievements of this didactic experimentation are listed below: 1. coordination of three\r\ncourses, in order to work on the same materials from different perspectives (Public History for the\r\nhistorical contextualization of the project, Text Encoding for XML-TEI models and technologies, and\r\nDigital Philology for the treatment of uncertain readings and for the creation of an optimized human\r\nreadable DSL); 2. engagement of students in the annotation process of a large sample of the postcards\r\ncorpus (learning by doing); 3. transfer of knowledge and experience from students of the University of\r\nPisa and High School students of La Spezia during meetings at the Museum; 4. involvement of students\r\nin the creation of Domain-Specific Languages meant to bridge the gap between the best practices of\r\nDigital Humanists and the simple practices of unskilled citizens that desire to participate in projects of\r\nPublic History.\r\n\r\n5\r\n\r\nConclusion and Future Work\r\n\r\nWe guess that the educational model that we experimented can be easily exported in other contexts, with a\r\nbroader involvement of multidisciplinary communities of practice and applied to different textual and\/or\r\niconographic (or multimedia) digital resources.\r\nIn the next academic year we will release an updated version of Euporia, which currently is just\r\na prototype, in order to allow High School students and volunteers to annotate further postcards in\r\nCartolineDSL."
	},
	{
		"id": 36,
		"title": "A digital review of criticals edition: A case study on Sophocles, Ajax 1-332",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Camilla Rossini"
		],
		"body": "In the next pages I will give an account of a still ongoing project conducted at the University of\r\nLeipzig under the supervision of professor G. Crane. The project aims at modelling a framework for\r\npublishing reviews of critical editions of classical works in a digital environment (Smart Reviews - SR),\r\nthus re-thinking the review genre at its roots. For a first experimental mock-up, the chosen case study is\r\nSophocles, Ajax, 1-332. The sequential steps we designed are: 1) to select from two or more editions some\r\nnoticeable readings; 2) to link them to the corresponding places in a text chosen as a base reference, with\r\nan unambiguous reference system; 3) to compare them with the aid of external tools, in order to explain\r\nthe editorial choices behind them. After a paragraph on the purposes, the uses and the shortcomings of\r\n\u2018traditional\u2019 reviews on paper, I will proceed to show in further detail the advantages of a SR and the\r\npassages to realize it. Finally, I will list some future developments.\r\nWhenever a new critical edition of an ancient text is published, other scholars carefully read it, compare\r\nit to previous texts and finally publish reviews of it on academic journals. Besides overall judgments\r\non the edition\u2019s quality, bibliographic suggestions and further comments on specific editor\u2019s remarks, a\r\nreview of a critical edition usually provides an account of the most noticeable editorial choices on the\r\ntext. Textual renditions of controversial readings, new conjectures or the recovery of old ones, and maybe\r\nthe comparison with the latest edition(s) on some crucial passages, are what really defines the work of\r\nthe editor on the text itself, and are thus the ultimate object of the reviewer\u2019s judgment.\r\nThe reviews of critical editions, finally, play an irreplaceable role for the users as well. Not only are\r\nthey often, at a practical level, the only way to access a new edition in the absence of it, while waiting,\r\nfor example, for University libraries to purchase it; even more importantly, they provide a list of the\r\ndifferences between critical texts in different editions, thus saving the time for the reader to detect them\r\nby manually comparing two or more printed books.\r\nNevertheless, such important tasks in this kind of reviews are, at least, hard to perform on a less-thanabstract level. An example will explain why:\r\nFinglass often succeeds in defending transmitted text: he agrees with OCT against Dawe\u2019s Teubner in about 22 cases (for\r\nexample 446, 771, 782, 790, 988, 1027, 1059, 1282, etc.), the reverse occurring about 15 times (for example 114, 191,\r\n420, 630, 1357, etc.)1.\r\n\r\nThis passage, from a review to Finglass\u2019 2011 edition of Sophocles\u2019 Ajax, is just one of many similar\r\nones. Finglass\u2019 work is compared with the two previous major editions (Lloyd-Jones \u2013 Wilson\u2019s and\r\n1Catrambone, 2013, p.169 on Finglass, 2011.\r\n\r\n227\r\n\r\n\fDawe\u2019s2), but only some specimens of agreement or disagreement are quoted, and for each of them the\r\nmere verse number is provided. The job of finding out where and how the three editions are unanimous\r\nor less so, is for the reader to do. Of course, the limited space of a review requires conciseness, and an\r\nextensive - rather than intensive - approach.\r\nSuch shortcomings are intrinsically linked to the printed (or printed-like, for the PDF distributed\r\njournals) format that the review articles have had so far. The main contents of a review, though, can\r\nbe described as links between corresponding passages in different editions. In such a way, they could\r\nperfectly support a digital metamorphosis of the genre. Moreover, a fully digital distribution (what we\r\ncould call a Smart Review, SR) could provide more effective comparisons between editions, and links\r\nto external resources could give the reader insights on the editors\u2019 choices. This way, not only the old,\r\nconsolidated tasks of the \u2018traditional\u2019 reviews are performed better and in a more feasible way; but what\r\nis more, a SR could improve and widen the usefulness of the reviewing and comparison on multiple\r\neditions3.\r\nThis shift in perspective is even more desirable if we think about the Scholarly Digital Editions (SDEs).\r\nMore and more as we move on, new versions of the same ancient texts become available online: not only\r\nas scanned out-of-copyright editions, but also as new uploads in large online repositories for plain or\r\nannotated texts, like treebanks4. Even though the sense to assign to the expression \u2018SDE\u2019 is controversial,\r\neach of those new documents bears a specific version of a text that becomes available to a large public;\r\nmoreover, both the digital-born plain texts and the linguistic annotated ones, often imply a critical revision\r\nby the digital editor. Unfortunately, the communication problems that have been acknowledged between\r\nprinted editions, stand for digital publications as well. The artificial sense of fixedness of each of those\r\n\u2018base texts\u2019 is often reinforced by the absence of critical apparatuses, that flattens the editor\u2019s opinions and\r\ntextual decisions in favour of a totally illusory objectiveness. It has been said that the technical possibility\r\nto publish all the witnesses and all the editions would lead to \u00ABa sort of \u2018B\u00E9dier effect\u2019\u00BB5, where everyone\r\npublishes an edition or a witness without establishing a critical text.\r\nAlthough the study of single editions or single manuscripts can have great applicability in many fields,\r\nthe differences among SDEs (broadly intended) is often underaddressed, and a great number of divergent\r\npassages remains unnoticed. This problem becomes even more visible when translations are involved.\r\nNot infrequently, the translations are made available online without their corresponding original text,\r\nmaking it difficult to address and explain the textual choices behind them6. To sum up, each digitally\r\npublished text is liable of becoming an arbitrary base text.\r\nThe idea behind a SR is the opposite. Its goal is to show the diverging readings in traditional or digital\r\neditions by juxtaposition, thus not necessarily stating a hierarchy between them, similarly to what happens\r\nin traditional reviews. It is true, though, that we can not do without a base text to anchor each reading to its\r\nproper position, because a section of the text where the two or more editions diverge doesn\u2019t have, by its\r\ndefinition, a lemma to unequivocally refer to. Thus, the first criticality to address is the need to provide an\r\nunambiguous anchoring of the noticeable readings. The most frequently implemented solution, the XML\r\nAppCrit module, is not suitable for our purpose. Firstly, it has a binary (and thus, hierarchical) distinction\r\nbetween lemma and reading. Secondly, a core need of a SR is to be flexible, updatable, reusable, and for\r\nthose necessities a standoff markup seems like a better choice7.\r\n2Lloyd-Jones and Wilson, 1994; Dawe, 1996.\r\n3 Gabler, 2010.\r\n4See Crane et al., 2014. On editorial interventions on treebanks see e.g. Bamman et al., 2009, 10: \u00ABA scholarly treebank\r\n[...] reflects an interpretation of a single scholar\u00BB. On textual variation and ambiguity in treebank annotation see also Bamman\r\nand Crane, 2010, p. 548; Beaulieu et al., 2012, p. 400.\r\n5Bartoli, 2015. In 1928, J, B\u00E9dier suggested that, as the Lachmannian method was practically unreliable, a single witness\r\n(codex optimus) should be chosen and edited. See B\u00E9dier, 1928.\r\n6 A basic example will show it. Accessing Soph., Aj. 35 on Perseus, one will find: \u03C3\u1FC7 \u03BA\u03C5\u03B2\u03B5\u03C1\u03BD\u1FF6\u03BC\u03B1\u03B9 \u03C7\u03B5\u03C1\u03AF (\u2018hand\u2019). The\r\ncorresponding English translation perfectly matches the text: \u00ABit is your hand that steers me\u00BB. Oppositely, if we take Romagnoli,\r\n1926, whose Italian translation is freely available e.g. on Wikisource, we read: \u00ABil senno tuo per guida io prender\u00F2\u00BB, whic\r\ntranslates as \u00ABI will always take your wisdom as a guidance\u00BB, and not \u00AByour hand\u00BB. Poetic license? No, only a varia lectio that\r\nis recorded in most editions. The tradition is divided between \u03C7\u03B5\u03C1\u03AF and \u03C6\u03C1\u03B5\u03BD\u03AF. Finglass, 2011, 80 chooses the former, Dawe,\r\n1996, 3, the latter.\r\n7See the fundamental benchmark of the database of latin texts by the Digital Latin Library (LDLT, 2019) that, in a much\r\nwider perspective, modified the XML TEI P5 module 12 for Critical Apparatus (Guidelines, 2019) for its own purposes (Cayless\r\n\r\n228\r\n\r\n\fFor these reasons, I tokenized and corrected an OCRed file of Pearson\u2019s 1922 out-of-copyright edition8.\r\nFrom this, I provided an automatically compiled list of references to each word, with unique identifiers (see\r\nfig. 1). To do so, my benchmark has been the CTS URNs model as implemented by the Perseus Catalog9.\r\nEach work in the Perseus Library (and in the new Scaife Viewer as well) has a string that identifies it. For\r\nexample, the greek edition of Soph., Aj. 1-332 is referenced by urn:cts:greekLit:tlg0011.tlg003.perseusgrc2:1-332, where tlg0011 and tlg003 are the traditional codes assigned by the TLG project respectively\r\nto the author Sophocles and to the work Ajax, and perseus-grc2 identifies the edition digitized by the\r\nPerseus team. The reference goes as far as pointing at a verse or a group of verses (in the example above,\r\nverses 1-332). Basing on the work already done on texts from the Perseus Digital Library and the First\r\nThousand Years of Greek Project, I extended the unique reference system down to the word level10. Thus,\r\neach word has an identifier with this ideal structure:\r\nurn.soph.ajax.pearson@134\u03A4\u03B5\u03BB\u03B1\u03BC\u03CE\u03BD\u03B9\u03B5[1]\r\n\r\nFirstly conventional abbreviations of the author, the work, and the edition are listed, separated by a\r\nmark (I used a dot); then, after an @, the verse and the word are reported and, finally, a number between\r\nsquare brackets that indicates the occurrence of the same word form in that verse. This formulation of the\r\nCTS URN is totally conventional. For our purposes here, it could be cited also in its abbreviated form:\r\n134\u03A4\u03B5\u03BB\u03B1\u03BC\u03CE\u03BD\u03B9\u03B5[1].\r\n\r\nFigure 1: A section of the CTS file from Pearson\u2019s edition, referencing Soph., Aj. 1: \u1FBF\u0391\u03B5\u1F76 \u03BC\u03AD\u03BD, \u1F66 \u03C0\u03B1\u1FD6 \u039B\u03B1\u03C1\u03C4\u03AF\u03BF\u03C5, \u03B4\u03AD\u03B4\u03BF\u03C1\u03BA\u03AC \u03C3\u03B5.\r\nNote the [2] in the cts with id 7, that denotes the second comma in the same verse.\r\n\r\nI then divided the material into four sections: an ordered list with vv. 1-332 of Pearson\u2019s edition, where\r\neach word is assigned with such a CST URN reference (see fig. 2a); a database containing the noticeable\r\nreadings found in the editions under analysis, and their position in reference to file 1 (see fig. 2b); another\r\ndatabase containing the matches between each edition and the readings that could be found in it11 (see\r\nfig. 2c); finally, in another database, the broadly meaning commentary material has been linked to the\r\ncorresponding readings (see fig. 2d).\r\nLinking the noticeable passages of each edition to the correct unit of text is not an easy matter. I\r\ncame up with a conventional set of rules. I considered lexical substitutions, additions, subtractions and\r\nmovements. For each of them I had to keep in mind that both the reading and the referenced passage\r\ncould be formed by one word (see fig. 3a) or by a group of words (see fig. 3c). To each reading I added\r\ntwo attributes: from and to. They respectively mark the point in the CTSized text where the variant\r\nbegins and ends; if they coincide, it means that the reading modifies only a word in the base CTSized\r\nand Huskey, 2018). See also the XML structure of the Euripides Scholia Project (Mastronarde, 2010), whose editor chose not\r\nto use the TEI module for the Critical Apparatus \u00ABbecause in a project of this kind it seems to me that it would involve an\r\nunjustifiably large overhead of markup\u00BB. About it, see Driscoll and Pierazzo, 2016, 213. For a theoretical comparison between\r\ninline and standoff markup see e.g. Schmidt, 2012; Eide, 2014; Petersen, 2016; Boschetti, 2007; Monella, 2008. For an overview\r\nof the criticalities of the XML TEI module 12, see the report issued by the Critical Apparatus Workgroup (Workgroup, 2014).\r\n8 Pearson, 1924.\r\n9 See the usage of CTS URNs and the Cite Architecture, both developed by the Homer Multitext project, by the Perseus\r\nCatalog. See Blackwell and Smith, 2014; Babeu, 2015; Blackwell and Smith, 2019b; Architecture, 2019; Tiepmar and Heyer,\r\n2019; Blackwell and Smith, 2019a; Babeu, 2019.\r\n10 See Celano, 2017 on texts taken from the Perseus Digital Library (Perseus, 2019a,b) and the First Thousand Years of Greek\r\nProject (OGL, 2016). See also the new Scaife Viewer (Perseus, 2019c).\r\n11 Thanks to this organization of the material, I reduced the redundance to much less than if, say, I had to list the noticeable\r\nreadings for each edition.\r\n\r\n229\r\n\r\n\f(a)\r\n\r\n(b)\r\n\r\n(c)\r\n\r\n(d)\r\n\r\nFigure 2: A reading (b) linked to its initial and final CTS URNs (a) and chosen in Dawe\u2019s edition (c), with comments on it by\r\nFinglass and Dawe (d).\r\n\r\n(a)\r\n\r\n(b)\r\n\r\n(c)\r\n\r\n(d)\r\n\r\nFigure 3: Types of variation. Interpretive (a), movement (b), substitution (c), subtraction (d).\r\n\r\ntext. This method works fine for substitutions (see fig. 3a12). For subtractions as well, it was enough to\r\nclearly show the reading as empty (see fig. 3d).\r\nIn the case of the word(s) addition, one needs to use a clear way to show it. I pointed at the space\r\nbetween two words by using the conventional formula 134\u03A4\u03B5\u03BB\u03B1\u03BC\u03CE\u03BD\u03B9\u03B5[1]+1 (to refer to the position after\r\nthe word \u03A4\u03B5\u03BB\u03B1\u03BC\u03CE\u03BD\u03B9\u03B5) or 134\u03A4\u03B5\u03BB\u03B1\u03BC\u03CE\u03BD\u03B9\u03B5[1]-1 (to refer to the position before it). Finally, movements have\r\nbeen pointed at with the self-closing element movement (see fig. 3b, that also shows the use of +1 and\r\n-1).\r\nThis system has multiple advantages: in the first place, it becomes machine-inferable (but quite clear\r\nto the human reader as well) where and how each edition differs from the chosen base text, and from\r\neach other. The material is kept separate and clean, with an easy way to add, change and modify parts\r\nof it without having to alter the structure of the existing files. Moreover, the overlapping of variants\r\nbecomes possible without complex systems as it is in the XML TEI. The basic types of intervention\r\nadopted by each edition can be easily inferred by an algorithm, by comparing the reading with the from\r\nand to attributes and, if necessary, by directing the reader to the comments (see footnote 12 about fig. 3a).\r\nWhatismore, in the exact same way as a group of readings is connected to an edition, other groups may be\r\nfigured out and collected under specific types that go beyond the core distinction between orthographic,\r\n12 When the reading is identical to the \u2018base text\u2019, the comment material could tell us if the word is listed as a variant because\r\nit is a homograph - like in the case of fig. 3a - or because it is just an interpretive variant on the same word form.\r\n\r\n230\r\n\r\n\f(a)\r\n\r\n(b)\r\n\r\nFigure 4: Link to treebank (a). Treebank and aligned translation: Finglass versus Pearson (b)\r\n\r\nmorphological and lexical variants that is provided, for example, by the Digital Latin Library13.\r\nAnother advantage of a SR is that it can point to external sources in order to give the reader insights\r\nabout the differences between texts. The variants chosen by each editor alter the surrounding text in\r\ndifferent ways. Some of them may generate syntactic differences, some other may remain on the lexical\r\nlevel. Finally, other variants are only due to different interpretations, and don\u2019t affect the texts themselves,\r\nbut are only visible in the translations. Through the \u2018comments\u2019 section, the available online tools can be\r\nlinked to specific passages in the considered editions to show these differences.\r\nFor the variants that have an impact on the morphology and the syntax, links to their treebank annotation\r\nand graphical visualization on the Arethusa Treebank Editor can be provided in the \u2018comment\u2019 database.\r\nIn this experimental case, the treebanks for each critical edition have been compiled using as a base the\r\nfile uploaded by the Ancient Greek and Latin Dependency Treebank project14. The comparison between\r\ntreebanks of corresponding passages in different editions makes us able to encode precisely the difference\r\nbetween editorial choices. Finally, not all variations affect the translation. For the ones that do, links to\r\nparallel translation alignments can be provided15 (see fig. 4).\r\nA theoretical framework for a SR addresses, on the one hand, some problems that are known to the\r\nlong-lasting debate over Scholarly Digital Editions (SDEs) and, more broadly, to the field of annotated\r\ntexts. The comparison between editions, core element of the SR, urges to find a way for handling the\r\ntextual variation in a digital fashion, i.e. to represent variants and to link them to the base text, which\r\nis itself the object of a dispute16. On the other hand, though, the SR\u2019s intrinsic differences from SDEs\r\ncompel us to find new solution. The main distinction is probably the programmatic desultoriness of\r\nthe provided data. Only the important readings, and not all the text as in SDEs, are named in \u2018printed\u2019\r\nreviews, hence the same principle should apply to SRs as well.\r\nA model for a SR, besides being a useful improvement of the current printed reviews, can prove to\r\nbe a valid testing ground for the cooperation and co-existence of various instruments to annotate and\r\nencode different features of the texts that are edited in critical editions. Moreover, such a model proves\r\nonce again that \u2018linguistic\u2019 instruments such as the treebank annotation can and should be integrated\r\ninto strictly speaking philological resources, as precious means to gain a better understanding of the text\r\nand the critical editors\u2019 choices17. Finally, the possibilities offered by the SR to its users would increase\r\nsignificantly from those of a traditional review, in what we could call a re-purposing of a known instrument\r\nthrough digital means. At the same time, though, its final goal of helping the reader in assessing the\r\ndegree of innovation or conservativity of an edition, and in evaluating specific editorial choices, would\r\n13See the LDLT Guidelines (Cayless and Huskey, 2018). One could group together, e.g., variants that affect the translation\r\nor the staging, or particular types or variants according to one\u2019s specific needs.\r\n14 See Alpheios, 2019. For the Guidelines for Greek Treebanking see Celano, 2014. See also Celano and Crane, 2015;\r\nCelano, 2019.\r\n15 I used Ugarit, 2019.\r\n16 On the base text see e.g. Andrews and Mac\u00E9, 2013, p. 506. About variants see e.g. Boschetti, 2007; Monella, 2012; Lana\r\net al., 2017.\r\n17 See Berti, 2019; Passarotti, 2019; Mambrini, 2016; Beaulieu et al., 2012; Bamman et al., 2009.\r\n\r\n231\r\n\r\n\fnot be altered; quite the opposite, they might be enhanced.\r\nFrom this starting ground, some crucial points need to be addressed. The connections traced between\r\nreadings, base text and editions could be properly defined semantic. Should the path of semantic\r\nannotation be embraced more fully, by developing an ontology18? What can (or should) the role of\r\nautomated processes both in variant detection and in word analysis be19? What can the visualization\r\nand the dissemination of the project be? Which platform will best suit the open source paradigm? The\r\nprevious pages only provided a first, experimental model that is still under development and that may\r\ntake various directions. As for now, my hope is that this paper might provide some additional discussion\r\nmaterial for some long known questions, more than answers to those very doubts.\r\n\r\nAcknowledgements\r\nI would really like to thank the members of the Department of Digital Humanities at University of Leipzig\r\nand especially professor G. Crane, professor M. Berti and professor T. K\u00F6ntges for their patient teaching\r\nand advising throughout my months as a visiting PhD student."
	},
	{
		"id": 37,
		"title": "Strategie e metodi per il recupero di dizionari storici",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Eva Sassolini",
			"Marco Biffi"
		],
		"body": "1 Introduzione\r\nIl progetto, nato per strutturare l\u2019intero elenco di voci del dizionario GDLI 1 , ha richiesto un articolato\r\nprocedimento di estrazione, data la complessit\u00E0 dei dati e la disponibilit\u00E0 di un formato digitale non\r\nstandardizzato. Il testo digitale da cui siamo partiti era costituito da un formato Word parzialmente strutturato,\r\nottenuto sottoponendo l\u2019originale cartaceo a procedure di OCR2, senza nessun tipo di collazione, parziale o\r\ntotale. Il processo di acquisizione ha evidenziato caratteristiche stilistiche e scelte di layout derivate\r\ndall\u2019originale che hanno reso l'OCR estremamente complicato. La versione edita presenta una suddivisione\r\ndella pagina in 3 colonne, un colore della carta non sufficientemente bianco, nonch\u00E9 un carattere tipografico\r\nrelativamente piccolo e una altrettanto minima interlinea. Per ragioni legate a tempo e costi dell\u2019impresa non\r\nabbiamo potuto migliorare la qualit\u00E0 dell'OCR, almeno in questa fase del progetto, come viene attualmente\r\nproposto in letteratura nei nuovi approcci. Nel caso specifico, utilizzando tecniche di pre- e\/o post-elaborazione\r\ndell'output eseguita attraverso l\u2019uso di un singolo o pi\u00F9 motori di OCR. Inizialmente abbiamo valutato\r\nl\u2019utilizzo di sistemi di estrazione automatici, sia basati su regole che su tecniche di machine learning\r\n(Khemakhem et al. 2017) ma l\u2019analisi dei dati ha escluso l\u2019opzione. La complessit\u00E0 strutturale non \u00E8\r\nl\u2019impedimento maggiore, pi\u00F9 rilevante \u00E8 il numero e la variet\u00E0 degli errori, a cui si aggiunge la mancanza di\r\nun training corpus opportuno per l\u2019addestramento. Tutto questo ci ha spinto verso un approccio sperimentale,\r\nbasato su strategie di definizione di regole di estrazione dal formato Word. Abbiamo inoltre evidenziato una\r\ndistribuzione non uniforme delle tipologie di errore nei vari volumi, probabilmente influenzata dalla lunga\r\ngestazione dell\u2019opera editoriale complessiva (vedi Tab. 1).\r\n\r\n1\r\n\r\n2\r\n\r\nGrande Dizionario della Lingua Italiana, di Salvatore Battaglia (poi diretto da Giorgio B\u00E0rberi Squarotti), Torino, UTET, 19612002, 21 voll.; con Supplemento 2004, diretto da Edoardo Sanguineti, Torino, UTET, 2004, e Indice degli autori citati nei volumi IXXI e nel Supplemento 2004, a cura di Giovanni Ronco, Torino, UTET, 2004.\r\nLa Optical Character Recognition \u00E8 una tecnologia che permette di convertire un\u2019immagine PDF o di altro tipo in testo digitale.\r\n235\r\n\r\n\fTabella 1: confronto dei tipi di errore tra volumi diversi\r\n\r\nIn un lavoro realizzato in un arco temporale di 40 anni, era probabilmente inevitabile la presenza di\r\ncambiamenti e aggiustamenti (anche minori), introdotti nel tempo sia a livello delle voci che nel corpus di\r\nriferimento del GDLI, che hanno avuto influenze sulle procedure di OCR. Questo contesto ha reso difficile\r\nappoggiarsi ad esperienze di altri, pur se indirizzati come noi verso approcci non standard. In alcuni progetti\r\nsimili si parla di \u201Cdigitalizzazione attraverso una procedura primitiva\u201D (Bausi, 2016), ma ci si appoggia poi\r\nprincipalmente alla ri-digitazione manuale da parte di studiosi ed esperti qualificati. Nel nostro caso, data la\r\ndimensione e complessit\u00E0 dei dati, \u00E8 necessario limitare il ricorso alla correzione manuale, per le stesse ragioni\r\ndi contenimento di tempi e costi indicate sopra.\r\n\r\n2 L\u2019approccio\r\nAbbiamo impostato un piano di lavoro a lungo termine, che comprendesse \u2018tappe\u2019 da raggiungere\r\nprogressivamente: 1) riconoscimento del lemma; 2) identificazione di tutti i campi del lemma principale; 3)\r\nnumero di sensi principali; 4) numero di sensi annidati; 5) campi di ogni senso principale; 6) campi di ciascun\r\nsenso annidato 7) mapping in formato TEI. L\u2019approccio seguito consiste in fasi di riconoscimento successive\r\nche, partendo dall\u2019identificazione del lemma dell\u2019entrata lessicale, ne eseguono la segmentazione progressiva\r\nindividuando, attorno a questo nucleo, gli altri campi dell\u2019entrata. Potremmo definirlo un processo di parsing\r\na pi\u00F9 livelli. Ogni campo ha richiesto strategie specifiche per l\u2019identificazione delle caratteristiche distintive,\r\nche, tradotte in vincoli di corretta attribuzione e impostati in modo incrementale, hanno portato ad un\r\nriconoscimento sempre pi\u00F9 granulare della struttura dell\u2019entrata. Oltre a definire procedure software di\r\nestrazione e codifica abbiamo implementato metodi di supporto alla correzione manuale e un sistema efficiente\r\ndi revisione e riallineamento successivo dei dati estratti, per contenere il pi\u00F9 possibile l\u2019intervento manuale.\r\nL\u2019articolo descrive l\u2019approccio generale e le prime tappe del progetto; la conversione in un formato standard\r\ndi rappresentazione, pur essendo un impegno rilevante e dall\u2019impatto non trascurabile sul progetto, esula\r\ntuttavia dai nostri intenti.\r\n\r\n2.1 L\u2019analisi dei dati\r\nIl GDLI \u00E8 il principale dizionario storico dell\u2019italiano pubblicato da UTET. I 21 volumi che lo compongono,\r\nterminati di pubblicare nel 2002, sono corredati da due supplementi integrativi, il primo del 2004 e l\u2019altro del\r\n2009, e da un Indice degli autori citati. Recentemente, \u00E8 stato firmato uno storico accordo tra la UTET e\r\nl\u2019Accademia della Crusca, che ha concesso a quest\u2019ultima i diritti per un\u2019edizione elettronica dell\u2019opera,\r\ndestinata alla consultazione gratuita. Dal maggio 2019 \u00E8 quindi possibile consultare e interrogare il GDLI con\r\nun motore di ricerca per forma applicato al testo in formato Word sopra citato (www.gdli.it). Per quanto il\r\ntesto elettronico presenti molte debolezze, l\u2019approdo finale di ogni ricerca \u00E8 la riproduzione in immagine\r\ndell\u2019originale a cui si rimane del tutto fedeli, anche in questa edizione, consentendone una comoda lettura con\r\nl\u2019ingrandimento a video, a differenza della versione cartacea in cui le dimensioni ridotte dei caratteri non\r\npermettono un facile accesso. Nella ricerca si possono certamente perdere alcuni risultati di forme \u201Coccultate\u201D\r\ndagli errori di OCR ma, una volta arrivati alla pagina, il consultatore pu\u00F2 attingere appieno a tutte le preziose\r\ninformazioni del dizionario. Questa rappresenta soltanto una fase iniziale del progetto: il contributo scientifico\r\ndell\u2019ILC si inserisce a fronte dell\u2019esigenza di fornire un accesso pi\u00F9 articolato alle informazioni. Grazie a\r\n236\r\n\r\n\fstoriche esperienze nella lessicografia computazionale (Calzolari et al., 1987; Calzolari et al., 1993) stiamo\r\nstati coinvolti per implementare il complesso processo di estrazione e riconoscimento della struttura delle\r\nentrate. L\u2019analisi dei dati ha evidenziato un input costituito da oltre 23.000 pagine di testo, rappresentate in un\r\nformato Word contenente diverse tipologie di errore. Come affermato nell\u2019introduzione, il testo cartaceo\r\noriginale presenta caratteristiche stilistiche e scelte di layout che hanno condotto il sistema di OCR verso\r\ninevitabili problemi di corretta interpretazione. Gli errori di riconoscimento sono stati analizzati su ogni singola\r\ncaratteristica strutturale del dizionario: lemma, varianti ortografiche, categoria grammaticale, codici d\u2019uso,\r\ndefinizione, etimologia, sensi principali e sensi aggiuntivi (annidati).\r\n\r\nTabella 2: esempi di errori del sistema di OCR\r\n\r\nCiascuno dei campi presenta errori di vario tipo, che vanno dalla mancata segmentazione dei paragrafi,\r\nall\u2019interpretazione errata della punteggiatura e dell\u2019ortografia delle parole, al mancato rispetto delle diverse\r\nsezioni della voce del dizionario: punti elenco, rientro, dimensione del carattere ecc. (vedi Tab. 2). La presenza\r\ndi errori ha assunto quindi un peso decisivo nel progetto e ha mostrato come le sole procedure automatiche,\r\nper quanto raffinate e puntuali, non sarebbero state sufficienti a produrre un risultato corretto.\r\n\r\n2.2 Le fasi di lavoro\r\nSiamo partiti da una sommaria classificazione dei problemi relativi all\u2019inesattezza del dato distinguendo tra\r\nerrori \u201Cbloccanti\u201D e \u201Cnon bloccanti\u201D, per poi procedere con i casi pi\u00F9 specifici. La differenza sta nell\u2019impatto\r\ndell\u2019errore sulla procedura di parsing dei dati. Gli errori bloccanti sono costituiti prevalentemente dal mancato\r\nriconoscimento di un nuovo lemma. In questo caso, non potendo chiudere correttamente la voce precedente,\r\nsi inficia il successivo processo di raffinamento, impedendo la definizione dei confini e campi dell\u2019entrata\r\n(vedi Tab. 3).\r\n\r\nTabella 3: alcuni tipi di errore nel lemma\r\n\r\nUn errore \u201Cnon bloccante\u201D interviene invece quando non \u00E8 possibile separare il codice grammaticale, da quello\r\nd\u2019uso, e\/o dalle varianti ortografiche e\/o queste dalla definizione. Questa tipologia di errori producono\r\n237\r\n\r\n\fun\u2019entrata non corretta, ma sulla quale si possono impostare le successive fasi di raffinamento progressivo,\r\nprocedendo in un certo senso a \u2018tappe\u2019 nella strutturazione della voce. Mentre per gli errori \u201Cbloccanti\u201D non\r\nabbiamo trovato un\u2019efficiente soluzione alternativa alla revisione manuale post-processing, per gli altri \u00E8\r\npossibile corredare il parser di meccanismi di annotazione puntuale. Segnalare quando mancano campi\r\nobbligatori o se il loro ordine non \u00E8 rispettato, e riferendo puntualmente il caso in un file di report. In alcuni\r\ncasi, quando \u00E8 possibile impostare un\u2019indagine pi\u00F9 puntuale del dato, i file di report sono finalizzati al controllo\r\ndelle soluzioni gi\u00E0 inserite in fase di parsing, cos\u00EC da alleggerire il lavoro di revisione manuale.\r\n\r\n3 Prospettive\r\nNelle fasi successive del progetto le risorse estratte hanno assunto una valenza autonoma, per esempio abbiamo\r\nprodotto un confronto tra i lemmi del GDLI e quelli del TLIO3: il primo dizionario storico di tutte le variet\u00E0\r\ndell\u2019italiano antico fino al 1375. Stiamo pensando di allargare il confronto anche ad altri dizionari, primo fra\r\ntutti il Dizionario Macchina dell\u2019Italiano (DMI) che \u00E8 patrimonio di storiche linee di ricerca dell\u2019ILC. Nel\r\nrecente passato le ricerche nel settore si sono concentrate principalmente sullo sviluppo di lessici\r\ncomputazionali in applicazioni di elaborazione del linguaggio naturale, ma oggi i metodi e le tecniche\r\nsviluppati per estrarre, strutturare e rappresentare dizionari, possono avere un ruolo potenziale per la\r\nprogettazione e costruzione di risorse orientate all\u2019uomo, nelle attivit\u00E0 lessicografiche dell\u2019editoria, soprattutto\r\ndigitale. I dizionari storici sono in grado di documentare l\u2019evoluzione diacronica della lingua, mostrando la\r\ndimensione storica del lessico. I potenziali vantaggi della digitalizzazione e strutturazione di un dizionario\r\nmonumentale come il GDLI risiedono anche nell\u2019importanza delle citazioni che vi si possono consultare.\r\nCome sostenuto da Beltrami e Fornara (2004), il vero fulcro del dizionario \u00E8 la presenza massiccia di citazioni\r\ndi testo, che coprono un\u2019ampia variet\u00E0 di usi linguistici, dalla lingua quotidiana e letteraria, alle lingue regionali\r\ne\/o specializzate\/specialistiche, ai neologismi e alle parole straniere. Le citazioni offrono preziose informazioni\r\nsulle prime attestazioni delle parole, sulle loro varianti formali\/diacroniche\/diatopiche; sugli autori che le\r\ncitano e sulle loro etimologie. Per questo motivo stiamo implementando procedure software che da un lato\r\nestraggano le varianti dalla struttura della voce e dall\u2019altro, attraverso l\u2019elaborazione delle informazioni estratte\r\ndal volume dell\u2019indice degli autori citati, consentano di predisporre filtri su autore ed epoca\/data per le\r\nrispettive citazioni.\r\n\r\n4 Conclusioni\r\nIl nostro impegno \u00E8 finalizzato a rendere una delle maggiori risorse lessicografiche dell\u2019italiano utilizzabile\r\nper il trattamento computazionale, ma l\u2019analisi conclusiva dell\u2019approccio adottato \u00E8 ancora prematura,\r\nsoprattutto per quanto riguarda l\u2019estrazione dei sensi annidati. A progetto in corso un\u2019analisi conclusiva del\r\nlavoro non \u00E8 possibile, tuttavia ci sembra di comune utilit\u00E0 descrivere la nostra esperienza, come aiuto per\r\npianificare progetti analoghi, per i quali mancano riferimenti certi in letteratura. Questi progetti, avendo un\r\nalto grado di complessit\u00E0 e di incognite, si sviluppano troppo spesso senza un\u2019adeguata divulgazione, il che\r\nsignifica che spesso i ricercatori e gli studiosi devono in un certo senso \u201Creinventare la ruota\u201D. L\u2019intento di\r\nquesto articolo \u00E8 proporre il nostro approccio come caso di studio in contesti in cui non \u00E8 possibile ricorrere a\r\nstrumenti e\/o procedure consolidate o sperimentali gi\u00E0 note in letteratura e magari offrire spunti per discutere\r\ndelle strategie specifiche che sono state utilizzate."
	},
	{
		"id": 38,
		"title": "Encoding Byzantine Seals: SigiDoc",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Alessio Sopracasa",
			"Martina Filosa"
		],
		"body": "1\r\n\r\nByzantine Sigillography and Digital Humanities\r\n\r\nSigiDoc is an effort providing XML-based and TEI-compliant encoding standards for Byzantine seals. It\r\nrepresents the first attempt to extend the digital approach, which in the past decade has already been applied\r\nto inscriptions, coins and papyri1, to Byzantine seals, and, with minor adjustments, to other coin-like objects,\r\nsuch as bread stamps. Already presented to the international community of Byzantine sigillographers during\r\nthe 12th International Symposium of Byzantine Sigillography held in May 2019 at the Hermitage Museum, St.\r\nPetersburg, SigiDoc will be presented at the 9th Annual Conference AIUCD for the first time to a wide community of digital humanists.\r\nSeals are the only survivors of the written documents used in daily administration of the Byzantine Empire,\r\nthe former Eastern Roman Empire (4th to 15th century). These small discs, mostly made of lead, commonly\r\nhave an iconographic representation on one side (a saint, the Virgin, etc.) and a legend \u2014 with name, at times\r\nsurname, dignities, functions, places \u2014 on the other. Hence, what we call \"seal\" is both the object bearing an\r\nimpression and the impression itself, left on the disk by a tool called boulloterion, i.e. the matrix (only six of\r\nthese tools survive nowadays in comparison to the extant 80\/100,000 seals' impressions). The seals greatly\r\n1\r\n\r\nA comprehensive and up-to-date list of projects related to inscriptions, coins and papyri within the Digital Humanities can\r\nfound\r\nat: <https:\/\/wiki.digitalclassicist.org\/Category:Projects>.\r\nbe\r\n240\r\n\r\n\fimproved our knowledge of the central and provincial administrative apparatus of the State, the Church, and\r\nthe Army, and thanks to them the Byzantine administrative, political, and social history is gradually being\r\nrewritten. But the impact of the seals is still limited, since access remains restricted to a very small number of\r\nresearchers, the great amount of unpublished material is widely scattered across museums, libraries, and private collections, and the paper publications are fragmented and far from being widely available.\r\nDespite this situation, unlike sister ancillary disciplines such as epigraphy, papyrology or numismatics,\r\nByzantine sigillography has not received much attention from the Digital Humanities for a long time, although\r\nthis has been a wish of at least part of the scientific community since the 2000s. Resumed and abandoned\r\nseveral times without significant results, the project has finally been revived by the authors. SigiDoc strives to\r\ncombine traditional research methods and scholarship in the field of Byzantine sigillography with the technologies offered by the Digital Humanities. In fact, standards for scientific publications within Byzantine sigillography have been established during the 20th century with the works by Vitalien Laurent2, George Zacos3,\r\nNicolas Oikonomides4, Jean-Claude Cheynet5, and Werner Seibt6: hence, one of the main tasks of SigiDoc has\r\nbeen to convert these standard publications into a digital and TEI-compliant form, while partially reassessing\r\nthem and providing them with more consistency.\r\n\r\n2\r\n\r\nWhat SigiDoc is and for what purpose it has been developed\r\n\r\nAt the time this paper is written, SigiDoc is in an advanced beta version and it will hopefully be close to\r\nits 1.0 version by the time the AIUCD annual conference takes place.\r\nSigiDoc is largely based on the ongoing experience of EpiDoc, a collaborative effort that provides guidelines and tools for the encoding of scholarly editions of ancient documents; alongside the scientific objectives,\r\nEpiDoc developed also an educational asset, with the didactic experiences of EpiDoc training weeks, showing\r\ngreat potential in teaching traditional epigraphy and papyrology.7 EpiDoc uses a subset of the TEI standard for\r\nthe representation of texts in digital form8, having clearly established itself as the most robust and widely\r\nsupported format for encoding editions of ancient texts on different text-bearing objects.\r\nFar from being only a simple adaptation and implementation of existing solutions, from a technical point\r\nof view, SigiDoc is: 1) a schema, merged with EpiDoc's one; 2) a template, i.e. SigiDoc\u2019s edition structure; 3)\r\na stylesheet for web visualisation; 4) a set of stylesheets for the critical edition of the legends on seals, derived\r\nfrom EpiDoc but adapted to the needs of Byzantine sigillography; 5) a highly customised version of EFES (see\r\nbelow); 6) a set of guidelines (for metadata, leidenisation, indexing, etc.); 7) a set of files intended to be shared\r\namong all the future SigiDoc projects and expanded through the time and the experience, in order to avoid\r\nsuperfluous duplications on one hand, and to ensure consistency on the other (IDs' lists, controlled vocabularies, authority lists, ontologies, etc.).9\r\nSigiDoc is intended for both the creation of digital-born editions of Byzantine seals as well as for the digital\r\nconversion of paper publications, and it has been conceived in order on one hand, to provide the users with a\r\ncommon ground for developing their projects, on the other, also to give them the freedom to customise their\r\napproach.\r\nDuring its presentation in St. Petersburg, the community of Byzantine sigillographers gave an enthusiastic\r\nfeedback to SigiDoc, and several projects aimed at the creation of online corpora are now waiting for SigiDoc\r\n1.0 to be released, thus ensuring both its dissemination and implementation. These projects involve leading\r\nscholars in the field of Byzantine studies \u2014 such as Prof. Jean-Claude Cheynet, Prof. Claudia Sode, Dr. Vivien\r\nPrigent \u2014 as well as important cultural institutions, such as the Biblioth\u00E8que Nationale de France (Paris), the\r\n2\r\n\r\nSee Vitalien Laurent. 1963\u20131981. Le Corpus Des Sceaux de L\u2019empire Byzantin, Voll. 1\u20135, Paris.\r\nSee George Zacos and Alexander Veglery. 1972. Byzantine Lead Seals, Vol. 1, Basel.\r\n4\r\nSee Nicolas Oikonomides. 1986. A Collection of Dated Byzantine Lead Seals. Washington, D.C.\r\n5\r\nSee Jean-Claude Cheynet, Turan Gokyildirim, and Vera Bulgurlu. 2012. Les Sceaux Byzantins du Mus\u00E9e Arch\u00E9ologique\r\nd'Istanbul, Istanbul; id. and Maria Campagnolo-Pothitou. 2016. Sceaux de la collection Georges Zacos au Mus\u00E9e d'art et d'histoire\r\nde Gen\u00E8ve, Geneva.\r\n6\r\nSee Werner Seibt. 1978, Die byzantinischen Bleisiegel in \u00D6sterreich I. Teil, Kaiserhof, Vienna; id. and Alexandra-Kyriaki Wassiliou. 2004. Die Byzantinischen Bleisiegel in \u00D6sterreich, Vol. 2, Zentral- und Provinzialverwaltung, Vienna.\r\n3\r\n\r\n7\r\n\r\nGabriel Bodard and Simona Stoyanova. 2016. Epigraphers and Encoders: Strategies for Teaching and Learning Digital Epigraphy, in Gabriel Bodard and Matteo Romanello (eds.), Digital Classics Outside the Echo-Chamber. Teaching, Knowledge Exchange\r\n& Public Engagement, p. 51\u201368, London, available: <http:\/\/dx.doi.org\/10.5334\/bat>.\r\n8\r\n\r\nTom Elliott, Gabriel Bodard, Hugh Cayless et al. 2006-2016. EpiDoc: Epigraphic Documents in TEI XML. Online material,\r\navailable: <http:\/\/epidoc.sf.net>, delivers thorough information about history and mission of EpiDoc and offers\r\nalways up-to-date versions of the EpiDoc Guidelines as well as documentation, software and tools to work with EpiDoc.\r\n9\r\n\r\nIn section 3, this paper will address only a selection of these topics.\r\n241\r\n\r\n\fDumbarton Oaks Research Library and Collection (Harvard\u2019s research institute for Byzantine Studies in Washington D.C.), the Epigraphic and Numismatic Museum of Athens and the Geneva Museum of Art. Researchers,\r\nmuseums, public institutions, as well as private collectors will be able to get a definite and stable record of\r\ntheir Byzantine seals, thus preventing deterioration and making their collections available for research, teaching, and presentation to the general public.\r\nThanks to the increasing number of SigiDoc-based projects, the long-term aim is to ensure dissemination,\r\nsharing, and sustainability of the data, and to make available a very wide range of published and unpublished\r\nmaterial, edited to a high scholarly standard. SigiDoc has not been conceived just to realise individual projects\r\n(an interesting, though limited aim): the use of the same guidelines and set of tools is intended to allow the\r\ncreation of a common search interface, through which all corpora will be virtually unified in a higher-order\r\ncatalogue, enabling actions going from the simple cross-referencing to the advanced search throughout every\r\ncorpus published in SigiDoc standard.\r\nThrough a dedicated website, the developers will ensure a proper dissemination of SigiDoc. The website\r\n(http:\/\/sigidoc.huma-num.fr\/) \u2014 which is, as for now, empty \u2014 will host SigiDoc\u2019s\r\ndocumentation as well as all the aforementioned materials needed to run it. Through this site the user will be\r\ninformed about the life of SigiDoc: status of SigiDoc-based projects, training sessions and events, technical\r\nupdates, etc.\r\nIn order to use SigiDoc, a formal training is needed. Training weeks as well as shorter training events\r\nwill take place regularly (once or twice a year): they are inspired by the well-established EpiDoc training\r\nweeks, which have repeatedly shown the feasibility as well as the effectiveness of this teaching format.\r\nThrough in-depth training, SigiDoc will be able to increase the dissemination and continuity of sigillography\r\nitself, while the creation of a new professional figure, i.e. the digital sigillographer, will facilitate the\r\nintegration between the traditional and the digital approach to Byzantine seals. Consequently, the foundation\r\nof a more interrelated scientific community will be laid: a network of digital Byzantine sigillographers is still\r\na desideratum within the larger community of Byzantine sigillography; in order to achieve this goal, a\r\ncommon scientific and prac-tical ground delivered by a standard like SigiDoc is much needed.\r\n\r\n3 Main Features of SigiDoc\r\n3.1 TEI-XML Template and Data Encoding\r\nSigiDoc XML template organises the information in hierarchical mark-up inside three main common\r\nele-ments of the standard TEI structure: 1) <teiHeader\/> for metadata; 2) <facsimile\/> for the digital\r\nreproduction of the artefact; 3) <text\/> for the legend\u2019s critical edition, commentary and bibliography.\r\nteiHeader: among the data nested inside this element, SigiDoc stresses the importance of providing each\r\nseal with a unique numerical identifier (being it the first attempt of systematic categorisation in\r\nByzantine sigillography).To preserve consistency, a common file of ID numbers will be shared among all\r\nthe SigiDoc projects.\r\nSeveral thesauri and controlled vocabularies are being prepared: among them, the classification of the\r\nseal (imperial, military, etc.), the milieu of the issuer, the language(s) of the legend, the work type (original\r\nimpres-sion, drawing, verbal description, etc.), the material, the layout (iconography only, text only, both,\r\netc.), the execution (struck, cast, printed, etc.), the shape, the iconography (see below). All these lists will be\r\nprovided in different languages (English, French, German, and Italian by default, but each project will be\r\nable to cus-tomise their languages).\r\nThe preservation history of the seal is a major concern, not only in establishing which is the current\r\nrepos-itory of a seal, but also in being able to follow it through its different displacements, which is of the\r\nutmost importance especially when the seal enters a private collection or is sold in an auction. Byzantine\r\nseals are increasingly present in online auctions: thanks to SigiDoc it will be easier to follow them before\r\nthey disappear in private collections; the leading journal in the field \u2014 Studies in Byzantine\r\nSigillography10 \u2014 includes a final section listing the seals sold through auctions, but its biannual\r\npublication (without photos of the seals) limits its effectiveness, whereas with SigiDoc the information will\r\nbe updated without any delay, thus promot-ing its circulation and its scientific study.\r\nAn important part of the metadata is devoted to the findspot and the find circumstances: these data contribute significantly to the historical interpretation of the seal, helping to establish both the areas directly administrated by the Byzantine Empire and those \u2014 outside the Empire \u2014 of contact or influence.\r\nUnfortunately,\r\n10\r\n\r\nSee <https:\/\/www.degruyter.com\/view\/serial\/36534>.\r\n242\r\n\r\n\fthis kind of data is often lacking for Byzantine seals, and this makes particularly valuable the preservation of\r\nthis information and its linking with similar data among different corpora.\r\nThe iconography deserves here a special mention: this is a key element to Byzantine sigillography, but also\r\none of the most challenging due the numerous and specific iconographic typologies to be found on Byzantine\r\nseals. In SigiDoc 1.0 this topic will be addressed in two ways: a short and general identification of the iconographic theme (according to a shared controlled vocabulary), and a detailed description. However, the degree\r\nof details in iconographic description is perhaps the most changing criterion in Byzantine sigillographic editions: this is the reason why a standard tool for the description of images is being developed, in order to introduce consistency in sigillographic editions, and to restore the importance of this feature, too often neglected.\r\nThis tool will be released after the launch of SigiDoc 1.0.\r\nLinks and relationships both within the corpora, and between texts and external datasets can be established\r\nand realised as hyperlinks in SigiDoc: the data is being enriched and its interoperability is being increased\r\nusing online resources and authority files such as prosopographies and geographical gazetteers.11\r\nFacsimile: The digital reproduction of the seals will be displayed as a digital facsimile above the edition:\r\nin case of seals in bad state of preservation, some of the ongoing projects (especially those based in Cologne\r\nand Paris) will provide images created with RTI technology (Reflectance Transformation Imaging).12 Of\r\ncourse, digital reproductions won't be always available, especially for seals edited between the 19th and the\r\n20th century and of which no trace can be found: in this case, we have often drawings or verbal descriptions.\r\nText: As far as the critical edition of the seals is concerned, variant readings and restorations are encoded\r\nin TEI, thus enabling the generation of apparatus criticus, parallel texts, and diplomatic editions; moreover,\r\nthe leidenisation of the legend allows for a full editorial interpretation based on the Leiden conventions (especially Panciera), but adapted according to the sigillographic editorial standards.13 For the diplomatic edition,\r\nSigiDoc uses AthenaRuby14, a Unicode-compliant font based on the lettering\/epigraphy of Byzantine coins\r\nand seals, and designed by Joel Kalvesmaki at the Center for Byzantine Studies at Dumbarton Oaks (Washington D.C.). AthenaRuby is currently the most accurate Greek font in terms of the lettering of Byzantine coins\r\nand seals. It is not yet widely used within the sigillographic (and numismatic) community due to the preference\r\naccorded by them to more abstract yet more approximate fonts, such as New Athena. Being the lettering of the\r\nlegend a key factor in dating and contextualising a specimen, SigiDoc\u2019s developers promote and strongly recommend the use of AthenaRuby for the encoding of Byzantine seals: the diplomatic edition carried out with\r\nthis font delivers a more accurate representation of the seal's lettering, thus facilitating its understanding even\r\nwithout the observation of the digital reproduction.\r\n3.2 Web Visualisation and Data Valorisation: Contribution from (and to) EFES (EpiDoc Front-End\r\nServices)\r\nEFES (EpiDoc Front-End Services)15, which builds upon existing tools such as Kiln16, is a highly customisable platform which allows expert and less expert users to get, in a relatively easy and fast way, four\r\nmain critical features for a TEI-XML based corpus: multiple indices, multilingual options, faceted search interface, and a (raw) webpage. TEI-XML files created in SigiDoc, their XSLT stylesheets, as well as part of\r\ntheir tagging have been designed to be best dealt with in EFES. In 2018 SigiDoc became one of the pilot\r\nprojects using and testing EFES and, in this way, actively contributing to its development, being also the only\r\nnon-strictly epigraphical project.\r\nThe most notable contribution of EFES to SigiDoc is certainly the creation, based on Authority Lists, of\r\nautomatic indices. The lemmatisation of words and the identification of relevant entities within the legend, as\r\nwell as the encoding of key words and terms, enable the indexing of words, personal names, geographical\r\nentities, offices, titulatures, and other features of philological, epigraphical, and historical interest at large.\r\n11\r\n\r\nSee, for example, Prosopography of the Byzantine World (<http:\/\/pbw2016.kdl.kcl.ac.uk\/>); Prosopographie der\r\nmittelbyzan-tinischen Zeit (<http:\/\/www.degruyter.com\/view\/db\/pmbz\/>); Pleiades (<http:\/\/\r\npleiades.stoa.org\/>).\r\n12\r\n\r\nFor further information regarding RTI technology and its application in sigillographic studies, see Franz Fischer and Stephan\r\nMakowski. 2017. Digitalisierung von Siegeln mittels Reflectance Transformation Imaging (RTI), Paginae historiae \u2013 Sborn\u00EDk\r\nN\u00E1rod-n\u00EDho archivu, 25\/1, p. 137\u2013141, available: <http:\/\/kups.ub.uni-koeln.de\/id\/eprint\/7882>.\r\n13\r\n\r\nFor example, the rendering after transformation of several kinds of <gap\/> tags has been changed.\r\nAthenaRuby is an OpenType and Unicode-compliant font. For documentation, tools, and selected bibliography visit:\r\n<https:\/\/www.doaks.org\/resources\/athena-ruby>.\r\n\r\n14\r\n\r\n15\r\nSee <https:\/\/github.com\/EpiDoc\/EFES\/wiki> for the technical documentation and <https:\/\/github.com\/\r\nEpiDoc\/EFES\/wiki\/User-Guide> for detailed guidelines and user guide.\r\n16\r\nSee <https:\/\/github.com\/kcl-ddh\/kiln> for the documentation.\r\n\r\n243\r\n\r\n\fSigiDoc users will be able to potentially index every feature deemed relevant for their corpus: the previous\r\nlisting of indexed features \u2014 featuring the most common indices in Byzantine sigillographic publications \u2014\r\nis what the authors recommend to all future SigiDoc projects in order to harmonise their indices. Nonetheless,\r\na higher degree of specialisation will be enabled: for example, thanks to the consistent use of AthenaRuby, it\r\nwill be possible to index \u2014 and, ultimately, to search for \u2014 single variant letters within the legend.\r\nThe using made by SigiDoc of EFES is essentially based on the customisation of the solutions offered by\r\nit: this is especially true for the use of Athena Ruby instead of generic Greek capital letters in the diplomatic\r\nedition; the XSLT stylesheet organising the webpage, in order to create a list of fields appropriate for Byzantine\r\nsigillography; the EpiDoc stylesheets used for the edition of the legends; and, of course, the customisation of\r\nthe indices and the search interface.\r\nDuring the next months some improvements related to EFES \u2014 mainly concerning further indexing features and automatic bibliographic references \u2014 will be delivered.\r\n\r\nFigure 1. From the seal (left) to the EFES-generated webpage (right), through the SigiDoc XML template\r\n(middle).\r\n\r\nConclusions\r\nDigital Byzantine sigillography is an entirely new discipline: SigiDoc has therefore been designed for users\r\nwith no prior computer skills but with a background in Byzantine sigillography or Byzantine history at large.\r\nSigiDoc\u2019s aim is to deliver a (reasonably) easy and ready-to-use tool, with a template ready to be filled in\r\naccording to the need of each project; but this also means that a more advanced user will be able to go further\r\nand customise it to a larger extent.\r\nThe visibility and availability of data coming from an increasingly number of seals, jointly with the possibility of establishing relations among them, will push further our knowledge of several aspects of Byzantine\r\nhistory, allowing the specialists to carry out analysis in several directions (i.e. the structure of Byzantine society\r\nand the relations among individuals and families, the organisation of the administration, the role of personal\r\npiety in the choice of the iconography, art history, but also sigillographic epigraphy or lettering, the \"writing\r\nuses\", the Greek language, etc., including the evolution of all these aspects throughout the centuries).\r\nAlbeit most of these research directions (and many others) already existed before SigiDoc, thanks to it\r\ntheir analysis will be greatly enhanced, in a way and to an extent extremely difficult to reach without this tool.\r\n\r\nAcknowledgments\r\nThe authors would like to thank the anonymous reviewers for their suggestions, which have been taken into\r\naccount where appropriate."
	},
	{
		"id": 39,
		"title": "Preliminary results on mapping digital humanities research",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Gianmarco Spinaci",
			"Giovanni Colavizza",
			"Silvio Peroni"
		],
		"body": "1\r\n\r\nIntroduction\r\n\r\nIt is a scientometrics trope to consider humanities research as poorly indexed in citation databases, and\r\nthus poorly understood in terms of research outputs (Hammarfelt, 2016). Several studies have pointed\r\nout to the limitations of indexes such as Web of Science and Scopus with respect to the humanities, both\r\nin terms of quantity and quality (e.g., lack of books) (Nederhof, 2006). Nevertheless, in recent years\r\nmore indexes have become available, such as Dimensions and Microsoft Academic, while coverage has\r\nbeen improving (Harzing and Alakangas, 2016). In view of these developments, a comprehensive and\r\ncross-index map of research in the (digital) humanities is still pending.\r\nSome previous work has considered the intellectual and social organization of the digital humanities\r\nusing bibliometrics. Nyhan and Duke-Williams (2014) focused on collaboration patterns in the journals\r\nComputers and the Humanities and Literary and Linguistic Computing (up to 2011), finding a propensity\r\nto collaborate within small, tight groups and a persisting tendency for single-author publishing. It is\r\nworth noting that more recent work on the humanities as a whole found an increasing propensity for\r\n\r\n246\r\n\r\n\fcollaboration, albeit with high variation among different disciplines\/departments (Burroughs, 2017).\r\nCitation analyses based on Computers and the Humanities, Digital Scholarship in the Humanities and\r\nDigital Humanities Quarterly highlighted instead a sparser organization, around thematic areas such\r\nas information studies, historical literature, linguistics, natural language processing and statistical text\r\nanalysis (Gao et al., 2017, 2018). Further work based on the Journal of Digital Humanities, Digital\r\nHumanities Quarterly, International Journal of Humanities and Arts Computing, Digital Medievalist,\r\nDigital Studies, Literary and Linguistics Computing assessed co-authorship, co-citation and bibliographic\r\ncoupling networks. The authors found a sustained growth in digital humanities publications, coupled\r\nwith increasing integration with respect to citation networks, and persisting fragmentation with respect\r\nto collaborations as mapped by co-authorship relations (Tang et al., 2017).\r\nA recent bibliometric comparison considered the annual conference of the Italian Association of\r\nDigital Humanities and Digital Culture (AIUCD) and the annual Italian conference on computational\r\nlinguistics (CLiC-it). Results show how collaborations are sparser in digital humanities, how research\r\nmethodologies usually are introduced in the computational linguistics conference and then readopted in\r\nthe digital humanities one, and how the citation behaviour in the latter one closely resembles that of\r\nhumanities scholarship (e.g., higher ratio of references to books) (Sprugnoli et al., 2019). Altmetrics data\r\nhas also been used to map the digital humanities community worldwide. In particular, Twitter follower\r\nand co-retweet networks were used to show how the community is organized around few \u201Cinfluencers\u201D\r\nand according to language and geographical region (Grandjean, 2016; Gao et al., 2018). Finally, some\r\nscholars attempted to position the digital humanities within the broader context of humanities scholarship\r\n(Leydesdorff and Akdag Salah, 2010; Salah et al., 2015).1\r\nIn this paper, we address the following research questions: a) what qualifies as digital humanities\r\nresearch, from a bibliometric point of view? b) What is the coverage of citation indexes with respect\r\nto digital humanities research? c) What is the organization of the resulting map of research? We\r\npropose an iterative method to individuate digital humanities publications by combining manual journal\r\nclassification and automatic citation clustering. One of the outcomes of our work is the first version of a\r\nlist which includes digital humanities journals. We use this list to assess the number of digital humanities\r\njournal publications indexed by Web of Science (WoS), Scopus, Crossref and Dimensions. Finally, we\r\nuse the citation data included in the index with most digital humanities publications, i.e., Dimensions\r\n(Hook et al., 2018), to present a map of digital humanities research based on journal articles. It is worth\r\nnoticing that our results are still preliminary and stem from ongoing work to create a comprehensive map\r\nof humanities research.\r\n\r\n2\r\n\r\nData and methods\r\n\r\nDatabase coverage limitations notwithstanding, individuating digital humanities (DH) publications is\r\nproblematic in itself. First of all, there is little agreement on what constitutes DH research among\r\npractitioners. Secondly, DH research tends to be highly interdisciplinary, so much so that clear-cut\r\nclassifications would be intrinsically arbitrary. We adopt here a combination of top-down journal level\r\nclassification, in view of expanding the ERIH-Plus journal list 2, and a bottom-up clustering approach,\r\nwhere we use citation clusters to find candidate journals to be added to the list.\r\nMore in detail, we perform the following steps:\r\n1. create a seed list of known journals in DH, by disseminating a survey to the participants to DH 2019\r\nand in the Humanist mailing list, which resulted in obtaining 14 replies;\r\n2. consider a fine-grained clustering of all publications within each citation index, obtained by using\r\nthe Leiden algorithm (Traag et al., 2019) and following the heuristics proposed in Waltman and van\r\nEck (2012);\r\n1A reasoned review of quantitative analyses of the digital humanities is maintained by Scott Weingart at http:\/\/\r\nscottbot. net\/dh-quantified\r\n2See https:\/\/dbh.nsd.uib.no\/publiseringskanaler\/erihplus\r\n\r\n247\r\n\r\n\fFigure 1: DH articles in Dimensions according to their journal category: \u201Cexclusively\u201D (green) and\r\n\u201Csignificantly\u201D (pink). Only articles with with one citation or more (34.4% of the total) were considered.\r\nThe network was visualized using Gephi 0.9.2 and the Force Atlas 2 layout. The size of the nodes (i.e.,\r\nthe articles) is proportional to number of citations they receive.\r\n\r\n3. detect which clusters contain a relatively high proportion of publications from the journals in the\r\nlist, in order to identify those which are also highly represented in the detected clusters but that are\r\nnot part of the list obtained in point 1. We do this by considering the top 5 journals (by number of\r\narticles) per cluster with more than 5% of articles already from DH journals within the list;\r\n4. manually assess each journal in the set identified in the previous point so as to add it in the original\r\nlist; and\r\n5. iterate again from point 2 until a convergence criterion is met.3\r\nAs convergence criterion, we iterate the proposed method twice (i.e., seed plus first iteration) and focus\r\nexclusively on research articles or review articles as publication typologies, published from the year 2000.\r\nThis approach follows previous work in citation indexing for the humanities (Colavizza et al., 2018).\r\n2.1 Journal classification\r\nGiven the highly interdisciplinary character of most publications in DH, we classify journals in the list\r\nusing three categories: exclusively, if we deem a journal to be solely devoted to DH; significantly, if we\r\ndeem that at least 50% of publications in the journal can be considered DH; marginally, if the journal\r\ncontains an estimated 5% to 50% publications in DH. Categories were assigned by survey participants\r\n(iteration 1) or the authors (iteration 2) independently, and disagreements solved by majority. We\r\nacknowledge as a limitation the subjective perspective and biases this approach might have introduced in\r\nthe resulting list of journals.\r\n\r\n3 Results\r\nThe first outcome of our study is a list of DH journals (Spinaci et al., 2019), arranged according to the\r\nproposed categories, and containing 19 \u201Cexclusively\u201D, 17 \u201Csignificantly\u201D and 64 \u201Cmarginally\u201D classified\r\njournals.4\r\n3Due to data access constraints, we worked with the following versions of the citation indexes under consideration: Web of\r\nScience: December 2018, Scopus: May 2019, Dimensions: December 2018, Crossref: August 2018. Coverage results might\r\nbe affected accordingly.\r\n4The list has been also made available in a Google sheet (https:\/\/tinyurl.com\/y6rfrsuw) which can be\r\ncommented for further feedback.\r\n\r\n248\r\n\r\n\fFigure 2: DH articles in Dimensions according to their related journal, if it contained more than 2% of\r\nthe visible articles. The underlying network and layout is created as in Figure 1. The journal names are:\r\n1) Language Resources and Evaluation; 2) AI & Society; 3) Literary and Linguistic Computing; 4) D-Lib\r\nMagazine; 5) Computational Linguistics; 6) Journal of Quantitative Linguistics; 7) International Journal\r\nof Humanities and Arts Computing; 8) Enthynema; 9) International Journal on Digital Libraries; 10)\r\nDigital Scholarship in the Humanities; 11) Virtual Archaeology Review; 12) Journal on Computing and\r\nCultural Heritage.\r\nTable 1: Database coverage for journals in the three categories.\r\nJournal\r\nExclusively\r\nSignificantly\r\nMarginally\r\nTotal\r\n3.1\r\n\r\nWoS\r\n1243\r\n1096\r\n39,655\r\n41,994\r\n\r\nScopus\r\n1858\r\n4421\r\n40,439\r\n46,718\r\n\r\nCrossref\r\n3989\r\n5395\r\n55,126\r\n64,510\r\n\r\nDimensions\r\n2751\r\n7259\r\n117,782\r\n127,792\r\n\r\nCoverage\r\n\r\nThe overall database coverage, expressed as the number of indexed articles per category, is shown in\r\nTable 1. Crossref has the best coverage with respect to \u201Cexclusively\u201D journals, while Dimensions has\r\nbetter coverage in the \u201Csignificantly\u201D and \u201Cmarginally\u201D categories.\r\nThe publication coverage of journals in the \u201Cexclusively\u201D category is shown in Table 2. Only few\r\njournals show a good coverage, including some non-active ones: Computers and the Humanities, Digital\r\nScholarship in the Humanities, International Journal of Humanities and Arts Computing, Journal on\r\nComputing and Cultural Heritage, and Literary and Linguistic Computing. Many other DH journals we\r\ncollected in (Spinaci et al., 2019) were either poorly represented or even not present in the indexes we\r\nused for the analysis. Crossref appears to be the most comprehensive database in this respect.\r\n3.2\r\n\r\nMap of research\r\n\r\nWe further present a preliminary map of DH research, focusing on journal articles from the \u201Cexclusively\u201D\r\nand \u201Csignificantly\u201D categories. We chose Dimensions for this analysis in order to better explore its\r\napparently complementary coverage with respect to both categories and with respect to previous work\r\n(Gao et al., 2017; Tang et al., 2017; Gao et al., 2018). Coverage in the \u201Csignificantly\u201D category is mostly\r\ndue to work in computational linguistics and digital libraries: Journal of Quantitative Linguistics (574\r\narticles), Computational Linguistics (759), D-Lib Magazine (1054), Language Resources and Evaluation\r\n(2076). We also highlight the presence of almost 1500 articles from the journal AI & Society, a topic of\r\nincreasing interest in DH. The map shown in Figures 1, 2, 3 considers all articles with at least one (given\r\nor received) citation, that is to say articles with a degree of one or more. The network initially contains\r\n10,010 articles and 5,283 citation edges, while the number of articles with citations is 3,446 (34.4% of the\r\n\r\n249\r\n\r\n\fFigure 3: DH citation clusters calculated using the citations available in Dimensions, considering only the\r\nclusters containing more than 2% of the visible articles in the dataset. The legend contains the five most\r\nfrequently occurring words in the titles of the articles within each cluster, after filtering out uninteresting\r\nones. The underlying network and layout is created as in Figure 1. The journal coverage for each cluster\r\nis as follows (we only include journals accounting for more than 10% of the articles within a cluster):\r\n1) Literary and Linguistics Computing (39.7%), Language Resources and Evaluation (19.3%), Digital\r\nScholarship in the Humanities (17.6%), Journal of Quantitative Linguistics (17%), 2) Computational\r\nLinguistics (55.4%), Language Resources and Evaluation (34.3%), 3) Journal of Quantitative Linguistics\r\n(91.2%), 4) Language Resources and Evaluation (91.1%), 5) Language Resources and Evaluation (55%),\r\nComputational Linguistics (32.5%), 6) AI Society (99%), 7) International Journal on Digital Libraries\r\n(50.8%), D-Lib Magazine (37.1%), 8) Computational Linguistics (53.6%), Language Resources and\r\nEvaluation (34.8%)\r\ntotal). Two thirds of the articles are not connected to any other article through citations. The network\u2019s\r\nlayout was created using Force Atlas 2 (Jacomy et al., 2014).\r\nFigure 1 shows the articles assigned to the categories \u201Cexclusively\u201D and \u201Csignificantly\u201D. The bulk of the\r\narticles in \u201Cexclusively\u201D journals include, somewhat predictably, the articles in Literary and Linguistic\r\nComputing or its successor, Digital Scholarship in the Humanities. However, we noticed that, when\r\nconsidering the most represented journals in our dataset (i.e., those with most articles) in Figure 2, the\r\narticles are visually arranged by journal and tend to follow field-specific patterns: digital libraries (IJDL,\r\nD-Lib), computational linguistics (Computational Linguistics, the Journal of Quantitative Linguistics),\r\nartificial intelligence and society (AI & Society), and DH (Literary and Linguistic Computing, Digital\r\nScholarship in the Humanities). Instead, the articles in Language Resources and Evaluation are more\r\nevenly spread across different field clusters.\r\nWhen we consider citation clusters detected using a modularity-maximizing method (Blondel et al.,\r\n2008), in Figure 3, we observe a modular structure with a high correlation with respect to the publication\r\nvenue. The main focus of the DH cluster (number 1 in Figure 3) are quantitative literary studies,\r\ne.g., stylometry and authorship attribution. Other clusters cover the DH-related areas of computational\r\nlinguistics and natural language processing (2,3,4,5,8), digital libraries (7) and AI and society (6). As far\r\nas we could notice from this graph, DH publications tend to connect to related disciplinary areas, even\r\nif each area maintained its distinctiveness. The publication venue remains a key trait of the intellectual\r\nstructure of the DH.\r\n\r\n4\r\n\r\nConclusion\r\n\r\nIn this article, we proposed an approach to find digital humanities publications by iterating between a\r\nlist of journals (top-down) and its expansion using citation clustering (bottom-up). In this way, we were\r\nable to propose a first version of a list of digital humanities journals split in three categories: those that\r\nare \u201Cexclusively\u201D, \u201Csignificantly\u201D and \u201Cmarginally\u201D related to the digital humanities. We assessed the\r\n\r\n250\r\n\r\n\fTable 2: Database coverage for journals exclusively devoted to digital humanities scholarship.\r\nJournal\r\nComputers and the Humanities\r\nDigital Humanities Quarterly (DHQ)\r\nDigital Medievalist\r\nDigital Scholarship in the Humanities (DSH)\r\nDigital Studies \/ Le champ num\u00E9rique\r\nDigit\u00E1lis B\u00F6lcs\u00E9szet \/ Digital Humanities\r\nFrontiers in Digital Humanities\r\nInternational Journal of Digital Humanities\r\nInternational Journal of Humanities and Arts Computing\r\nJournal of Cultural Analytics\r\nJournal of Data Mining and Digital Humanities\r\nJournal of Digital Archives and Digital Humanities\r\nJournal of Digital Humanities\r\nJournal of the Japanese Association for Digital Humanities\r\nJournal of the Text Encoding Initiative\r\nJournal on Computing and Cultural Heritage (JOCCH)\r\nLiterary and Linguistics Computing\r\nRevista de humanidades digitales\r\nUmanistica Digitale\r\nTotal\r\n\r\nWoS\r\n663\r\n\r\nScopus\r\n806\r\n38\r\n\r\n254\r\n\r\n237\r\n\r\n15\r\n\r\n75\r\n251\r\n\r\n178\r\n584\r\n\r\n1243\r\n\r\n1858\r\n\r\nCrossref\r\n1465\r\n\r\nDimensions\r\n\r\n67\r\n327\r\n230\r\n\r\n66\r\n388\r\n\r\n58\r\n\r\n19\r\n66\r\n\r\n253\r\n13\r\n\r\n475\r\n27\r\n\r\n15\r\n73\r\n\r\n21\r\n\r\n1465\r\n23\r\n\r\n202\r\n1450\r\n37\r\n\r\n3989\r\n\r\n2751\r\n\r\ncoverage of Web of Science, Scopus, Crossref and Dimensions in this respect, finding that Crossref has\r\nthe best coverage of \u201Cexclusively\u201D digital humanities journals, while Dimensions has the best coverage of\r\nthe number digital humanities-related articles overall. We discussed a first map of research using citation\r\ndata from Dimensions, highlighting how just one third of the articles in Dimensions are connected with\r\neach other via citations. We further found that digital humanities articles are connected via citations to\r\ncomputational linguistics and natural language processing, digital libraries and other developing areas\r\nsuch as AI and society. Nevertheless, we also found that the venues (i.e., the journals) strongly overlap\r\nwith citation clusters, and are a key trait of the intellectual organization of digital humanities research.\r\nWe acknowledge that our work is still in progress, and thus it has a set of limitations which we plan to\r\naddress in the future. In particular, we plan to include additional bibliographic entity types in addition to\r\njournal articles (e.g., books), and to also include the COCI (Heibi et al., 2019) and Microsoft Academic\r\ncitation indexes to the comparison. Coverage will also be assessed at the article level (i.e., which citation\r\nindex contains which articles) and chronologically. Lastly, we will elaborate on the map of research by\r\nincluding a comparison across all indexes.\r\n\r\nAcknowledgements\r\nThe authors would like to thank the Centre for Science and Technology Studies (CWTS), Leiden University, for providing access to their databases and computing facilities. This work was in part conducted\r\nwhen Spinaci was visiting CWTS."
	},
	{
		"id": 40,
		"title": "Epistolario De Gasperi: National Edition of De Gasperi‚Äôs Letters in Digital Format",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Sara Tonelli",
			"Rachele Sprugnoli",
			"Giovanni Moretti",
			"Stefano Malfatti",
			"Marco Odorizzi"
		],
		"body": "1 Introduction\r\nAlcide De Gasperi (1881-1954) was one of the most important statesmen in European history, recognized\r\nas the father of the Italian Republic and one of the founding fathers of Europe. Despite the rich\r\nhistoriography about him, many aspects of his biography are still waiting to be enlightened. Indeed, up\r\nuntil now, the historical analysis on De Gasperi has been based almost exclusively on institutional sources,\r\non his published works and on his public speeches (De Gasperi, 2006, 2008a,b, 2009), also available in\r\ndigital format1 (Sprugnoli et al., 2016). What is still missing is his extraordinary correspondence, which\r\ncovers all the critical steps of his biography and often offers a different point of view on many events\r\nof the time. So far, only very few letters written or received by De Gasperi have been made available\r\nthrough anthological publications, see (De Gasperi, 1955, 1974) among others, or through the publication\r\nof the correspondence with specific people, e.g. (Antonazzi, 1999). Nevertheless, a large number of\r\nletters has not been published yet. This has paved the way to this ambitious project, coordinated by a\r\nscientific committee that brings together all the major scholars of De Gasperi, whose goal is to provide\r\nan exhaustive collection of De Gasperi\u2019s correspondence in digital format, covering both his public and\r\nprivate life. A preliminary search of the letters to be digitized and edited lists about 5,000 documents\r\nwritten in multiple languages, currently stored in 114 archives all over the world. More than 1,500 of\r\nthese letters are already available online at the time of writing, all searchable and accessible through an\r\nonline platform: https:\/\/epistolariodegasperi.it\/#\/archivio_digitale\/lettere.\r\nThe Italian Ministry of Culture supports the project, which has been recognized as a \u201CNational\r\nEdition\u201D. Significantly, this is the first Italian National Edition exclusively conceived through digital\r\ntools.\r\n1http:\/\/alcidedigitale.fbk.eu\/\r\n\r\n253\r\n\r\n\f2\r\n\r\nRelated Work\r\n\r\nEditorial projects involving the digitization of documents are always increasing and the standards that\r\ncharacterize them are constantly evolving (Pierazzo, 2014). The literature reports a rich and varied\r\npanorama of digital editions of works by writers, intellectuals and historical figures (Franzini et al.,\r\n2016). In recent years, also the digitization of letters has received much attention (Hankins, 2015) with\r\nprojects dedicated to individual characters, such as Darwin2 and Van Gogh3, or that aim to reconstruct\r\nlarge epistolary networks (Ravenek et al., 2017; Baillot and Seifert, 2013). A centralized web service\r\nto search within the metadata of diverse scholarly editions of letters has been also developed (Dumont,\r\n2016). If we look at the Italian landscape, we can see that the development of digital editions of letters is\r\nrather limited and that current initiatives have mostly focused on the collection and digitization of literary\r\ndocuments created in the Late Medieval and modern era, such as in the Progetto Datini4 and in the digital\r\nedition of the letters by Vespasiano da Bisticci (Tomasi, 2012).\r\nIn our opinion, the project presented in this paper has at least four significant aspects to be highlighted\r\nif compared with other works and that constitute our contribution: (i) the strong interdisciplinary synergy\r\nthat led to define a rich set of metadata and editorial choices taking into consideration different aspects (i.e.,\r\nlinguistic, historical, philological); (ii) the development of a transcription infrastructure, characterized\r\nby three access levels and a graphical interface that is both intuitive and compliant with existing editing\r\nstandard and that we plan to release for the research community so to be adopted in other projects; (iii)\r\nthe subject of the edition, that is the exhaustive collection of correspondence related to contemporary\r\nhistory until now closed in archives and difficult to reach; (iv) the attention to different types of final users\r\nso that mechanisms have been implemented in order to allow a customization of the reading complexity.\r\n\r\n3\r\n\r\nEditorial Practice\r\n\r\nSeveral transcribers, mostly history scholars, have been involved in the project to digitally acquire De\r\nGasperi\u2019s correspondence. They have been provided with an ad-hoc tool (Moretti et al., 2018), through\r\nwhich they have inserted various metadata for each document, including: sender, recipient, chronotopical data, type of document (letter, telegram etc.)5 (Malfatti, 2019). They also specify if the document\r\nis an original, a copy or a draft and if the topic is of a personal or institutional nature. Presence of\r\nenvelope, letterhead, autograph signatures, attachments have to be reported as well. Transcribers must also\r\nindicate for each document the number of cards, writing technique (manuscript\/typewritten), conservation\r\nstatus, as well as the reference code for the correct identification of the document. Transcriptions must\r\naccurately reflect the text and the layout of the document, including possible mistakes, illegible words\r\nand cancellations, punctuation and paragraph division, etc. Margin notes have to be reported in a specific\r\nfield, possibly with the corresponding position and author. Historical (or commentaries) notes clarify\r\nand integrate the content of the text. The transcript must also be preceded by an abstract to contextualise\r\nand summarize its content.\r\n\r\n4\r\n\r\nTranscription Tool\r\n\r\nFor the acquisition of De Gasperi\u2019s letters we developed a new infrastructure that allows different\r\ntranscribers to work in parallel and to smoothly publish the transcribed documents online as soon as\r\nthey have been revised by a small pool of experts. Specifically, the infrastructure functionalities are the\r\nfollowing:\r\n\u2022 possibility to work both offline and online, so that the transcriptions can be performed also in archives\r\nwithout an Internet connection;\r\n\u2022 use of a wide set of metadata defined by the Scientific Committee and compliant with the Dublin\r\nCore standard and the standard for the cataloging of manuscripts in Italian libraries;\r\n2https:\/\/www.darwinproject.ac.uk\/\r\n3http:\/\/vangoghletters.org\/\r\n4http:\/\/datini.archiviodistato.prato.it\/\r\n5The editorial guidelines are available online: https:\/\/epistolariodegasperi.it\/#\/risorse\r\n\r\n254\r\n\r\n\f\u2022 clear division of roles into three types of contributions: (i) the transcriber, who creates a digital\r\npicture of the original document and uploads it in the transcription tool, inserts metadata, transcribes\r\nand annotates the text, (ii) the supervisor, who adds the critical apparatus, and (iii) the editor, who is\r\nin charge of validating the correctness of the transcription and publishes the letter in the final online\r\nplatform;\r\n\u2022 automatic upload and update of each letter on a central database; (v) easy-to-use transcription\r\ninterface not requiring explicit knowledge of tag annotation and coding schemes;\r\n\u2022 easy-to-use transcription interface not requiring explicit knowledge of tag annotation and coding\r\nschemes;\r\n\u2022 presence of autocomplete features to reduce the risk of mistakes in the insertion of metadata.\r\nFrom the technical point of view, the infrastructure includes a MySQL database and three software\r\napplications (one for each role) written in Javascript\/ECMAScript 6. The interface is implemented using\r\nthe ReactJS framework.\r\n\r\n5\r\n\r\nDigital Archive\r\n\r\nCurrently, the digital archive allows to search by title, sender, recipient, type (i.e., letter, telegram,\r\npostcard, note, other), theme (i.e. private life, national politics, international politics, local politics,\r\nreligion, culture, economy, society, political party), free text. A slider can be used to select a specific\r\ntime period from 1902 to the year of De Gasperi\u2019s death in 1954. The interface responds in real time\r\nto what the user types, modifying the list of letters that correspond to the search key. Searches can\r\nalso be combined: for example, it is possible to search for all the notes sent by Giulio Andreotti after\r\n1950 or for all the postcards containing the word \u201Caugurio\u201D. When clicking on a letter, all the metadata\r\ncompiled by the transcriber are displayed together with an abstract written by a historian that summarizes\r\nthe content of the document. The photograph and the transcription are placed next to each other and\r\nthe transcription respects the annotation made by the transcribers showing, among others, words that are\r\nunderlined, erased parts, spelling errors. Footnotes explain content that could be unclear to the reader\r\nadding contextualization. In addition, 380 proper names of persons are accompanied by a biographical\r\nnote written by historians. Given that the aim of the project is to attract both general public and experts,\r\nreaders can access the National Edition from different perspectives. More specifically, with a simple click\r\nthe user goes from a complete transcription of the letter containing all the annotations to a simplified one\r\nthat is more suitable for the general public: in this reader-friendly view of the letter the deleted parts are\r\nnot displayed, the typos are corrected and the abbreviations are extended.\r\n\r\n6\r\n\r\nCurrent State of the Project\r\n\r\nThe transcription process started in August 2018 and it currently involves 38 transcribers and 10 supervisors. At the moment of writing (September 2019), 1,549 letters from 106 different private and public\r\narchives in Italy and abroad have been transcribed, annotated, revised and published online. The number\r\nof letters written in each year currently present in the digital archive is shown in Figure 1. As expected,\r\nmost letters are exchanged between 1945 and 1954, when De Gasperi was a very prominent political\r\nfigure with key roles in the Italian government. However, we observe an interesting element for the years\r\nbetween 1926 and 1942, which historians describe as De Gasperi\u2019s internal exile, because he did not\r\ncover official roles under fascism: while the archive of public documents of Alcide De Gasperi6 contains\r\nonly few documents for that period (Moretti et al., 2016), 132 letters have been found for the same years,\r\nand 24 of them have been tagged as being about Politics. This shows that, even if De Gasperi did not\r\nhave any official role, he was still involved in political discussions and was expressing his opinion on the\r\ncurrent situation.\r\n6Available at http:\/\/alcidedigitale.fbk.eu\/\r\n\r\n255\r\n\r\n\fFigure 1: Letter distribution over time\r\nAdditional statistics are reported in Table 1, where we list the five senders and receivers that are\r\ncurrently most present in the correspondence. De Gasperi appears in most letters either as sender and as\r\nreceiver. Only in few examples he is not explicitly mentioned, for instance when the receiver is a collective\r\nentity (e.g. \u2018Governo italiano\u2019, \u2018Soci della Tridentum\u2019) where De Gasperi was included. Some persons\r\nappear in the table because a version of their correspondence had already been curated and published\r\nbefore, see for example Sturzo (Antonazzi, 1999), so that the Epistolario project could take advantage\r\nfrom already transcribed blocks of letters.\r\nSenders\r\nAlcide De Gasperi\r\nLuigi Sturzo\r\nPiero Malvestiti\r\nLuigi Granello\r\nGuido de Gentili\r\nAgostino Gemelli\r\n\r\n#\r\n816\r\n143\r\n71\r\n39\r\n28\r\n22\r\n\r\nReceivers\r\nAlcide De Gasperi\r\nLuigi Sturzo\r\nGiulio Delugan\r\nPiero Malvestiti\r\nGiuseppe Micheli\r\nAmintore Fanfani\r\n\r\n#\r\n704\r\n107\r\n55\r\n41\r\n34\r\n31\r\n\r\nTable 1: Top-5 senders (left) and receivers (right) in the digital collection at the moment of writing.\r\n\r\nFigure 2: Topic distribution in the letters\r\nAnother interesting analysis is related to the topic(s) of the letters. Indeed, each transcriber has to\r\nassign one or more topics to each letter chosen among the following categories: Private Life, National\r\nPolitics, International Politics, Local Politics, Political Party, Economy, Society, Culture, Religion. The\r\nchoice to allow multiple annotations was guided by domain experts, who identified in each letter multiple\r\n\r\n256\r\n\r\n\ftopics and opted not to select only the main one. We report in Figure 2 the distribution of topics in the\r\nletters transcribed so far. As expected, national politics is the most present topic. The fact that private\r\nlife is the second most frequent topic, instead, is rather surprising, since the majority of De Gasperi\u2019s\r\nletter exchanged with other family members are still kept private by his descendants, and have not been\r\nincluded in the Epistolario. This means that De Gasperi tended to mention personal information and tell\r\nabout private aspects of his life in letters to members of the christian-democratic party, politicians and\r\nother public figures, mixing political, cultural and private topics.\r\n\r\nFigure 3: Keyphrases extracted from eighteen English letters sent by\/to De Gasperi.\r\nAnother intriguing aspect of the collection is the fact that, given the international role played by De\r\nGasperi, especially after WWII, several letters are either in English or in German (resp. 33 and 10 at\r\nthe moment of writing). Figure 2 shows a preliminary content analysis of the English transcribed letters\r\nperformed with the keyword extraction tool KD (Moretti et al., 2015). The letters, dated 1945-1948,\r\nwere exchanged between De Gasperi and important US personalities, such as President Truman and the\r\nSecretary of Defense Robert A. Lovett. The focus of these letters is on the international cooperation\r\nbetween the two nations, the treaty after WW2, the necessity of peace for Italy reconstruction and the\r\nrole of supranational communities.\r\n\r\nFigure 4: Network of letters\r\nFigure 4 presents the correspondence network of the collection at the time of writing, considering only\r\nexchanges of at least two letters. The network is a direct graph in which node and edge size is related to\r\nthe amount of exchanged letters and edge colour represents different types of document. We can notice\r\n\r\n257\r\n\r\n\fthat the strong majority are letters (75%, in red) but there are also telegrams (20%, in blue) and short\r\nnotes (4%).\r\n\r\n7\r\n\r\nConclusion and Future Work\r\n\r\nA digital edition presents, with respect to a print edition, new opportunities both for curators and users.\r\nFirst of all, there are virtually no limits to the amount of material that can be included in a digital collection;\r\nin addition, archiving on a web platform makes the organisation and management of documentary material\r\nmore flexible, and provides users with interactive tools that are easier and more attractive for non-experts.\r\nAt the same time, however, a digital project also requires an interdisciplinary approach. In particular, it\r\nis necessary to involve different competences already in the planning phase to create effective synergies\r\nbetween historians, DH experts, computer scientists, archivists and publishers. The National Edition of\r\nDe Gasperi\u2019s letters in digital format is an example of this interdisciplinary approach: all the choices\r\nabout the documentary editing are the results of discussions among the project members that have guided\r\nthe choices implemented in the platform, for example the possibility to annotate the letters from different\r\nperspectives (linguistic, historical, philological).\r\nThe development of the digital archive is still ongoing thus several improvements and additions are\r\nplanned for the future. Search options will be extended to take advantage of the many metadata provided\r\nby the transcribers through the transcription tool: e.g. language used in the letter, presence or absence of\r\nheaded paper, presence or absence of a handwritten signature. Thematic paths will also be designed to\r\nguide the user in discovering the collection of letters, for example by navigating the various biographical\r\nphases of De Gasperi\u2019s life or the relationship between De Gasperi and his correspondents. Moreover,\r\nit will be possible to browse the letters stored in a specific archive or library using an interactive map.\r\nUsers will also be allowed to download the letters in different format, including pdf and TEI-XML. In\r\naddition, we plan to release to the research community the transcription infrastructure with a Creative\r\nCommons 4.0 license.\r\n\r\nAcknowledgments\r\nMarco Odorizzi wrote Section 1 of this paper, Stefano Malfatti wrote Section 3, while Rachele Sprugnoli,\r\nSara Tonelli and Giovanni Moretti equally contributed to the analyses and the writing of the other sections.\r\n"
	},
	{
		"id": 41,
		"title": "Visualizing Romanesco; or, Old Data, New Insights",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Gianluca Valenti"
		],
		"body": "1 The sociolinguistic background\r\nIn the past decades, dialectology studies have exponentially increased. All linguists nowadays acknowledge\r\nthe importance of the dialects in the social and cultural history of humanity, and put them on an equal footing\r\nwith standard languages. In particular, in the context of Italian sociolinguistics, the study of the dialect of\r\nRome is considered as an extremely relevant topic.\r\nSince the beginnings, Italian dialects are divided into three main groups: Northern, Tuscan, and Southern\r\nvarieties. The medieval and renaissance periods marked an irreversible revolution in the Roman social\r\nbackground, and consequently, in the Roman language\u2014the Romanesco.\r\nAlthough Romanesco formed part of the Southern dialects, before the second half of the 16th century at the\r\nlatest, it came to resemble the Tuscan varieties\u2014a process called \u2018Tuscanisation\u2019 (Ernst, 1970). This change,\r\nwhose dynamics remain largely unclear, represents a unique episode in the history of the Italian language. The\r\nuniqueness of the \u2018Rome case\u2019 has been stressed on many occasions, and several explanations have been\r\nproposed for it, but no agreement has been reached yet.1 Indeed, the mutation of Romanesco was so deep (and\r\nwithout similar precedents) that scholars tend to refer to it as \u2018disintegration, decay\u2019 (Migliorini, 1932), instead\r\nof \u2018evolution\u2019 (as is the usual case for most of the languages).\r\nMore than fifty years after Migliorini\u2019s work, Mancini (1987:41) argued that, until then, scholars \u2018placed\r\ntoo much emphasis on some demographically macroscopic events,\u2019 such as the Sack of Rome of 1527. Instead,\r\naccording to him, the mutation of Romanesco was a slow event, already in place, at least in its germinal stages,\r\nin the Trecento and the Quattrocento. Trifone (1990:92) quickly reacted with a detailed linguistic analysis of\r\nnew documents, and concluded that the demographic de-southernisation of Rome (caused by the sack) and the\r\nensuing repopulation of the city post-1527 were the main reasons for the de-southernisation of the spoken\r\nlanguage of its lowest social classes. The debate has continued for several years (cf. also Trifone 1992 and\r\nMancini 1993) without a consensus.\r\nFrom one side, it is unquestionable that, as De Caprio (1988:453) states, \u2018the sack of Rome of 1527 is a\r\ntraumatic caesura in Roman cultural history,\u2019 but, from the other side, its role in the context of the\r\nTuscanisation is admittedly unclear. Even at the present time, instead of positioning themselves on one side or\r\n\r\n1\r\n\r\nCf. e.g., Vignuzzi (1988, 1995), De Mauro (1989), Palermo (1991), and, recently, Coluccia (2011), who argues that the exceptionality\r\nof the phenomenon lies in the untimely Tuscanisation at a spoken level.\r\n260\r\n\r\n\fthe other, scholars tend to report both opinions, at most trying to harmonise them into a single whole.2\r\nCurrently, and maybe because no one has established a definitive answer to the issue, researchers seem to\r\nprefer to focus on the currently spoken Romanesco, a language that has still many points of contact with its\r\nrenaissance variety.3 Interest in the old Romanesco has not waned, though, as is clear by the recent organisation\r\nof the roundtable \u2018Rome in history, in linguistics and in literature\u2019 (Rome, 23th July 2016) and the international\r\nconference \u2018Il romanesco tra ieri e oggi\u2019, which I organised in Li\u00E8ge the 9th September 2019.\r\n\r\n2\r\n\r\nOld data, new insights\r\n\r\nThe old epistemological framework\u2014where single scholars tried to explain the evolution of the Roman\r\nlanguage by studying analytically one or few texts\u2014has proved to be unfit for the task of understanding the\r\ndynamics of the Tuscanisation of Romanesco. At the present time, it is necessary to look at a broader picture,\r\nand consider the Roman texts as if they were a single whole. Indeed, up to now, scholars essentially based\r\ntheir findings on qualitative research, but did not exploit the potentialities of databases and digital tools. Most\r\nof the current papers focused on Romanesco analyse the language of a specific text (or a bunch of texts), while\r\na general overview that takes into account the huge amount of data pertaining to the Roman sources collected\r\nby the scientific community during the past hundred years is still missing.\r\nI recently made the first step in order to fill this gap, by building and putting online a database that allows\r\nusers to make queries into the whole corpus of texts written in Romanesco from the Origins to 1550. It\r\nis available online, free of charges, at the address http:\/\/www.romanesco.uliege.be\/.\r\nRegularly updated, the database makes available metadata concerning not only the texts, but also the\r\nphysical supports (printed books, manuscripts, and places, such as churches or catacombs) that transmit\r\nthem.\r\nWorking on digital data leads scholars to new findings, and allows them to answer old research\r\nquestions, which could not be solved with the traditional approach. In this paper, with the help of some\r\nvisualisations, I show that\u2014due to the scarcity of the sources\u2014we do not have sufficient data to get new\r\ninsights about the language of Rome and its evolution through time if we only look at the languages of the\r\ntexts. Therefore, to have a sharp understanding of the Tuscanisation process, we need to reanalyse the\r\nlinguistic features of the epigraphs, whose language has been often defined, maybe too quickly, as\r\ngenerically \u2018Vernacular\u2019.\r\n2.1 Corpus and tool\r\nTo conduct this study, I put in plain text files the metadata of 372 texts, written from 800 to 1550, with at\r\nleast some features of Romanesco in them. I took the data from D\u2019Achille and Giovanardi (1984). With\r\nregard to the languages, notice that a text can contain some features that do not belong to its original\r\nlinguistic system. In consequence, for each text, I identified its primary language and all the potential\r\nsecondary languages (i.e., the languages that occur to a lesser extent). The condition for a text to be\r\nincluded in this corpus is to have Romanesco as its primary or, at least, its secondary language. The corpus\r\nis thus composed of texts written in Romanesco\u2014which may contain some pieces of Tuscan or Latin\u2014, but\r\nalso of texts that contain only a small amount of features of Romanesco, while their primary language is\r\nTuscan, Latin, or Vernacular.\r\nEach text is transmitted by one physical support: a) \u2018places\u2019 transmit epigraphic texts; b)\r\n\u2018manuscripts\u2019 transmit handwritten texts; and c) \u2018printed books\u2019 transmit printed texts.\r\nAll the visualizations are made with the software Tableau.\r\n2.2 Results\r\nFigures 1\u20134 show the total number of occurrences of primary and secondary languages, and their evolution\r\nover time. The figures provide some interesting insights that, in a way, strengthen both hypotheses of Mario\r\nMancini and Pietro Trifone.\r\n\r\n2\r\n3\r\n\r\nCf. e.g., Vignuzzi (1994), Giovanardi (1998:61), D\u2019Achille and Petrocchi (2004:122\u2013123).\r\nCf. e.g. the projects VRC. Vocabolario del Romanesco Contemporaneo and ERC. Etimologie del romanesco contemporaneo.\r\n261\r\n\r\n\fFigure 1. Primary languages\r\n\r\nFigure 2. Primary languages over time (I)\r\n\r\nFigure 3. Primary languages over time (II)\r\n\r\nFigure 4. Secondary languages\r\n\r\nWe notice that the total amount of occurrences of texts written in Romanesco and Tuscan starts to diverge (in\r\nfavour of the former) in the 2nd half of the 14th century, maybe due to the success of Anonimo Romano\u2019s\r\nCronica (1357\u20131358), the most important Roman text of its century.\r\nHowever, the Tuscan language is attested all over the centuries, from the 14th to the 16th century. I do not\r\nregister any dramatic increase right after 1527 (cf. figure 2). This observation seems to endorse Mancini\u2019s view\r\nof the Tuscanisation as a slow process, already in place in the Quattrocento.\r\nOn the other hand, if we look at figure 3 we notice, in the first half of the 16th century, a slight decrease of\r\ntexts written in Romanesco, and a parallel growth of texts written in the Tuscan language. Admittedly, this\r\noutcome may be related\u2014as Trifone states\u2014to the de-southernisation of the Roman population after the Sack,\r\nand the subsequent increase in the number of Tuscan people moving to the town.\r\nThese visualisations improve significantly our perception of the evolution of Romanesco through time.\r\nNonetheless, they do not provide any irrefutable evidence that would end the debate on the timing and\r\nmodalities of its Tuscanisation. Therefore, we need to look at the problem from another perspective.\r\nIndeed, an aspect has escaped the scrutiny of most of the past scholars: observing the physical supports that\r\ntransmit Roman texts over the years, we notice some interesting insights.\r\n\r\nFigure 5. Physical supports\r\n\r\nFigure 6. Physical supports over time\r\n\r\n262\r\n\r\n\fThe prevalent supports of literary texts are, up until and including the 15th century, manuscripts, and afterwards,\r\nprinted books. In the present corpus, though, the total number of epigraphs is significantly high (cf. figure 5).\r\nFurthermore, if we consider only the sources ranging from 800 to 1550, texts transmitted by epigraphs are\r\neven more than texts transmitted by manuscripts (cf. figure 6).4 This is due to the fact that most of the texts of\r\nthis corpus are practical documents (such as receipts, letters, and private notes), and have no literary value.\r\nIndeed, texts that are transmitted by perishable material\u2014such as pieces of paper\u2014get easily lost, while texts\r\nthat are carved on the column of a church are more likely to be preserved.\r\nBy their very nature, epigraphs are dramatically short, and in consequence, linguistic features that are\r\ntypical of a given area are less likely to be detected in epigraphs than in other textual typologies. The high\r\nnumber of epigraphs included in this corpus may be related to the high number of occurrences of texts written\r\nin a language that has been defined, generically, as \u2018Vernacular\u2019. Therefore\u2014maybe because of their\r\napparently low linguistic value\u2014the past surveys on the Tuscanisation of Romanesco did not take enough into\r\naccount epigraphic texts.\r\nHowever, the low number of handwritten documents of Romanesco and, in contrast, the high number of\r\nepigraphic texts, make the latter a critical source to understand the linguistic mutations in the medieval and\r\nrenaissance Rome. Moreover, we should not forget that, as a starting point for the research, we have at disposal\r\na solid documentary basis, the volume on Vernacular texts found in churches, edited by Sabatini et al. (1987).\r\nThere, the authors provide detailed linguistic analyses, which could serve as a model for further studies focused\r\non the language of the newly discovered epigraphic texts of the past thirty years.\r\nOnce we have collected a significant amount of new data, it will be possible to look again at the linguistic\r\nfeatures of the Roman sources\u2014including but not limited to manuscripts and printed books\u2014thus refining our\r\ntheories and reaching new conclusions about the Tuscanisation process of Romanesco.\r\n\r\n3 Conclusions and new perspectives\r\nWithin the traditional approach, scholars tried to explain timing and causes of the Tuscanisation of Romanesco\r\nby analysing the linguistic features of a small selection of texts. The results of this approach\u2014albeit essential\r\nin many respects\u2014did not lead to a sharp understanding of this particular linguistic process. I have shown that\r\nthe reason for this failure is not entirely due to the little number of texts analysed. Indeed, even though we\r\nconsider all texts at our disposal, we are not able to recognise a clear pattern in favour of one or the other\r\ntheory. In order to resolve this issue, we need more data, i.e., we need to look back at those texts that\u2014until\r\nnow\u2014have been catalogued as written in \u2018Vernacular\u2019 language.\r\nThe high number of texts that we did not assign to any specific linguistic system is probably related to the\r\nhigh number of epigraphs that transmit them. Nowadays, scholars should approach the epigraphic texts with\r\nrenewed attention, looking for pieces of evidence of their linguistic features. While awaiting additional archival\r\nfindings, this is the only way to increase the number of texts whose language is known, which is our only\r\nchance to make new assumptions that could explain the Tuscanisation of Romanesco."
	},
	{
		"id": 42,
		"title": "What is a last letter? A linguistics/preventive analysis of prisoner letters from the two World Wars",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Giovanni Pietro Vitali"
		],
		"body": "1 Introduction\r\nThis paper aims to report some of the first results of the Marie Curie project entitled Last Letters from the\r\nWorld Wars: Forming Italian Language, Identity and Memory in Texts of Conflict, which started in September\r\n2018. The project analyses the linguistic and thematic features of the last letters of people sentenced to death\r\nduring the two World Wars, and is conducted with digital humanities tools. The documents concerning the\r\nFirst World War have been collected mainly in the Central Archives of the Italian State in Rome and thanks\r\nto a kind donation by Professor Giovanna Procacci, who offered her letters of Italian prisoners (Procacci,\r\n2000), published and unpublished, for analysis in the Last Letters project. The letters from the Second World\r\nWar were collected in close collaboration with the Ferruccio Parri National Institute (ex INSMLI) and the\r\nCentre for Contemporary Jewish Documentation (CDEC), both in Milan. The majority of the Second World\r\nWar texts were collected through these two organisations, although some also come from other Italian institutes\r\nof resistance connected to the Ferruccio Parri National Institute, which is the central organisation of the Italian\r\nNetwork of Institutes for the History of the Resistance and the Contemporary Age. Other letters were found\r\nthanks to the National Association of Ex-Deportees in the Nazi Camps (ANED), again in Milan. In other\r\nwords, we were able to collect letters from Italian prisoners captured by the Austrian or German armies as far\r\nas the First World War is concerned, whereas we composed a corpus of letters from partisans and Jewish\r\ndeportees for the Second World War. The total number of letters is 1203 for letters from WWII and 960 for\r\nWWI. I selected those documents, which were analysed for this paper, from a total of approximately 3500\r\nletters collected in the first six months of my archival research.\r\n2 Objectives\r\nThe objective of this paper is to display the main differences between these two corpora (WWI-WWII). In\r\nfact, as far as the Second World War is concerned, the prisoners who wrote those letters were mainly partisans\r\nwho knew for certain that they were going to be executed, whereas for the First World War the writers did not\r\nhave a precise notion of what their fate was going to be, despite the precariousness of their situation. This\r\ndichotomy between letters from prisoners who knew they were sentenced to death and prisoners who still\r\nbelieved in a chance of survival, is underlined by textual and extra-textual elements. These elements were\r\nmade evident by a digital analysis that allowed a greater understanding of these letters. The purpose of this\r\npaper is not to provide a complete interpretation of the subject but to propose a framework for the genre \u2018last\r\nletter\u2019 in order to understand if it is possible to give a preliminary answer to such a question. The texts analysed\r\nwere submitted to a first NLP analysis carried out with TreeTagger (Schmid, 1994, 1995). To conduct this\r\n265\r\n\r\n\fpreliminary analysis, I used, in TreeTagger, the Italian parameter file of Professor Achim Stein (University of\r\nStuttgart). I then scanned the text for potential errors. Several writers for example, especially during World\r\nWar I, often write a group of words as a single word. An example of this is \u2018saperesezicarlo\u2019 which means \u2018to\r\nknow if uncle Carlo\u2026\u2019 The transcripts of these texts normally respect the language and spelling of the original\r\ndocuments. This way of writing, combined with dialectal forms, could not be read by TreeTagger. In order to\r\nrectify the pos-tagging procedure, I decided to correct manually the repertories of tagged word so that the work\r\ncould be more precise, considering that I had to divide those groups of words. Regarding the stop words, I\r\ndecided to include them in the analysis because they represent another characteristic habit of the writers. As I\r\nexplained, these texts often display groups of united words, and in some cases, this happens because the writers\r\ndo not know the correct spelling of these phrases, nor the concept of collocating, for instance, a preposition\r\nand a noun. An example of this tendency is the recurring intrincea [in the trenches], which should be written\r\nin two separate words, or aggorizia, which is a case of syntactic gemination (Repetti, 1991), a typical\r\nphenomenon that occurs in spoken Italian. In three different cases, the writers wrote aggorizia instead of a\r\nGorizia [to Gorizia], imitating the sound they reproduce orally. This example is typical of the main category\r\nof writers who are part of the two corpora, that is, partially literate people. They often write groups of words\r\nattached together, like alacamba, literally alla gamba [to the leg], with the palatalization of the velar consonant\r\n(Pellegrini, 1985: 272). Another interesting example is the assimilation of the verb \u2018have\u2019 to the following past\r\nparticiple. In 1,2% of the passato prossimo [present perfect] forms it is possible to read expressions like\r\noreclamato [I reclaimed] or oscrito [I wrote]. Finally, a third kind of word grouping, which occurs quite rarely\r\n(0,02% of the tokens), consists in the writer attaching entire phrases in a unique form. Some examples of this\r\ntendency are to be found in cases like nosischersa [don\u2019t joke about it] that display the low level of education\r\nof the writer, who obviously does not know the correct spelling of the verb scherzare, in which there is no \u2018s\u2019\r\nbefore the final suffix. The preliminary analysis that I am proposing in this paper presents the very first results\r\nof my ongoing research, namely, the first comparison between WWI and WWII corpora.\r\n3 Letters\r\nSo what is a \u2018last letter\u2019? Is it the last text written by someone before his or her disappearance or should it have\r\nsome precise characteristics in terms of language and contents? Can a prisoner who was ignorant of his\/her\r\nfate really write a last letter? Can we consider a \u2018last letter\u2019 one written by a prisoner who is then pardoned?\r\nTraditionally in the history of epistolary memoirs, the last letter has always been vaguely described as the last\r\nmessage that remains to us from someone who died. However, there are several types of documents that fit\r\nthis description, and yet also have other, distinct features. For example, the Jewish partisan Emanuele Artom\r\n(Aosta, 23\/06\/1915 \u2013 Torino, 7\/04\/1944) kept a diary (Artom, 1966) during his imprisonment by the Nazis.\r\nHis spiritual testament is contained within this diary, but not at the end of the text. Could this message be\r\nconsidered Artom\u2019s last letter even though it is a diary page, simply because it contains the last message he\r\nwrote? Considering that the World War II corpus is mainly composed by attested last letters, and the World\r\nWar I corpus comprises letters written by prisoners who were, in most cases, unaware of their possible\r\nexecution, I will compare them in order to see what the main differences between these two corpora are. Then\r\nI will determine whether, among these differences, there are shared, distinctive features that can generally be\r\nattributed to a last letter genre, as people\u2019s final messages obviously present some recurring peculiarities. One\r\nof the main characteristics of last letters is the request to the family for forgiveness. An example of this, taken\r\nfrom the WWI corpus, is the famous letter by Fabio Filzi (Pisino, 20\/11\/1884 \u2013 Trento, 12\/11\/1916), an Italian\r\nvolunteer and irredentist executed by the Austrians. In his last message to his parents, one can immediately\r\nrecognise a request for forgiveness which is one of the common traits of letters in both WWI and WWII\r\ncorpora.\r\nCari genitori, prima di morire non posso fare a meno di esprimere il mio profondo rincrescimento, per il fatto che\r\nmi sovrasta, invero non per la mia esistenza, ma per voi che avete fatto tanto per me e che non approvaste i miei\r\nsentimenti italiani. Io ho sempre adempito il mio dovere con scrupolosit\u00E0 seguendo sempre l\u2019impulso della mia\r\ncoscienza. Prima di morire rivolgo il pensiero a voi e alla mia cara Emma, che si trova a Padova, e contro i cui\r\nconsigli ho agito arruolandomi. Addio per sempre, baci ai miei fratelli. Fabio Filzi.1\r\n\r\n1\r\nDear parents, before I die I cannot help but express my deep regret, for the fact that it overwhelms me, indeed not for my existence,\r\nbut for you who have done so much for me and who did not approve of my Italian feelings. I have always fulfilled my duty with\r\nscrupulousness, always following the impulse of my conscience. Before dying I turn my thoughts to you and to my dear Emma, who\r\nis in Padua, and against whose advice I acted by enlisting. Goodbye forever, kiss my brothers. Fabio Filzi.\r\n\r\n266\r\n\r\n2\r\n\r\n\fThis letter stands out as one of the few examples of surviving letters written by Italian WWI soldiers who were\r\nexecuted. The red part shows the request for forgiveness to his parents for the pain that his own death will\r\ncause them. This sentence could also be easily contained in the letter of a partisan sentenced to death. On the\r\ncontrary, the green part is something totally different compared to the letters of the Second World War. In the\r\nletters of this period there are no ideological clashes between the people sentenced to death and their families.\r\nThe contrast is always linked to the desire of the condemned person\u2019s loved ones not to lose them. Another\r\nfeature that differentiates the two corpora is that First World War letters were written only by adult men while\r\nin the WWII corpus there are several other categories of writers, like teenagers and women. An example is\r\nRenato Mantovani (Treviso, 16\/12\/1928 \u2013 Pieve di Teco (IM), 26\/01\/1945), one of the youngest partisans in\r\nthe corpus, who was 16 years old.\r\nNotizia ai genitori. \u2018Sono accusato di appartenere alle bande comuniste, vi domando perdono, ora mi fucilano\u2019.\r\nRenato.2\r\n\r\nAgain, as you can see in green, Mantovani apologizes to his family for the pain that his shooting will cause\r\nthem, as in the previous case of Fabio Filzi. These two texts show that there are characteristics in common\r\namong the last letters, although these two texts were produced 20 years apart and were written by different\r\nprofiles of writers. Another common feature of all these letters, even those not written with full awareness of\r\ncertain death, is to entrust one\u2019s family to the care of the recipient of the message. This is because the writer\r\nthinks that they will never be able to see their loved ones again. An example from the First World War is the\r\nfollowing excerpt from the letter of a soldier who is writing from a trench shortly before an assault, with an\r\nobvious fear of dying.\r\n[\u2026] e bacia i banbini ch\u2019io sara dificile a poterli rivedere ancora una volta; mediante il mio scritto la lagrime\r\ncadono dalli occhi che una volta ti rimirava. adio. [\u2026]. (Procacci, 2000: 414).3\r\n\r\nWe only know the name of this infantryman, Beppe, and we cannot say with certainty that this is his last letter\r\neven if it is plausible to think so. In red, it is evident that the letter refers to never seeing one\u2019s loved ones again\r\nwith the probable intention of exorcising the fear that this soldier had that this eventuality would actually occur.\r\nAn example of a letter from World War II with the same kind of tone is that of Vanda Abenaim (Pisa, 6\/05\/1907\r\n- Auschwitz, unknown), a Tuscan Jew who did not survive Auschwitz. The case of Abenaim is very interesting\r\nbecause it is one of the few that presents coded messages in a last letter. Based on her son\u2019s accounts, we know\r\nthat the family had devised a coded means of communicating in case they should find themselves in a\r\ndangerous situation (Pacifici, 1993: 129).\r\nFirenze 30\/11\/1943\r\nGent.ma Signora, Mi farebbe tanto la gentilezza di consegnare a mio fratello la presente perch\u00E9 purtroppo sono\r\nferita gravemente e non so quale destino mi sono destinata. Sono molto avvilita perch\u00E9 non so se potr\u00F2 essere salva\r\ne rivedere pi\u00F9 i miei cari. Gi\u00E0 sono in camerata. Pregata tanto per me. I bimbi sono stati salvati. Per ora sono\r\nsempre a Firenze. Mando tanti baci al mio caro Carlo e mando baci alla mia mamma e chiss\u00E0 quando la rivedr\u00F2.\r\nSaluto tanto anche lei e pure la sua signorina. Sua aff.m a nipote Vanda.4\r\n\r\nThe letter is theoretically addressed to a woman, but in truth it is addressed to her brother. In red you can see\r\nthe parts where the woman asks her brother in code to take care of the children because she fears she will never\r\nsee them again. The Abenaim family had established a secret code if they were captured by the Germans\r\n(Abenaim, 2015). The examples in the text are in green. With the sentence sono gravemente ferita e non so\r\nquale destino mi sono destinata [I am seriously injured and do not know what fate I am destined for], she\r\nwarns her family that she has been taken prisoner by the Germans. Moreover, with the expression Gi\u00E0 sono in\r\ncamerata [I\u2019m already in my dormitory], she informs her brother that she is already on the train to the\r\nconcentration camp. Considering that these texts have some common traits despite their many differences, I\r\ntried to keep the characteristic elements of the original documents to better underline the differences between\r\n2\r\n\r\nNotify the parents. I am accused of belonging to communist gangs, I ask your forgiveness, now they shoot me \u2018Renato\u2019.\r\nAnd kiss the children as it will be difficult for me to see them again; through my writing the tears fall from the eyes that once gazed\r\nat you. Farewell.\r\n4\r\nDear Madam, could you be so kind as to bring this letter over to my brother because unfortunately I am seriously injured and do not\r\nknow what fate I am destined for. I am very discouraged because I do not know if I will be saved and I will be able to see my loved\r\nones again. I'm already in my dormitory. Please pray a lot for me. The children have been saved. For now I'm still in Florence. I send\r\nmany kisses to my dear Carlo and I send kisses to my mother and who knows when I will see her again. I also greet you and your lady.\r\nYour affectionate nephew Vanda.\r\n3\r\n\r\n267\r\n\r\n3\r\n\r\n\fone corpus and the other, through a semantic and morpho-syntactic analysis of the two. I mainly used\r\nTreeTagger and I displayed some results through the use of the Links tool of Voyant-Tools. The next step of\r\nthe project will consist in the comparative analysis of another group of texts carried out with TreeTagger and\r\nother lemmatizers such as Tint (tint.fbk.eu), UDPipe (http:\/\/ufal.mff.cuni.cz\/\r\nudpipe) and T2K (http:\/\/www.italianlp.it\/demo\/t2k-text-to-knowledge\/).\r\n4 Part-of-Speech analysis\r\nIt is the first time that such an analysis is applied to these texts, despite the fact that in some cases these letters\r\nhave already been studied and analysed, both for the WWI corpus (Spitzer, 2016) as well as the WWII one\r\n(Bozzola, 2013), but in any case, this is the very first digital analysis that has been applied to these texts. The\r\npossibilities given by digital tools have enabled us to clearly see the differences between World War I and\r\nWorld War II, and to establish some of the characteristics of the last letter genre. The result of the pos-tagging\r\nand the following manual corrections enabled me to gain a better understanding of the language of these letters.\r\nThe following table summarises all the characteristics of these texts.\r\nWorld War I\r\n\r\nWorld War II\r\n\r\nLetters\r\n\r\n960\r\n\r\n1203\r\n\r\nTokens\r\n\r\n63.637\r\n\r\n134.103\r\n\r\nTypes\r\n\r\n9.953\r\n\r\n12.912\r\n\r\nLemmas\r\n\r\n6122\r\n\r\n8031\r\n\r\nType-Token Ratio (TTR)\r\n\r\n0,156\r\n\r\n0,096\r\n\r\nLemma-Token Ratio (LTR)\r\n\r\n0,96\r\n\r\n0,6\r\n\r\nAverage Words Per Sentence\r\n\r\n19,2\r\n\r\n32,3\r\n\r\nAccuracy\r\nCorrected Tokens\r\n\r\n89,8%\r\n\r\n95,5%\r\n\r\n6505 (10,2% of the total)\r\n\r\n6044 (4,5% of the total)\r\n\r\nThe data concerning the accuracy are very interesting, and are linked to the linguistic nature of the two corpora.\r\nAs a matter of fact, the World War I corpus is linguistically more problematic, and Treetagger had a harder\r\ntime analysing it. As you can see, on World War I texts, it had an index of accuracy of 89,8%, which is 5.7\r\npercentage points less than the accuracy score it had on World War II letters. A manual correction confirmed\r\nthese data, but I also noticed, thanks to a close reading approach to World War I letters, some linguistic\r\npeculiarities that I did not think were canonical peculiarities, such as local and dialectal traits. As I explained\r\nin the introduction, World War I writers were not confident about the spelling of Italian, owing to their\r\neducation and their being dialect speakers. These characteristics, which TreeTagger cannot tag, are extremely\r\nrepresentative of the World War I corpus, but totally absent from the other one. Moving from these general\r\ncomments about parts of speech to a more detailed analysis, it is possible to verify the differences between the\r\ntwo corpora in terms of what grammatical categories are used, and where TreeTagger struggled the most.\r\n\r\nWorld War I\r\n\r\nCorrected\r\n\r\nTreeTagger\r\nOccurrences\r\n\r\nAccuracy\r\n\r\nOccurrences\r\n\r\nPercentage\r\n\r\nPercentage\r\n\r\nAbbreviations\r\n\r\n308\r\n\r\n0,48%\r\n\r\n0,07%\r\n\r\n45\r\n\r\n0,413%\r\n\r\nAdjectives\r\n\r\n4843\r\n\r\n7,61%\r\n\r\n8%\r\n\r\n5093\r\n\r\n0,392%\r\n\r\nAdverbs\r\n\r\n5842\r\n\r\n9,18%\r\n\r\n8,65%\r\n\r\n5504\r\n\r\n0,531%\r\n\r\nConjunctions\r\n\r\n4268\r\n\r\n6,7%\r\n\r\n6,5%\r\n\r\n4141\r\n\r\n0,199%\r\n\r\nArticles\r\n\r\n4283\r\n\r\n6,73%\r\n\r\n7,84%\r\n\r\n4988\r\n\r\n0,568%\r\n\r\nNouns\r\n\r\n12038\r\n\r\n18,91%\r\n\r\n21,95%\r\n\r\n13970\r\n\r\n3,03%\r\n\r\nProper Names\r\n\r\n1550\r\n\r\n2,43%\r\n\r\n1,91%\r\n\r\n1217\r\n\r\n0,523%\r\n\r\nPunctuation\r\n\r\n5263\r\n\r\n8,27%\r\n\r\n1,99%\r\n\r\n1269\r\n\r\n6,276%\r\n\r\nPrepositions\r\n\r\n9674\r\n\r\n15,20%\r\n\r\n13,12%\r\n\r\n8349\r\n\r\n2,082%\r\n\r\nPronouns\r\n\r\n9078\r\n\r\n14,26%\r\n\r\n13,11%\r\n\r\n8347\r\n\r\n1,148%\r\n\r\nVerbs\r\n\r\n15890\r\n\r\n24,97%\r\n\r\n20,10%\r\n\r\n12792\r\n\r\n4,868%\r\n\r\nAccuracy\r\n0,055%\r\n0,118%\r\n\r\nOccurrences\r\n84\r\n10809\r\n\r\nTreeTagger\r\nPercentage\r\n0,063%\r\n8,06%\r\n\r\nCorrected\r\nPercentage\r\nOccurrences\r\n0,12%\r\n159\r\n8,18%\r\n10968\r\n\r\n268\r\n\r\nWorld War II\r\nAbbreviations\r\nAdjectives\r\n\r\n4\r\n\r\n\f0,428%\r\n0,051%\r\n0,040%\r\n2,427%\r\n1,191%\r\n0\r\n0,255%\r\n0,548%\r\n9,994%\r\n\r\n11213\r\n8629\r\n8806\r\n26940\r\n3237\r\n3515\r\n16990\r\n19935\r\n27154\r\n\r\n8,36%\r\n6,43%\r\n6,57%\r\n20,09%\r\n2,41%\r\n2,62%\r\n12,67%\r\n14,86%\r\n20,25\r\n\r\n8,79%\r\n6,49%\r\n6,6%\r\n17,66%\r\n3,61%\r\n2,62%\r\n12,41%\r\n15,41%\r\n30,19%\r\n\r\n11787\r\n8698\r\n8860\r\n23685\r\n4835\r\n3515\r\n16647\r\n20670\r\n40485\r\n\r\nAdverbs\r\nConjunctions\r\nArticles\r\nNouns\r\nProper Names\r\nPunctuation\r\nPrepositions\r\nPronouns\r\nVerbs\r\n\r\nWORL WAR I\r\n\r\nWORL WAR II\r\n\r\nThe biggest differences between the two corpora lie in the use of verbs. If the present, the most common tense\r\nof the indicative, appears in both corpora with almost the same frequency (WWI: 7,82 \u2013 WWII: 8%), the same\r\ncannot be said of the past tenses. The use of the past \u2013 simple past, imperfect, and present perfect \u2013 in World\r\nWar II letters is more than twice as frequent (11,41%) as in WWI texts (4,9%). Combining these data with a\r\nclose reading approach, it is possible to affirm that this linguistic trait is one of the peculiarities of the last letter\r\ngenre. As I said, WWI letters were written by prisoners who wanted to tell their families about their everyday\r\nlives. On the other hand, WWII letters were written by people sentenced to death, who often used the memories\r\nof their past experiences as a way of exorcising the fear of capital punishment and entrusting their loved ones\r\nwith the memories of happy times when they were together. The use of parts of speech being so meaningful, I\r\ndecided to highlight the correlation between these grammatical elements by using a graphic tool. Then I\r\nsubmitted these tagged texts to Voyant-tool. The application then showed which parts of speech are the most\r\ncommon (blue rectangles) and which combinations they form (orange rectangles).\r\n\r\nIt is immediately evident that the texts from the First World War give greater attention to textual construction.\r\nFor instance, they display a greater use of punctuation. As Spitzer (2016: 108) noticed, this use is often made\r\nincorrectly (Cortelazzo, 1972: 119-123). Nevertheless, the writers demonstrate awareness of the fact that\r\npunctuation must be there as an indispensable element of the text, (Restivo: 2018, 249), while in the Second\r\nWorld War letters, due to the strong emotionality of the moment, language becomes mimetic of speech. First\r\nWorld War morpho-syntactic chains highlight a higher number of nouns, prepositions and adverbs with an\r\nindirect relationship between substantives and adjectives. Analysing the letters with a close reading approach,\r\nI can suppose that sentences are more complex and present a higher number of indirect objects. On the other\r\nhand, the Second World War corpus, with a higher number of nouns and adjectives, displays a more prominent\r\nuse of direct objects or nominal sentences.5 Indeed texts of the First World War show a greater hypotaxis,\r\ntherefore complexity, in comparison to those of the Second, precisely because of the different emotional\r\nconditions of writing but not only. In fact, the writing of the last letters of deportees and condemned to death\r\nof the resistance is very often clandestine and can literally be visualised on the page as a stream of\r\nconsciousness, as the writers were trying to make the most of every moment in which they could find time to\r\nwrite. In a possible definition of the last letter, it will therefore be necessary to consider the morphological\r\nconstruction of the discourse as one of the discriminating factors in the reflection on genre.\r\n5 Most Frequent\/ Characteristic words\r\nIn order to better display the use of the lexicon, I used Voyant as a tool to visualize collocations and links\r\nwithin the texts. The representation of the connections of the most frequent words (blue rectangles) with the\r\nothers (orange rectangles) is the following:\r\n\r\n5\r\n\r\nIn any case, in the next phases of the project I will go further in the syntactic analysis using tool like CohMetrix (http:\/\/terence.fbk.eu\/services\/api\/computeReadability\/v2\/).\r\n269\r\n\r\n5\r\n\r\n\fWorld WAR II\r\n\r\nWORLD WAR I\r\n\r\nOn the green background is the corpus of the First World War while on the red one is the corpus of the Second.6\r\nThis analysis is surprising because it clearly shows what the nature of the two corpora is. The words sempre,\r\nora and guerra, on the left, show a descriptive lexical approach to war writing, which aims to tell the stories\r\nof the front to the families of the writers. On the contrary, the most frequent words in the last letters corpus of\r\nthe Second World War illustrate a familiar lexicon that reveals the emotional character of this writing. All this\r\ninformation allows us to say more about the generic features of the last letter. In fact, both corpora, WWI and\r\nWWII, are texts written in a tragic moment and, if we bracket the substantial differences between a person\r\ncondemned to death and a soldier in prison, the condition of deportees of the Second World War can in fact\r\nbe compared to that of prisoners of the First World War. The substantial difference lies in the codification that\r\nthe subject makes of the reality he or she is living. A soldier learns to experience the daily realities of war as\r\npart of a group of like-minded people; imprisonment and death become a codified consequence of a tragic but\r\ncommonly accepted situation. Those sentenced to death, on the other hand, find themselves alone before death,\r\nin some cases feeling incredulity, such as the case of fourteen or fifteen-year-old boys who do not expect to\r\nbe shot or tortured; in other instances, they cling onto the hope that their comrades can make an exchange for\r\na Fascist or a Nazi prisoner. On the other hand, deportees, especially for racial reasons, are faced with the\r\nunknown while in chains, considering that their imprisonment is not the result of their actions but of their\r\npersonal identity, and they are kept in the dark as to their fate. The letters of many deportees also contain\r\nappeals to hope. A very interesting fact is that this concerns the lexicon in its entirety. If we observe in fact the\r\nlemma\/token ratio (Jurafsky, Bell, Girand, 2002) it is evident that the WWI corpus is more lexically varied\r\n(0.96) than the WWII corpus (0,6). This is due to the fact that the letters written a few hours before execution\r\nwith the certainty of having to die, are characterized by a basic lexicon that often returns. I preferred lemmatoken to type-token because it is better suited to treat inflected forms of a word as the same type (McCarthy,\r\n1990: 73). To give an example of how the lexicon of these texts works, we can for example cite the use of the\r\nword dolore [pain] which is often used in phrases in which the writer apologizes for the pain that death will\r\ncause his\/her loved ones as in the previous case of Filzi and Mantovani. In the corpus of the First World War\r\nthere are 43 occurrences of the word dolore and in no case is it collocated with adjectives. By contrast, in the\r\ncorpus of the Second World War, the word \u2018pain\u2019 appears 185 times, 57 of which are accompanied by a\r\ndemonstrative adjective such as: grande [big], immenso or immane [immense], tremendo [tremendous],\r\nprofondo [deep], accorato [heartfelt], straziante [heartbreaking] or ultimo [last]. The language in these last\r\nletters is therefore more descriptive, especially when it describes the feelings and therefore distinguishes the\r\nstory of imprisonment or of life in the trenches from an inner narrative that must condense a greater\r\ncommunicative intent into a few lines. It should also be noted that of the 57 occurrences of the noun pain with\r\nthese adjectives, 7 have the adjective post-placed to the noun while 50 have it placed before the noun (Serianni,\r\n1989: 199-205). This is typical of the syntactic structures commonly found in literary texts (Scarano, 2000: 5).\r\nIt is no wonder that there should be a similar lexicon as well as sentence construction in the letters, given that\r\ntheir authors learnt how to write in Italian through the example of literature, for instance Dante. These letters\r\nare as diverse as the materials on which they were written. During my research in the archives, I never found\r\na single letter from a soldier on the front of the First World War that was written on a precarious medium. In\r\ncontrast, the partisans and deportees wrote their last messages really wherever they could (Bozzola, 2013: 26).\r\nThere are also, for instance, \u2018letters\u2019 composed of three words on the edge of a book or even a list of names\r\nengraved on a loaf of dry bread. These letters represent in essence what the two wars were, and testify to their\r\ndifferences. Thanks to the function \u2018oppose\u2019 of the R package Stylo, I identified the most characteristic words\r\nof each of the corpora. For World War I it found austriaci [Austrians] (34), macello [slaughterhouse] (21),\r\n6\r\n\r\nIn the rectangles the numbers are the occurrences of the given word. An English translation is provided in italics. The blue rectangles\r\ncontain the most frequent words, or parts of speech, and the orange the ones those that appear most frequently in connection with them.\r\n270\r\n\r\n6\r\n\r\n\flicenza [license] (19), francesi [French] (19), stanchi [tired] (18). On the other hand, for World War II, muoio\r\n[I die] (212), sii [be] (117), chiedo [ask] (105), perdonatemi [forgive me] (51), non piangete [don\u2019t cry] (45),\r\nricordatevi [you remember] (43). It is interesting to note how, even in these few cases, the most characteristic\r\nwords of World War I are related to the conflict, describing it as a slaughterhouse, its protagonists \u2013 the\r\nAustrian enemies and the French allies \u2013 the authors\u2019 desires to escape the war while on leave and to one of\r\nthe most common feelings of the soldiers: tiredness.\r\n6 Conclusion\r\nThese \u2018last\u2019 letters focus on the content of the message, whereas World War II letters are most concentrated\r\non the emotive and conative functions because the language focuses on the sender and the addressee (Jakobson,\r\n1960). The sentenced to death ask to be remembered by the people they love. They want to be forgiven and\r\nfor their families to be happy. We could therefore assume that one of the most salient peculiarities of a last\r\nletter is when the message mainly focuses on the sender himself\/herself and on the addressee. The last letters\r\naim to describe emotions rather than facts, and to tell about the past more than about the present, because there\r\nis no future for the sentenced to death. In the next future, I will include other letters in the corpus and I will\r\ncross the analysis done until now with TreeTagger with the use of other lemmatizers and tools. The analysis I\r\nhave conducted revealed some problems in comparing World War I and World War II letters but it also\r\nhighlighted changes in the writing of Italian. Most importantly, this phase of my research proved that a \u2018last\r\nletter\u2019 was thematically, linguistically and pragmatically definable.\r\nAcknowledgements\r\nI would like to thank Dr Rachele Sprugnoli for having shared with me her scientific expertise, and Professor\r\nMatthew Reynolds and Dr Anita Jorge for their precious help, advice and patience."
	},
	{
		"id": 43,
		"title": "L‚Äôorganizzazione e la descrizione di un fondo nativo digitale: PAD e l‚ÄôArchivio Franco Buffoni",
		"abstract": {
			"it": "",
			"en": ""
		},
		"authors": [
			"Paul Gabriele Weston",
			"Primo Baldini",
			"Laura Pusterla"
		],
		"body": "1 Introduzione\r\nIl progetto PAD-Pavia Archivi Digitali dell\u2019Universit\u00E0 di Pavia \u00E8 nato nel 2009 con lo scopo di preservare\r\ndalla scomparsa gli archivi delle memorie digitali di autori contemporanei. L\u2019Universit\u00E0, che attraverso il\r\nCentro per la tradizione manoscritta di autori moderni e contemporanei dal 1969 salvaguarda i documenti\r\ncartacei di scrittori e giornalisti italiani, ha voluto estendere questa esperienza ai documenti nativi digitali. Fu\r\nil giornalista e scrittore Beppe Severgnini, ex alunno dell\u2019Universit\u00E0, che, partendo dalla constatazione che una\r\nparte maggioritaria della produzione culturale letteraria si basa ormai sull\u2019utilizzo di supporti informatici,\r\nsollecit\u00F2 questo ampliamento di prospettive. Per rendere tali documenti ricercabili e leggibili \u00E8 necessario\r\nservirsi di infrastrutture hardware e software in continua evoluzione, un ostacolo che rende le procedure della\r\nconservazione progressivamente pi\u00F9 impegnative con il passare degli anni. La volont\u00E0 di arginare questa\r\nperdita di testimonianze della nostra storia culturale \u00E8 stata alla base della creazione del progetto PAD.\r\nPAD conserva diverse tipologie di materiali digitali, garantisce la tutela a lungo termine dei fondi ed\r\neventualmente pu\u00F2 essere accessibile agli studiosi, nel pieno rispetto, come \u00E8 ovvio, delle disposizioni ricevute\r\ndagli autori.\r\nFino ad ora il progetto si \u00E8 focalizzato soltanto sulla conservazione a lungo termine degli archivi digitali in\r\nlocale, ospitati cio\u00E8 sui dispositivi di scrittura correntemente utilizzati dagli scrittori o su apparecchiature non\r\npi\u00F9 utilizzate, ma da essi conservate, nonch\u00E9 sui supporti di archiviazione utilizzati dagli stessi nel corso degli\r\nanni (nastri magnetici, floppy di diverse dimensioni e densit\u00E0 di archiviazione, cd, dvd, unit\u00E0 compatte di\r\narchiviazione massiva). La crescente tendenza ad avvalersi della rete per comunicare ed archiviare dati ha reso\r\nnecessario mettere a punto una strategia e dei dispositivi finalizzati alla salvaguardia di risorse digitali, siti web\r\ne contenuti sui social media. A questo modulo del sistema \u00E8 stato dato nome PAD Web Archiving. L\u2019intento\r\ndi PAD non \u00E8, ovviamente, quello di competere con analoghi progetti internazionali di ben altro respiro, ma\r\nmantenersi come un progetto sostenibile, sia tecnologicamente, sia finanziariamente, che garantisca, ad onta\r\ndelle sue dimensioni contenute, dei risultati di qualit\u00E0. Spetta agli autori stessi o alle istituzioni culturali alle\r\nquali fanno capo i siti interessati richiedere espressamente che anche questa componente venga inserita nel\r\npiano complessivo di preservazione dell'archivio. L'accordo \u00E8 indispensabile al fine di interagire direttamente\r\ncon il committente per stabilire tempi e metodi per il salvataggio e la consultazione. Tutto il materiale resta\r\novviamente di propriet\u00E0 dell\u2019autore, che pu\u00F2 in ogni momento decidere di rimuoverli dall'archivio e di\r\nrinunciare al prosieguo del progetto.\r\n273\r\n\r\n\f2 Il fondo Franco Buffoni\r\nFranco Buffoni, anglista, poeta, prosatore e traduttore, il cui archivio cartaceo si trova gi\u00E0 in deposito presso il\r\nCentro Manoscritti della stessa Universit\u00E0, avendolo lui conferito in anni precedenti, \u00E8 uno degli autori che,\r\nnel corso degli anni, hanno conferito a PAD i propri archivi digitali. Nel 2016, nel rispetto dell\u2019iter messo a\r\npunto allo scopo da PAD, una copia del suo ampio archivio \u00E8 stata riversata in PAD. L'iter a cui si fa qui\r\nriferimento prevede che, dopo la firma di un contratto legale, un operatore di PAD si rechi presso la residenza\r\ndell\u2019autore per prelevare una copia dei file che lui stesso ha selezionato per la conservazione. Il riversamento\r\ndell'archivio Buffoni ha riguardato 1065 elementi, per complessivi 758 MB, comprendenti tipologie di file di\r\ndiversa natura: documenti di testo, immagini, video, audio, link. Nel 2019, con l'intenzione di sperimentare lo\r\nstrumento messo a punto per la salvaguardia a lungo termine delle risorse web, PAD ha concordato con\r\nl'autore di utilizzare il suo sito personale (www.francobuffoni.it), ritenendolo particolarmente\r\nidoneo allo scopo a motivo della ricchezza di contenuti e della variet\u00E0 di formati e tipologie.\r\nPer prima cosa, su richiesta esplicita del proprietario del sito, PAD ne ha prelevato una copia. Dato che i\r\nsiti web possono essere modificati o aggiornati anche molto di frequente, si \u00E8 concordato con l\u2019autore di\r\nprocedere con l'effettuazione di salvataggi a cadenza prestabilita. In questo modo si possono conservare le\r\nvarie versioni del sito, che possono essere messe a disposizione dell\u2019utenza secondo la volont\u00E0 del proprietario.\r\nAttraverso un software per il web scraping, il sito dell\u2019autore \u00E8 stato riprodotto in locale, in modo da garantirne\r\nil browsing offline. Cos\u00EC l\u2019utente futuro potr\u00E0 navigare liberamente nella copia dell\u2019intero sito. Per progettare\r\nquesta implementazione, si \u00E8 dovuto tenere conto della struttura anche molto complessa che i siti possono\r\ntalvolta presentare, comprendente riferimenti numerosi ad altre pagine, interne o esterne nel web. Per questo\r\ndi ogni pagina che compone il sito web, PAD memorizza, oltre alla pagina stessa, i link anche alle pagine\r\nesterne, con un\u2019immagine della pagina a cui il link conduce, nonch\u00E9 i documenti allegati. In questo modo si\r\npu\u00F2 tenere meglio traccia dei path che il creatore del sito ha voluto valorizzare. Se, ad esempio, un link a una\r\npagina esterna non fosse pi\u00F9 funzionante o se la pagina non risultasse pi\u00F9 esistente, una parte di ci\u00F2 che l\u2019autore\r\nintendeva comunicare, una componente probabilmente significativa del suo pensiero, andrebbe perduta.\r\nQuando il progetto ha avuto inizio \u00E8 stata presa in considerazione l'idea di utilizzare principalmente il\r\nformato di archiviazione WARC (Web ARChive). Sebbene questo formato sia stato standardizzato nel 2009\r\n(ISO 28500:2017) il suo utilizzo da parte delle grandi aziende informatiche (Microsoft, Apple, Google ecc.)\r\nnon ha mai goduto negli anni della diffusione che sarebbe stata auspicabile. Un'accurata serie di verifiche ha\r\npermesso di accertare che i browser pi\u00F9 diffusi non lo riconoscono. Allo stesso tempo il sistema di\r\nmemorizzazione sembra essere stato realizzato per essere installato e usato solamente da un sistemista esperto,\r\nci\u00F2 che rischia di creare notevoli problemi agli utenti comuni. Si \u00E8 preferito quindi adottare un prodotto di pi\u00F9\r\nfacile utilizzo per l\u2019elaborazione del sito. Il formato WARC \u00E8, invece, stato mantenuto per la parte del progetto\r\nche si occupa di preservazione a lungo termine. Quindi in PAD per il Web Archiving vengono gestiti due\r\nsistemi diversi. Il primo utilizza il software Heritrix, sviluppato da Internet Archive, mentre il risultato dei\r\nprocessi di crawling viene memorizzato in file con formato WARC.\r\nPer l\u2019elaborazione delle pagine il software che PAD utilizza prevalentemente si chiama HTTrack, un Web\r\ncrawler open-source. Consente di scaricare un sito web da internet in una directory locale, ottenendo HTML,\r\nimmagini e altri file dal server al computer. HTTrack mantiene la struttura originale del sito, compresi i link,\r\npermettendo all'utente di navigare da una pagina all'altra, come se la stesse visualizzando online. Esso preleva\r\nanche tutte le altre tipologie di documenti e immagini che si trovano allegate alle pagine web. Pur non\r\nbasandosi su uno standard, HTTrack presenta il vantaggio della semplicit\u00E0 nell\u2019aprire le pagine web o\r\nnell'estrarre il testo per elaborarlo. Alcuni autori, come ad esempio Francesco Pecoraro, hanno richiesto di\r\ncreare una copia offline del proprio sito, con l'intenzione di rimuoverlo successivamente dalla rete. Questa\r\ncopia resta ovviamente a loro disposizione per l\u2019accesso, qualora ne facciano richiesta, anche a distanza. Si \u00E8\r\nvisto come la versione \u2018mirror\u2019 del sito \u00E8 stata considerata come quella pi\u00F9 semplice da inviare e da consultare\r\nda parte di utenti non particolarmente esperti. La possibilit\u00E0 di navigare all\u2019interno del sito locale, senza\r\nl\u2019obbligo di installare preventivamente specifici software, ha reso questo servizio estremamente semplice da\r\ngestire. Al contrario, per le questioni ricordate in precedenza, l\u2019utilizzo di un archivio WARC avrebbe\r\ncomportato la necessit\u00E0 di assistere l\u2019utente nel corso della procedura di consultazione.\r\nTutto il materiale cos\u00EC raccolto \u00E8 stato sottoposto alle procedure di conservazione sperimentate da PAD nel\r\ncorso degli anni. La prima operazione \u00E8 creare pi\u00F9 copie dell\u2019archivio, ubicate su diversi server. Oltre che sul\r\nserver interno di PAD infatti, esso viene replicato sui server dell\u2019Universit\u00E0 di Pavia e su quello della sede\r\n274\r\n\r\n\fdistaccata dell\u2019Universit\u00E0 a Cremona, citt\u00E0 distante da Pavia circa 70 chilometri, in modo da salvaguardare la\r\nsicurezza delle informazioni in caso di disastro ambientale. Un\u2019ulteriore copia viene memorizzata su supporto\r\nhardware esterno. Una volta assicurata la preservazione dell\u2019originale, l\u2019archivio passa in un\u2019area di working.\r\nVengono estratti i metadati, fondamentali per poter poi svolgere l\u2019operazione di normalizzazione, che\r\ncomporta il salvataggio di ogni documento in diversi formati, a seconda della tipologia. Terminate le\r\noperazioni preliminari, si procede a descrivere i file.\r\n\r\n3 La descrizione\r\nRispetto al trattamento di un archivio cartaceo, un archivio nativo digitale presenta peculiarit\u00E0, come ad\r\nesempio il numero dei file che lo costituiscono, che talvolta possono assommare a molte migliaia, che rendono\r\nla descrizione effettuata seguendo consuetudini e procedure tradizionali inadeguata e persino non sostenibile.\r\n\u00C8 stato, perci\u00F2, necessario individuare strategie che potessero consentire di sfruttare al massimo le potenzialit\u00E0\r\nofferte dall'informatica.\r\nAl contempo, si registrano problematiche simili, come la questione dell\u2019accessibilit\u00E0 e della riservatezza.\r\nTrattandosi di documentazione prodotta molto di recente, si \u00E8 dovuto tener conto del fatto che, probabilmente,\r\nuna parte anche significativa dei file non possano essere resi disponibili all\u2019utenza senza che ci\u00F2 comporti la\r\nviolazione di disposizioni legislative, come quelle sulla privacy o sulla propriet\u00E0 intellettuale. Anche il fatto\r\nche i documenti conferiti o salvati dal web siano stati indicati espressamente dal conferente, non \u00E8 sufficiente\r\na garantirne la libera consultazione da parte degli studiosi. Si rende perci\u00F2 necessario, durante le fasi\r\npreliminari della descrizione, sottoporre ogni singolo file ad una attenta disamina volta ad escludere che\r\ncontenga dati sensibili o creazioni intellettuali la cui responsabilit\u00E0 non sia in capo al conferente, al soggetto\r\nproduttore dell'archivio o al titolare del sito. Anche lo scrittore che ha conferito il proprio archivio potrebbe\r\nrichiedere, per ragioni personali, che alcuni documenti siano secretati e di conseguenza esclusi dalla\r\nconsultazione, anche per motivi di studio, per un determinato periodo di tempo, il cosiddetto embargo. Per\r\ntener conto di queste evenienze, l\u2019operatore di PAD, nel vagliare ogni file dell\u2019archivio digitale, gli assegna\r\nuna categoria di rischio, in base alla quale esso viene automaticamente reso o meno consultabile da parte degli\r\nutenti.\r\nSi passa poi alla fase del riordino. Come per gli archivi cartacei, viene creata una struttura ad albero\r\nrovesciato che comprende le diverse serie, alle quali vengono poi assegnati i singoli file. Gi\u00E0 in questa\r\nprocedura si manifestano quelle potenzialit\u00E0 dell\u2019informatica, prima ricordate, che offrono un significativo\r\ncontributo agli archivisti e nuove opportunit\u00E0 agli utenti. In primo luogo, il sistema offre la possibilit\u00E0 di\r\nassegnare un singolo file a pi\u00F9 di una sezione dell\u2019archivio. In secondo luogo, se nell'archivio cartaceo il\r\nriordino comporta la modifica della sistemazione pensata dallo scrittore, l'archivio digitale pu\u00F2 consentire di\r\nmantenere ad un tempo l\u2019aspetto originale e contemporaneamente collocare i documenti in un ordinamento\r\nche segua altri criteri. Questi criteri possono anche essere pi\u00F9 di uno, quando le esigenze lo richiedano. Durante\r\nla descrizione, infatti, l\u2019archivista assegna a ogni documento dei tag, che hanno lo scopo di aiutare l'utente a\r\ncomprendere meglio la tipologia del materiale in questione (se, ad esempio, si tratta di un testo, di una\r\nrecensione, di un\u2019immagine, di un video e cos\u00EC via). In ogni archivio, ovviamente, non tutti i file sono prodotti\r\nda colui o colei che conferisce l'archivio stesso. Sono molto frequenti i casi di documenti frutto del lavoro\r\nintellettuale di terze persone. La possibilit\u00E0 di collegare a ogni file uno o pi\u00F9 nomi di persona che abbiano in\r\nqualche modo contribuito alla sua produzione \u00E8 funzionale anche a questo scopo. Al tempo stesso, il nome\r\ndella persona o dell'ente viene associato ad una tipologia di responsabilit\u00E0 intellettuale, espressa attraverso un\r\nvocabolario controllato, implementabile a seconda delle esigenze mediante l\u2019inserimento di nuovi termini in\r\nuna tabella. Ricorre, poi, il caso di documenti - il termine viene qui utilizzato in senso generale, senza cio\u00E8 fare\r\nriferimento a funzioni di natura amministrativa - che siano stati estratti o ricavati da pubblicazioni pi\u00F9 ampie\r\n(ad esempio, un capitolo da un libro, una poesia da una raccolta, un brano da un'intervista o da una recensione\r\ne cos\u00EC via). Qualora il collegamento tra i due documenti sia riconosciuto dall'archivista, il sistema consente di\r\nesplicitare la relazione anche a beneficio dell'utente, dal momento che \u00E8 possibile stipulare un collegamento\r\ntra l\u2019oggetto e il titolo della risorsa che lo contiene o del quale \u00E8 parte. L'inserimento del codice ISBN o DOI\r\nnel caso di un libro o dell'ISSN per una rivista \u00E8 funzionale a rendere pi\u00F9 riconoscibile e in modo inequivoco\r\nla fonte e, di conseguenza, a consentirne pi\u00F9 facilmente il recupero.\r\nLa presenza di tutte queste informazioni inserite dall\u2019archivista (tag, nomi, responsabilit\u00E0, identificativi\r\nunivoci), unitamente ai metadati tecnici estratti direttamente dalla macchina, consente a PAD di mettere a\r\ndisposizione dell'utente percorsi di ricerca alternativi, o, per meglio dire, complementari e trasversali, rispetto\r\na quelli consueti. Infatti, vi \u00E8 la possibilit\u00E0 di estrarre i dati dei documenti secondo l\u2019ordinamento per serie\r\n275\r\n\r\n\farchivistiche, oppure secondo la disposizione originale dello scrittore, o ancora ordinati per soggetto produttore\r\no per provenienza. In questo modo PAD cerca di venire incontro alle variegate necessit\u00E0 dell\u2019utente che si\r\ntrova a consultare l\u2019archivio.\r\n\r\n4 La descrizione del sito web\r\nAnalizzando i conferimenti effettuati dai diversi autori ci si \u00E8 resi conto che spesso l\u2019autore ha conservato i\r\ntesti che poi vengono riversati sul web sotto forma di file allegati alla pagina o come link. In altri casi, i testi\r\ndella pagina web si ispirano, prendono spunto oppure sono almeno in parte i medesimi di quelli presenti\r\nnell'archivio. Operando sui metadati estratti, avendo selezionato quelli pi\u00F9 importanti per identificare in modo\r\npuntuale la risorsa web, tali correlazioni tra nativo digitale e web possono venire ricercate ed individuate.\r\nLe procedure occorrenti non si discostano molto da quelle normalmente messe in atto in PAD al momento dei\r\nconferimenti per analizzare la struttura e la consistenza dell'archivio. Come primo passo, si procede a creare\r\nuna mappa del sito; in secondo luogo si estraggono i metadati, il cui numero viene ridotto in seguito alla\r\nscrematura di quelli di scarsa utilit\u00E0; infine, i documenti vengono sottoposti ad operazioni di normalizzazione.\r\nConclusa questa fase, i documenti del web possono essere descritti.\r\nIl sistema di descrizione \u00E8 in parte automatizzato, dato che la grande quantit\u00E0 di materiale disponibile sul\r\nweb richiederebbe tempi eccessivamente lunghi e un'attivit\u00E0 dispendiosa, se ad occuparsene fosse un operatore.\r\n\u00C8 evidente, infatti, che, una volta ricevuto il comando, il computer possa elaborare per diverse ore il materiale,\r\nfino ad arrivare alla conclusione. Per ottenere un risultato molto simile, un operatore dovrebbe lavorare per\r\ngiorni. Il software PAD Web Analyzer confronta ogni pagina web - prendendo in considerazione sia il testo\r\nvero e proprio della pagina, sia eventuali file allegati - con i documenti presenti nell\u2019archivio. Procede, quindi,\r\na mostrare, affiancati, i testi del web e i corrispondenti testi del nativo digitale, sulla base di indicatori di\r\nsimilitudine. Per ciascuna corrispondenza viene stabilito un indice di similitudine, che indica in percentuale\r\nquanta parte del testo sul web sia uguale a quella di un documento presente nell\u2019archivio nativo digitale. Un\r\nrisultato molto basso, indicativamente inferiore al 50 %, non viene tenuto in considerazione dall'operatore. Al\r\ncontrario, come hanno permesso di accertare le prove effettuate, la certezza di aver individuato il medesimo\r\ndocumento si ha quando l'indice presenta un valore intorno al 98%. In presenza di valori intermedi sar\u00E0 compito\r\ndell'operatore effettuare i riscontri necessari, ma anche in questo caso il sistema \u00E8 in grado di offrire assistenza.\r\nSe il valore \u00E8 relativamente elevato, il computer, assumendo di aver individuato due file \"probabilmente\"\r\ncorrispondenti propone l'assegnazione ad una delle serie inserite con la descrizione manuale dell\u2019archivio. Se,\r\nviceversa, il valore \u00E8 relativamente baso, la casella delle assegnazioni resta vuota e deve essere quindi\r\nl\u2019operatore a riempirla sulla base di una ricognizione autoptica. Per effettuare queste valutazioni si \u00E8 tenuto\r\nconto del fatto che talvolta i documenti in archivio possono essere in formati differenti rispetto a quelli\r\npubblicati o allegati sul web (ad esempio un file Word solitamente viene convertito in PDF per essere messo\r\nsul sito) e questo comporta un ragionevole abbassamento dell\u2019indice. Poich\u00E9 la macchina non ha le stesse\r\ncapacit\u00E0 di discernimento di un operatore, \u00E8 opportuno che il controllo finale sia effettuato esaminando in\r\nparallelo i due testi. Una specifica funzione del software di PAD mostra i due testi affiancati in modo che la\r\nricognizione possa procedere speditamente. \u00C8 ovviamente \u00E8 possibile che il testo sia stato prodotto direttamente\r\nsulla rete, oppure che il file originale sia stato eliminato o non conferito: in tal caso non si evidenziano\r\ncorrispondenze.\r\nA questo scopo PAD ha implementato all\u2019interno del software l\u2019algoritmo Levenshtein distance,\r\nadattandolo alle proprie esigenze. La procedura appena descritta risulta attualmente vantaggiosa solo se la\r\nparte dell\u2019archivio digitale nativo sia stata gi\u00E0 trattata e viene quindi utilizzata in questo momento unicamente\r\nper la descrizione dei siti web o in caso di secondi (o comunque successivi) conferimenti.\r\nLe informazioni ottenute attraverso questo procedimento serviranno all\u2019archivista per assegnare il\r\ndocumento a una particolare serie inventariale, riguardante una specifica opera dell\u2019autore o una specifica\r\ntipologia documentale. Integrando i documenti provenienti dai siti web con quelli nativi digitali si ottiene un\r\narchivio il pi\u00F9 completo possibile. Se all\u2019interno di una serie viene inserito del materiale proveniente dal web,\r\nviene creata una sottoserie apposita, in modo che l\u2019utente possa trovare aggregato tutto il materiale avente lo\r\nstesso argomento e al contempo conoscerne la provenienza.\r\n\r\n5 Prospettive per il futuro\r\nCome nella tradizione delle realizzazioni informatiche, il sistema PAD viene costantemente implementato per\r\nfornire nuove funzionalit\u00E0 e migliorare quelle attuali. La scelta di configurarsi come un progetto di piccola\r\n276\r\n\r\n\fscala, limitato a autori o a istituti culturali selezionati, permette di dedicare grande cura nel perfezionare le\r\nsoluzioni tecniche, secondo le necessit\u00E0 dell\u2019archiviazione e della descrizione. L\u2019architettura di PAD \u00E8 stata\r\npensata per la conservazione, ma negli anni si \u00E8 evoluta con la finalit\u00E0 di consentire lo studio del materiale\r\nconferito. La stretta collaborazione con gli scrittori conferenti garantisce il rispetto delle loro decisioni sul\r\ntrattamento e la gestione del loro archivio e permette di andare incontro alle esigenze che essi manifestano.\r\nSeguendo le tendenze dei cambiamenti sociali nell\u2019uso di internet e delle sue risorse, PAD sta sperimentando\r\nuna funzionalit\u00E0 che permette la conservazione a lungo termine e la consultazione delle pagine personali di\r\nsocial network, come Facebook, Twitter o Instagram, dei canali YouTube e delle e-mail. Questa tipologia di\r\ndocumenti informatici, che potrebbero anche essere di rilevante importanza per studi futuri, sono per ora solo\r\npensati in funzione della conservazione. Su richiesta esplicita di un autore, verranno raccolti i dati direttamente\r\ndai social e preservati. Con lo stesso criterio verrebbero trattate le e-mail, utilizzando criteri analoghi a quelli\r\nche negli archivi tradizionali si applicano ai carteggi e agli epistolari.\r\nAttualmente \u00E8 al vaglio la possibilit\u00E0 di affidare un\u2019ulteriore copia degli archivi al progetto nazionale\r\nMagazzini Digitali, avviato nel 2006 dalla Fondazione Rinascimento Digitale, dalla Biblioteca nazionale\r\ncentrale di Firenze e dalla Biblioteca nazionale centrale di Roma. La conservazione digitale assicurata dai\r\ndepositi digitali affidabili o fidati (trusted or trustworthy digital repositories) di un servizio pubblico \u00E8 una\r\nulteriore garanzia che archivi digitali di autore cos\u00EC preziosi e che rischiano l\u2019oblio vengano adeguatamente\r\nconservati nel lungo termine."
	}
]