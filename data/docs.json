[
	{
		"id": 1,
		"title": "Digital HumanitiesFrom Wikipedia, the free encyclopedia",
		"authors": [
			"Petris,  Marco"
		],
		"body": " Introduction Digital humanities is an area of research and teaching at the intersection of computing and the disciplines of the humanities . Developing from the fields of humanities computing, humanistic computing, Humanistic Computing, Proceedings of the IEEE, Vol. 86, No. 11, November, 1998, Pages 2123-2151. and digital humanities praxis http://dhpraxisf13.commons.gc.cuny.edu/tag/dhpraxis/ digital humanities embraces a variety of topics, from curating online collections to data mining large cultural data sets. Digital humanitiescurrently incorporates both digitized and born-digital materials and combines the methodologies from traditional humanities disciplinesand social sciences Digital Humanities Network. University of Cambridge. Retrieved 27 December 2012 with tools provided by computing, and digital publishing . As well, related subfields of digital humanities have emerged like software studies , platform studies, and critical code studies . Digital Humanities also intersects with new media studies and information science as well as media theory of composition and game studies , particularly in areas related to digital humanities project design and production. Example of research which includes the use of digital methods: network analysis as an archival tool. League of Nations archives, United Nations Office in Geneva. Network visualization and analysis published in Grandjean, Martin. La connaissance est un réseau. Les Cahiers du Numérique 10: 37–54. Retrieved 2014-10-15. Areas of inquiry Digital humanities scholars use computational methods either to answer existing research questions or to challenge existing theoretical paradigms, generating new questions and pioneering new approaches. One goal is to systematically integrate computer technology into the activities of humanities scholars, Opportunities/tabid/57/Default.aspx Grant Opportunities. National Endowment for the Humanities, Office of Digital Humanities Grant Opportunities. Retrieved 25 January 2012. as is done in contemporary empirical social sciences . Such technology-based activities might include incorporation into the traditional arts and humanities disciplines use of text-analytic techniques; GIS ; commons-based peer collaboration ; and interactive games and multimedia . Despite the significant trend in digital humanities towards networked and multimodal forms of knowledge, spanning social, visual, and haptic media, a substantial amount of digital humanities focuses on documents and text in ways that differentiate the fields work from digital research in Media studies, Information studies, Communication studies, and Sociology. Another goal of digital humanities is to create scholarship than transcends textual sources. This includes the integration of multimedia, metadata and dynamic environments. An example of this is The Valley of the Shadow project at the University of Virginia, the Vectors Journal of Culture and Technology in a Dynamic Vernacular at University of Southern California or Digital Pioneers projects at Harvard. A growing number of researchers in digital humanities are using computational methods for the analysis of large cultural data sets such as the Google Books corpus. Roth, S., Fashionable functions. A Google ngram view of trends in functional differentiation, International Journal of Technology and Human Interaction, Band 10, Nr. 2, S. 34-58. Examples of such projects were highlighted by the Humanities High Performance Computing competition sponsored by the Office of Digital Humanities in 2008, Bobley, Brett. Grant Announcement for Humanities High Performance Computing Program. National Endowment for the Humanities. Retrieved May 1, 2012. and also by the Digging Into Data challenge organized in 2009 Awardees of 2009 Digging into Data Challenge. Digging into Data. 2009. Retrieved May 1, 2012. and 2011 NEH Announces Winners of 2011 Digging Into Data Challenge. National Endowment for the Humanities. January 3, 2012. Retrieved May 1, 2012. by NEH in collaboration with NSF, Cohen, Patricia. Humanities Scholars Embrace Digital Technology. The New York Times. ISSN 0362-4331. Retrieved 2012-06-07. and in partnership with JISC in the UK, and SSHRC in Canada. Williford, Christa; Henry, Charles. Computationally Intensive Research in the Humanities and Social Sciences: A Report on the Experiences of First Respondents to the Digging Into Data Challenge. Council on Library and Information Resources. ISBN 978-1-932326-40-6. Environments and tools Digital humanities is also involved in the creation of software, providing environments and tools for producing, curating, and interacting with knowledge that is born digital and lives in various digital contexts. Presner, Todd. Digital Humanities 2.0: A Report on Knowledge. Connexions. Retrieved 2012-06-09. In this context, the field is sometimes known as computational humanities. Many such projects share a commitment to open standards and open source. Bradley, John. No job for techies: Technical contributions to research in digital humanities. In Marilyn Deegan and Willard McCarty. Collaborative Research in the Digital Humanities. Farnham and Burlington: Ashgate. pp. 11–26 [14]. ISBN 9781409410683. History Digital humanities descends from the field of humanities computing, of computationally enabled formal representations of the human record, Unsworth, John. What is Humanities Computing and What is not?. Jahrbuch für Computerphilologie 4. Retrieved 2012-05-31. whose origins reach back to the late 1940s in the pioneering work of Roberto Busa . Svensson, Patrik. Humanities Computing as Digital Humanities. Digital Humanities Quarterly 3. ISSN 1938-4122. Retrieved 2012-05-30. Hockney, Susan. The History of Humanities Computing. In Susan Schreibman, Ray Siemens, John Unsworth. Companion to Digital Humanities . Blackwell Companions to Literature and Culture. Oxford: Blackwell. ISBN 1405103213. Other aspects of digital humanities were descended from the IRIS Intermedia project on hypertext at Brown University in the 1980s. The Text Encoding Initiative, born from the desire to create a standard encoding scheme for humanities electronic texts, is the outstanding achievement of early humanities computing. The project was launched in 1987 and published the first full version of the TEI Guidelines in May 1994. Hockney, Susan. The History of Humanities Computing. In Susan Schreibman, Ray Siemens, John Unsworth. Companion to Digital Humanities . Blackwell Companions to Literature and Culture. Oxford: Blackwell. ISBN 1405103213. In the nineties, major digital text and image archives emerged at centers of humanities computing in the U.S., Rossetti Archive , Institute for Advanced Technology in the Humanities, University of Virginia, retrieved 2012-06-16 and The William Blake Archive Morris Eaves, Robert Essick, and Joseph Viscomi, The William Blake Archive , retrieved 2012-06-16 ), which demonstrated the sophistication and robustness of text-encoding for literature. Liu, Alan. Transcendental Data: Toward a Cultural History and Aesthetics of the New Encoded Discourse. Critical Inquiry 31: 49–84. doi: 10.1086/427302. ISSN 0093-1896. Retrieved 2012-06-16. The Blake archive, in particular, was designed by its editors to take advantage of the syntheses made possible by the electronic medium and thus accomplish an editorial transformation in the publication of Blakes work which was, from the authors hands, multimedia. Editorial Principles. The William Blake Archive. Retrieved 17 December 2014. The terminological change from humanities computing to digital humanities has been attributed to John Unsworth and Ray Siemens who, as editors of the monograph A Companion to Digital Humanities, tried to prevent the field from being viewed as mere digitization. Fitzpatrick, Kathleen. The humanities, done digitally. The Chronicle of Higher Education. Retrieved 2011-07-10. Consequently, the hybrid term has created an overlap between fields like rhetoric and composition, which use the methods of contemporary humanities in studying digital objects, Fitzpatrick, Kathleen. The humanities, done digitally. The Chronicle of Higher Education. Retrieved 2011-07-10. and digital humanities, which uses digital technology in studying traditional humanities objects. Fitzpatrick, Kathleen. The humanities, done digitally. The Chronicle of Higher Education. Retrieved 2011-07-10. The use of computational systems and the study of computational media within the arts and humanities more generally has been termed the computational turn. Berry, David. The Computational Turn: Thinking About the Digital Humanities. Culture Machine. Retrieved 2012-01-31. In 2006 the National Endowment for the Humanities, launched the Digital Humanities Initiative, which made widespread adoption of the term digital humanities all but irreversible in the United States. Kirschenbaum, Matthew G.. What is Digital Humanities and Whats it Doing in English Departments?. ADE Bulletin. Digital humanities emerged from its former niche status and became big news Kirschenbaum, Matthew G.. What is Digital Humanities and Whats it Doing in English Departments?. ADE Bulletin. at the 2009 MLA convention in Philadelphia, where digital humanists made some of the liveliest and most visible contributions Howard, Jennifer. The MLA Convention in Translation. The Chronicle of Higher Education. ISSN 0009-5982. Retrieved 2012-05-31. and had their field hailed as the first next big thing in a long time. Pannapacker, William. The MLA and the Digital Humanities. Brainstorm. Retrieved 2012-05 Organizations and Institutions The field of digital humanities is served by several organisations: The Association for Literary and Linguistic Computing, the Association for Computers and the Humanities, and the Society for Digital Humanities/Société pour létude des médias interactifs, which are joined under the umbrella organisation of the Alliance of Digital Humanities Organizations. The alliance funds a number of projects such as the Digital Humanities Quarterly, supports the Text Encoding Initiative, the organisation and sponsoring of workshops and conferences, as well as the funding of small projects, awards and bursaries. Vanhoutte, Edward. Editorial. Literary and Linguistic Computing 26: 3–4. doi: 10.1093/llc/fqr002. Retrieved 2011-07-11. ADHO also oversees a joint annual conference, which began as the ACH/ALLCconference, and is now known as the Digital Humanities conference. CenterNet is an international network of about 100 digital humanities centers in 19 countries, working together to benefit digital humanities and related fields. About. CenterNet. Retrieved June 16, 2012. Caraco, Benjamin. Les digital humanities et les bibliothèques. Le Bulletin des Bibliothèques de France 57. Retrieved 12 April 2012. Criticism and controversies An edited text, Debates in the Digital Humanitieshas identified a range of criticisms of digital humanities: a lack of attention to issues of race, class, gender, and sexuality; a preference for research-driven projects over pedagogical ones; an absence of political commitment; an inadequate level of diversity among its practitioners; an inability to address texts under copyright; and an institutional concentration in well-funded research universities. The literary theorist Stanley Fish claims that the digital humanities pursue a revolutionary agenda and thereby undermine the conventional standards of pre-eminence, authority and disciplinary power. Fish, Stanley. The Digital Humanities and the Transcending of Mortality. The New York Times. Retrieved 2012-05-30. There has also been some recent controversy amongst practitioners of digital humanities around the role that race and/or identity politics plays in digital humanities. Tara McPherson attributes some of the lack of racial diversity in digital humanities to the modality of UNIX and computers, themselves. An open thread on DHpoco.org recently garnered well over 100 comments on the issue of race in digital humanities, with scholars arguing about the amount that racialbiases affect the tools and texts available for digital humanities research. This is a current source of debate within the digital humanities. At present, formal academic recognition of digital work in the humanities remains somewhat problematic, although there are signs that this might be changing.[ citation needed ] Some universities offer programs related to the field Digital Humanities Programs and Organizations. UCLA Digital Humanities. Retrieved 2 November 2014. and some have dedicated Digital Humanities programmes. See also Centers The Alliance of Digital Humanities Organizations maintains a comprehensive list of digital humanities centers Department of Digital HumanitiesHumanities Advanced Technology and Information InstituteInstitute for Advanced Technology in the HumanitiesMaryland Institute for Technology in the Humanities Roy Rosenzweig Center for History and New MediaUCL Centre for Digital HumanitiesCenter for Public History and Digital HumanitiesJournals Digital Medievalist Digital Humanities Quarterly Literary and Linguistic Computing Southern Spaces Meetings Digital Humanities conference HASTAC THATCamp Miscellaneous Computers and writing Computational archaeology Cybertext Cultural analytics Digital Classicist Digital Humanities Summer Institute Digital library Digital Medievalist Digital history Digitizing Digital rhetoric Digital scholarship Electronic Cultural Atlas Initiative Electronic literature EpiDoc E-research Humanistic informatics Multimedia literacy New media Systems theory Stylometry Text Encoding Initiative Text mining Topic Modeling Transliteracy Categories: Digital humanities External links The Alliance of Digital Humanities Organizations CenterNet A Day in the Life of the Digital Humanities Lev Manovich, Computational Humanities vs. Digital Humanities "
	},
	{
		"id": 2,
		"title": "Single Image Super Resolution Approach to the Signatures and Symbols Hidden in Buddhist Manuscript Sutras Written in Gold and Silver Inks on Indigo-Dyed Papers",
		"authors": [
			"Aida, Toshiaki",
			"Aida, Aiko"
		],
		"body": " Image super-resolution can obtain a high-resolution image from multiple low-resolution ones. Conventional image super-resolution is realized by summing the information included in low-resolution images of the same scene. Therefore, it has the advantage of exact replication of details in the resulting high-resolution image from the original, although it necessarily requires many low-resolution images with various shifts of subpixel order. However, single-image super-resolution allows the inference of high-resolution images from relations to their low-resolution ones. Following the method of Yang et al., we determined the relations among more than 100,000 image patches. More concretely, various high-resolution images were first prepared and transformed to low-resolution ones by blurring and down-sampling to one-third of their original sizes for threefold super-resolution. Thus, we obtained pairs of high- and low-resolution images. Then, corresponding 9 × 9 and 3 × 3 image patches were extracted respectively from them and combined into coupled image patches. The method of Yang et al. enables construction of their basis vectors, called a coupled dictionary, under which the corresponding high- and low-resolution image patches can be represented by their common coefficient vectors. The coupled dictionary is the essence of the relation between low- and high-resolution images. With the obtained dictionary and a low-resolution image, the estimation of a coefficient vector for each 3 × 3 patch directly permitted inference of the corresponding 9 × 9 high-resolution patch. In this way, a high-resolution image can be inferred from a single low-resolution one, although the details of the resulting high-resolution image are not identical to those of the original. In this presentation, we will report the results of our analysis of the signatures and symbols hidden in Buddhist manuscript sutras written in gold and silver inks on indigo-dyed papers during the late Heian period in Japan. Conventional image super-resolution requires original subjects of fine texture and cannot obtain clear images of cultural assets that have been damaged or experienced deterioration. Single-image super-resolution can overcome the above difficulties and assist in research of cultural assets, transforming their images into fine-textured ones. We applied super-resolution to low-resolution infrared images of Buddhist manuscript sutras written on indigo-dyed papers to facilitate the investigation of hidden signatures or symbols behind them. Because the purpose of our study is not archiving cultural assets but deciphering their meaning, the inference of details by single-image super-resolution can effectively increase visibility. Here, characters not originally present could possibly be reconstructed from low-resolution images, which could cause misinterpretation. Usually, the contribution from visual noise on paper is small-sized and high-frequency. Therefore, it can be easily removed by decreasing the number of dispatched basis vectors of the coupled dictionary in order to represent low-resolution image patches. In East Asia since ancient times, respected Buddhist sutras have been written in gold and silver inks on deep-blue papers dyed with natural indigo. Digital infrared cameras have detected various characters, pictures, and signs under the dyes on some deep-blue papers handed down in Japan. Researchers have recently proposed that some of these sutras were made using recycled papers, generally public documents. We focus on black signatures and stumpshidden on the blue papers on which Buddhist manuscript sutras were copied in gold and silver inks during the Heian periodin Japan. We detected similar marks on indigo-dyed papers from several scrolls of one of the most famous Japanese Tripitaka, formerly called Jingoji-kyo, planned and made by the retired emperor Tobaat the contemporary capital city of Kyoto. Recent researches have shown that the Chusonji-kyo, another famous Japanese Tripitaka written on deep-blue papers, also bears many signatures and signs under the indigo dye. The marks on the Jingoji-kyo closely resemble those on the Chusonji-kyo, which was made by the Northern Fujiwara clan who had ruled northeast Japan. Therefore, we naturally presume that these hidden signatures and pictures are connected somehow with the central culture or public power. Furthermore, the hidden signatures and symbols detected by digital infrared cameras, including those on the Jingoji-kyo, are apparently limited to only decades in the Heian period. At this time, the production of luxury Japanese paper was entering a transition from centrally authorized paperto local production. After attempting analysis and classification of these signatures and marks, we concluded that they indicate that some paper studios, aristocrats, or noble priests drew signatures on the papers to show possession. "
	},
	{
		"id": 3,
		"title": "Hidden in a Breath: Tracing the Breathing Patterns of Survivors of Traumatic Events",
		"authors": [
			"Akdag Salah, Alkim Almila",
			"Ocak, Meral",
			"Kaya, Heysem",
			"Kavcar, Evrim",
			"Salah, Albert Ali"
		],
		"body": " Introduction Many people experience a traumatic event during their lifetime. In some extraordinary situations, such as natural disasters, war, massacres, terrorism or mass migration, the traumatic event is shared by a community and the effects go beyond those directly affected. Today, thanks to recorded interviews and testimonials, many archives and collections exist that are open to researchers of trauma studies, holocaust studies, historians among others. These archives act as vital testimonials for oral history, politics and human rights. As such, they are usually either transcribed, or meticulously indexed. In this project, we look at the nonverbal signals emitted by victims of various traumatic events and seek to render these for novel representations that are capable of representing the trauma without the explicitcontent. In particular, we propose to detect breathing and silence patterns during the speeches of trauma patients for visualization and sonification. We are hoping to glean into cultural and contextual differences of bodily expression of trauma through automatic processing of thousands of hours of testimonials from all over the world. Trauma Diagnosis DSM-IV defines a traumatic event as an event that is generally outside the range of usual human experience and would evoke significant symptoms of distress in almost everyone. In the diagnosis of Posttraumatic Stress Disorderand acute stress disorder, DSM symptoms are essential. If the duration of symptoms is less than 3 months, it is defined as Acute Stress Disorder, otherwise, it is defined as Post Traumatic Stress Disorder. There are scarcely any studies that detect PTSD and acute stress disorder without clinical data. Some of these studies include information about heart rate, pulse and breathing patterns of individuals. Although vocal parameters are used in order to detect clinical depression, no studies investigating the relationship between voice and trauma were found. Some recent work investigates PTSD symptom severity in a multimodal fashion, using questionnaires and skin conductance physiology. In this work, we focus on the relationship between trauma, speech and breathing. It is important to note that we do not work with PTSD, as not all trauma survivors have PTSD. General Approach Our general approach is to use speech features to automatically segment trauma survivor testimonials into speaking, breathing, and silence classes. The duration of the silences, the frequency of breathing and its quality, the whole dynamics of the testimonial are investigated through unsupervised learning and visualization. Statistical testing will be used to measure the effect of cultural and contextual factors. Speech and Breath Features The most commonly used speech processing techniques in the recognition of emotions and clinical depression in the literature are related to prosody, as well as the spectral featuresand cepstral features. Prosodic, source, and acoustic features, as well as vocal tract dynamics are speech-related features affected by depression. Researchers have found that depressed subjects are prone to possess a low dynamic range of the fundamental frequency, a slow speaking rate, a slightly shorter speaking duration, and a relatively monotone delivery. Breathing consists of two phases called inspiration and expiration. During inspiration, the diaphragm is used to increase the volume of the chest cavity, causing the air to enter by mouth or nose to fill the low-pressure lungs. During a normal expiration, the diaphragm and external muscles are relaxed, the chest volume is lowered, the pressure increases, and breath is dispelled. During speech activity, some of the air stored in the lungs are spent to produce sounds. Subsequently, long bouts of speaking also require taking a breath. Breathing is intimately connected to oxygen flow, and thus is involved in all internal and external systems, including circulation, hormone systems, and the nervous system. Deep breathing in general seems to allow emotional release and processing of the energy stuck in the body from the trauma. Additionally, Ogden and Mintonobserved that subjects can have breathing difficulty when they are talking and extemporizing about their trauma. Many features can be extracted from the speech signal for the automatic detection of breathing, including MFCC parameters, short-time energy, zero crossing rate, spectral slope, and duration. However, this is a difficult problem to tackle in-the-wild. The testimonials we are planning to use in this work have many sources of noise to confound the automatic algorithms, including background music, other persons talking, and external noise sources. Sometimes the level of the sound is far from ideal, and the performance of the breath detection will not come close to the ideal situation where the speaker is isolated in a lab environment with high quality recording equipment. Dataset and Annotations In order to test the quality of various collections, as well as if the language, culture, and the traumatic events nature and date have a measurable effect on breathing patterns, we have collected short clips from survivors testimonials. We sampled different languages, and different mass-traumas. In the first phase of the project, 20 clips from 10 survivors were manually annotated for speaking, silence, breathing, lip noise, other people speaking classes. For each survivor, we selected a normal speech and a more traumatic speech segment. For some survivors, the date of the events were decades ago, for some, the memories were fresh. Each speech segment is annotated to extract the time span of speech, silence, and breathing periods. Analysis In Figure 1, each line represents one survivors speech segments, where light blue circles are speech, red circles are breathing, and green circles are silences. The horizontal dimension shows the time, and the circle radii are proportional to duration. On the left side, the normal speech segments are aligned, whereas the ones on the right side are emotionally charged. Typical features in the latter include long silences, pierced by deep breath -especially before telling about the most traumatic event-, sometimes frequent and sharp breathing. As expected, even in such a small sample size, certain characteristics prevail: for instance, personal speech patterns are different, and set the tempo of the speech as well as breathing, but within the personal tempo, deep breathing emerges. Some of these patterns are not heard while listening to the video/audio recordings themselves, but become visible only after the annotated portions of breathing patterns are visualized. Cultural approaches to trauma and disasters might dictate the way events are described, but this small sample suggests the possibility that the breathing and silence patterns that occur while telling a traumatic event are shared across cultures. A much larger scale empirical investigation will follow. � unable to handle picture here, no embed or link Figure 1: Visualization of Breath, Silence, SpeechPatterns "
	},
	{
		"id": 4,
		"title": "Newton virtually meets Euler and Bernoulli",
		"authors": [
			"Alassi, Sepideh",
			"Schweizer, Tobias",
			"Hawkins, Michael",
			"Iliffe, Robert",
			"Rosenthaler, Lukas",
			"Mattmüller, Martin",
			"Harbrecht, Helmut"
		],
		"body": " Introduction Historians of science usually face challenges in accessing the literature for their research. In the past, they had difficulties with acquiring the data they needed due to missing digital formats as the manuscripts were lying in archives not accessible to all researchers. Although in the present time manuscripts are mostly digitized and transcribed, the digital editions are not connected. Today there are many online digital editions available, but each presented in an individual platform without any connection to other editions. For example, a historian of science interested in the mathematics of the 17th-18th century must search for the data in various platforms; works of Leonhard Euler and the mathematicians of the Bernoulli dynasty are available in the Bernoulli-Euler Online platform, Bernoulli-Euler Online, BEOL , https://beol.dasch.swiss/ works of Isaac Newton in The Newton Project , The Newton Project, http://www.newtonproject.ox.ac.uk/ works of Gottfried Leibniz in the Leibniz Archive , Leibniz Archive , https://www.gwlb.de/Leibniz/Leibnizarchiv/english/introduction/ etc. Although most of these mathematicians were corresponding and had cited each others works in their books and articles, this linkage of the data is not evident because the works of these mathematicians are currently presented in individual digital platforms which are not semantically linked. For a historian of mathematics who tries to track these mathematicians lines of thought, having access to the information about how the data is linked plays a crucial role. From this information, the researcher can find the origin of ideas, estimate the effects of one mathematicians contributions on works of others, observe the relation between ideas, etc. Therefore, having access to the literature of all contemporary mathematicians along with a graph representing the semantic connections of the data through one single platform will facilitate historians research dramatically. In our project, we aim to provide such a platform without locally storing all digital editions. Project Description As the base platform which will provide access and semantic links to other edition projects, we are going to use the BEOL platform. Because it is based on Knora, a web-based virtual research environment for the humanities, providing various research tools to historians as an HTML based Restful API. As for editions, BEOL contains the correspondence of Leonhard Euler, Leonhard Euler exchanged more than 3100 letters with 300 correspondents. Currently BEOL contains just Eulers correspondence with Christian Goldbach but eventually, it will integrate all of Eulers and the Bernoullis correspondence as well as their articles and books. and of various members of the Bernoulli dynasty as XML-based online editions. On the other hand, The Newton Project is a TEI XML based digital edition project of Newtons correspondence and works. Since data in both projects are in a structured XML format, they prove to be the best candidates as a prototype. As proof of concept, we first attempt to connect the letter networks of the two projects. Although Knora provides a generic storage system, an RDF triplestore, our aim is not to store all the data of The Newton Project in BEOL , Unlike the similar project ePistolarium which stores all the editions of works of Descartes and Huygens locally, we will attempt to avoid the redundancy of data by retrieving the data from The Newton Project on demand. In this way, there wont be any need to update the database of BEOL by every editorial change in third-party repositories instead we just store the metadata of letters, The original facsimiles of Newtons correspondence are served externally by the Cambridge university digital libraries. such as the date of creation, authors, recipients, URIs pointing to the facsimiles of Newton letters, subjects discussed in the letters, etc. These metadata will be extracted from letters of The Newton Project and will be imported into BEOL . The correspondents and mentioned persons of the letters will then be linked to the existing persons of BEOL , and the letters containing specific subjects will be connected to BEOL letters about the same topic, see Figure. 1. Figure.1 Brief depiction of project framework and data model as RDF triples Furthermore, with the search features of Knora, a BEOL user can perform specific queries on the data, for example, to acquire all letters written in a specific date interval by a specific person. Knora will then check the entire BEOL database to retrieve all the letters complying with this search criteria. The search results which are originally part of BEOL will be directly presented to the user in SALSAH , SALSAH, System for Annotation and Linkage of Sources in Arts and Humanities https://dhlab-basel.github.io/Salsah/ the graphical user interface of Knora, and the results which are originally part of The Newton Project will be retrieved from the server of The Newton Project and presented to the user in SALSAH . Moreover, there will be a search component integrated into The Newton Project with which the users of The Newton Project can benefit from the search functionality of Knora to perform advanced queries on the database of The Newton Project . Lastly, since BEOL users have their own work-space in which they can access the data and annotate it, it is important that when users are working on resources retrieved from The Newton Project , they have permanent access to the state of the letters which they were initially presented. Therefore, in case, a user starts to annotate a resource, a stable version of the retrieved Newton letter will be cached and stored in Knora in order to both preserve users annotations and keep track of the users works on a resource through versioning functionality of Knora. Caching the data can be avoided if the third-party repository- in this case The Newton Project -provides versioning of data. Then the required version of the data will be fetched from that repository. Conclusion Connecting the repositories of digital editions will preserve the information which would be difficult to access without semantically linking the data. In general, establishing a link between repositories will be useful for all disciplines of humanities. In BEOL focus is on the early modern mathematics but all the features developed for this project will be generic and can be used for other projects in humanities. "
	},
	{
		"id": 5,
		"title": "Visualizing Shakespeare’s Sonic Signatures",
		"authors": [
			"Alexander, Eric Carlson",
			"Nichols, Elizabeth",
			"Bayer, Estelle"
		],
		"body": " Good authors imbue their characters with distinctive voices that are often discernible devoid of explicit dialog labels, both by their word choice as well as sometimes by the actual sound of the words. For instance, in Shakespeares Othello, the speech of the titular character is said to be characterized by longer, rounder vowel sounds than the quick speech of his counterpart, Iago. This phenomenon provokes a wide variety of questions: Can we detect these differences in speech computationally? If so, what would it tell us about these characters? What would it tell us about the author? We set out to see if we could detect a difference between Shakespearean characters based on the sounds they made: what we call their sonic signatures. To be clear, we were interested in the sounds associated with the words themselves, not the voice of a particular actor. We used a library called NLTKto convert plain text versions of every characters lines into strings of phonemes, or perceptually distinct units of sounds. For instance, the word cheese is made up of three phonemes, which can be written CH IY Z. Having converted each character into 39 counts of these different phonemes, we used a machine learning technique called Naive Bayes to create a classifier for differentiating them. We initially chose to try and tell the difference not between individual characters but between different common character roles in Shakespearean plays: protagonists, antagonists, and fools. We achieved partial success, finding that we were able to predict a characters role based on the sounds they made significantly better than chance, though with far from perfect accuracy. It may be possible to improve our accuracy, and perhaps train a classifier to detect more specific differences between individual characters, but this was not the immediate next step. Our results serve as a proof of concept, showing that we can detect differences in sonic signatures between characters. However, this had been reliant on modern pronunciation of the source text, which is sometimes quite different from how Shakespeares actors might have pronounced his words. As such, we then converted our data to original pronunciations derived from a Shakespearean pronunciation dictionary, and found that our accuracy changed very little. Aside from problems with early modern pronunciation, a perfect classifier is not precisely the end goal. Rather, the motivating research question is what insight can differences in sonic signatures provide scholars regarding the differences between characters and authors. For instance, do some authors write such that their characters signatures are more distinguishable? For such insight, it is important to be able to present scholars with analysis of the important phonemes in the context of the passages themselves, such that they are able to apply practices of close reading to investigate differences. To afford such investigation as it relates to a potential interpretation of individual phonemes, we have created a prototype of a visual tool for comparing the lightness and darkness of the speech of individual characters. Our tool, called Ophelias OH, has grown into a system for comparing the prevalence within characters speech of vowel sounds that have found to be associated with light and dark connotations. Its colored visualizations highlight characters that use dramatically more or less of a particular vowel sound than the rest of the characters in the play. Upon identifying a character of interest, a researcher can click to drill down into a view of that characters lines, annotated with colored tags showing relative proportions of different phonemes within the passages themselves. Such tagged text representations help place vowel differences in more specific context within the plays. Figure : Screenshot of Ophelias OH, a prototype for detecting differences in the lightness and darkness of characters characteristic speech patterns. Going forward, we plan to work with readers to refine Ophelias OH to better highlight the most important characters and phonemes, and extend it to be able to discern differences amongst authors. Finally, we mean to extend our scope to determine whether the ability to distinguish characters by sonic signature is done by different authors with differing levels of success. "
	},
	{
		"id": 6,
		"title": "Waqf Libraries And The Digital Age",
		"authors": [
			"Alshanqiti, Ahmed Mohammed"
		],
		"body": " This short paper raised as a part of my ongoing PhD thesis that aims to introduce and study the concept of a Digital Waqf library. The paper argues that the current rules and guidelines of the concept of Waqf needs to be reviewed and updated in order to adapt with the digital age and the new digital innovations. Waqf is an Islamic concept. It has existed since the days of the prophet Mohammed. Waqf can be defined as a kind of pious endowment with special requirements and conditions. It aims to continuously benefit the community as well as seek a reward and forgiveness from God for the donor of the Waqf. The word Waqf in the Arabic language means stop. Therefore, the concept of Waqf aims to stop an asset from being sold, given away, or even inherited, and extracts the benefit from it to certain individuals, groups, or organizations. Waqf can come in different forms, such as real estate, libraries, schools, wells, etc. However, there is a set of mandatory requirements which must be applied for any donation to be registered as a Waqf. If these requirements cannot be met, then the donation cannot be registered as a Waqf; rather, it could be a normal donation and will not fall under the rules and protections of the Waqf. These requirements are mentioned in many different Islamic studies which discuss Waqf, such as Aldawood, Kbha, Haggar, and Alshangity. Waqf libraries are a type of Waqf. They are ancient libraries, and many old Waqf libraries still exist today and contain valuable historical collections that include manuscripts, books, and artefacts. An example of an existing Waqf library is the Arif Hikmat Waqf library in the city of Medina. Hence, a Waqf library is a kind of public service body where the owner maintains the collection for the benefit of certain people or organizations. However, Waqf libraries have not received as much attention as other libraries, even in the Islamic world. The literature lack studies concerning traditional Waqf libraries, let alone digital Waqf libraries. The majority of users in the Islamic world do not have enough knowledge about Waqf in general, let alone Waqf libraries. Waqf libraries have been important contributors to shaping Islamic civilization due to their valuable collections that include books, manuscripts, and artefacts. Henniganexplains that and states, it is not an exaggeration to claim that the Waqf, or a pious endowment created in perpetuity, has provided the foundation for much of what is considered Islamic civilization. Waqf libraries spread through the Islamic world into cities such as Damascus, Istanbul, Mecca, Medina, and Cairo; some still exist, while others have been destroyed due to wars, such as the conflict in the western Kosovo town of Gjakova/Djakovica. In this example, the Waqf library of Hadum Suleiman Aga, which was founded in 1595, was burned by the Serb military around 1999, which resulted in the loss of its complete ancient collection. Waqf libraries, like other types of libraries, have the ability to benefit from any type of technology to develop their services, which will include creating a digital version of their existing library. Reading through the literature about Waqf libraries, it mainly discusses the traditional version of these libraries, not a digital version. Therefore, one of the main and important questions of my thesis is to determine whether we can introduce the concept of a digital Waqf libraries, which leads us to the question: is a digital Waqf library still a Waqf library? The challenge in this is that the traditional Waqf libraries are already following strict requirements in order to be recognized and registered as Waqf libraries, but how can we apply these to a digital version? Therefore, can we simply create a digital library and decide that we want it to be recognized as a digital Waqf library? The initial findings of my thesis argue that based on the current requirements and rules of Waqf, not any type of digital libraries would qualify to be registered and recognized as a Waqf digital library. I concluded that based on the current requirements of Waqf, only a hybrid digital library would qualify as a Waqf library. And by a hybrid digital library I mean a digital library that is generated and based on an approved physical Waqf library. Therefore, the current rules and requirements of Waqf would prevent the registration and recognition of any born digital libraryas a Waqf. Hence, that raised the need to advocate to an immediate review and an attempt to update the current requirements and rules of Waqf in order to regulate how electronic innovations such as born digital libraries, virtual schools, digital books, can qualify to be registered and donated as a Waqf. At any rate, though Waqf is an ancient concept, it has not yet been reviewed by an official Islamic committee to adjust its rules and regulations to fit the current digital age. Increasingly, however, this kind of adjustment is vital and necessary, because of the great many new things that now exist, though a high portion cannot be accepted and registered as a Waqf; the current rules and requirements of Waqf would prevent such registration, and no one can simply issue a statement to adjust Waqfs rules and requirements. An official committee of recognised Islamic scholars would have to study this issue and come up with new regulations on how to deal with Waqf in the digital age, thereby fulfilling the need for a detailed Islamic study to be conducted to study this important matter. Such review and update will allow different types of digital innovations including born digital libraries to be qualified to be registered as a Waqf . "
	},
	{
		"id": 7,
		"title": "Epistolary networks in Italian Humanism: Collecting, editing, analysing Italian humanistic letters - 1400-1499 (with a critical edition of familiar letters of Iovianus Pontanus)",
		"authors": [
			"Amendola, Cristiano",
			"Alfredo, Cosco"
		],
		"body": " Introduction: During the 15th century the increase of letter-usage and the philological establishment of the Latin language according to the classical purity, promoted the development of a cosmopolitan movement of researching and sharing knowledge. Due to these means of communication, scholars from all over Europe gave life to an ideal society based on the classical values of Humanitas. The longest-lived intellectual community of the Old country, the so-called Republic of letters, rooted in this system of rhetorical and cultural values. The Italian writers, who during the 16 th century initiated one of the most remarkable phenomena of the Italian literature – namely, the production of Libri di lettere – referred to these values as well. Nevertheless, there are just a few critical studies on the relationship between these remarkable cultural movements and the massive production of letters in the 15 th century. The most up-to-date list of projects that use technology to catalogue, digitise, and edit letters concerning these cultural movements – list available on the E.M.L.O. project web page –, shows that none of the 84 researches in progress studies the early stages of these processes: namely, the 15 th century. The current project, started in December 2018, aims to encourage a critical review of this massive epistolary production through the creation of an online platform intended to map and catalogue letters, collect metadata, analyse prosopography and epistolary networks, and edit texts drawn from editions free from copyright and from unpublished manuscripts. Corpus: an initial review allowed us to define a body of 34 epistolaries– of which more than half derived from editions free from copyright – which totals around 8500 documents. The cataloguing of these materials will be based on collecting of the following metadata: date of the letter; sender of the letter; people mentioned in the letter; recipient of the letter; place of origin of the letter; place of destination of the letter; origin; document type; repository; shelf mark; printed copy details; digital copy details. Meanwhile, we will edit the digital critical text of the familiar letters of Iovianus Pontanus, an Italian humanist, politician, and leading figure of the court of the kingdom of Aragona. Methodology: The project consists of three phases: Get; Analyse; Represent & disseminate. To execute these three phases, we will use the native XML-db application server eXist-db. The XML format used to store and index data is TEI. The choice to use an XML native db and TEI-XML format from the beginning, although less straightforward than others, will enable us to extend the metadata collected to the full-text at any time, without facing platform migrations. For the collecting data phasethe ARACNE frameworkwill be used: an open source software developed by the Centro di Ateneo delle Bibliotecheof our university for managing and publishing archival document collections in TEI-XML format in eXist-db. For the next phaseswe will use several open source tools, such as: Apache Luceneand Elastic searchto aggregate the data. To publish the results in machine-readable format, to create linked data in the eXist-db API, and to project data in other formatsApache Jenawill be used. Furthermore, for human-readable results, ARACNE, which provides the possibility to create a website for a published archival collection, will be used. "
	},
	{
		"id": 8,
		"title": "Batavia and the Gold Coast: Mapping Textile Circulation in the Dutch Global Market",
		"authors": [
			"Anderson, Carrie J.",
			"Kehoe, Marsely L."
		],
		"body": " PROJECT SUMMARY The Dutch Republic established its importance on the world stage through its early successes in global trade, becoming for a time the preeminent circulator of luxury and wholesale goods for the European market. This success was due in part to the new countrys nimbleness in adapting to local circumstances to undermine their Iberian competitors, and their financial creativity and business acumen in creating the first multinational publicly-traded stock company, the Dutch East India Company, and its counterpart, the Dutch West India Company. Our project is a collaboration between specialists in the VOC and the WIC, who have both worked to establish the centrality of trade and the circulation of goods to Dutch Golden Age art history, and now join forces to bring the previously siloed considerations of these companies, East and West, together, through the examination of different modes of textile circulation. The types of textiles that circulated the globe on VOC and WIC ships were varied—ranging from Indian cotton to Haarlem wool to Silesian linen—and their values rose and fell according to demand. Data on textile circulation are likewise varied and dynamic, often movingeasily between quantitative and qualitative categories. For example, whereas textiles frequently circulated as wholesale commodities, they were also presented as diplomatic gifts, whose value was determined by social—rather than monetary—terms. Moreover, beginning as early as the second half of the seventeenth century, textiles were the favored bartering currency in the inhumane brokering of human life known as the Atlantic slave trade, giving them an uneasy status in early modern social and economic history. Textiles—re-presented as garments in paintings and prints—also became potent signifiers in an increasingly global world, where clothing played a critical role in shaping identity in colonial and European circles. Our project, Batavia and the Gold Coast: Mapping Textile Circulation in the Dutch Global Market, seeks to make connections between these economic, social, and visual data—which so often exist as discrete epistemological categories—through the development of an open-access database and an interactive map. While our project was inspired by foundational research questions that engage both art historical and economic methodologies, they are united by factors that are fundamentally spatial: What types of textiles circulated in Batavia and the Gold Coast and where was their point of origin? How were representations of textiles linked to their circulation? What were the internal and external factors in each geographic location that impacted the textile trade? What impact did the slave trade have on the trade/circulation of textiles by both the VOC and the WIC? What role did indigenous communities play in facilitating or hindering the local and global circulation of textiles? Despite the disparate geographies served by the VOC and WIC, how were the Companies linked? The intellectual and spatial breadth of our questions necessitated a project that was both collaborative and digital, one that we hope will provide a model for future work in Digital Art History. THE CONTENT The Interactive Map The richness of our data—which includes visual images, archival data, and geospatial coordinates—requires a robust web-based database upon which other scholars of Dutch trade can eventually build. We are working closely with a developer to create an interactive map using Leaflet, an open-source JavaScript library, which will visualize our textile data in geographical space. This platform will enable users to geolocate our VOC/WIC data, which is drawn from published and archival cargo lists, according to variables such as textile type, geographical origin and destination, and time. Image Database Beyond the archival sources, we are also compiling a database of images featuring textiles that have been re-presented as garments on both exotic and European bodies in a colonial context, such as Albert Eckhouts famous series of so-called ethnographic portraits in Copenhagen; the three portraits of African envoys also in Copenhagen and attributed to Eckhout; Andries Beeckmans 1661 Castle of Batavia in the Rijksmuseum; the many portraits of VOC governors that would have originally hung in the Casteel Batavia, also in the Rijksmuseum; and Dirck Valkenburgs 1706 Slaves on a Sugar Plantation in Surinam, among others. This database of images will provide what the mapped data cannot: a visual record of the ways in which textiles were closely linked to complex categories like race, gender and economic status. Visual Textile Glossary One of the most important contributions of Batavia & The Gold Coast is our visual textile glossary—a much-needed resource that is currently lacking in the scholarship of the early modern period. Our glossary will include each textiles name, a standard definition, and a visual example of that textile—either in the form of internal links to our database of painted images, external links to textiles housed in museum collections, or both. Our visual textile glossary will also link to the mapped data, so the user can see—alongside the visual material—how particular types of textiles circulated the world as objects of trade. ADHO 2019: SHORT PAPER PROPOSAL This project fits well with the ADHOs theme complexity, as it visualizes the myriad ways in which textiles were valued in the early modern global world: as trade goods, as mediators in diplomacy, as garments with profound social significance, and as physical objects comprised of raw material from across the globe. In our 10-minute presentation we will introduce our methodological approach; share our in-progress maps, images, and data; and demonstrate the interdisciplinary importance of understanding the dynamic movements and meanings of textiles in the early modern world. "
	},
	{
		"id": 9,
		"title": "Thinking Computationally in the Digital Humanities: Toward Block-Based Programming for Humanists",
		"authors": [
			"Anderson, Clifford B.",
			"Ramey, Lynn T."
		],
		"body": " What is computational thinking in the digital humanities? The question whether digital humanists should learn to code has been highly-contested. The hack versus yack debate has lost its edge as scholars concede that theory and programming praxis can be brought together productively through what Davidson terms collaboration by difference. Given that the majority of digital humanists will not need to program professionally, to what extent ought computation be taught in the digital humanities? Jeannette M. Wing coined the term computational thinking to address the teaching of digital literacy beyond computer science. She contended that computational thinking is a fundamental skill for everyone, not just for computer scientists. In Digital Humanities, Berry and Fagerjord comment at length on Wings definition, concluding that a critical understanding of computing at its different levels is a prerequisite for a digital humanist.... There is little agreement, however, about the best way to teach computational thinking to humanists. We review the potential of visual or block-based programming languages for teaching computational literacy in the digital humanities. We argue that digital humanists should learn from these tools emphasis on the ludic over the pragmatic. We also offer suggestions about how digital humanists might adapt and critically adopt block-based programming as they seek to expand their understanding of fundamental concepts of computer science. Review of educational programming environments The use of visual or block-based programming has become a mainstay for computer science education in the K-12 arena. Block-based programming involves the manipulation of graphical elements to create units of computation. The authors of Learnable Programming: Blocks and Beyond argue that block-based programming makes it easier to learn to program for three primary reasons: emphasizing recognition over recall, chunking code, and constraining options. These pedagogical advantages would also seem to apply in the digital humanities though, as a quick review of the evolution of these languages demonstrates, they were not created to teach computational thinking to adults. Logo. The origins of block-based programming stretch back to Logo, a programming language and graphical environment for computer science education. While not a visual programming language, when paired with Turtle Graphics Logo provides students with the ability to visualize their computations. Logo has gained renewed popularity among the elementary age set due to Gene Luen Yang and Mike Holmes Secret Coders, a series of graphic novels that employs Logo to teach basic computational literacy. Scratch. The designers of Scratch sought to create a computational environment for kids and teens to become active manipulators rather than passive consumers of digital media. The designers stripped away many of the complexities of software developmentto create what they term a tinkerable environment, pioneering the use of blocks for syntax. Snap! While Scratch succeeded in developing an extensive community of users in the K-12 arena, its emphasis on semantic simplicity inhibits its usefulness for teaching students computer science at the postsecondary level. The Snap! programming environment emerged from a collaboration between Brian Harvey at Berkeley and Jens Mönig, a software developer currently at SAP. Drawing on long experience teaching functional programming in Scheme, the authors created a semantics with lambda expressions, recursion, and high-order functions using a Scratch-like syntax. NetsBlox. NetsBlox is an adaptation of Snap! that makes it straightforward for users to communicate with internet services and to communicate peer-to-peer. Students can draw on these features of NetsBlox, for example, to place markers representing art museums in the vicinity on a Google Map or to create a shared digital whiteboard. By fostering the ability to communicate beyond the boundaries of the programmers laptop, NetsBlox paves the way for creating data-driven digital humanities projects. Data may also be persisted in the cloud, making it possible to preserve state. Currently, NetsBlox comes with the ability to call out to a select number of services. However, the developers envision adding a lot of new services and data sources to NetsBlox.... This raises the question whether a version of NetsBlox could be developed specifically for digital humanists, integrating web-based application programming interfacesfor platforms like the DPLA, the HathiTrust, and Europeana, among others. NetsBlox with prototype RPC block for Wikidata The digital humanities community also embraces visual programming models. Voyant Tools, for instance, provides a graphical interface for scholars seeking to study textual corpora. To date, there appears to be little to no scholarship about the pedagogical effectiveness of using visual programming environments in the digital humanities. Programming as ludic rather than pragmatic Is learning block-based programming a means to an end or an end in itself? While computer science students will inevitably move from block-based to text-based programming, the designers of Scratch claim that many students will fruitfully remain within its environment. At the secondary and post-secondary level, The Beauty and Joy of Computing curriculum likewise promotes the enjoyment of programming within the Snap! environment: having fun is an explicit course goal. Leading digital humanists also acknowledge the playful aspects of programing. In On Building, Stephen Ramsay remarks, Learn to code because its fun and because it will change the way you look at the world. Nick Montfort argues that motivations for learning programming go beyond the merely instrumental, remarking it is enjoyable to write computer programs and to use them to create and discover. By customizing the visual representations and selecting domain-specific exercises, block-based programming could find wide application in the digital humanities, promoting the joy of learning computation for its own sake while providing humanists with a better conceptual grounding for the evaluation and application of algorithms and software in their digital research. Prolegomena to any future visual programming environment for the digital humanities What would a digital humanities version of a block-based programing environment look like? By way of conclusion, we suggest how NetsBlox might evolve past its origins in Scratch to provide a shared platform for teaching computational thinking in the digital humanities. We propose three developments: 1. creating default sprites that represent the domains of digital humanities research; 2. establishing libraries of blocks to call commonly-used web-based APIs in the digital humanities; 3. providing a curriculum focusing on major research areas in the digital humanities, including distant reading, educational gaming, geospatial analysis, and steganography, among other topics. By developing a block-based environment for the digital humanities, we hope not only to advance computational thinking in our field, but also to provide resources for introducing the digital humanities into secondary and postsecondary courses on computational thinking. "
	},
	{
		"id": 10,
		"title": "Evaluation of a Semantic Field-Based Approach to Identifying Text Sections about Specific Topics",
		"authors": [
			"Adelmann, Benedikt",
			"Andresen, Melanie",
			"Begerow, Anke",
			"Franken, Lina",
			"Gius, Evelyn",
			"Vauth, Michael"
		],
		"body": " Introduction With the increasing availability of large corpora, humanist scholars gain opportunities to choose their material in a more data-driven way. How can we identify texts or text sections relevant to our research question if we abandon prior knowledge as a determining factor? In this paper, we explore the potential of semantic fields for finding text sections about a topic of interest. We use the term topic in the sense of subject of a text. We do not want this to be confused with the term used for the results of topic modeling methods. By topic, we do not refer to the topic modelling concept, but to different subjects in a text. Additionally, we want to address the major issue of evaluating a task involving a great deal of interpretation. The use case we present is the identification of text sections about body and illness. This is motivated by our larger research project that focuses on health. As test data, we use extracts of about 7,000 words from diverse research domains addressed in our research project: the novel Corpus Delictiby Juli Zeh; the novel Ellen Olestjerneby Franziska zu Reventlow; an interview with a dying patient; and a protocol from the German Bundestag. Manually annotating topics Our guidelines for the manual annotation are available in German http://doi.org/10.5281/zenodo.2634297 . They determine that we consider a text span to be about illness or body if it is depicted as such by the text. The exact decision of what to annotate remains somewhat arbitrary, as is unavoidable in hermeneutic annotations. In order to agree on a concept of body and illness across disciplines, both were defined in a narrow way. The annotation was carried out by two independent annotators who did not receive any input beyond the guidelines. We used the annotation tool CATMA. We calculate the agreement between the annotators in order to estimate the difficulty of the task and the quality of our guidelines. For this calculation, we compare the annotations sentence by sentence. If any word was annotated, we consider the whole sentence to be annotated. The objective of the task is to identify text sections and not phrases, so this abstraction is adequate. It also facilitates comparison as we do not need to deal with overlaps. In terms of agreement this is a rather tolerant approach. As a result, the agreement is relatively high, given the interpretative nature of the task. The chance-corrected scores range between 0.54 and 0.90, showing the varying difficulty in the texts and topics. Some of the disagreement could potentially be avoided by further refinement of the guidelines. Corpus Delicti Ellen Olestjerne Interview Protocol body 0.90 0.86 0.66 - illness 0.54 0.71 0.74 0.84 Table 1: Inter-annotator agreement, measured by KappaFor the gold standard, the annotators and two other researchers resolved the discrepancies between the two annotations. Table 2 shows the absolute numbers of annotated sentences. Corpus Delicti Ellen Olestjerne Interview Protocol body 113 38 38 - illness 22 27 67 13 Table 2: Number of annotated sentences for the two topics Semantic field generation We generated semantic fields in the following ways: The German Integrated Authority File GND http://www.dnb.de/gndis a controlled vocabulary with a hierarchy of concepts and references. All hyponyms for bodyand illnesswere extracted. GermaNetis a lexical-semantic net that is structured in hypernym and hyponym relations. We extracted all hyponyms to bodyand illness. We excluded all hyponyms to illness from the semantic field of body. We created word embeddingswith the gensim implementationof word2vecon a collection of more than 2,500 full-texts of German literature from around 1900. We took the 100 words most similarto body and illness, respectively. All words of the semantic fields were expanded by all possible inflection forms using SFSTand the model by. The texts were automatically tagged with the three semantic fields using CATMAs query function. Evaluation For the evaluation of the semantic field approach we compare it sentence by sentence with the gold standard. Table 3 shows the results for precision, recall and F1 scores. As can be expected for an annotation task involving much interpretation, not even half the scores reach more than 0.5. The GND semantic field has a better recall than precision as it is very large, especially for illness. GermaNet and WE score higher on precision than recall. The combination of all three semantic fields results in a clear improvement for the semantic field of body. illness body Precision Recall F1 Precision Recall F1 GND 0.20 0.41 0.31 0.55 0.69 0.62 GermaNet 0.85 0.23 0.54 0.30 0.13 0.21 WE 0.60 0.34 0.47 0.57 0.23 0.40 mean 0.55 0.33 0.44 0.47 0.35 0.41 combination 0.21 0.45 0.33 0.43 0.79 0.61 Table 3: ResultsFor example, words like Handas a part of the body or Virusas an indicator for illness were found both by the manual annotations as by our queries using the semantic fields. Our approach generates false negatives when the topics of interest were mentioned in an indirect way, as it is frequently the case in literature such as zu ihren Füßen. Additionally, our semantic fields consist of nouns only, so all other parts of speech were neglected. False positives were produced when words about body or illness were used metaphorically as for example aus dem Auge verlierenor mentions of Herzin the context of offenherziges Lächeln. Conclusions The identification of specific topics using existing or automatically generated semantic fields does not fully reproduce what human annotators do. Researchers relying on this method should be aware that they systematically lose texts with specific features such as a more indirect style which results in a biased corpus. There are many false positives that can be manually removed. For scenarios with large corpora, an approach like this is still a feasible one. If we apply the method to identify units of text larger than sentences, the results might improve. We intend to conduct experiments to this end in the future. A higher-level question is how we can adequately evaluate tasks involving a great deal of interpretation. There are many possible ways of operationalizing, the topic body and our annotations guidelines represent only one. We consider our contribution to be a rough first approximation to a solution of this issue. "
	},
	{
		"id": 11,
		"title": "Continuous Integration Systems for Critical Edition: The Chronicle of Matthew of Edessa",
		"authors": [
			"Andrews, Tara Lee",
			"Safaryan, Anahit",
			"Atayan, Tatevik"
		],
		"body": " We present here a project to prepare the digital critical edition of the Chronicle of Matthew of Edessa, which is due to finish its first stage in April 2019. The Chronicle is a 12 th century Armenian-language historical text covering events in the Near East during the 10 th-12th centuries. This takes the reader from the apogee of the medieval Armenian kingdoms to the fall of most of them during the eleventh century as their lands were annexed to Byzantium and ultimately lost to the Seljuk Turks. The Chronicle is also an important source for the history of the First Crusade, and particularly the Crusader County of Edessa. There are about 40 known manuscripts that contain the text of the Chronicle in whole or in part, copied between the end of the 16 th century up to the 19 th. In our workflow we have adopted the stages of digital critical edition suggested by Robinson– transcription, collation, stemmatic analysis, edition and publication. We found, in the process, that these stages do not occur in a strict succession; it was quite regularly necessary to move back and forth between stages, refining earlier steps on the basis of later results. One of the central features of our project was to adopt a continuous integrationsystem In this case, the software in question is Concourse. in order to manage the work across these stages in a sensible manner. The primary challenge we then had to overcome was the need to ensure that the data was cleanly maintained from beginning to end, as the nature of CI design does not allow for modifications in the middle of the pipeline. Beginning with the transcription, where we also followed general guidelines proposed by Robinson and Solopova, diplomatic transcriptions of all available manuscripts were made in T-PENaccording to guidelines maintained in the projects GitHub repository, and converted from T-PENs native Shared Canvas JSON format into valid TEI-XML using a Python library developed for the purpose. A major advantage of the digital approach is the ease with which entire categories of transcription error can be identified and corrected automatically. The CI framework enabled parsing and validation errors to be spotted immediately. Rare cases could be corrected manually in T-PEN, but widespread mistakes required us to revise our workflow tools to behave in a way more akin to functional programming, so that we could insert custom code to handle peculiarities of our specific texts without compromising the more general design of the tools themselves. Given transcriptions that passed our litmus tests for validation and basic accuracy of content, we were then ready to collate them using the JSON input functionality of CollateX. Here again we relied on custom programming within the CI setup – while programmatically taking TEI files and tokenise the text contents into individual readings is a straightforward task in its basics, the specifics will vary massively from text to text. Our tokenizer software thus provides a number of code plug-in interfaces that can be used to generate a correct token, also in the specific XML context of that reading in the document. The CI setup also allowed us to make, and preserve, a number of custom modifications to how we used CollateX, in order to maximise its accuracy. Since one of our transcription guidelines was to leave abbreviations unexpanded, we also developed a tool using a combination of base text collation, regular expression logic, and user interactivity toautomatically expand these abbreviations and store the results in a database, which could in turn be fed into the tokenisation step on later runs of the pipeline. Here too we retained the principles of diplomatic transcription: if the word was abbreviated the way that it could be expanded in a canonical orthography, we did so, otherwise, we tried to follow the intendedspelling of the scribe. Given full collations of the chronological sections within the text, our editorial analysis could begin. We have used the Stemmaweb toolboth to normalise and to specify classes of relationship between individual readings throughout the text, which in turn eases the stemmatic analysis of the manuscripts. Recent versions of Stemmaweb also provide a means of indicating the lemma reading among a set of variants, so that critical edition text can be produced directly. At this time a system for annotation of the textual content is under development, which will enable us to provide a digital commentary on the historical content of the Chronicle, as well as the edition itself. Although our project has not extended the CI model past the stage of collation – this would require a system to save and re-play editorial decisions concerning the collated text, which would have required more of an engineering effort than we had resources for within the framework of the project – we consider this to be an important direction for producing a critical edition that is truly reproducible from the textual evidence at its base. "
	},
	{
		"id": 12,
		"title": "Mapping the Complexity of Ancient Codices in Databases: the Syntactical Model",
		"authors": [
			"Andrist, Patrick"
		],
		"body": " Most antique and mediaeval codices preserved today present one or more layers of complexity. Codices are usually considered complex when they contain more than one text; but there are other aspects to be taken into consideration. Sometimes, they are complex objects because several scribes collaborated in copying several texts into one book, at times using different ink or layouts, or because their writing material was reused from pre-existing books. Other times, a manuscripts owner might also restore damaged leaves or choose to add new texts on new quires. Readers too could contribute to an evolving complexity as they added comments, drawings or small pieces of content in the margins or in blank spots. The historical complexity of ancient codices is in itself also a complex notion. How can the various levels of complexity of a codex be coded into a database in such a way that the complexity is not only clearly understandable but also correctly searchable? The aim of this paper is to present a language-independent model for achieving this goal, and some of the new challenges it now faces. It is called the syntactical model and is already in use in several MySQL as well as XML/TEI databases. The first section of the paper sets out the problem by showing a few slides of a complex codex and explaining the problem one encounters in many current databases. Even though they record accurate information searches with multiple criteria often result in inaccurate answers. Let us for example consider a codex C, which contains both a text by Aristotle copied in the twelfth century, and a text by Chrysostom copied in the fifteenth; let us now imagine a scholar searching databases for witnesses of Chrysostom copied in the twelfth century; by far most of todays databases will return codex C as a positive match, though it is not! Why is this so? Because the single pieces of information is unrelated to its context of production: these databases fail to consider the codex as an object evolving over time. The second section presents the principles of the syntactical model. The basic idea is that all the contents are linked to a more or less conscious act of production. Let us imagine that someone orders a new copy of Homers Iliad ; many years later, someone adds some texts by Hesiod to this book; to these a subsequent reader adds the commentaries of Eustathius in the margins along with some notes of her or his own; later still someone takes out the pages containing the Hesiodic texts, binds them together with Pindars Odes and adds a colophon… every stage of the transformation represents a unique act of production. The Syntactical model holds that each content in a printed or electronic description of a codex ought to be clearly and unambiguously related to its production unit. The third section explains how this model has been implemented until now. A standard description according to the syntactical model operates on three data levels: the data level related to the codex as it is today; the data level related to its constitutive production units i.e. its historical parts; and the data level of the pieces of content, always situated within a production unit. I will show how the syntactical model is implemented in our MySQL shared databaseand I will also show an example from the XML/TEI Beta maṣāḥǝft database of Ethiopian manuscripts. This very structured and hierarchical way to describe manuscripts enabled us to develop a new tool, with the specific purpose of both visually relaying the overall structure of a codex, as well as the content of each stratum; elements which may not be immediately evident in a written description. It generates graphic representations on demand from a manuscript description based on the syntactical model. The whole codex, as well as its historical parts and their contents, is represented on a single screen; extra details are displayed by clicking on individual elements of this graphic representation. These are created by an open-access web application, developed through a proof-of-concept funded by the ERC. A promising future development of this tool is the reconstruction of now dispersed manuscripts; by drawing elements from existing graphic representations, the users will be able to create a new representation by joining diverse parts of various manuscripts and arranging them in order. The final part of the paper mentions some of the main current challenges: the main one is a human one, that is to say, when people are unfamiliar with or misunderstand the historical layers of a codex and consequently do not represent them correctly in the database; from a more technical point of view, the hierarchical way in which the syntactical model is currently implemented in the databases results in very static descriptions and poses certain problems. For example, when the production units of a codex are not easy to identify, or when one wants to simultaneously visualise all the pieces of content currently on one page even though they were produced at different stages. Moreover, other structure levels of the codex within or above the production units are not easy to represent. This is why we are currently thinking of other ways to implement the syntactical model in the databases, which would allow us to visualise the codex according to various interpretation of the syntactical model or, when needed, according to other models. Mapping the major layers of complexity of ancient codices in databases is possible with the syntactical model, even though there is a lot room for improvement. We hope that in the future more projects will take advantage of the potential of this model. "
	},
	{
		"id": 13,
		"title": "Modelling The Editorial Reading Process: The Case Of Giulio Einaudi Editore",
		"authors": [
			"Antonietti, Laura"
		],
		"body": " The paper will present the results of the analysis and the modelling of the reading process within the Italian publishing house Einaudiin the aftermath of World War II. The research focuses on the activity of one of the most important contemporary Italian publishers and, ultimately, on the dissemination of Italian literature of the last century. This is part of a larger project, which focuses on narrative works and on Natalia Ginzburg, one of the leading collaborators of the publishing house and one of the most original writers of the period. Reading processes were carried out within the publishing house or thanks to external consultants, and had the purpose of verifying the economic and literary viability of a work; this activity resulted in the production of reading reports, containing a summary and an evaluation of the proposed work. Through these documents, mostly unpublished, the books to be printed were selected. Reading reports embodied therefore cultural models that need to be interpreted according to a specific historical and cultural context. The presentation will highlight the fundamental contribution of tools and methods of Digital Humanities in the context of my research. More specifically, I will discuss data modelling and encoding as speculative activities of epistemological value. In fact, by modelling the reading process, I was able to understand and represent its complexity; in addition, by encoding the documents I was able to focus in a consistent and speculative matter on aspects such as textuality, style, rhetoric, highlighting differences of the communicative process. The potential of the DH tools and methods have yet to be exploited and applied to this research field: at present there are no digital editions of reading reports. This lack of direct reference models is one of the most stimulating challenges of my project. The model was created thanks to a class diagram developed in UML. This allowed me, on the one hand, to organize a heterogeneous corpus, and on the other hand it provided the basis for the conceptualization and analysis activities necessary to the creation of a relational database. The database allows to efficiently account for sources, gathering the fundamental information for the description of the corpus in a structured way. It currently contains metadata of a sample of documents, which were then transcribed and encoded according to the TEI XML schema. The metadata in the database and the encoded texts form the basis of the work that will follow: the creation of a digital edition of the documents themselves. To describe the UML scheme, I focus my attention first on the document, which is stored in a particular archive. UML calls their entities classes, where each class can have attributes that describe a series of their characteristics: the class documento, for example, contains the date, length, type of writing and form fields. Each documento conveys what I have called an editorial text. It was fundamental to make the distinction between documentary unit and textual unit, considering also the fact that the same editorial text can be transmitted by several documents: often, in fact, many copies were made of the same reading report, in order to reach different collaborators who were called upon to express their opinion within a collective decision-making process. Editorial reportsare transmitted by different types of editorial texts, among them, of particular interest are minutesof editorial meetingswhich report upon the opinions and the sometimes heated exchanges between the various collaborators; these documents allows us to reconstruct the collegial decision-making process, although this reconstruction is, of course, only partial, since it summaries on a much denser oral communication. Other textual typologies that I have identified are: • Editorial department decisions in which the opinions of the individual collaborators were reported indirectly by the documents compiler. • Structured reading reports. • Letters by the individual collaborators, which are the emblem of the dialogic and collegial character of the decision-making process within the publishing house. I distinguish between editorial texts and the editorial reports because a single editorial report doesnt occupy the entirety of any given editorial texts, and sometimes, many editorial reports are contained by a single editorial text. The editorial readerhad different natures. It is not always easy to identify the reviewer: the documents can be signed or unsigned, sometimes containing an identifier of a given collaborator, but when completely anonymous, an analysis of the handwriting is required. It is important to underline that, as shown in the diagram, the editorial reader who expresses an editorial opinion may not coincide with the compiler of the document containing it. This is evident in the case of the minutes, where the writer is often a stenographer. The model I have produced is essential for the re-organization of the corpus and for the rationalization of the reading process. This model is a preparatory work for the study of the corpus itself, which ultimate aim is the valorization of a literary micro-genrethat deserves to be studied through appropriate tools. The model itself will allow to understand the reasons behind the decisions and the relative weight of any given collaborator, providing an insight on how the Italian literary canon of the aftermath of World War II was built. FIG. 1 "
	},
	{
		"id": 14,
		"title": "Reading in Europe - Challenge and Case Studies of READ-IT",
		"authors": [
			"Vignale, François",
			"Benatti, Francesca",
			"Antonini, Alessio"
		],
		"body": " This paper aims to present the READ-IT project and the first set of case studies collected by DH and HSS researchers. Case studies in reading are one of the first deliverables and results of a joint construction between DH, ICT, LIS and HSS scholars. Case studies occupy a central place in the definition of READ-IT data model and tools, guiding the identification on common issues, dimension of analysis and sources for validating and testing both the conceptual framework and the database. The case studies include different sources, such as social media, students diaries and letters, from the 18th up to today, in Czech, French, German, Italian and Dutch. The importance of books and reading is unquestionable in modern society, but at the same time there are still unaddressed questions. Up to now, we can study the circulation of books and therefore the idea they convey, we can identify the factors that may facilitate or impending the reception of such ideas in different cultural groups, but we still cannot comprehend the impact of reading in our history and in our society. Regarding history of reading practices, knowledge has significantly increased over the last decades about what, where and when people read. Nevertheless, two major questions remain unanswered: why and how do people read? We still lack a systematic approach and tools to study the experience of reading, what are the effects on readers and their lives, what are the outcomes of reading and what is affecting the reading experience of the general public. The two main research questions indicated above can also be decomposed in a series of subordinated questions: a. What kind of transaction exists between a reader and a text? b. What role does the environment play in this transaction? c. Is it possible to list and model the emotions caused by reading? d. Have these emotions changed throughout time and space in Europe? e. Is it possible to sketch out the portrait of something like the European reader? Through a unique large-scale, user-friendly, open access, semantically-enriched investigation tool to identify and share groundbreaking evidence about 18th-21st century Cultural Heritage of reading in Europe, READ-ITproject wants to address these questions. It is a 3-yeartransnational, interdisciplinary Research and Development project funded by the Joint Programming Initiative for Cultural Heritage. READ-IT consists of a robust consortium of 5 academic partners from 4 European countries. The interdisciplinary collaboration between digital humanists, human and social sciences scholars and computer scientists investigates innovative ways of gathering new resources through crowdsourcing and web-crawling as well as linking and reusing pre-existing datasets. READ-IT thus aims to ensure the sustainable and reusable aggregation of qualitative data allowing an in-depth analysis of the Cultural Heritage of reading. The corpus encompassed is a rich human archive in multiple media and languages depicting a transaction between reading subjects and reading material from the 18th century up today, to web scraping and social media crowdsourced evidence of reading experiences. With regard to the work plan of READ-IT, the collection of use cases is considered to be the first significant milestone. Use cases collected in READ-IT are challenging the previous approaches adopted in projects such as UK-Reading Experience Database, the ANR-funded Reading in Europe: Contemporary Issues in Historical and Comparative Perspectives projectand the Listening Experience Database projectby going beyond the current state of the art of use cases and by requiring a much deeper analysis of sources. Specifically, in READ-IT we collected the following seven use cases: 1. Studying Contemporary Digital Reading Experiences Through Social Media. How can we document contemporary digital reading experiences? Are there any new ways of describing reading experiences out there?Can READ-IT help us preserve a collection of present-day heritage for the future? How can we find and collect data about such digital experiences? 2. Self-reflection. Which readerstend to report self-referencing as part of their reading experiences? Which texts prompt self-referencing more than others?3. The places where we read. What text genres are read in what physical environments? What physical environments are conducive to quality reading experiences as perceived/valued by readers?4. Four use cases examining a. the semi-automated extraction of evidence of reading from challenging sources written in inflected languagesthat have undergone significant historical change, using evidence in non-Latin alphabets, some in manuscript form. 5. whether such extracted evidence can be interpreted by historians of reading both at scale and in detailto address questions such as the influence of the state and of censorship, the development of reading in educational contexts, the emergence of a European identity in 19th-century readers. A preliminary analysis of use cases highlighted a challenging set of requirements to be considered in the modelling of READ-IT data model, such as the ability to study the embodiment of the reader in a physical context, the ability to identify the relation between reading and the personal life of the reader, the patterns emerging from the relation between the content, the type of sources and the socio-political context. This examination of a variety of case study facilitates a dialogue between the ICT and DH scholars who create the underlying data model and the HSS researchers who adopt it. This rigorous critique supports the development of a conceptual framework of reading, enables greater interoperability of data sources across different use cases, and allows scholars to address both macroscope and microscope questionsin terms of geographical scale, time frame and types of sources which are at the core of READ-IT vision. "
	},
	{
		"id": 15,
		"title": "Locating Absence with Narrative Digital Maps",
		"authors": [
			"Applegate, Matt",
			"Evans, Sarah",
			"Cohen, Jamie"
		],
		"body": " This paper outlines an interdisciplinary undergraduate digital humanities course, study abroad trip to Rome, Italy, and consultation with representatives from the non-profit organization Shoot 4 Changefocused on teaching students to visualize space critically. Utilizing the Spatial Humanities Kit, deployed via both Molloy College and Hofstra University, we showcase narrative geospatial humanities work, media production, and a simple mix of HTML and GeoJSON as vehicles for our students critical analysis. Our maps prioritize student experience, encapsulated in still images, written description, VR video, and vlogs embedded within them. Our course prioritizes methods for researching and unearthing embattled histories of public space, particularly within architecture, monuments, and urban design. Combined, our maps and critical framework result in a practice of teaching students to visualize cultural conflict that prefigures their experience of the space they inhabit--what is formative of, but currently absent or obscured from, the landscape they engage with. Context & Background What does it mean to map something that is no longer present? Inversely, what does it mean to visualize something that has been co-opted and ideologically obscured? These questions guided our course and directed our GIS project. Historical absence, charted through existing architecture, is perhaps easier to visualize. For instance, by researching and visiting the Domus Palazzo Valentini , a palace that showaces its ancient history through digital projection and glass floors that reveal earlier structures, students saw how architecture layers over time. What one sees in the present may physically elide what once stood in the exact location in the past. Politically speaking, ideological obfuscation, especially for American students abroad, is often more difficult to understand and visualize. Students were asked to consider contemporary political ideologies that co-opt Roman culture and redeploy it. In August, 2017, for example, famed historian Mary Beard characterized a BBC cartoon depicting a high-ranking black soldier as emblematic of a typical Roman family. As noted by The Telegraph , Beard argued that the cartoon was indeed pretty accurate, and, theres plenty of firm evidence for ethnic diversity in Roman Britain. What should have been an innocuous observation by an influential historian gave rise to a fierce attack on Beards credentials and character by the alt-right. This incident, among others, revealed a trend in alt-right thought now documented through a growing set of popular literature: the alt-right, particularly in the US, consistently appropriates Roman monuments, culture, and history in its effort to garner support for its ethno-nationalist agenda. This fact reveals itself over and again in the alt-rights digital footprint. Memes, video, and forum presence form a battleground for manifold acts of alt-right appropriation. Equally as compelling, however, are growing attempts to map and visualize alt-right violence, cultural appropriation, and concomitant modes of exclusion, both historical and contemporary. Several activist map making movements currently document where alt-right organizations congregate as well as the violence they commit. Italian ANTIFA groups have mapped hate crimes perpetrated by neo-nazi movements across the country, and similar projects called Fashmaps have taken shape in the US. Finally, Shoot 4 Change, a European non-profit focused on humanitarian reportage, has documented the formation and removal of refugee camps in Southern Italy, a direct result of the countrys closed border policies. Parallel to these events, critical GIS work in the humanities, especially geospatial work in DH, has been mobilized to document both present and past inequalities predicated on race and ethnicity. Most recently, the Torn Apart / Separados team visualized the effects of the USs 2018 zero tolerance policy for asylum seekers. DH scholar-practitioners at the University of Iowa have visualized histories of segregation in the US via Placing Segregation, and DH scholar-practitioners at the University of Richmond have visualized events leading to slaverys end during the American Civil War via Visualizing Emancipation. In so many words, DH utilizes digital maps to visualize violence, appropriation, and exclusion so as to speak back to dominant histories of national formation and containment. Taking inspiration from these projects, this paper showcases approachable entry points to humanities GIS work for undergraduates and mobilizes them toward the critical visualization of space. We are specifically interested in the act of locating absence via digital mapping technologies--researching, exposing, and visualizing what has been obscured over time and excluded by nationalist formations in public space, both physical and imagined. Our paper offers multiple ways in which spatial humanities methods can be made accessible to a layperson audience via interactive and accessible media production tools that document a present space and time as it relates to an array of social, political, and cultural issues. Our focus on pedagogy is primary, and to draw a stark divide between our critical approach and the pedagogical methods we deploy would be anathema to our project-based work. Map & Production Methods Our student map of Rome is an outgrowth of prior international trips and deployments of the Spatial Humanities Kit. It demonstrates competence in three areas: basic knowledge of HTML and GeoJSON, media production, and critical visualization. For ease, we refer to each theme by its essential function in the kits deployment: site, narrative, and knowledge produced. Site: Student training and historical work began months prior to our departure for Rome. Students were introduced to historical texts, maps, and virtual exhibits of the locations we would eventually visit. Concomitantly, students were introduced to contemporary political movements, both US-based and Italian-based, that prioritize appropriation and exclusion toward nationalist goals. On location in Rome, students were prompted by push notifications sent to their phones to reflect on absence and exclusion in one of three areas, depending on the site they inhabited: migration, labor, and infrastructure. Students visited tourist sites, famous monuments, as well as lesser known locations tied to contemporary political turmoil to reflect, gather data, and ultimately visualize absence and exclusion. Narrative: Our focus on media production borrows methods from Jason Farmans Mobile Storytelling and non-profit Shoot 4 Change. On the one hand, students were asked to incorporate two methods into their documentation process. First, following Farmans method for mobile storytelling, students were asked to account for multiple aspects of the locations they documented, including its histories, cultural conflicts, communities, and architecturesand makes these aspects foundational for the experience of the space. On the other hand, students consulted with Shoot 4 Change to gain awareness around the sensitivities and necessities of digital storytelling in a global context. In addition to still images taken by students of their experiences, students created vlogs and VR video to capture the times and spaces of their experience, guided by our three areas of reflection above. Knowledge: Our final area of focus, critical visualization, is an outgrowth of combining site-specific media assignments in Rome with digital mapping technologies. Our maps construction combines simple methods for data collection and an introduction to basic coding. Prior to our departure, students learned best practices for organizing and archiving content on three platforms: Google Drive, Omeka, and cPanel. Students were also introduced to video production and editing technologies, while more advanced students focused on the production of VR video. Combined with their narrative training, our students produced a map that both accounts for their own situatedness as American students abroad as well as Romes multilayered history. Students draw out Romes diverse cultural history in particular as a means of visualizing what is excluded contemporarily by those that would co-opt it for authoritarian gain. Outcomes As part of our presentation, we will make our syllabi, methods, and maps available to our audience and discuss the importance of digital mapping projects that prioritize criticality. Our work underscores how historical research, media production, and simple mapping techniques lead undergraduates to view public space as a multilayered and ideologically driven. We demonstrate how digital map making can create critical awareness of exclusion and ideological obfuscation as authoritarian power takes shape. "
	},
	{
		"id": 16,
		"title": "VR Video Production for Interactive Digital Maps",
		"authors": [
			"Applegate, Matt",
			"Cohen, Jamie",
			"Evans, Sarah"
		],
		"body": " This poster session showcases a combination of gear, open source code, and teaching materials for producing VR video experiences that correspond to narrative GIS projects. Derived from multiple international trips utilizing the spatial humanities kitvia Molloy College and Hofstra University, the materials provided at this poster session form a full suite of resources to create, analyze, and share the integrative possibilities of combining GIS mapping projects with VR video production. The following high and low cost kits will be available for faculty to interact with at DH 2019: High Cost Kit: GoPro Fusion 360 degree camera; Sennheiser AMBEO VR 3D Microphone; VariZoom Chickenfoot tripod, and Garmin eTrex 20 GPS Low Cost Kit: Samsung 360 camera; Zoom H4N Handy Recorder; VariZoom Chickenfoot tripod, Garmin eTrex 20 GPS or iPhone with Google Maps. Storage for both Kits: Class 10 U3 micro-SD cards; computer for downloading and backing up footage, backpack with side pocket and lashings for tripod to be attached externally. Code: See the Deployment section of The Spatial Humanities KitProject Description & Framework: VR and GIS integration is perhaps most commonly associated with urban planning projects and 3D modeling. Esri CityEngine, for example, allows users to design and model 3D landscapes of entire cities, subsequently allowing them to experience their designs via a proprietary app. Contra 3D modeling, VR video gives the maker the ability to capture live events that correlate to points on her map, allowing for immersive experiences of present circumstances. Our proposed poster session is meant to enhance GIS related work in the humanities. We do so by showcasing how VR video experiences can correspond to map projects using GeoJSON in correspondence with basic web design. Simply put, we connect VR video to a web deployable map that is easy to use and share. The work emphasized in this poster session prioritizes three approaches to narrative-based GIS projects that deploy VR video: mobile storytelling, accessibility, and site specificity. Each emphasize a sense of place that is meant to move beyond the 2D experience of the map through VRs immersive possibilities. VR video augments mobile storytelling techniques by design. In his 2018 Storytelling for Virtual Reality, for example, John Bucher looks specifically to immersive theatre and the theatre of the oppressed to contextualize this feature of VRs narrative potential. VR experiences ask the viewer to become a participant in an immersive digital environment--a spect-actor in Augusto Boals terminology--requiring sound effects, performers voices, and even recorded music to structure their experience. These necessities immediately lead to questions of accessibility in VR video production, insisting that the maker consider how auditory cues make demands on the users body as she or he participates in the narrative. Finally, VR video can give further insight into the place and time a map represents. Site specificity, a concept prized by mobile storytelling communities, embraces the a location, including its histories, cultural conflicts, communities, and architecturesand makes these aspects foundational for the experience of the space. It asks that the viewer become a participant in a narrative of a particular place and of a particular time, but also places a demand on the maker to integrate a sites locative features into their VR video production in ways that might be accessible to all. When combined with narrative GIS projects, these three methods for producing VR video create an experience of a space that extends beyond its 2D visualization. It opens the map up to questions of design and interactivity through which geospatial data might be experienced differently. Markers on a map become access points to scenes of participation. As a result, this poster session is not proscriptive in its method. It aims to feature a set of possible relationships shared between data, narrative, and production that faculty can take and make their own. Interactive Experience: Our proposed poster session will offer faculty the opportunity to experience VR made for narrative-based digital maps and a tutorial for producing VR experiences with accessibility in mind. Faculty will also be able to access the gear in both high cost and low cost kits, maps to which VR video corresponds, and instructional materials outlining each piece of gears use. In addition, we will offer faculty syllabi, access to the Molloy and Hofstra University projects, as well as the source code for our maps. "
	},
	{
		"id": 17,
		"title": "A deep learning approach to Cadastral Computing",
		"authors": [
			"Ares Oliveira, Sofia",
			"di Lenardo, Isabella",
			"Tourenc, Bastien",
			"Kaplan, Frederic"
		],
		"body": " Introduction Among all the diverse typologies of administrative systems, the fiscal-cadastral sources retracing the ownership of lands are undoubtedly the richest and, in a sense, the most coherent records. The need for efficient tax collection processes pushed different administrations and political regimes in the 18th century to develop scale-invariant coherent property tracing system. These systems were progressively generalised at the turn of the 19th century with the abolition of privileges for certain classes of citizens and the establishment of republican administrations for managing cities and countries. The cadastre, by introducing a constant collection of taxes calculated on a fixed percentage, defines a new conception of the city and of the urban space. It marks the transition from the Ancient Regimes to the Modern State. The introduction of a constant, proportional and impartial tax determined a new conception of the city, adapted to statistical computations. In some sense, for the first time, the cadastre transforms the city into a computational object. We use the neologism cadastral computing to refer to the operations performed on such primary sources. This article presents a generic approach for processing automatically the information contained in the Napoleonic cadastres. The cadastres established during the first years of the 19th century cover a large part of Europe. For many cities they give one of the first geometrical surveys, linking precise parcels with identification numbers. These identification numbers point to register lines with the names of the parcels owners. As the Napoleonic cadastres include millions of parcels, it therefore offers a detailed snapshot of large part of Europes population at the beginning of the 19th century. Figure 1. Identification numbers in parcels and in the register To develop a generic approach adapted to the processing of these administrative documents, one needs to solve two difficult challenges:developing algorithms capable of robustly segmenting maps into parcels and administrative tables into cellsdeveloping solutions for transcribing handwritten text containing people or places mentions and identification numbers. Until recently, these two problems were considered much beyond the state-of-the-art. The results of this article are based on the important progress recently made on both issues, using deep learning architectures. In 2017, an initial study on cadastre extraction showed promising results in parcel extraction and identifier recognition. However, it was entirely designed as an ad hoc pipeline fine-tuned for a particular cadastre. A year later, a generic deep-learning segmentation engine, relying on a convolutional neural networkdemonstrated that it was possible to design a generic architecture to segment many typologies of documents. In this article we use the genericity of this approach to develop the first fully automatic pipeline to transform the Napoleonic Cadastres into an information system. Method The automatic processing of the cadastre maps aims at extracting the parcels as geometrical shapes and also at transcribing the parcels identification numbers. It is composed of three main steps: 1. Training of the deep neural network on manually annotated data 2. Segmentation of the maps into meaningful regions and objects 3. Transcription of the identification numbers Our segmentation network is a fully convolutional neural network inspired by the U-Net architecture. It uses a ResNet-50 pretrained networkas encoder, which speeds up the training, reduces the amount of training data needed and helps generalization. The full details of the architecture and open-source implementation can be found in. The network is trained to extract the parcel contours and text using annotated data from the Venetian cadastre. The training data corresponds to roughly 1/3 of one map sheet among the 26 maps of the city of Venice. Figure 2. Sample of training data for cadastre maps segmentation. Parcels contour are in red, text is in green. The transcription network is a convolutional recurrent neural networkwhich produces a chain of characters when given an image segment containing text. The network is trained on samples of numbers from the Venetian archives and on numbers synthetically generated with MNIST digits. Figure 3. Example of training data for the transcription system. Left: synthetically generated numbers; right: numbers from the Venetian archives. The segmentation model obtained after the training is able to predict the parcel contours and text region at pixel level. Watershed by flooding algorithmis applied on parcel contours predictions, which allows the extraction of parcel objects as polygonal shapes. Text regions are cropped, horizontally aligned and converted into grayscale image segments for further processing by the transcription system. Figure 4. Output of the segmentation network. Left: text extraction; right: contour extraction. The image segments containing text are fed to the transcription network, which outputs a prediction of a number, i.e. a chain of digits. Each transcription is then linked to its corresponding parcel. The contours of the parcelsare saved as polygonal shapes and are exported into JSON format. In our case, since the images have previously been georeferenced, the coordinates are exported as geographical coordinates and can therefore directly be imported in any geographic information system. Results Two evaluations are performed in order to assess the performances of the system: the geographical accuracy of the extracted parcels and the transcription of the identification numbers. Figure 5. Visualization of the results of the automatic extraction of parcels. The red rectangle indicates the parcels used as training data for the segmentation network. Geometrical evaluation The number of geometrical shapes extracted and manually annotated are listed in Tab. 1. After a filtering step, which keeps only shapes which area range from 2 m 2 to 15000 m 2, the total number of extracted parcels is 28711, among which 18138 contain a transcription. Table 1. Total number of geometries in the automatic extraction and manual annotation. The first three rows relate to automatically extracted parcels, the two last rows show the statistics for manually annotated parcels. Geometries Number Geometries extracted automatically 31 342 Geometries remaining after filtering 28 711 ---- with ID number 18 138 Manually annotated geometries 16 946 ---- with ID number 15 634 The quality of the parcels extraction is evaluated by measuring the intersection over unionbetween the geometries produced automatically and almost 17000 manually annotated shapes. Precision and recall with three different IoU thresholds 0.5, 0.7, 0.9are reported in Tab. 2. The recall value shows that a large majority of parcels are extracted. The low precision value is mainly due to the incorrect extraction of streets, squares, canals, etc. that are currently not filtered out. Figure 6. Example of false extraction of streets and canalsTable 2. Evaluation of the geometrical shape extraction with different Intersection over Unionthresholds. IoU Correct parcels Precision Recall t=0.5 15 999 0.557 0.944 t=0.7 15 292 0.533 0.902 t=0.8 14 440 0.503 0.852 Transcription evaluation We assess the performance of the transcription of parcels identifier numbers by computing the number of correct predictions and report the precision and recall values in Tab. 3. The current method assumes that the identifiers are located within the parcel, thus, identifiers partially or completely outside the geometrical shape are not correctly transcribed, resulting in a lower recall. Figure 7. Examples of identifiers numbers outside or partially outside the parcel. In order to increase the precision value and since we can assume that spatially close parcels will have numerically close identifiers, we tried to discard false predictions by determining if a transcription was plausible or not, using information from its spatial neighbourhood. Thus, a transcription is considered as an outlier if thedifference between the predicted number and the median of its 5 neighbouring transcriptions is greater than 10. This results in a significant increase in precision, but at the expense of a decrease in recall. Table 3. Evaluation of the transcriptions of parcels identifiers numbers Correct transcriptions Precision Recall Transcriptions 11101 0.612 0.710 Transcriptions after outlier detection 8070 0.927 0.516 Perspectives This initial study demonstrates on a particular case that 90% of the urban geometrical structure and more than 50% of a city population can be automatically remapped with high precision using only generic pipelines. Even if these numbers need to be confirmed on the basis of other case studies, the genericity of the methods used makes us optimistic about the possibility of conducting a large-scale study in the coming years. Such datasets call for a confrontation with the large number of historical hypotheses that have been formulated on the urban society at the beginning of the 19th century based on much smaller sets of evidence. Thanks to the standardisation processes of the Napoleonic administration, we hope in the coming months to extend this systematic processing beyond Venice to a large part of Europe. "
	},
	{
		"id": 18,
		"title": "Cultural Analysis of Spoken Linguistic Signalling: A Pipeline for the Alignment of Audio, Text, and Prosodic Features",
		"authors": [
			"Arnold, Taylor Baillie",
			"Ballier, Nicolas"
		],
		"body": " Linguistic elements are known to be powerful signals for social categories such as class, race, education, political affiliation, and gender. Significant research has been conducted within the field of digital humanities to explore the ways in which language function to form communities across large corpora. The vast majority of work on linguistic signalling in the digital humanities has focused on the analysis of print culture due to the availability of large textual datasets and readably available methods. Spoken language, however, is known to vary considerably within communities, even when they share a common written language and dialect. Phonetic features such as tone, rhythm, and phoneme variation all serve to signal social identity. Methods for collecting and studying such variation offer, therefore, important insights into linguistic signaling that fail to be recognized by the study of text-only corpora. In this poster, we present a general pipeline for the construction, alignment, and analysis of spoken linguistic data. Our pipeline uses a combination of open-source tools in the R programming language and will be made available as an open-source toolkit through GitHub. The goal in our alignment workflow is to produce a single table collectively representing each of the elements collected in the multiple annotations. As the smallest unit of analysis, we chosen to align the corpus at the phoneme level. Other larger linguistic units—such as syllables and words—and metadata are simply duplicated across the relative phonemes. Unique identifiers for each unit are also included, allowing for reconstruction of the original annotations. Once the data was collected as a single table, we were able to compute new lemmatised word forms, part of speech tags, dependency relationships, and named entities. As a way of illustrating how this linguistic data pipeline is able to produce new scholarship, the poster focuses on an application to a corpus of spoken British English curated by the French-led Aix-MARSEC project. The dataset provides features for analysing vowels, pitch, rhythm, phonosyntax, for prediction of phrase breaks for text-to-speech systems. It has even been used as a baseline for psycholinguistic experiments. In our analysis, we suggest that we can contribute to a finer-grained analysis of cultural and situational factors on the prosodic hierarchy by taking into account the original annotations of the corpus and adding new layers. We synthesize the earlier stages of the corpus, from the Spoken English Corpus to the Aix-MARSEC speech database. Our poster lays out two experiments: the analysis of major and minor boundaries acknowledged in the corpus on the basis of a multidimensional analysis of the different subgenres of the corpus and of its prosodic and syntactic annotation when analysing the final nuclear contours of the prosodic units. Results of the distribution of the main intonation valuesacross the final tonal segment types in final positions according to discourse genres are shown in Figure 2. As, Brierly and Atwell explain, prosodic parsing can be based on the speakers desire to highlight specific aspects of the syntax producing a break after the item she wishes to highlight as in ...The idea that its important | for developing countries to become self-sufficient | in food | is widely | and uncritically accepted | not just in Brussels; | but from the orthodox economic standpoint | its without foundation... whereas the syntactic model would predict a break after idea and before to. Highlighting as a strategy means emphasizing the role of adjectives in final position of the intonation unit. The relative proportion of adjectives in final position of minor units should be monitored in relation to this highlighting strategy. The dominance of Higherpitch targetsfor minor units confirms our previous observation as does the clustering of Same and Bottom for major units. Our 3-gram analysis of final pitch targets in intonation units reveal phonosyntactic patterns. Considering the number of S-S-S sequences, a 4-gram analysis might be more relevant for the definition of the span of the pitch targets that characterize these units. Figure 3 illustrates a clear gender difference in tonetic stress marks and intonation unit. An overall patterning of major units with tonetic stress markings suggesting finalitywhereas rises co-occur with minor units, marking continuity. Figure 1. Example of the input dataand aligned corpususing our alignment pipeline. Figure 2. Distribution of the main intonation values in final positions according to discourse genres. Figure 3. Distribution of tonetic stress marks by gender and intonation unit. "
	},
	{
		"id": 19,
		"title": "Tracing the Development of Digital Humanities in Australia",
		"authors": [
			"Arthur, Paul"
		],
		"body": " The field of digital humanities has expanded rapidly over the past decade, with new centres, associations and activities proliferating worldwide, including in Australia and New Zealand. Digital research in the humanities is linking scholars and practitioners across Australasia, Asia-Pacific and globally. In Australia, government policies for research funding and infrastructure, including most recently the 2016 National Research Infrastructure Roadmap, have acknowledged the importance of digital humanities as a collaborative framework that connects the arts and sciences and is bringing together researchers and information technology experts with collecting institutions and the public to address complex social and cultural challenges in new ways. This paper traces the development of digital humanities in Australia, with reference to major projects and events leading to the founding of the Australasian Association for Digital Humanities. It discusses national exemplar projects, as well as significant activities and initiatives that formed a basis for the Australian field. It outlines the history of the establishment of aaDH as a regional association, reflecting on its directions over the past decade, and describes the parallel development of large-scale infrastructure that has supported the fields further growth. Looking back, it becomes clear that many projects and activities were pointing in the same direction without necessarily being linked directly, but as they progressed and connected with each other, the dispersed and disparate work across disciplines, institutions and sectors began to cohere with a common purpose—and from this confluence the Australasian Association for Digital Humanitieswas formed. A range of large-scale digital projects and events provided the key context and impetus for the development of the digital humanities field in Australia. Projects discussed in this paper include those of the Centre for Literary and Linguistic Computing at the University of Newcastle; the Archaeological Computing Laboratoryat the University of Sydney; the eScholarship Research Centre at the University of Melbourne; the Consortium for Research and Information Outreachat the Australian National University; AustLit; AusStage; Design and Art Australia Online; Paradisec; the Australian Dictionary of Biography; and the Trove digitisation and aggregation project of the National Library of Australia. As is the case in many parts of the world, such institutional activities tend to be associated with particular research centres or groups, and are typically collaborative enterprises. There were many centres, teams, projects and individuals that played a part in the long-term development of digital humanities in Australia prior to the establishment of the Australasian Association for Digital Humanities. The intention is to provide a representative sample. The paper highlights the role of foundational projects and events for the development of the digital humanities field in Australia, and in this context, it traces activities of the Australasian Association for Digital Humanities as a peak body representing the interests of researchers and connecting them with an international community. aaDH has been active for almost a decade and is now expanding its engagement with the next generation of scholar-practitioners in universities and the GLAMsector in Australasia. Meanwhile, since the associations establishment, the field has expanded exponentially to become a major mode of connection between research communities operating in very different national contexts, serving as a means of global linkage, promoting and opening up possibilities for a higher degree of international collaboration. In Australia, as in other parts of the world, libraries have played a pivotal role in supporting the evolution of digital humanities as a field and set of activities that spans academia and GLAM. National and state digitisation programs led by collecting institutions are increasing researcher and public access to a wealth of otherwise hidden resources. Institutional projects have helped to determine priorities and interests for the field as a whole and to foster communities of practice. In historical and disciplinary terms, many can be linked directly back to the humanities computing tradition that predated and provided the foundation for digital humanities as it is now known. Leading projects have facilitated cooperation across geographical, cultural and disciplinary boundaries and integrated data in ways that enhance capacity, scale and accessibility. Through these characteristics they have been able to stimulate, generate and support new kinds of research activity. Smaller-scale projects can be equally innovative, and in the Australian context they have also contributed to the expansion of the digital humanities field over time. However, the aim here is to highlight a selection of the most visible and active projects that can be regarded as iconic exemplars. They are large-scale collaborative initiatives that have been influential in the field, and through their high profile and long reach, they have played a significant role in raising awareness of new digital approaches and possibilities. Linking the examples discussed in this paper is a common theme: they all have the goal of capturing, preserving, building upon and articulating the richness of Australian culture and history for current and future generations. Digital humanities points to a future in which researchers will be able to utilize comprehensive global data from many different sources and countries, and across languages, to form diverse and inclusive infrastructures and methods for data sharing and analysis, to further advance knowledge and understanding and to develop new skills in the broad fields that the humanities and social sciences represent worldwide. "
	},
	{
		"id": 20,
		"title": "A National Library’s digitisation guide for Digital Humanists",
		"authors": [
			"Atanassova, Rossitza Ilieva"
		],
		"body": " This short paper will give practical advice about the British Librarys digitisation planning process for scholars who wish to use digitised resources in their research. The information will help scholars understand the institutional context, the roles involved in digitisation, the preparation stages and documentation required, typical timelines and the decision-making that happens at different stages. With this knowledge it is hoped that Digital Humanitiesscholars will be better prepared for the process and will factor it in their research funding proposals. They will also gain an understanding of the Librarys considerations and policy for making available for reuse existing digitised resources and how scholars could request this for their projects. In making the policy and processes at the institution more transparent, the presentation will expose some of the hidden labour undertaken by cultural heritage staff to enable DH research, as well as the opportunities to use the research outputs for the enrichment of culture heritage collections. Institutional context The British Library has a mixed-economy approach to drive digitisation and works with external partners from across the commercial, academic and GLAM sectors. The Librarys collections and areas of expertise are wide-ranging and we have a significant experience of collaborating on research projects in the humanities and other disciplines. Collaborations with DH scholars contribute to the Librarys digitisation priorities to ensure a more complete digital offering and an enhanced research potential of the collections to the benefit of future users and audiences. A timely engagement with the Library means proposals are carefully reviewed by relevant staff and lead to mutually beneficial outcomes. Digitisation planning process New proposals for digitisation are assessed and approved by a group of stakeholders representing strategic and operational teams at the Library. The digitisation planning process involves significant information gathering through the coordination of a number of assessments by different teams to establish the suitability and readiness of a collection for digitisation. The data captured relates to the provenance and physical state of the collection, level of descriptive metadata, copyright status and sensitivity issues, evaluation of the imaging process and necessary preparation of the material, post-processing and output file requirements, resource capacity and scheduling, and overall costs. During the final stages of approval strategic considerations are made about the optimal approach for digitising the collection, its fit with the Librarys overall digital offering, the most appropriate partnership model, opportunities for the enrichment and innovative use of the data, and the value for the intended users groups. Legal compliance is an essential stage in the digitisation planning and approval of new proposals, and standard due diligence procedures are in place for undertaking copyright assessments and rights clearance, data protection and sensitivity checks, and the assignment of usage terms. Facilitating easy access and onward reuse of the digital collections is set out as a clear objective in the Librarys long-term vision and strategy, and where appropriate and permissible digital content is released under the most open licences. The paper will outline the Librarys policy with regards to access and reuse of its digital content, the process in place and the stakeholders group involved. Scholars will understand how decisions are made and will be able to apply this knowledge when planning for the use of the Librarys digitised resources in their research. Challenges and opportunities Digitisation at the British Library provides an excellent opportunity for staff to contribute to digital culture heritage studies. This paper will bring to light some of the research and innovation that underlines the digitisation activities and how this benefits scholars. Despite efforts to standardise the digitisation process, the range of formats and content types in the Librarys collections, the items dimensions, age and condition present challenges and require much original thinking and experimentation in order to create the best digital surrogates. Careful investigation and evaluation of the digitisation approach from the outset guarantees that digital assets will be of the highest quality and long-term value. Therefore, decisions made in the planning process about the preparation of material, use of equipment, image capture and properties, post-processing and presentation influence the outcomes of digitisation and would have an impact on how the digitised resources are used in research. With its vast and significant holdings the British Library plays a major role in supporting digital research in the humanities. Members from different teams across the Library, including the Digital Scholarship department, contribute to the digitisation process and enable the scholarly use of digitised resources. Whilst DH projects often help fund new digitisation and make the Librarys collections more accessible, they generate a deeper understanding of culture heritage and enable the enrichment and reuse of digital collections. In explaining the institutional context, the digitisation planning process and the involvement of different stakeholders, this short paper will help scholars appreciate the significant investment in digitisation by the Library and also the opportunity DH projects present for the Library to develop its collections. In this way research collaborations between culture heritage institutions and DH scholars will be strengthened, ensuring that outcomes are of greater mutual value and paving the way for future innovative use of digitised resources. Bibliography British Library. Digital Scholarship. https://www.bl.uk/subjects/digital-scholarshipBritish Library. Heritage Made Digital. https://www.bl.uk/projects/heritage-made-digitalBritish Library. Proposing a new research collaboration with the British Library. https://www.bl.uk/help/proposing-a-new-research-collaboration-with-the-british-library"
	},
	{
		"id": 21,
		"title": "Norse World – The Complexities Of Spatiality In East Norse Medieval Texts",
		"authors": [
			"Backman, Agnieszka",
			"Petrulevich, Alexandra"
		],
		"body": " The Norse Perception of the World project is creating an online, searchable index with mapping capabilities of foreign place names and other spatial references in medieval Swedish and Danish texts. East Norseliterature is a mine of information on how foreign lands were visualised in the Middle Ages: What places were written about and where? Are some places more popular in certain text types or at certain times? How do place names link different texts? Is there a shared concept of spatiality? How is space gendered? Spatial humanities, the spatialisation of literary studies, and cognitive mapping are growing fields within digital humanities, but the study of spatial thinking and knowledge in medieval Scandinavia and its development as an area of enquiry are hampered by a dearth of information on place names in literary texts. Any research aiming to uncover what pre-modern Scandinavians understood about places abroad requires as a minimum an index of foreign place names in East Norse literature, an infrastructure that has not existed until now. The overall aim of the project is to create an infrastructure in the form of an online, open access searchable index and mapping of foreign place names found in medieval East Norse texts. This is accomplished by a bespoke back-end MySQL database containing the data, Norse World, an interactive search and mapping resource based on the Leaflet library, and a REST-API, a separate back-end application that allows end-users to access the database. The REST-API uses JSON as its open standard format and is compatible with both GeoJSON and JSON-LD. The data are freely accessible to scholars and other interested parties around the world under a CC-BY 4.0 licence and can also be downloaded as a CSV-file, usable in for instance Microsoft Excel or offline GIS applications as QGIS. The data are extracted from a corpus of all Swedish and Danish literary, non-biblical medieval texts extant in a handful of medieval runic inscriptions as well as around 210 manuscripts, manuscript fragments, and early prints, such as romances, travel stories, pilgrim guides, saints lives, devotional literature, revelations, prayers, didactic works, and sermons as well as encyclopaedic works. Some of these texts exist in editions of varying quality while others are as yet unedited and housed in different archives and libraries in Scandinavia. The project is thus collecting data that are scattered and otherwise difficult to access, sorting and presenting them in a uniform manner. It also means that it is hard to estimate how many entries the finished database will hold. Outside of the East Scandinavian context several similar resources exist: Recogito, World Historical Gazetteer, and Icelandic Saga Map. One of the challenges of the project is to represent the complexities of the data accurately from an onomastic and philological point of view. First of all, geo referencing itself can be an issue. If a text was translated in the late 14th century, is preserved in a manuscript from around 1450, and was read until the end of the medieval period, it is impossible to decide what borders for instance France would have had in the context of a readers reception of the text. For this reason, among others, we have chosen primarily to make use of a modern gazetteer, GeoNames. Furthermore, just because Paris, for instance, is mentioned, that does not mean that the reader had any clear idea of where it was or how to get there, but its context would have impacted how it fitted into the readers world-view. A further complication is place name identification, both when a name is hard to interpret and when the same place name has different geographic meanings. The former can be studied by using the filter Level of certainty on the Norse World map. For the latter we create different locations by closely following the context of the text. If for instance a place, Paris, is called a lake, it will be distinguished from the city of Paris. Secondly, the different forms of place names and non-name spatial references play a major role in structuring the data. The Original form, which is closest to the source in presentation, is transcribed according to diplomatic principles and also contains the immediate textual context. To facilitate orthographic, morphological, and onomastic investigations there are two normalised forms based on the Original form. The Variant from is slightly normalised, for instance, place names begin with a capital letter but spelling variation is preserved. The Lemma form is normalised further so that spelling variation is not preserved. A new Lemma form is created when there is a new place-name formation. The top level is Standard form, usually a modern English form of the place name or spatial reference. The structure, Original form > Variant form > Lemma form, runs parallel for Old Swedish and Old Danish, but the Standard form ties them together. It is the Standard form that has assigned coordinates from GeoNames. In some cases the Lemma forms might be completely different from the Standard form, e.g. Bern and Verona for the city of Verona. By having a Standard form the user does not have to be aware of all possible variations of a place name or non-name spatial reference to use the resource. To add further layers to the data every Original form is also linked to information on the work and its dating and description; the source, whether a runic inscription, manuscript, or early print, and its dating, material composition, and repository; edition used; and links to available electronic digitised sources or editions. Every Original form is thus at the core of a complex network linking it to other medieval texts and sources, modern digital resources and linguistic information ready to be used by researches and students as a starting point to study the world-view presented in East Norse texts. "
	},
	{
		"id": 22,
		"title": "CAT tools in DH training",
		"authors": [
			"Baillot, Anne",
			"Barrault, Loïc",
			"Bougares, Fethi"
		],
		"body": " Introducing Digital Humanities training in philology curricula seldom involves Computer-Assisted Translation, despite the fact that students in modern philologies are likely to use such digital methods or work environments during their training time, or even end up using them on a daily basis in their future job. Considered as a tool, CAT methods dont really stand in their own right in a DH curriculum. Considered in a user-interface perspective though, or as an approach allowing to reflect on the impact of machine learning methods on the Humanities, CAT methodscan legitimately be integrated in such a curriculum. One core aspect of this approach consists in reflecting the impact human translators can have, with their input in CAT tools, on such aspects as the enrichment of style, inclusion of context, idiomsin the learning process of algorithms, e.g. how the interaction of the Humans with the machine can be conceived as a reciprocal one. This poster presents the way we are integrating CAT-based translation classes training in a B.A. DH curriculum at the Institute for Modern Philologies in Le Mans Université. A first part of the poster is dedicated to the training setting. The two translation environments proposed to the students are MateCAT on the one hand, allowing them to learn how to edit a pre-processed translation, and multimodal approach with image descriptions[2], allowing to consider the impact of mental representations in the translation process. This infrastructure is based on the open source CAT tool MateCAT [3] equipped with machine translation systems trained with Moses[4]. The poster will show the role the Computer Science research department played in setting up a solid infrastructure for these two environments as well as the type of data that has been gathered from the students input. Another aspect of this teaching setting that will be presented in the poster is the way the CAT-tools class is integrated in 1) the whole curriculum and 2) a DH training concept that starts at B.A., continues at M.A. level and extends up to PhD candidates. The poster will also pay a special attention to the coordination between year-long teaching of regular classes and the organization of intensive training events such as summer schoolsor Translation Marathons like those held in February and March 2019 in Le Mans Université and to which B.A. students contribute. We will also discuss the reason why such a pedagogical experiment can be beneficial to small disciplines like German Studies are in France. One final aspect to present in the poster concerns the connection of the pedagogical setting to research. The input gained from the CAT-training allows to gather useful data for research projects in the Computer Science department. More specifically, working with the German Studies department allows to collect high quality Human translations in German-French/French-German. We are organizing a machine translation shared task at the fourth Conference on Machine Translationfor a language pair not involving English. The gathered corpus will be used as data to train state of the art machine translation systems involving deep learning techniques. In the philologies, the class allows to envision translation studies not solely as a specialty anchored in one linguistic area only, in which German, English and Spanish studies work independently from one another, but on the contrary to foster a transdisciplinary approach enriched by NLP-based research. "
	},
	{
		"id": 23,
		"title": "The Leonardo Code: Deciphering 50 Years of Artistic/Scientific Collaboration in the Texts and Images of Leonardo Journal, 1968-2018",
		"authors": [
			"Bardiot, Clarisse",
			"Broadwell, Peter",
			"Oiva, Mila",
			"Suarez, Pablo",
			"Wevers, Melvin"
		],
		"body": " In 1959, C.P. Snow gave an influential lecture on the split between the two academic cultures – the sciences and the humanities. Snow deplored the growing divorce between these worlds and their inability to enter into a dialogue. Nonetheless, around the same period, collaborations between art, science and technology began to develop. Snows address, which was widely publicized and commented upon, shows that these collaborations cannot be taken for granted. From the 1960s to the present, collaborations between art, science, and technology flourished, leading to the establishment of dedicated cultural institutions and programs, university departments, and academic journals. Among them is Leonardo, published by MIT Press, which aims to be a forum for interaction, collaboration, and the sharing of ideas between artists, scientists, and engineers across the globe. The spirit of the journal was inspired by the multifaceted activities or Renaissance polymath Leonardo da Vinci, and by the modern utopian vision of bringing together the best minds of humankind. Leonardo is the leading international peer-reviewed publication on the relationship between art, science and technology, making it an ideal dataset to analyze the emergence and dynamics of such complex collaborations. This research studies how interactions between art, science, and technology in Leonardo evolved between 1968 and 2018. We use the approximately 3,100 articles and around 7,800 illustrations in the 232 digitized issues of the journal and their metadata as our source data set. Each document is available as a PDF file with full-text searching and an accompanying XML file containing the documents publication metadata. While text analysis remains dominant, image and other media analysis is an emerging field that may offer unexpected insight, especially where the arts are concerned. This short paper relies on metadata, textand imagesto offer a fuller examination of the development and interactions networks between communities of researchers and artists in the journal. We applied multiple analytical approaches to the different materials in the dataset. For example, we analyzed illustrations, topical evolution and semantic transformations within the text, and cross-referenced respective results. We also studied the shifting dynamics of the author/institution network interactions collaborations over time. To study the visualizations, we isolated 7,836 illustrations from all issues of the journal using the PDF Figures 2.0 open-source package, which is based on PDFBox tool for parsing scholarly publications . We examined the features of the images in order to determine which modes of representingart, science or engineering were represented in the images. To this end, we used visual and conceptual clustering based on automatic feature extraction from the images. We ran the PixPlot toolon the images to perform the feature extraction and feature-based similarity clustering; PixPlot uses an Inception3 Convolutional Neural Network, trained on the ImageNet 2012 data set, and projects its similarity inference results into a two-dimensional manifold with the UMAP algorithm such that similar images appear proximate to one another . The algorithm allowed us to identify 16 individual clusters that could be considered as lying along a visual genre continuum from art to science, including archetypes such as technical diagrams, portraits, installations, and sketches. Some clusters are strongly oriented towards science or art, while others exhibit a mixture of both genre polarities, such as one that weaves kinetic art displays together with more scientific optical imaging devices like radar readouts. Some clustering is representative of periods of time: for example, illustrations from the 1970s exhibit a strong association of technical computer science and engineering diagrams with the notion of science. Other features remain constant from the 1950s to the present: for example, portraits of the people involved in the projects, photographs of the installations, standard scientific figures. To understand the longitudinal evolution of our image corpus we will adapt some functionalities of the SIAMESE toolkit , which visualizes changes in advertisements over time. As a next step, we will tag the images according to their clusters to determine, as with a latent text topic modeling approach, the proportion of each class of images that is present in each paper. This information can subsequently be paired with the information generated through text mining methods. We will use both unsupervisedand supervisedclustering techniques to determine to what extent a text fell within the arts or science domain SKlearn and Spacy were used for clustering and NLP. . Moreover, we will extract entitiesand other linguistic features from the text. Pairing the information gathered from the image and text analysis will allow us to answer questions like: in an article describing an art/science collaboration, is it customary to include representative art and science images, and if so, do these types appear in similar or dissimilar proportions? Also, we will be able to see whether specific actors or networks of actors were responsible for advancing particular visual styles. Figure 1: Overview of the Leonardo images dataset as visualized via PixPlot. "
	},
	{
		"id": 24,
		"title": "Complexities And Compromise – User-Centred Interfaces For Public Humanities projects",
		"authors": [
			"Barget, Monika Renate",
			"Schreibman, Susan",
			"MacCarron, Pádraig"
		],
		"body": " Drawing on recent experiences of the Letters 1916-1923 team in re-designing their project website, this paper will elaborate how changing user expectations, academic standards and special requirements of source material can be reconciled in the creation of database-driven interfaces designed for public humanities projects. Studies in the field of Human-Computer Interactionand usability, which first flourished in the 1990s, have been a major concern of the digital humanities for the past decade. Digital Humanities scholars agree that interfaces are part of the designand need to visually tell the projects story. But despite extensive theoretical discourse on user-centered designs, DH projects tend to fall short in practice. A lack of long-term funding and the interdisciplinary nature of DH projects often result in the development of websites, databases or mobile applications that make many compromises and are no longer updated when user expectations and technical possibilities change. In their 2016 conference paper Usability in Digital Humanities, Natasa Bulatovic and her co-authors found that more resources need be spent on testing of digital tools and infrastructure components and that it is especially important to conduct user tests covering the whole knowledge process. Jason Slipphas analysed a broad range of digital editions, online archives and biographical websites, pointing out their failure to permanently engage their users. One of the projects he discussed was the William Blake archive, which won the 2003 Modern Languages Association Prize for a Distinguished Scholarly Edition and was last updated in May 2015. Slipp notices that the site offers a vast amount of content, but [lacks] the aesthetic layout to help users find and process this information without frustration. In Slipps opinion, a consistent site navigation, basic information about the objectives of the project on the landing page, and a video or other visualization explaining the usage of the site are key elements of a successful digital humanities project. The Letters 1916-1923 project, first launched in 2013, also faced the challenge of re-designing its user interface in 2018 to engage a broad range of users from school children to professional researchers, both as end users of the project and as contributors. In contrast to some of the projects analysed by Jason Slipp, Letters 1916-1923 had to cope with a potentially unlimited, constantly expanding dataset whose metadata were not only added by members of staff but also by students and members of the public. Therefore, extensive data cleaning and careful data management was an inherent aspect of the website redesign, and compromises had to be made when presenting such complex sources. The fact that many women had different maiden and married names, that Irish republicans often used Irish variants of their English names, and that letter-writers fearing censorship and criminal prosecution resorted to pseudonyms had to be taken into account, and a balance had to be struck between respect for the variety of spellings and names used in the original sources and the technological necessities of a user-friendly digital archive. Onsite user testing provided insights how website visitors would interact with the site and how self-evident navigation would be. One important result of the user testing was that our clean and reduced design overwhelmed users with little IT experience. An additional section was added to the projects landing page to offer alternative ways of accessing information on how to participate in the project. Menu categories were renamed, and the number of keywords in browse filters was limited. Every step towards simplification was always balanced by the introduction of spaces for the ambiguity and complexity of content elsewhere. Feedback received from the focus-group revealed that the average website user is more willing to accept complexity at least in the design of search results. Frequent contact with sophisticated search engines in the world of online shopping, streaming services or digital newspapers have prepared users to handle large amounts of search results they themselves need to narrow down. When the Letters 1916-1923 project first invited members of the public to test its website in 2015, this type of display was rejected by most users. In only two years, the public expectations of how to navigate search results have changed considerably. By describing some of our initial problems and design choices in the light of recent user testing, we provide a case study from which similar public humanities projects may benefit . "
	},
	{
		"id": 25,
		"title": "Creating Complex Digital Narratives for Participatory Cultural Heritage",
		"authors": [
			"Basaraba, Nicole"
		],
		"body": " Many academic projects have produced digitised archives that provide public access, but public engagement is difficult to sustain, and museums are integrating digital media into their visitor experiences to engage the public. As Schreibmanargues, public participation in cultural heritage can generate new ideas and could challenge the top-down division between the researchers and them. In a move towards convergence of academia and the public, this paper proposes a novel approach to producing interactive digital narrativesthat combines expert-produced content and user-generated contentto create participatory cultural heritage narratives. Heritage is socially constructed and communicated; heritage meanings are not frozen in time, they are the result of constantly evolving values, beliefs, knowledge and traditions. Thus, it is important to allow heritage narratives to keep evolving as society changes which calls for public participation. Some contemporary constructions of heritage have been criticised as being overly selective, presenting glamourised, nostalgic presentations of the past and excluding many social groups within a multi-cultural society. The democratisation of knowledge-building is growing because audiences are not passively consuming content produced by expert historians and digital media has allowed anyone – with the access and ability – to produce, record and display representations of the past. The affordances of new media create a bridge for potential collaboration between experts and the public. Participatory heritage is a form of citizen scholarship, which could result in more engaged access to digitised cultural heritage content. It is hypothesised that IDNs present an opportunity to increase the breadth of the social groupsinterested in heritage, the types of historiesshared in the digital space, and allow for evolving interpretations and continued public contributions to cultural heritage narratives. Interactive digital narrative is a term used by scholars who work in the areas of intelligent narrative technologies, interactive drama, interactive storytelling, and narrative games. It encompasses the many different formats of digital narratives, such as interactive fiction, transmedia storytelling, web documentaries and video games. If IDN is separated into its three components – namely interactivity, the digital medium, and narrative – each part has historically been researched in different disciplines. However, IDN is a marriage of computation and narration that brings together perspectives from computer science and the humanities. IDNs combine technical developments, advances in artistic expression, and the expansion of analytical perspectives. IDNs can be created for different cultural heritage applications and audiences such as, museum experiences, location-based storytelling, or other heritage installations. Cultural heritage IDNs could help preserve local history, uncover lost cultural stories and customs, and allow cultural heritage tourists to explore their own and other cultures as facilitated by a narrative framework. Examining the potential of IDN formats for creating cultural heritage narratives involves many facets of complexity. Firstly, the concept of a participatory cultural heritage narrative is complex as it involves different interpretations, representations, perspectivesand participants. Secondly, UGC is difficult to verify for authenticity and factuality, thus the extent to which it is incorporated into a non-fiction IDN is challenging. Thirdly, the different perspectives or levels of complexity on the subject matter can be designed into an IDN system, which are designed to tell complex stories. More specifically, creating a cultural-heritage IDN involves considering: how the non-linear digital narrative should be structured, the type and method of delivery of multimodal content, and how content from different corpora – such as that produced by galleries, libraries, archives, museumsand governments, creative-industries, and UGC – can/should be incorporated into a new complex narrative to achieve a participatory cultural heritage while maintaining narrative control. This conference paper will discuss these complexities in the context of how cultural analyticscan be used to create remixed IDNs. A beta IDN system will be built as a reverse-engineered or remixed, web-based transmedia narrative drawing upon data acquired through cultural analytics of numerous sources of cultural heritage content. The results of user testing will inform whether the convergence of a top-down and bottom-up approach is feasible to produce a participatory narrative and inform whether IDNs may be sustainable forms for the future of cultural heritage storytelling. "
	},
	{
		"id": 26,
		"title": "A Halt for Hearsake: Towards a Digital Genetic Edition of Finnegans Wake II.2§6.",
		"authors": [
			"Bayramova, Halila"
		],
		"body": " This poster provides a short overview of the challenges of building a digital genetic edition of James Joyces Work–in–Progress . In doing so, it uses the textual genetic analysis of Finnegans Wake Chapter II.2 Section 6to demonstrate how the writers composition habits impact editorial decisions in data modelling. This process includes determining how exactly the text grew, what constitutes building blocks of the genetic development, and establishing an optimal level of granularity for draft representation and textual collation. This research addresses one of the central dissonances in digital editing between generalisation and customisation. The Text Encoding Initiative, for example, as encoding guidelines for digital editors, presents a certain way of dealing with text modelling, but it is by no means all–inclusive. The impetus of the present project is to promote a double movement toward not only process–over–product editions, but also to apply the same approach to the editorial process itself. This will contribute to the idea of a robust digital ecology, where the end–user has more control over the text, its constituent elements and analysis, engendering further research and recycling of the material. Operating from this standpoint, the poster demonstrates a test–case analysis of FWII.2§6. The inceptive assumption is that the architecture of a digital genetic edition should reflect the writers compositional style and the texts genesis. This requires an inductive analysis of the genetic material of FWII.2§6, which determines prerequisites for its effective digital editorial model. One of the key prerequisites is the level of granularity of the given data. Considering that the nature of the data is not only textual, i.e. the edition aims to represent not only the text of Work–in–Progress but also the manuscripts, their physical attributes and sequentiality, the established smallest unit must accommodate multiple functions from representation to collation. In particular, Joyces Buffalo–notebooks material and its transmigration from sources to the final text, which contributes to the greater part of his revisions, adds additional complexity to this task. Another challenge is the ghost draftsand how to account for their semi–existence in a database where their physical attributes are as important as the text they carry. Some of these attributes allow to reconstruct a better picture of the various aspects of Joyces compositional process, such as the time, agent, or even the purpose of textual changes. Notably, it is possible to employ Joyces multicoloured crayon deletions as a guide for distinguishing draft levels: how many times a manuscript underwent changes or so–called revision campaigns. These and many other aspects of the rich data set presented by the manuscript sources need to be ascertained before the data model of FWII.2§6 is fully functional. "
	},
	{
		"id": 27,
		"title": "Data Science & Digital Humanities: new collaborations, new opportunities and new complexities",
		"authors": [
			"Alex, Beatrice",
			"Alexander, Anne",
			"Beavan, David",
			"Goudarouli, Eirini",
			"Impett, Leonardo",
			"McGillivray, Barbara",
			"McGregor, Nora",
			"Ridge, Mia"
		],
		"body": " Panel overview David Beavan, The Alan Turing Institute; Barbara McGillivray, University of Cambridge & The Alan Turing Institute This panel highlights the emerging collaborations and opportunities between the fields of Digital Humanities, Data Scienceand Artificial Intelligence. It charts the enthusiastic progress of the Alan Turing Institute, the UK national institute for data science and artificial intelligence, as it engages with cultural heritage institutions and academics from arts, humanities and social sciences disciplines. We discuss the exciting work and learnings from various new activities, across a number of high-profile institutions. As these initiatives push the intellectual and computational boundaries, the panel considers both the gains, benefits, and complexities encountered. The panel latterly turns towards the future of such interdisciplinary working, considering how DS & DH collaborations can grow, with a view towards a manifesto. As Data Science grows globally, this panel session will stimulate new discussion and direction, to help ensure the fields grow together and arts & humanities remain a strong focus of DS & AI. Also so DH methods and practices continue to benefit from new developments in DS which will enable future research avenues and questions. This panel has been enabled by the establishment two years ago of a new DS & DH Special Interest Group, with all co-organisers forming part of this panel. The group brings together 40+ practitioners, researchers and organisations from academia, the commercial sector and the cultural heritage institutions, all with an interest in exploring the benefits to DH from DS & AI and vice-versa. This includes new work in areas such as advanced statistics, algorithms, natural language processing, machine learning and artificial intelligence coupled with big data, high performance computing and high throughput computing techniques. At the same time, the SIG has created new directions and prospects for the institute in humanities scholarship, highlighting the unique challenges brought about, such as greater human interpretation and a more iterative research process, the vast potential of data sources, variety of modalitiesof content and the social importance of research findings. For more information about the group, see https://www.turing.ac.uk/research/interest-groups/data-science-and-digital-humanities The panel is structured as a series of short papers, introduced and facilitated by one of the co-authors, all addressing different perspectives and personal reflections of the intersection between DH, DS & AI. Starting with access to source material, the first paper will discuss the future of archives, focusing not only on the opportunities digitisation can bring, but also on how AI can enhance the role of archives, archivists and of course their users. The next paper moves to advanced Machine Learning experiments as pedagogical tools, how these can help us understand the creative process in a digital realm and what the experiments can tell us about ourselves. The infrastructure to support DS & DH work is essential, and the third paper draws on extensive experience from the cultural heritage sector on building collaborations and the complexities of data available. The rise of these advanced digital methods and techniques has led to the need for advanced training for those working and providing access in the cultural heritage sectors, as discussed in the next presentation. A more personal reflection on the importance of interdisciplinary work in the DH, DS & AI space follows, with advice on what works and what can be improved. The final paper ends on a high, with the focus on the future, towards a manifesto of Data Science and Digital Humanities. There will be ample timeto engage the panel and the audience through a moderated discussion to close the session. What is the role of Data Science and Digital Humanities in the Future Archive? Eirini Goudarouli, The National Archives Trust has always been central to archives. However, the intangible record is fundamentally changing the landscape as well as the role of archivists and archival institutions. Undoubtedly, the emergence of new generation technologies is rapidly leading to an epistemological shift in archival science, or, to put it in Thomas Kuhns words to a scientific revolution, by moving from a relatively settled scientific framework to the urgent need for a profound change to its principles, methods and practices. For example, the use of new technologies, such as Snapchat, Google Docs, neural networks, blockchains, hashing algorithms, cryptography and the cloud have profoundly altered the nature of archives, by disrupting how information is created, recorded, captured, encoded, curated, shared, made available and used. These shifts require fundamentally new capabilities and approaches on how best to capture, preserve, contextualise and present increasingly digital public record. Therefore, the archivists relationship to emergent technologies needs to become multi-layered. We need to understand the digital landscape, and the changing nature of how society creates and shares records in the light of new generation technologies, and be willing to apply these new advanced technologies in our archival response to these changes. This sparks a new era for archives, as todays archivist must become equipped with emergent technologies as their own tools of the trade. As the nature of records and archives evolves more quickly and the digital contests long-standing archival practices, trust comes to the forefront of the discussion. One of the major questions related to this, is how archives retain the legitimacy they confer on the digital evidence they capture, preserve, contextualise and present. Archives and collection-holding institutions should rethink the nature of the record and our archival practices around the record in the light of digital and combine practical considerations with explorative research into infrastructure, methodology, tools, techniques and user requirements, drawing on innovation across cultural heritage, academia and relevant industries. As we moving forward to a future fully-functioning digital archive, we need to embed new generation technologies in our recordkeeping practices to help us manage our rights and responsibilities as we go about capturing, preserving, contextualising and presenting digital records. This paper will suggest that, in an era of AI-assisted recordkeeping, the need to demonstrate the adaptability, value and sustainability of archives has never been more acute. In this context, it will discuss the conceptual and epistemological challenges relating to trust and openness in rapidly evolving digital archives. The paper will also discuss the role of Data Science and Digital Humanities within the archival context and will argue that research plays a central role not only to inform but also to innovate around these challenges, helping to define future directions and lead to the shaping of the future archive. Ways of machine seeing: experiments in critical pedagogy Anne Alexander, University of Cambridge; Leonardo Impett, Bibliotheca Hertziana - Max Planck Institute for Art History This paper explores the theoretical, methodological and practical challenges of developing Machine Seeing experiments as pedagogical tools, using as a case study a set of experiments created for graduate workshops between 2016 and 2019. These workshops aimed to facilitate interdisciplinary encounters between humanities scholars and computer vision researchers, and were framed by engagement with John Bergers seminal work Ways of Seeing, which we see as offering critiques of visual ideology. In the spirit of Bergers original documentary, our design experiments aim to appropriate image-machinesin his case colour television, in ours Machine Learning systems) and turn them into critical pedagogical tools for the critique and re-understanding of historical and contemporaryimage cultures. The object of study is not merely digital or digitally-created images, but the ways in which computation informs our ways of seeing. In attempting to combine hermeneutic criticismwith theoretically-engaged software development - not by alternating between them, but by doing each within the other - our workshop model is in the spirit of Critical Technical Practice. It is precisely the difference ofcomputer vision systems which makes them ideal tools for the critique of visual ideology. The object of this critique is not only contemporary digital image-culture, but historicaland machine-generatedimages. We see this critique as fundamental to understanding creative practice in an age of computational automation, in order to develop methods of enquiry which render visible hidden social and technical dimensions of image creation and circulation, such as the production histories of commonly-used image training datasets, the mechanics of learning models, and the impact of factors such as personalisation in web search. Machine visual perception is explored through several performative experiments, developed in the course of the workshops by live-coding, with technical experiments and critical discussion developing in parallel. In one experiment, Generative Adversarial Networks, twin pairs of image-generating and image-decoding networks, are used to create infinite series of images from certain classes of dataset - Google image search-results from different professions, high-rent and low-rent photographs from rental websites, or classical paintings. In another experiment, object detection networks semantically identify segments of images, and live-swap the resulting crops. Our experiments encouraged workshop participants to see Machine Learning systems simultaneously as lenses and mirrors. In other words, ML systems are not simply devices through which we can view the world; we can make them show us something about ourselves. Our experiments thus attempted to reveal the ways of knowing encoded in ML systems through a kind of Verfremdungseffekt where the strangeness of machine visual perception causes viewers to question aspects of the composition and context of images which they previously took for granted. Like Bergers experiments discussing Caravaggio with school children, we utilised the GAN as a surrogate for a human naive observer in order to encourage workshop participants to temporarily unlearn implicit aspects of their own image culture in order to facilitate deeper human understanding. In search of the sweet spot: infrastructure at the intersection of cultural heritage and data science Mia Ridge, British Library This paper explores some of the challenges and paradoxes in the application of data science methods to cultural heritage collections. It is drawn from long experience in the cultural heritage sector, predating but broadly aligned to the OpenGLAM and Collections as Data movements. Experiences that have shaped this thinking include providing open cultural data for computational use; creating APIs for catalogue and interpretive records, running hackathons, and helping cultural organisations think through the preparation of collections as data; and supervising undergraduate and MSc projects for students of computer science. The opportunities are many. Cultural heritage institutionshold diverse historical, scientific and creative works – images, printed and manuscript works, objects, audio or video - that could be turned into some form of digital data for use in data science and digital humanities research. GLAM staff have expert knowledge about the collections and their value to researchers. Data scientists bring a rigour, specialist expertise and skills, and a fresh perspective to the study of cultural heritage collections. While the quest to publish cultural heritage records and digital surrogates for use in data science is relatively new, the barriers within cultural organisations to creating suitable infrastructure with others are historically numerous. They include different expectations about the pace and urgency of work, different levels of technical expertise, resourcing and infrastructure, and different goals. They may even include different expectations about what data is – metadata drawn from GLAM catalogues is the most readily available and shared data, but not only is this rarely complete, often untidy and inconsistent, it is also a far cry from datasets rich with images or transcribed text that data scientists might expect. Copyright, data protection and commercial licensing can limit access to digitised materials. Orphaned works, where the rights holder cannot be traced in order to licence the use of in-copyright works, mean that up to 40% of some collections, particularly sound or video collections, are unavailable for risk-free use.While GLAMs have experimented with APIs, downloadable datasets and SPARQL endpoints, they rarely have the resources or institutional will to maintain and refresh these indefinitely. Records may be available through multi-national aggregators such as Europeana, DPLA, or national aggregators, but as aggregation often requires that metadata is mapped to the lowest common denominator, their value for research may be limited. The area of overlap between computationally interesting problems and solutions useful for GLAMs may be smaller than expected to date, but collaboration between cultural institutions and data scientists on shared projects in the sweet spot - where new data science methods are explored to enhance the discoverability of collections - may provide a way forward. Sector-wide collaborations like the International Image Interoperability Frameworkprovide modern models for lightweight but powerful standards. Pilot projects with students or others can help test the usability of collection data and infrastructure while exploring the applicability of emerging technologies and methods. It is early days for these collaborations, but the future is bright. Computing in Cultural Heritage: adventures in training GLAM staff to support digital scholarship Nora McGregor, British Library The cultural heritage sector of galleries, libraries, archives and museumsshares a fundamental purpose to preserve and provide access to digital and digitised culture, and to support and stimulate the innovative use of these extensive digital collections and data in research. Professionals across the sector will often have come to their role, many years ago, having deep domain expertise in a particular academic subject, yet now find themselves with increased responsibility for assisting on the design and delivery of complex digital projects, without a foundation in computing to empower them. Though the GLAM sector is facing a broadening digital skills gap as rapid technological transformations impact nearly every role in the sector, there is much progress and innovation to report in the area of digital skills development in the area of supporting digital scholarship. From deploying simple scripts to make everyday tasks easier, advising on the building of new digital systems & services to support collaborative, computational and data-driven research, through to large-scale collaborativeprojects, todays cultural heritage professionals require, and most importantly, are actively seeking, a grounding in computational approaches to navigate this new landscape confidently. Through a diverse mixture of approaches, such as bespoke course development, guest lectures, network building, reading groups and hack & yacks, the British Library is finding a variety of innovative ways to provide their staff with the space and opportunity to delve into and explore all that digital content and new technologies have to offer in the research domain today. This paper will report on these latest initiatives, including a newly funded project to develop a Post-Graduate Certificate in Computing for Cultural Heritagerelaying experiences particularly from British Library, as well as other GLAM institutions, currently investing in upskilling their staff to support data science and the digital humanities. The paper will conclude with a discussion on matters of equality, diversity and inclusion, and report on efforts to identify and mitigate barriers for staff in taking up such digital skills training. Benefits and challenges of interdisciplinary Data Science and Digital Humanities research Beatrice Alex, University of Edinburgh Data scienceand digital humanitiescollaborations can be extremely successful if participants involved are willing to find common ground and value each others research and contributions. Speaking from experience, they often take time to develop. The following section summarizes lessons learnt at hand by one of our co-authors in a number of interdisciplinary projects and some insights overlap with those shared by Siemens et al., 2016, for example. Computer scientists tend to want to develop and evaluate algorithms or come up with new and interesting ways to visualize information. This requires data and in some cases manually annotated data. Humanists and social scientists tend want to prove or disprove hypotheses or study specific questions or queries using a particular dataset. Running projects successfully combining these goals requires willingness and compromise on each side. It is important to be clear about differences at the start and discuss how area-specific and joint dissemination will be handled. Project publications with many co-authors are something to embrace but they also need to be better recognized. It is sometimes worth running a pilot to gain common understanding and knowledge of what is possible computationally. This can help to attract further funding to be able to scale up research in a larger project. In practice, that is not necessarily how it works as one often gets involved in last-minute grant proposals and needs to make quick decisions about what is feasible computationally, which is far from ideal. Funding councils need to make more grants for interdisciplinary work availableboth nationally and internationally. Small network grants are not sufficient. Leading on from them, we need more funding calls like Digging into Data and the Trans-Atlantic Platform allowing participants from different countries to collaborate. To drive research and innovation and to encourage interdisciplinary work, such as that between DS & DH, national research funding bodies like UKRI must also provide similar opportunities on a national level. Once a project goes ahead, a lot of time is spent on data preparation. Often there is an understanding about the necessary datasets being available but complexities in terms of data access, data being spread across different collections or released in different formats and data quality are underestimated. For example, in the Trading Consequences and Palimpsest projects several large text collections were processed. Converting them into one common format took time. Even with common guidelines being advocated, in reality most collections come with their own format and metadata, or no metadata at all. Those who have found themselves doing data wrangling will know that one rarely is able to re-use an existing conversion script. However, this initial, tedious preparation step is necessary and the experience gained doing it helps to get going faster each time. Finally, computational methods are not always perfectly accurate. We need to inform collaborators about what technology, for example, a text mining pipeline, is capable of and how it can be used to assist scholarship, rather than replace it. When making a tool available, it is also important to report about the data it was originally developed for or trained on to avoid disappointment when used on data from a different domain. Sharing and being transparent about a technology can lead to positive outcomes and new interesting collaborations. Towards a Manifesto for Data Science and Digital Humanities Barbara McGillivray, University of Cambridge & The Alan Turing Institute; David Beavan, The Alan Turing Institute This paper takes the first steps towards a manifesto for the field at the intersection between Data Science and Digital Humanities. Over the past few years there has been an increasing level of activity in the area of computational humanities, quantitative humanities, and humanities computing, as testified by several initiatives, groups, workshops, and publications. Research in this area has focussed on developing new computational techniques to analyse and model humanities data. Digital Humanities work has partially tackled these questions, but its scope is broader, and has historically concentrated primarily on the creation of digital resources, editions and tools, as well as the application of existing computational methods to humanities data, rather than on new computational challenges. We believe that the development of cutting-edge computational research that aims to promote and answer new research questions, including methodological ones, in the Humanities deserves a specific and coordinated reflection. Such effort is needed to shape the scope of the field, suggest a methodological framework for it, and highlight strategic priorities to drive it forward. We propose to make this contribution in the form of a collaboratively-written manifesto. The structure of this document will first include a scoping section, which defines the types of challenges addressed by the field in question, such as: how do we automate and scale up tasks in Humanities research? How can we train machines to perform creative tasks? Further, the manifesto will briefly describe the current landscape, from the point of view of existing methods and approaches, projects, and institutional initiatives. Next, we will review the challenges and opportunities that characterize the research of this emerging field, to highlight areas of progress. Finally, we outline a future vision, providing a thought-leadership piece that suggests methodological frameworks and interdisciplinary models of collaboration to support the field, help it grow, and facilitate its broader dissemination. We will also provide grounding of an educational strategy to train the current and next generations of scholars and practitioners to operate in and interact with this field, and a reflection on the funding routes appropriate to support the research in this field. DH2019 is the premiere Digital Humanities conference worldwide, with many opinion leaders expected to attend. Therefore, we look forward to presenting an initial outline of the manifesto in this ideal venue, gathering feedback and stimulating a discussion amongst the panel and the audience. "
	},
	{
		"id": 28,
		"title": "The Intellectualisation of Online Hate Speech: Monitoring the Alt-Right Audience on Youtube",
		"authors": [
			"Tuters, Marc",
			"de Keulenaar, Emillie",
			"Kisjes, Ivan",
			"Bach, Daniel",
			"Beelen, Kaspar"
		],
		"body": " This paper looks at YouTube as a platform for the circulation of far-right pseudo-intellectual ideas and focuses on its alleged tendency to act as a machine for radicalization. Whilst social media platforms such as Google have promoted the notion of connectivity as an unalloyed social good, it has recently been observed that such connectivity, mediated, for example, by recommendation systems, is equally capable of strengthening communities bound by narrow preferences for the same information, including political ideas. Its resulting so-called filter bubbles would be creating communities bound by common aversion for users with different informational diets -- particularly political and ideological others. While a significant amount of attention has of late been focused on the general problem of fake news within digital humanities research, this abstract takes an empirically-focused approach to the broader, seemingly philosophical, problem of epistemological relativism in the landscape of contemporary social media. Specifically, it considers how it is that social media platforms, in this case, YouTube, has become a site of dissemination and development of ideas originating from far-right subcultures, particularly via the medium of pseudo-intellectual debate. Recently referred to as the Dark Intellectual Web, YouTube is today replete with what had in the past been referred to as balkanized speech markets. These extreme speech markets, as we will refer to them here, traffic in a variety of ideas considered to be too controversial to be discussed by the academic establishment. While they may be understood to have deep roots in the subcultural margins of the web, they can be seen to extend across mainstream social media platforms, and perhaps nowhere more so than on YouTube. But while YouTube can justifiably censor imminent threats of physical violence, extreme speech -- hitherto marginal -- rarely fits the platforms actionable descriptions of hateful content. This thus leaves watchdogs and authorities to doubt where, exactly, to place the distinction between expressions of ideological extremisms -- which have been further normalised into popular content, and which many platforms have sought to tolerate in deference to American free speech principles -- and simply illegal expressions of ethnic, religious and sexual hatred. While the latter discourse might perhaps be ignored if it remained on fringe websites, extremism experts and media scholars have argued that the design of YouTubes recommender system may provide users with the affordances to develop these ideas into inconspicuous intellectual expressions — making Youtube an accidental machine for normalising political extremes within closely-knit political communities. The problem in question, then, is at the intersection of computational linguistics and intellectual history. As hate speech has largely been defined as products or by-products of far-right political thought, and as content associated to this political culture has been largely normalised as popular content, to what extent would hate speech detection equate ideological persecution or censorship? The problem of censorship poses theoretical and methodological challenges. Social Media platforms, and their comment space in particular are notorious for their opaque censorship policies. Based on our preliminary results, however, we observed that videos are more rigorously monitored than comments. In a related study on the vaccine debate, we noted that extreme content, promoting the anti-vaxxers point-of-view, was more likely to be removed than pro-vaccines channels. Would it not be inadvertently aggravating contentions between polarised communities? Instead, how can hate speech detection be sensible to the intellectual history and transformation of ideas substantiating traditional hate speech? Discussions of ideas traditional to far-right political thought is widespread enough on Youtube to, as this paper will assess, call for computational techniques sensible to the ambiguity of such online vitriol. In this respect, this paper adopts an approach that might be considered a variation of reception studies traditions in studying extreme speech on YouTube. Our approach is a response to Lewiss analysis of far-right political culture on YouTube: it seeks to follow the audience as opposed to profiling the uploaders of content — the latter whom Lewis referred to as the Alternative Influence Network. While in dialogue with Lewiss research, we take issue with her use of the concept of influence — informed here by a critique from reception studies of the media effects tradition. With this point of distinction in mind, our paper looks at how we can detect hate speech terms uttered by channels and commenters on Youtube, as defined by a number of international hate speech watchdog datasets, by associating them to intellectual corpora having substantiated their discriminatory viewpoints. While racially discriminatory slangs -- to name the deeply inflammatory term, nigger -- may not be detectable on YouTube comments and video transcripts, terms originating from nineteenth-century biological racism -- negroid -- are instead abundant. We consider how and whether such terms appear in channels and commenters presenting known for debating their arguments in such pseudo-academic format. We then attempt to understand what happens when commenters who have previously been using terms associated to hate speech come into contact with intellectualised extreme speech markets. It is worth mentioning that YouTube has often been characterized as an important hub and source of intellectual resources for new far-right users and talking heads. Self-described sceptics and freethinkers engage in discussions of topics such as race and IQ or the role of Jewish elites in the new world order as matters of genuine intellectual concern. While the substance of these discussions appears as hateful or conspiratorial to both experts and laymen users, their participants tend to see their points of view as alternative, both ideologically and intellectually. Within the digital humanities, there is a growing interest in the automatic identification of political ideology and ideas, often relying on techniques created within the field of natural language processing. The latter discipline has tended to focus on the detection of hate speech as though it were a category distinguishable from normal or acceptable speech. Following Pohjonen and Udupas critique of this binary conception of hate speech, our approach is concerned with a broader range of equally problematic forms of speech that do not necessarily constitute hate speech. With this problem in mind, the abstract explores text mining techniques to monitor attitudes of users over time. This yields a computational lens on the behavioural effects of the platform, i.e., how the beliefs and behaviours of users shift as a result of the content they engage with. Our approach of following the audience thus differs from most YouTube research which centred on content creators and largely relies on standard metrics made available through the API. We thus map the debating culture of the new right on YouTube as a process resultingfrom the interaction of commenters and channels. By measuring the word usage of the audience of what has elsewhere be called the Alternative Influence Network or the Dark Intellectual Web, our approach may thus be understood as mapping the debating culture of the new right on YouTube from below in order to determine how audiences become associated to the cause. Methodologically, we have developed quantitative techniques for gathering all the comments made by individual usersin order to map their movement through and across YouTube channels. In collaboration with journalist and extremism expert Dimitri Tokmetzis, we have processed 1027 right-wing YouTube channels initially harvested using the YouTube Data Tool. The data was collected based on a list of actors that extracted from a database created by the Dutch right-wing watchdog organization KAFKA. The resulting set has a broad an internationalfocus, though the majority of actors are Anglo-American. The data retrieved via the YouTube API record consists of video titles, channel references, tags and comments. To analyse this content -- and establish the extent to which users agreed with extreme opinions -- we defined two vocabularies: one is a set of intellectual extreme notions originating from two, main sources. One source comes from tags in right-wing videos and commenters, e.g. eugenics, race and IQ and white identity. The second source is a collection of frequently mentioned terms from Metapedias article on race and intelligence. The second vocabulary is made of 267 common English language comments that indicated a commenters consensus with a video. Using such comments, we identified those commenters agreeing with videos tagged as extreme speech. We then identified the five preceding comments by those users, which we visualised in a directed network that indicates where and when users have commented in videos from our selected channels. To find movements between hate and extreme speech, we have developed a similar list of keywords to identify hate speech comments. We define hate speech based on a corpus of terms as collected by the hatebase.org, which bills itself as the worlds largest structured repository of regionalized, multilingual hate speech, and which has been compiled by a recognized Canadian NGO. We manually selected items from that database to make up the list. Applying these to comments allows us to compute extreme and hate speech scores for users over time. Comparing movements over time in terms of both hate speech and extreme speech will provide us with the data to test the hypothesis of whether commenters move from hate speech to extreme speech. Figure 1: Scientific racism and hate speech in right-wing comments and transcripts, 2006-2018 At the crux of the problem then is the attempt to define what counts and does not count as hate speech — which is itself a concept that has come under considerable criticism from the subject being studied in this case study. Our objective is thus to bring critical perspectives to those digital spaces where users consume and reproduce intellectual content. More specifically, our objective is to monitor pseudo-intellectual debates in the comment section and explain them in terms of user behaviour. It is thus our hope that this approach will help researchers concerned with hate speech to contextualise concepts having to do with various intellectual processes online, including radicalization, indoctrination, andeducation. "
	},
	{
		"id": 29,
		"title": "New Beginnings: Using Keystroke Logging For Literary Writing",
		"authors": [
			"Bekius, Lamyk",
			"Buschenhenke, Floor"
		],
		"body": " In this poster presentation we will report on a first collaborative pilot study within the ongoing five-year project Track Changes: Textual scholarship and the challenge of digital literary writing. The larger project aims to discover what the creative processes of present-day literary writers look like, and what the implications of their largely digital practices are for textual scholarships theories and methodologies. There have been a few trailblazers studying born-digital literary materials. But despite passionate pleas from them and others, the field as a whole has yet to embrace born-digital textual materials as research subject. None have so far used keystroke logging to investigate the literary process, which makes our project quite innovative. The aim of this poster presentation is to show and discuss our methodology and data, and to present a small-scale example of the insights we could gain. Although born-digital processes leave traces, the conventions of word processing programmes are such that immediate corrections have become invisible. Versioning has become a matter of personal preferences; which in some cases leads to a further decrease in evidence of the writing process. Related to this issue of changes in the type of traces a writing process generates, is the way in which the medium affects the process. From studies into non-literary texts, we know that digital writing processes differ from paper-based work processes – one difference is that there is more rewriting during composition. Inputlog We chose to work with the keystroke logger Inputlogbecause of its seamless integration into a normal writing experience. Once switched on, it logs writing activities performed in Word. Using keystroke logging, we are thus able to catch all traces of digital writing and editing processes, including immediate corrections, and including a separate version of the Word document at the end of every writing session. Project data This poster presents a pilot study that focusses on nine writing sessions of the Dutch author Walter van den Berg, which encompasses all his writing for one short story. This story was – together with three other stories – commissioned during our small-scale precursory study. The writing process of these stories was logged in Inputlog. In Track Changes we will analyse the writing process of the novel Roosevelt by the Flemish author Gie Bogaert – which contains 446 writing sessions – as well as 8 newly commissioned stories. Pilot study The pilot study shows how the logging data can help interpret revisions made during the writing process. By both focussing on small revisions and on the incipit of the story – the beginning of the story in the published text – we will try to examine if and how these small revisions can be related to the genesis of the incipit. Revisions Deletions and substitutions will be manually labelled and interpreted. Van den Berg deleted quite a bit of his freshly produced materials, sometimes iterating over multiple possibilities before settling. The deletions and substitutions within each writing session present a chronological story that does not follow a linear path through the unfolding text. Thematically connected passages, distributed through the text, are connected in chronological revision patterns. Incipit Incipits are paradoxical as they are random but always decisive and primordial for the storys development. Looking at the drafts of the story, what do the revisions tell us about the composition of the incipit? During the writing sessions, Van den Berg made extensive revisions to the incipit. In the first draft of the story, he starts with introducing the neighbour of the main character, but in the finished story, this neighbour is reduced to someone. Results As this is a work-in-progress, we cannot yet provide results. We hope to reconstruct the genesis of the incipit through studying both the versions of the text and the process data from Inputlog; we cannot only see which revisions were made to create the ultimate incipit but also when – in the complete writing process. Revisions in the incipit could thus perhaps be linked with additions or revisions elsewhere in the text. We wish to elucidate parts of the writing processes we will study through visualisations based on TEI-XML. "
	},
	{
		"id": 30,
		"title": "Towards a New Approach in the Study of Intangible Cultural Heritage",
		"authors": [
			"Bellia, Angela"
		],
		"body": " Introduction The application of new technologies to cultural heritage has led to important methodological changes in the protection and enhancement of monuments. This new approach is stated in the objectives of the International Council on Monuments and Sites, 1 which aims to restore meaning and preserve the memory of historic buildings, promoting the application of technology in the assessment of monuments: this is particularly interesting with regard to the recovery of archaeological heritage. The European Commission report, New Technologies for the Cultural Heritage Sector, 2 states that virtual reality will provide an impressive, immersive and involving product. Within this context, archaeoacoustics is being used as a new method for the analysis of historical heritage, enabling the evaluation of the sound quality of a space3 by using auralisation techniques which allow cognitive and physical elements to be reproduced and combined. 4 The STESICHOROS 5 project aims to assess the cultic theatres acoustics through the acoustic reconstruction of a particular case study at Selinunte in Sicily. It is also hoped that the results might provide some foundations from which to create experimental interpretative reconstructions of what the cultic theatres might have sounded like, using acoustics and digital technology in order to recover their lost intangible heritage. The Cultic Theatre at Selinunte and its Acoustics The ancient site of Selinunte is recognised today as one of the most important archaeological sites of the Greek period in Italy. In 2006, the Institute of Fine Arts at New York University began a research project on the acropolis in collaboration with the Archaeological Park of Selinunte. The project consists of a new, systematic and interdisciplinary study of the archaeology and architecture of the main urban sanctuary, beginning with its southern sector. The investigation included a systematic programme of documenting the buildings in the area and its digital reconstruction. Several elements suggest the identification of the South Building as an impressive theatral viewing area with particular acoustic qualities. 6 This building belongs to a group of theatral structures found in various regions of the Greek world. Many of these structures were not proper theatres, but rather primitive rows of seats, with linear and non-circular theatra and/or orchestra. The acoustics of the linear theatra in the Greek world have never been analysed: 7 no study has focused on the acoustics of these theatres in order to understand how and why these spaces were chosen for auditory and synaesthetic experiences. 8 Fig. 1. Digital reconstruction of the theatral area in Selinunte. Research Objectives The STESICHOROS project has emerged to address the gap in international scholarship. Previous acoustic research has concentrated only on circular theatres in the Greek world. It will establish a new line of research to complete a study on the acoustics of the linear and non-circular theatra. The overall objective of the STESICHOROS project is the assessment and recovery of the lost intangible heritage of cultic theatresʼ acoustics through acoustic reconstruction of these spaces of the past. Using archaeoacoustics as an emerging discipline that involves the study of ancient sites, 9 the reconstruction will recreate what the cultic theatre in Selinunte might have sounded like. The project aims to verify whether it is possible that this building was built in a precise place for its acoustical qualities. Research methodology and approach In order to analyse the acoustic characteristics of the cultic theatre at Selinunte, we plan three main tasks: 1. current acoustic simulation with the creation of a geometric 3D model of the building in its current state; 2. simulation, acoustics and auralisation with a creation of geometric 3D models of the building. This process allows the use of visual and sound virtual reality to perceive and experience the sound response of a non-existent space which we have recreated thanks to digital technology; 10 3. acoustic parameters, simulations, auralisations includes a comparative study and perception of the acoustic variations in the cultic theatre during its history. Expected results This project will provide the first acoustic model for the study of acoustical properties of cultic theatres in the ancient Greek world. It is also hoped that the results will provide some foundations from which to create experimental interpretative 3D reconstructions integrating acoustic models. The results will establish a new framework, which future researchers can use to advance their knowledge of the application of 3D technology to virtual acoustics in order to offer an innovative research method in visualisation and immersive experiences related to the reconstruction of archaeological sites and their soundscapes. "
	},
	{
		"id": 31,
		"title": "APOLLONIS: The Greek Infrastructure for Digital Arts, Humanities and Language Research and Innovation",
		"authors": [
			"Constantopoulos, Panos"
		],
		"body": " APOLLONIS is the Greek national infrastructure for Digital Arts, Humanities and Language Research and Innovation. It brings together the leading strengths and capacities in the field by providing high-level computational tools, interoperable datasets and services. APOLLONIS was recently formed by the union of two existing ESFRI-related national research infrastructures: CLARIN:EL, the CLARIN-related Greek network for language resources, technologies and services; and DARIAH-GR/DYAS, the DARIAH-related Greek network for digital research in the Humanities, creating bridges where commonalities are observed. The development of APOLLONIS is being carried out by a consortium comprising of a number of institutions across Greece: Athena Research Centre, Academy of Athens, National Center for Scientific Research Demokritos, GRNET, University of Athens, Institute of Communication and Computer Systems, Athens School of Fine Arts, Foundation for Research and Technology – Hellas, University of the Aegean, and Ionian University. The development of the APOLLONIS infrastructure advances the existing CLARIN:EL and DARIAH-GR/DYAS services within a common framework that will ensure interoperability and reach to broader user communities, as well as promoting open science principles. CLARIN:EL will provides a permanent, stable infrastructure for accessing language resources and language processing web services, supports all kinds of language-related activities, and collaborative workspace for application development environment. DARIAH-GR/DYAS provides access to curated digital resources and services for the development, analysis and visualization of data, best practice guidelines, and dissemination and training activities on the use of digital methods and tools in the Humanities. A Digital Humanities Observatory monitors the penetration of digital practices in the Humanities. Establishing a unified, open virtual workspace that will enable access to interoperable digital resources, curation and editing tools, good practice guidelines and support, APOLLONIS allows Greek humanities research and education communities, as well as professionals in the media, culture and tourist industry, to carry out their creative activities more efficiently and less costly. Furthermore, through the CLARIN:EL and DARIAH-GR/DYAS national networks, APOLLONIS continues Greeces participation to the European infrastructures CLARIN and DARIAH, respectively. This poster will enable DH2019 audiences to engage with, comment and discuss the four main lines of action of the APOLLONIS infrastructure: 1. Tools and Services: APOLLONIS provides tools and services for supporting the entire life cycle of digital arts and humanities and language resources, including processes of acquisition, creation, documentation, knowledge extraction, analysis, annotation, curation, visualisation and publication of resources at various levels. 2. Resources: APOLLONIS creates, integrates, curates and offers access to digital repositories, registries, text corpora, lexica, thesauri, datasets and metadata, and bibliography. 3. Education and Training: The infrastructure develops and provides training on digital tools, practices, methods and services through a series of events, workshops, seminars and summer schools aimed at researchers, students and professionals. APOLLONIS furthermore offers digital training material such as webinars, tutorials and manuals. 4. Communities of practice: APOLLONIS fosters emerging communities of practice in the digital humanities and language technology. This is effected through sharing tools, services and resources; promoting relevant good practice guidelines and standards; focused training activities; supporting a sustained dialogue on theoretical and methodological trends and approaches, as well as intra- and inter-community interactions. "
	},
	{
		"id": 32,
		"title": "A World of Immersive Experiences: The Scottish Heritage Partnership",
		"authors": [
			"Konstantelos, Leo",
			"Pittock, Murray",
			"Benardou, Agiatis",
			"Economou, Maria",
			"Hughes, Lorna"
		],
		"body": " With ca. 17 million visitors annually to culture and heritage sites in Glasgow and Edinburgh alone, tourism is a major driver of the Scottish economy - it is worth some £6bn annually, ca. 5% of Scottish GDP; and supports 196,000 jobs Scottish Government201711112Scottish Government,Introduction to TourismStatistics topics2018October 222017October 13websitehttps://www.gov.scot/Topics/Statistics/Browse/Tourism?> . The centrality of cutting-edge immersive experiences for tourism, the heritage industry and audience development has been increasingly evident in recent years, with the development of Ars Electronica Linz; the Robert Burns Birthplace Museum; The Battle of Bannockburn, and other venues. Immersive experiences describe all forms of perceptual and interactive use of technologies and physical spaces in order to create a hybrid reality , in which visitors feel part of the experience as a whole, encompassing all spheres of attention – immersion can be Sensory, Challenge-basedand/or ImaginativeErmi200522247Ermi, LauraMäyrä, FransFundamental Components of the Gameplay Experience: Analysing Immersion2005 DiGRA International Conference: Changing Views: Worlds in Play2005Vancouver, CanadaPaperhttp://www.digra.org/digital-library/publications/fundamental-components-of-the-gameplay-experience-analysing-immersion/?> . For tourism and cultural heritage, immersion represents a pathway towards a mixed-mode experience economy , which reflects the nuances of differing experience dimensions embodied by different elements of a site Suntikul201633317Suntikul, WantaneeJachna, TimothyThe co-creation/place attachment nexusTourism ManagementTourism Management276-28652C2016?> . In this mixed-mode experience economy, visitor engagement combines activities across the Realms of an Experience. Despite the opportunities afforded by immersive experiences – and relevant investment in such experiences in Scotland – there has been a lack of substantive evidenceto evaluate current approaches and guide future developments: How successful are the approaches to immersive technologies at major heritage sites in Scotland, in terms of outcomes against business plan expectations and in terms of visitor response? What kinds of future development are supported by existing evidence? The Scottish Heritage Partnership These questions are the remit of the Scottish Heritage Partnership project. The Scottish Heritage partnership is a 2018-19 Engineering and Physical Sciences Research Council/Arts and Humanities Research Council funded project at the University of Glasgow, aiming to address the existing practice and future potential of immersive experiences and technologies in the culture and heritage industry in Scotland. The team is led by Professor Murray Pittock, with Professor Lorna Hughes and Dr Maria Economou as the Co-investigators and Dr Agiatis Benardou and Dr Leo Konstantelos as Research Associates. The National Trust for Scotland, Glasgow Museums and The National Library of Scotland are key partners in the project and so is our industry partner, Soluis Heritage. The objectives of the project are to: Build on and expand existing partnerships to explore the efficacy of immersive experiences at major heritage sites. Build a decision-making tool and gather evidence for policy development. Explore how we can best harness and shape cutting-edge digital technology and develop effective, meaningful content into leading edge inclusive and impactful immersive experiences. Outline the kind of social/group experiences facilitated or limited by immersive technology, and study how these affect the visitor experience overall. Examine the consequences of service-wide adoption of immersive technology in Scotlands leading heritage and collections resource provider, Glasgow Life. Produce a website, a digital decision-making tool, a policy paper and a risk assessment together with: An evidence-based market model for use with Scottish Government, VisitScotland, local tourist authorities and nongovernmental agencies A route to developing suitable immersive technologies which can be scaled/developed to meet the criteria identified under, in the process benefiting our digital partner, Soluis. Support National Trust for Scotland, National Library of Scotland and Glasgow Museums strategic use of their collections in interpretation and exhibitions development. Our methodology With support from our partners at Glasgow Museums, the National Trust for Scotland, and the National Library of Scotland, we have designed, collected and processed a substantial corpus of empirical evidence. To date, hundreds of questionnaires have been completed by visitors at six major heritage sites across Scotland, including the Battle of Bannockburn; Culloden Battlefield; the Robert Burns Birthplace Museum; the Riverside Museum; the Kelvingrove Art Gallery and Museum; and the NLS at Kelvin Hall. We have conducted observations of visitor experience at Riverside and RBBM, harvested and processed visitor comments from online websites, and we are using secondary analysis of existing visitor experience data to answer the projects research questions. Development of an evidence-based, decision-making model was completed. Formulated as a policy and risk assessment document, the model is meant to help heritage institutions identify the kinds of future immersive experiences that are supported by our evidence; as well as assess how to develop effective, meaningful content into leading edge inclusive and impactful immersive experiences. Our digital partner, Soluis Heritage, developed a visualisation of both the model and project findings, as a decision-making tool to illustrate the wider implications for policy and good practice, making the projects findings accessible and clearly showing the underlying data and empirical evidence used. This output was made freely available online, as a resource illustrating the creative and critical processes, and key decision points, of developing immersive technologies in a cultural heritage environment. What have we found: Our findings provide valuable pointers and evidence on how cultural heritage institutions in Scotlandcan prepare for dynamic change in the immersive experiences economy. Based on visitor feedback and other data sources, we have investigated the components of perceived value in a digital experience; the audiences most interested in immersive technologies; as well as characteristics of good immersive design and content. A summary of our findings was presented in a decision-making visualisation, while the entire corpus of evidence informed the development of a policy document for decision-making in incorporating immersive experiences in the cultural heritage sector. To stay up-to-date with our work, follow us on Twitter @scotmimmersives ; and visit our project website . "
	},
	{
		"id": 33,
		"title": "Immersive Experiences And Difficult Heritage: Digital Methods As Re-interpreters Of Historically Contested Sites",
		"authors": [
			"Benardou, Agiatis"
		],
		"body": " As methods of immersive experience, virtual and augmented reality, artificial intelligence, as well as mixed methods, are all means of memory re-composition in the cultural heritage domain. Immersive experiences describe all forms of perceptual and interactive use of technologies that blur the line between the physical world and a simulated or digital world, ie create a hybrid reality aiming at embracing all spheres of the users attention. Immersion can thus be sensory, challenge-based, and/ or imaginative. The beginnings of immersive experiences can be traced at the end of 1960s, when philosopher and cinematographer Morton Heilig invented Sensorama, a simulator for up to four people of a motorcycle ride in Brooklyn, which created an illusion of reality through a three-dimensional film with stereo sound, vibrations of the seats, wind on the face and even smells of the city. Around the same time, Ivan Sunderlanddeveloped the Sword of Damocles the first VR system with interaction possibilities. Since then, technologies which create immersive experiences have developed radically, primarily in the context of gaming and thematic parks, and have thus emerged as new interactive and narrative means. Some of the most creative applications of immersive experiences appear nowadays in museums and cultural heritage sites, proposing innovative narratives and interpretations through an impressive range of digital methods. Moreover, scenarios complementing immersive experiences as well as personalized storytelling have transformed visitors experience in cultural heritage sites immensely. Difficult heritage was a term coined by Sharon Macdonaldwhich was defined as concerned with histories and pasts that do not easily fit with self-identities of the groups of whose pasts or histories they are part. Instead of affirming positive self-images, they potentially disrupt them or may threaten to open up social differences and conflicts. Difficult heritage deals in unsettling histories rather than the kinds of heroic or progressive histories with which museums and heritage sites have more traditionally been associated. Difficult heritage is relevant not only to museums focusing on the recent past, but also to all archaeological and historical sites which may present controversial and sensitive subjects. Immersive technologies are so far limited to very few difficult heritage sites: In 2017, a virtual reality experience titled The Day the World Changed was to commemorate the work of the International Campaign to Abolish Nuclear Weapons. The experience began with an explanation of the conception, development, and implementation of the atomic bomb and then proceeds to a second part focusing on the aftermath of the attack. Visitors could walk through the ruins of the city and examine artifacts from the bombing. Similarly, a virtual reality experience developed in 2016 allows visitors to Jerusalem to see the city as it looked during the heyday of the Second Temple . In Europe, immersive technologies have not been employed widely in historically contested sites. In Scotland, however, immersive experiences are used to enhance visitor experiences in two difficult heritage sites, Bannockburn and Culloden. In 1324 in Bannockburn, near Stirling, the Scots under Robert the Bruce defeated the English army marking a significant Scottish victory in the First War of Scottish Independence, and making the site a landmark in Scottish history. The Battle of Bannockburn immersive experience takes visitors through a series of 3D films depicting the events during and surrounding the battle, and culminates in a visit to the Battle Room in which they have the option to participate in their own interactive battle game and are even give the chance to turn the course of history. In Culloden, east of Inverness, the 1746 battle marked the end of the Jacobite rising. The sites visitor centre is comprised of an immersion cinema, real-life reenactments of everyday life and a game. In Britain amidst Brexit and in the context of a possible new Scottish independence referendum, both sites bear characteristics of difficult heritage and immersive experiences shape new, objective yet diverse narratives around them. Α study by G. Yairshowcased that, due the trauma the Third Reich engendered, German memorials appear to be reluctant to advocate, direct, or lead, thus shying away from manipulative educational strategies that they associate with the Third Reich. They intentionally avoid manipulating emotions or directing visitors to arrive at pre-determined moral conclusions, and refrain from using expressive props to create authentic experiences. Yair argued that despite the many possibilities for introducing visitors to authentic and challenging exhibits and artefacts, German memorial sites and documentation avoided using emotive cues. Consequently, without a more challenging approach, these sites are unlikely to create key experiences with lasting effects on visitors identities. Despite the fact that the Nazi Occupationin Greece has influenced public debates, inspired artistic displays and performances, the sites of memory remain invisible. The proposed talk will use the infamous Block 15 of the Haidari Concentration Camp in West Athens, the largest and most notorious concentration camp in wartime Greece, as a case study of a largely neglected site of difficult heritage and will attempt to showcase that immersive technologies would be best fit to make accessible, highlight and re-interpret both the site and the narrative surrounding it. The Block 15 showcase will employ a mixed methodological approach, in which digital methods will be applied in the re-composition of difficult memory through immersive technologies, ethnography, history and digital narratives. It will be argued that, through original scenarios based on primary and multimedia archival sources, immersive experience developedcould not only bring back to life the actual Block 15, currently an endangered monument, but could also function as reminders of the atrocities of prisoners, in an attempt to reintroduce a historically and politically contested site to heterogeneous audiences, both in situ as well as in sites outside the Concentration Camp. "
	},
	{
		"id": 34,
		"title": "Developing a Community of Practice: The CHASE Model for Digital Humanities Researcher Training",
		"authors": [
			"Benatti, Francesca",
			"Gooding, Paul",
			"Sillence, Matthew"
		],
		"body": " Introduction The provision of digital humanities training to graduate students who have no previous experience of the field is a challenging task. In the United Kingdom, such training is still unevenly distributed across universities, varying from dedicated Masters and Doctoral programmes to more informal seminars and research groups. For smaller or less research-intensive institutions, the establishment of a digital humanities programme usually begins with the recruitment of a single specialist lecturer, who may however struggle to cover the breadth of expertise required to provide suitably wide-spectrum teaching. This paper will address these challenges in relation to a specific training programme, including our pedagogical approach to addressing these challenges, and how we can develop DH syllabi that simultaneously address the individual needs of the learner alongside a broader understanding of DH as a field of practice. The 11 Doctoral Training Partnershipsfunded by the UK Arts and Humanities Research Council offer the possibility of pooling the expertise of several DH specialists to provide more extensive coverage. The Consortium of Humanities and Arts South-East EnglandDTP, funded in 2014, comprises nine institutions: the universities of East Anglia, Essex, Kent, Sussex; Goldsmiths, University of London; the Courtauld Institute; The Open University; Birkbeck College and the School of Oriental and African Studies. Since 2015, CHASE has employed its Cohort Development Fund to pioneer an innovative doctoral training programme, Arts and Humanities in the Digital Age, which has to date trained over 80 students. The programme is built around a three-day winter school, a series of optional methods-based workshops, a two-day mid-project residential, and a final plenary session. This programme is supported by compulsory group work, where each group is tasked with producing a wireframe for a proposed digital humanities project. Methodology We faced two key challenges in developing the course: first, the DTP is comprised of a diverse range of institutions with broad remits across the arts and humanities; and second, we do not know the disciplinary backgrounds of our self-selecting cohort until they sign up. These students have already identified the relevance of the programme to their training needs, but they arrive with very different concepts of what the digital humanities actually are, and very different learning objectives. Some want to address a personal skills deficit; others want to develop existing skills to address a specific research goal; and others want to engage with DH as a field of study. This has necessitated an iterative approach to programme development that has cohered into a teaching philosophy focused on introducing digital humanities as a reflective methodological space. The broad possibilities of DH can necessitate a self-deterministic element to skills development, and this is reflected in our approach. It is important that our students understand the central methods that are considered core to DH, so that they may situate these methods in relation to their own disciplinary practices. The four iterations of the programme so far have trained students in the following: Text Encoding; Text Analysis; Information Visualisation; Digital Images; Web Authoring; Databases; Project Management. Our approach has therefore been twofold: to establish a series of workshops on key methods in digital humanities; and to support this work with a residential school that introduces a degree of meta-discussionalongside an explicit self-reflective component framed in terms of threshold concepts. This understanding is then developed through group work that allows students to situate their work within a broader sense of collaboration in digital humanities. We have experienced relatively high variation in workshop attendance in the programme, which can be attributed in part to the difficulty of addressing this latter point. Our approach foregrounds individual skills development, while not necessarily making explicit that the methodological component of the programme reflects communities of practice of DH that draw together several disciplinary groups and ways of knowinginto a so-called methodological commons. For new entrants to the field, the meta-discussions that shape our understanding of interdisciplinarity in DH are neither obvious, nor necessarily relevant insofar as interdisciplinarity can be understood to combine approaches or methods from more than one discipline. The extent to which the engagement of the digital humanities with various disciplines is truly interdisciplinary has been contested on the basis that it is not clear how the field contributes to a larger shared agenda with other fields. This has led others to propose alternative models: Svensson, for instance, introduces the notion of trading zoneswhere different traditions are maintained while still carrying out intersectional work. Such approaches foreground the importance of methodology in defining the nature of these intersections. Outcomes AHDA has always introduced the domain of digital humanities through methodology as a reflective, iterative process. It does so because this is specifically a research-level programme. As such, developing confidence in the affordances and limitations of certain methods of analysis, and the quality of data itself, is recognisable to most research degree students. It echoes the conceptual underpinnings of their own, very specific, research projects, a key doctoral criterion. In the final presentations, participants are able to diagnose the technical and conceptual issues as they relate to their own research context, but also the methodological limits of their group project. By this process of reflection, students identify what could and could not be achieved, and in the course of cohort discussion, what approach might improve the reliability and validity of their project. The second outcome is that, through this reflective approach, students are developing a degree of academic socialization. That is, not only what constitutes an appropriate method or technique for the data, but how it relates to a discipline or across disciplines. For example, in presenting their research findings, our students are not simply evidencing the work they have undertaken, but are also modelling scholarly communication. This might involve explicitly articulating the method, demonstrating clarity and accuracy in terminology, and being able to defend their choice of project design within the timescale and resources at their disposal. The key challenge in the second outcome - as Cordellnotes in relation to undergraduate DH teaching - is that it is very difficult to develop those meta-discussions about digital humanities as a field. Programmes often fail to do this because the domain knowledge of the novice is insufficient, and frequently tied to their home discipline. For our programme, it is possible that these two broad outcomes have limited potential for those intent on progressing beyond the methodological. For the majority of participants, practical application and a critical approach to analysis may be sufficient; in short, addressing a skills deficit. However, the kind of academic socialization noted above may not tackle the epistemological or hegemonic dimensions of digital humanities practices. These are better likened to Lea and Streetsconcept of academic literacies, which allow the student to better understand and critique the social nature of knowledge production in terms of what counts as known, and who or what makes those decisions. The challenge for programme design is embedding these literacies while still enabling students to achieve other key learning objectives. Conclusion The application of the pedagogical framework of AHDA has been continuously refined through student feedback. The four iterations of AHDA so far have privileged a self-directed style of learning, giving students the freedom to choose among a number of elective workshops. Student feedback has been positive, with those who complete the programme stating that it addressed their learning needs effectively, and that the cohort development aspect of the course helped them to take ownership of their learning within a supportive network of common interest. However, the teaching team has noted that the highest levels of engagement have been with those topics, such as Information Visualisation, that have the greatest cross-disciplinary applicability. This feedback, combined with the difficulty of providing specialised teaching for an always changeable student cohort, has led the teaching team to innovate the 2018/19 provision, which will therefore focus on leading the students through the lifecycle of a DH project. Through Open Source tools and platforms, students will learn how to clean, manage, store and analyse either their own data or those provided by the British Library; how to present their results and understand the nature of the DH community of practice; how to make informed decisions about the extent of their participation in DH, whether through deep engagement or in the form of legitimate peripheral participation. We therefore will require mandatory attendance to all workshops in order for students to engage with each step of the project development process. In this presentation we will introduce the development of the programme and report and evaluate this years iteration in relation to the pedagogical framework described above. The unique contribution of this presentation will be to consider how theories of self-reflective learning can combine with an understanding of DH as a community of methods to inform the development of syllabi for postgraduate researchers. "
	},
	{
		"id": 35,
		"title": "Towards a Common Model for European Poetry: Challenges and Solutions",
		"authors": [
			"Bermúdez-Sabel, Helena",
			"Díez Platas, María Luisa",
			"Ros, Salvador",
			"González-Blanco, Elena"
		],
		"body": " Introduction This paper stems from the analysis of multiple poetic resources that were available on-line, as well as the results of methodological discussions with scholars of European Literature. The goal was to retrieve the informational needs of all these different sources in order to build a common data model for European Poetry. Thus, by implementing a reverse engineering method, we have created the Domain Model for European Poetry, which is an important milestone for making existent poetry resources interoperable. In this paper, we will present some of the challenges we encountered while conceptualizing the information relevant to poetic analysis and how we have worked around them. Rationale During the history of European Literature, there have been different cultural centers that have irradiated their influence. Some traditions, due to historic and socio-political reasons, have leaned at some point in their history on other literary models. Thus, the relations between the different literary traditions are many and heterogeneous. This poses some difficulties for literary research, since these relations are not always easy to trace. An additional handicap is that it demands for researchers to closely know traditions and languages other than the ones of their specialization and the accumulation of all that knowledge is not always humanly possible. We can find many poetic resources on-line. However, the access to these resources is fragmentary: there is no way to retrieve all relevant information at once. Researchers need to look for multiple resources and then, for each one of them, carry out different queries in order to retrieve the required information. To work around this problem, the project POSTDATA Poetry Standardization and Linked Open Datais an ERC-funded project. Please visit the projects website for more details: has a proposal that depends on two key concepts: standardization and interoperability, according to the linked open data paradigm. After presenting some brief notes about the objectives, this paper will focus on modeling issues. Contextualization Linked data must endorse a semantic model before being published. This semantic model can take different formats one of them being an ontology. Considering that one of the main aims of POSTDATA is to provide a means to publish European poetrydata as LOD, this project is building an ontology for this domain. With ontologies, shared and distributed knowledge can be managed in such a way as to allow the integration of information from different data sets. The starting point of the ontology construction was the analysis of different databases with contents related to one or more EP traditions in order to represent the informational needs of the community of practice, that is, the EP one. For a detailed exposition about how these informational needs were elicited and other methodological aspects, please see Bermúdez-Sabel et al.. Our goal is to enhance interoperability between existent repertoires and to facilitate the creation of new poetry resources. With such an ambitious objective in mind, we must be very exhaustive when eliciting the data needs of our target. Our sources to draw out the informational needs of the EP community were, on the one hand, a representative sample of existing resources and, on the other hand, a survey that allowed us to consult the EP community. See the map available at to see the projects that have collaborated with us. In Curado Malta et al.there is more information about all the resources that were analyzed and what type of study was done to each one of them. In addition, there were different validation processes through which we received the direct input of experts in EP in order to refine the model. To learn about the validation processes, please see Curado Malta et al.. We are dealing with miscellaneous sources of information that incorporate data of multiple languages and cultures. This matter complicates the process of modeling. In the following section we will present some of the issues we encountered. Modeling challenges The creation of a data model that covers all required concepts to analyze any European poem causes some difficulties. a) Multilingualism: The most obvious problem we ran into arises from working with a multilingual reality. The modelers had to analyze on-line resources in languages they are not familiar with. The perfect team would have an expert on every poetic tradition, that is, a scholar for every European language and literary period. Regretfully, it is hard to find a project in Humanities with that type of resources. This knowledge gap is covered with either the documentation translated to English by the project being analyzed, or with additional bibliography. Nevertheless, the direct contact with the people in charge of that resource is at times inevitable but the response and willingness to collaborate is, for the most part, very positive. b) Polysemic terms: Occasionally, the difficulties are due to ambiguities in the same language. For instance, we find that many European languages have a term derived from the Latin versus to describe the poetic line. In English, however, the term verse can describe either the line of poetry, a bigger division like the stanza or the whole poetic composition. c) Synonymicterms: Literature scholarship is a field with thousands of years, which means that some of the concepts we are analyzing have been defined for many centuries and from different perspectives. The domain experts of the team cannot prioritize any school of thought or theory. On the one hand, we may find different terms for similar concepts, but the use of one term over the other is related to philological schools. In these cases, the less aligned term is selected. On the other hand, we may find the same term in different technical vocabularies but with distinct meanings. For example, the term dieresis in syllabic verse traditions describes the separate pronunciation into two syllables of two sounds which usually form one syllable. However, in the quantitative verse, a dieresis expresses the pause that occurs when the end of a foot coincides with the end of a word. Therefore, it is unavoidable to rank the suitability of certain terms since absolute neutrality is unattainable. In the aforementioned issue with the word diaeresis, we selected that term to describe the type of pause and used hiatus to express the separation into two syllables, taking the term from its counterpart concept in Linguistics. d) Semantic interoperability: Like in any other process of semantic modeling, there is some tension between interoperability and semantics. For instance, poetry of the Western world is most commonly divided between qualitative and quantitative meter. Thus, meter may depend on the length of syllables and their distribution, or on the pattern created by stressed syllables coming at regular intervals. In the case of qualitative meter, instead of demanding a fixed pattern of all the stresses, some traditions only care about the position of a certain stressed syllable, like the last one. Some of the types of qualitative verse have many attributes that are interoperable with the quantitative ones. Therefore, we decided to make a conceptual division between metrical schemes that depend on patterns and those that are defined by counting elements. In this manner, little semantics are lost, because other properties make the distinction between qualitative and quantitative. However, with this conceptualization we enable the comparison between types of meter that, even if they focus on different linguistic properties, have many things in common. The development of a data model that expects to serve a whole community of practice in the LOD ecosystem entails a great complexity. The type of final user that will consume that data is very diverse. Also, the applications that might be built with these data are many and very heterogeneous. This factor complicates the elicitation of the functional and no functional requirements, thus arising very interesting issues during the modeling process . Acknowledgments The authors would like to credit Mariana Curado Maltaas the semantic modeler who engineered the method and its implementation. We would also like to credit Clara Martínez Cantónfor her examination of the metrical concepts which are part of the data model referenced in this paper. Finally, would like to thank the delegates of the analyzed repertoires for their availability and willingness to share information and to discuss issues related to their projects with the POSTDATA team. Research for this paper has been achieved thanks to the Starting Grant research project Poetry Standardization and Linked Open Data: POSTDATAobtained by Elena González-Blanco. This project is funded by the European Research Councilunder the research and innovation program Horizon2020 of the European Union. "
	},
	{
		"id": 36,
		"title": "Complexities, Explainability and MethodMedia Philosophy and Artificial Intelligence",
		"authors": [
			"Berry, David M.",
			"Fazi, M. Beatrice",
			"Dieter, Michael",
			"Roberts, Ben",
			"Bassett, Caroline",
			"Salway, Andrew",
			"Tkacz, Nathaniel"
		],
		"body": " Abstract This panel addresses the relationship between explanatory complexities involved in understanding knowledge and method. One of the perennial debates related to the problematic raised by computation is that knowledge and its processing are enveloped within a black-boxed structure which obscures or hides the internal workings of the machine. This has implications not just for programmers, but also for those who rely on computational techniques, such as the digital humanities. Moreover, as algorithms continue to penetrate into broader society, major difficulties are raised when important decisions are made which cannot be understood or checked – this has democratic implications. But not only are decisions and the decision-making process often obscured; the form of knowledge and mode of thought itself are also equally veiled. These debates have been given greater intensity with the rise of machine-learning systems that are fully able to automate much more complex decision-making processes than the previous generation of algorithms. Not a day goes by without a news report detailing a new front in the automation of labour, the creation of a new form of robot, or a report warning of mass unemployment unless re-training and re-skilling is begun in earnest. These concerns have also given rise to renewed intellectual debate about the future of work and proposed remedies for automation-induced under employment, such as UBI. But such debates need to be extended to the fundamental problem of understanding automated computational decision making and algorithmic forms of knowledge. In this panel, we explore these questions by probing a number of different ways of thinking about and representing this thematic. Are there ways in which we can use new digital methods to uncover these systems in order to present them in a human-readable form? Do we need new theoretical frameworks for understanding these complex issues? What are the broader implications for the way in which the digital humanities can contribute to these important questions over complexities in knowledge, thought and data? Explainable Digital Humanities Berry, David M. In the UK, the Data Protection Act 2018 has come into force, which was the enabling legislation for the European GDPR. It has been argued that this creates a new right, in Article 22, in relation to automated algorithmic systems that requires the controller of the algorithm to supply an explanation of how a decision was made to the user– the social right to explanation. This right has come to be known as the problem of explainability for artificial intelligence research, or Explainable Artificial Intelligence. Although limited to the European Union, in actuality the GDPR is likely to have global effects as it becomes necessary for global companies to standardise their software products and services but also to respond to growing public disquiet over these systems. In this paper I want to explore the implications of explainability for the digital humanities, and particularly the concept of explainability it gives rise to. This is increasingly relevant to the growing public visibility of digital humanities projects and the potential for the use of machine learning in these approaches. The discussion I wish to open in this paper is largely speculative. It seems to me that we have two issues that are interesting to consider. Firstly, that the GDPR might require digital humanities projects to have or to be explainable in some sense and therefore subject to the same data protection regime as other algorithms. This may mean they are required to provide their processing descriptions under this right to explanation. Secondly, the interpretation problem faced by algorithm programmers seems to me exactly the kind of expertise that digital humanists possess and who could therefore help inform the debate over explainability. Digital humanists tend to be familiar with technical systems and the questions raised by understanding and interpretation more generally, for example in the discussions over close and distant reading. There is now an emergent field of Explainable AI, also sometimes known as transparent AI, which attempts to design AI systems whose actions can be easily understood by humans. These third-wave AI systems are designed to produce more explainable models, while still maintaining a high level of learning performance and prediction accuracy and helping humans to understand, appropriately trust, and effectively manage the emerging generation of artificially intelligent partners. This means that XAI systems will have to have the ability to explain their rationale, characterize their strengths and weaknesses, and convey an understanding of how they will behave in the future in order to strengthen their public accountability. In this paper, I explore this discussion and argue that the digital humanities can enrich the debate by extending the notion of explainability with what I want to call understandability. That is, from the domains of a formal, technical and causal model of explanation, to that of understanding, given through a notion of comprehension modelled after the humanities and stressing complexity, interpretability, hermeneutics and critique. Beyond Human Knowledge: Explainability and Representation in Deep Learning Fazi, M. Beatrice This paper addresses some of the philosophical implications of computer programs being no longer constrained by and to the limits of human knowledge. I will interpret this freedom from human knowledge as an exemption from human modes of abstraction. In the paper, I will develop this argument by discussing and relating this claim to epistemological questions about representation, as these have been developed both within philosophy and computer science. I will do so in order to argue for the necessity to engage—philosophically—with the contemporary success of computational procedures for which there are not adequate human representations, and to which human representation is also no longer necessary. Artificial intelligence research in machine learning will be my main case study. More specifically, I will look at the case of deep learning. The latter expression denotes a set of machine-learning techniques that rely on artificial neurons to process information, somewhat analogously to what a biological brain is understood to do. A lower layer of neurons carries out a computation and transmits this result to the layer above, contributing to the final outcome that is to be outputted by the layer at the top. Although they have been around for decades, these deep learning techniques have today capitalized upon the contemporary increase in computational power and the availability of vast amounts of data. Because of their recent successes in pattern recognition, for example, deep learning techniques are today much talked about, and have attracted interest from the general public, academia, government and business alike. In this paper, I will address this condition and this debate in order to discuss how deep learning operates in computational ways that are opaque and often illegible. I will thus consider the black-box aspect of deep learning, and I will characterize it in terms of a technical condition. My claim is that this technical condition requires us to reconsider and reassess the abstractive capacities of these AI technologies. The scope of the paper is precisely to offer such a reflection by entering the multifaceted debate about explainability in AI, and thus assessing the ways in which technoscience and technoculture, alongside the digital humanities, are addressing the possibility to re-present algorithmic procedures to the human mind. By engaging with examples from the field of explainable AI, I will argue that deep learning is not only transforming the epistemic spaces and scopes of what we consider to be a valid explanation: computationally automated techniques such as deep learning are changing the meaning and reach of abstraction too. I will thus conceptualize explainability in AI as a problem that asks us to surpass a strictly phenomenological analysis of machine representation. The challenge for philosophy and for the humanities, I will claim, is that of advancing a theory of knowledge that would be able to account for the epistemic specificities of the artificial cognitive agents that populate our present. Walking Through the Security Apparatus: On Verification, Explainability and Apps Dieter, Michael; Tkacz, Nathaniel Apps appear as bounded digital objects, but operate through multiple data flows that vary according to diverse infrastructural settings. Indeed, the coordination of such flows not only make apps functional, but also economically viable. Studying the embeddedness of apps within these wider situations, however, presents a number of methodological challenges, particularly in developing approaches that are able to delve into their multi-situatedness. This presentation aims to contribute to the study of these complexities by focussing on walkthroughs, specifically by exploring banking apps using a form of hybrid walkthrough analysis. While walkthrough methods are well-established across a number of research fields and commercial practices, including user-focused walkthroughs, cold walkthroughs that focus on data, and recent post-phenomenological approaches from human geography, we apply a more interdisciplinary approach with an emphasis on how designerly ways of knowing and doing are interwoven with specific techno-economic and cultural niches. Practically speaking, this involves not only systematically documenting a normative scenario of app use, but thickening the approach on technical dimensions by decompiling apps as software packages to perform diagnostics on the codebase; identifying points of user engagement by reflexively tracing interface design patterns; and capturing data-flows to cloud and platform infrastructures through dynamic network analysis. From a critical perspective, this hybrid approach also means paying attention to the heterogeneous kinds of expertise that intersect with apps, including the legislative frameworks, epistemologies and practices of software development, strategies from data-driven marketing and other established ways of doing and knowing within a specific sector. In this presentation, we demonstrate the novelty of this approach with a focus on European digital-only challenger or neo-banks – that is, new actors in banking whose financial services are delivered exclusively as apps. Importantly, these banking apps are distinct since they rely on relatively high levels of security. Secure identity verification processes for banking are closely configured according to a raft of judicial and regulatory frameworks broadly referred to as Know Your Customer. In general, the transaction thresholds for mandatory KYC in Europe has in recent years been significantly reduced, while being extended into emerging contexts like cryptocurrencies. Many of the latest regulatory reforms incorporate new actors, while requiring more frequent and intensive monitoring. Framed in terms of risk and operationalized through digital technologies and techniques – including smartphones and apps, but also machine-learning systems and social media profiling – these forms of securitization seek to explain who users are through new technical and automated means. In this paper, we discuss how hybrid walkthrough analysis can be used to study these ways of knowing app users and provide a context for critically reflecting on how these appified situations drive the habituation of security as convenience. Automation Now and Then: Interpreting Automation Fevers, Anxieties and Utopias Roberts, Ben; Bassett, Caroline; Salway, Andrew The way that automation is currently investigated is primarily either in terms of the technological developments themselves, or in relation to the implications of automation for the world of work. Our approach is different. We set out to understand automation anxiety, that is, we analyse ways in which automation is socially justified, imagined, understood, designed, and critiqued. We call such social and cultural concerns automation anxiety. Such anxiety is long-standing and demands urgent and sustained attention in its own right. In this paper we probe earlier automation scares of the 1950s and 1960s. Automation anxiety in this period produced a plethora of gray literature from government agencies and committees, labour organisations, political parties, and corporations, among others. These include discussions between labour, civil rights activists, left public intellectuals, and emerging industrial figures over automation, particularly over the question of who benefits/when?, which remain highly pertinent today. In our own time there is also a burgeoning collection of similar literature produced in relation to the new application of machine learning to more complex tasks and its implication for the future of work. This paper reports the findings of a pilot project conducted with a small set of this literature, including UK government and US congress reports on automation from 1956. It informs the wider project which will use computational approaches, including topic modelling and corpus text analysis, to analyse and compare corpora of gray literature from the 1950s and 1960s with material from the present. Corpus linguistic techniques such keywords and n-grams, along with topic modelling, will be used to identify technology-specific terminology and a set of automation concepts. The varying uses, meanings and connotations of these automation concepts over time will be explored with concordance and collocation analysis. Keyness analysis will also be used to contrast the UK and US cases. Our goal in this research is twofold: firstly to understand better and account for the cyclical nature of automation anxiety, as reflected in the language of gray literature, and the recurrence of automation debates in culture, particularly with reference to the 1960s and today. Of interest here is the media-archaeological concept of​ topos drawn from Erkki Huhtamo as a way we might think about the return of automation anxieties. We are also interested in the idea of revived salienceapplied to the way in which tropes evident in these debates are reproduced and re-embedded today. The second concern explored in this paper concerns the forms of knowledge the methods adopted can deliver. We address the tension between interpretive critical theoretical methods and empirical DH methodologies which brings into question the forms of knowledge the historical probes we are deploying can deliver. A mode of complex interpretation, we suggest, may be usefully developed to take us beyond Morettis sense of operationalization, in which the test of big data methods is that they change theory, in thinking about how to engage critical theorisation and data. "
	},
	{
		"id": 37,
		"title": "Digital Fragmenta Historicorum Graecorum (DFHG)",
		"authors": [
			"Berti, Monica"
		],
		"body": " This long paper presents the Digital Fragmenta Historicorum Graecorumproject. The DFHG is the digital version of the five volumes of the Fragmenta Historicorum Graecorum, which is the first big collection of ancient Greek historical fragments published by Karl Müller. The FHG is a corpus of quotations and text reusesof 636 ancient Greek fragmentary historians preserved by Classical sources. Fragmentary authors date from the 6th century BC through the 7th century CE and, except for the first volume, are chronologically distributed. Fragments are numbered sequentially and arranged by works and book numbers with Latin translations, commentaries, and critical notes. The DFHG is not a new edition of ancient Greek fragmentary historians, but a new digital resource to provide textual, philological, and computational methods for representing fragmentary authors and works in a digital environment. The reason for choosing the Fragmenta Historicorum Graecorum depends not only on an interest in Greek fragmentary historiography, which provides a rich collection of complex reuses of prose texts, but also on the necessity of digitizing printed editions and preserving them as structured machine readable corpora that can be accessed for experimenting with text mining of historical languages. Moreover, the FHG is still fundamental to understand recent editions of Greek historical fragments and in particular Die Fragmente der griechischen Historikerby Felix Jacoby, who spent his life to change and improve the collection created by Karl Müller. Finally, the corpus of the FHG is open and big enough to perform computational experiments and obtain results. This paper presents tools and services that have been developed by the project, not only for accessing the entire collection of the FHG, but also for providing a new model that can be applied to other collections of fragmentary authors in order to visualize and explore their complex data and connect it with external resources for further developments. The presentation is organized according to the following topics: Visualization of DFHG contents. The DFHG appears as an Ajax web page automatically generated by a PHP script querying an SQL database of the entire FHG, which is accessible by browsing the whole collection or single volumes through a slide in/out navigation menu. The navigation menu allows scholars to navigate the FHG with a comprehensive and detailed view of the structure of the entire collection and to jump to the relevant section without reloading the page. This kind of visualization is very helpful because the printed version of the FHG doesnt contain detailed tables of contents of its volumes, but only short and sometimes incomplete lists of authors published in the collection. Access to the DFHG. The DFHG Digger filters the whole collection according to authors, works, work sections, and book numbers, while the DFHG Search function is performed on fragments, translations, commentaries and source texts. Results show the number of occurrences in each DFHG author and searched words are highlighted in the text. They also display, when available, the lemmatization of inflected forms and the disambiguation of named entities through external resources. The DFHG provides a web API that can be queried with author names and fragment numbers. The result is a JSON output containing every piece of information about the requested fragment. The DFHG exports data to CSV and XML format files. Integration with external resources. One of the main goals of the project is to make the DFHG part of a bigger infrastructure of processed data. This is the reason why the DFHG is integrated with external resources such as textual collections, authority lists, dictionaries, lexica and gazetteers. These resources are fundamental for disambiguating and annotating DFHG data, which in turn offers a collection of parsed texts for enriching external libraries of Greek and Latin sources. The DFHG is currently connected to different resources that provide morpho-syntactic information and named entities disambiguation of textual data of the FHG. The DFHG provides also a Müller-Jacoby Table of Concordance, which is a complete correspondence between fragmentary historians published in the FHG and in Die Fragmente der griechischen Historiker including the continuatio and the Brills New Jacoby. The goal of this resource is to go beyond the FHG corpus and produce a complete catalog of fragmentary authors of Greek literature published in different digital editions. This resource is progressively ingested into the Perseus Catalog. Data citation. It is possible to retrieve and export citations of DFHG fragments and source texts down to the word level using URN identifiers. These URNs are combinable with a URL prefixto generate stable links. The syntax of each URN represents the editorial work of Karl Müller, who has arranged the fragments in a sequence and has attributed them to fragmentary authors, works, work sections and book numbers. The DFHG provides also CITE URNs according to the guidelines of the CITE Architecture. Source Catalogs. The DFHG includes a Fragmentary Authors Catalog and a Witnesses Catalog that have been created from FHG data. These catalogs allow users to search and visualize the 636 Greek fragmentary historians of the collection and each of their witnesses. Data from both catalogs has been used to generate charts for visualizing chronological distributions and statistics of FHG authors and their source texts. This data integrates also Pleiades identifiers with geo-locations that have been used for producing maps that visualize the geographical distribution of FHG authors and their witnesses. Text Reuse Detection. The DFHG project offers experimental text reuse functionalities for automatic text reuse detection of FHG fragmentary historians. This resource allows users to automatically detect text reusesof FHG authors in their witnesses. Users can insert an XML file URL or select one of the PerseusDL or Open Greek and Latin editions available in the DFHG. Results display quotations and text reuses of FHG authors within their source texts. The DFHG allows scholars to download complete XML files of the source texts of the fragments with dfhg attributes that mark up the presence of DFHG text reuses in the relevant passages of the source texts. DFHG text reuse detection is based on the Smith-Waterman algorithm that performs local sequence alignment to detect similarities between strings. OCR Editing. The digital version of the DFHG has been produced starting from the OCR output of the printed edition of the FHG. Even if it is possible to obtain very good results when OCRing 19th-century editions of ancient Greek and Latin sources, OCRed texts still contain errors. The DFHG offers an interface for manual OCR correction of source texts, fragments, Latin translations and commentaries. Corrections are validated or rejected by the project team through an administration page. Further developments of the DFHG project aim at implementing named entities recognition in the texts of Greek and Latin fragmenta and in contributing to enrich the number of lemmata and inflected forms of Greek and Latin thesauri. The final goal of the project is to offer a new methodology based on digital and computational approaches to represent complex historical text reuse data. The DFHG also offers an open collection of quotations and text reuses of Greek fragmentary historians. This resource provides the community of scholars and students with machine processable data for historical and computational research. "
	},
	{
		"id": 38,
		"title": "COSME² - Complexities: 30 Years Of Research Of Medievalists DH Concerning A Thousand Years Of Medieval Sources",
		"authors": [
			"Bertrand, Paul",
			"Levenson, Matthias Gille",
			"Ferrand, Margot",
			"Pinche, Ariane"
		],
		"body": " Cosme² is the Consortium Sources Médiévales, it is dedicated to digital approach of the medieval sources, for french mediaevalists. It wants to bring together a large part of the French medievalist communityaround the digital processing of medieval sources, mainly written. Its aims: to gather all mediaevalists around their sources, to build the community by proposing frameworks, standards, ideas around Digital Humanities, in collaboration with the group Menestrel http://www.menestrel.fr/ , and help to the creation and publication on line of digital corpora. The complexity of the medieval digital landscape is due to its ancientness: medievalists were among the first to be concerned about the digital processing of their sources. Databases and corpora digitised in various forms are therefore many and varied, many remain dormant or need to be upgraded, others lack metadata, others are no longer online or on outdated media, others lack interoperability, even if their content allows them to do so. The hundreds of thousands of digitised medieval charters are not yet effectively linked. Medievalists were among the first to design electronic publishing platformsbut they are not yet interoperable. These corpora themselves are complex, associated with the sources which are just as complex, by their diversity, by the concepts implemented but also by the chronological span. Pioneering strategic choices have been made, such as the adoption of the TEI, but paradoxically many projects stand out from it, because the creation of interoperability no longer necessarily requires fine markings but automatic indexing tools based on complex referentials, fed by artificial intelligence training. The implementation of data exploitation tools, text mining procedures, instruments for extracting named entities and qualifying data within the corpuses of digitised medieval sources, but also the development of tools for analysing these corpuses, within COSME², make it possible to consider the connection of these complex and still poorly connected corpuses. The development of good practice guides and support manuals, the organisation of summer schools to train researchers in these tools and techniques are essential complements to strengthen this connection. The poster aims to present the complexity of documentary files, corpora and data, the complexity of historical uses and the complexity of the communities concerned, but also to evoke the operations envisaged to solve the complexities of communication and structuring by implementing integrated solutions. "
	},
	{
		"id": 39,
		"title": "The Frankenstein Variorum Challenge: Finding a Clearer View of Change Over Time",
		"authors": [
			"Beshero-Bondar, Elisa",
			"Mulligan, Rikk",
			"Viglianti, Raffaele"
		],
		"body": " No cultural artifact lasts forever in a single, static, universal form. Texts change with editors and audiences, and tracking their changes challenges our ways of reading and understanding, heightening awareness of mediations, censorships, interventions, reinventions. Yet, telling the story of change over time in a variorum edition can be tedious, disruptive of joy in the reading experience, alien to the popular experience of getting lost in a book. As Brett D. Hirsch and Hugh Craig have discussed of Shakespeare editions, the print variorum has yet to be surpassed for its highly developed economy of presentation.Can we find a humane way—attuned to readerly impatience and excitement in the reading process—to build an edition that teaches a non-specialist how to read change over time, without sacrificing the standards of textual scholarship? Must digital methods that comprehensively document variation at their best mimic the printed critical edition in burying the experience of change over time to the footnote or the end space? Or, can we design a variorum edition that engages not only scholars but also fans and new readers in the dynamic complexity of a changing text? Since the early days of hypertext, Mary Shelleys novel Frankenstein has inspired experiments using electronic texts that invite new ways of reading. The novel about assembling a new monstrously superior life from dead bodies easily becomes, in the age of computers, a metaphor for attempting to unify a super-text complexly assembled from multiple variant pieces, since the process of computationally comparing texts necessitates editors to identify the smallest meaningful units to align and process. Like Victor Frankensteins assembly of the Creature from multiple corpses, variorum editors aspire to animate a new composite reading experience. Such was the experiment to make the 1818 and 1831 editions of Frankenstein simultaneously readable in the Pennsylvania Electronic Editionproduced by Stuart Curran and Jack Lynch in the mid-1990s. Designed in a framework of small pieces like the hypercard novel popular at the time, the PAEE encouraged the reader to wander between two editions, explore hyperlinked ancillary material, and resequence the novel at will. The transfer of this edition to Romantic Circles, a scholarly web resource in British Romanticism, conserved its textual data without exceeding its early intertextual vision of a complex, particulated reading experience with many reading paths always available. Our project to design a new Variorum of Frankenstein began with an encounter with these earlier digital editions. Originally called the Pittsburgh Digital Frankenstein project, and inspired by the worldwide bicentennial celebrations of Frankensteins first publication, we started work in October 2016 as a voluntary collaboration of Pittsburgh-area digital humanists with the Maryland Institute of Technology in the Humanities to update the Frankenstein edition on Romantic Circles. The original goal was to revise and update the TEI code of the Romantic Circles edition to find ways to connect it with the Shelley-Godwin Archive sdiplomatic edition of the Frankenstein notebooks . The mission shifted to design a variorum edition to collate five editions of the novel produced in the authors lifetime, rather than simply to update and interlink the two most famous earlier editions. Our team from the University of Pittsburgh, Carnegie Mellon University, and the University of Maryland are now producing a new variorum edition of the novel encoded and published in TEI XML. Inspired in part by Barbara Bordalejos Online Variorum of Darwins Origin of Species, the Frankenstein Variorum incorporates annotations and visualizations that highlight change over time to guide readers to discover the hotspot moments of most intense variation in the novel. The Frankenstein Variorum is developing a scholarly edition that augments but does not replace previous print and digital editions. Our project pulls earlier editions into a new comparative perspective, to view the elaborately encoded S-GA edition of the manuscript through a variorum interface designed to illuminate where the manuscript contains material lost in later editions, and where later editions preserve or adapt its original content. At DH2018 and at the Balisage Markup Conference in June and July 2018, Beshero-Bondar and Viglianti presented on the first phase of these efforts: preparing the differently-encoded source documents for computed collation with CollateX and building the foundational structure of the Variorum as a spine of standoff pointers into the separate edition files. This DH2019 paper addresses a new phase of work: developing a browser-based user interface that incorporates accessibility and responsive design features, inviting visitors to read specific editions, review variants, support edition-specific and cross-edition annotations, and visualize GIS data of the movements of characters, authorand co-editors in space and time. One of the most complex elements of the Frankenstein Variorum is its application of stand-off XML to point to rather than recreate the diplomatic TEI encoding of the 1816 manuscripts in the S-GA. Our interface should guide readers in navigating the complexity of the edition, how to find its most interesting moments of variation, and how to read the richly encoded textual information about the writing process available in the markup. To help guide the reader, the annotations team on the Frankenstein Variorum seeks to balance contextual annotationswith a more innovative commentary designed specifically for navigating the Variorum: a set of tunneling annotations to highlight internal changes across the five editions. These annotations will augment previous scholarship on the persistence of various hands in the composition, emendation, and revision process. Incorporating these annotations with the variant apparatus poses a challenge for designing our interface that we look forward to discussing in our paper. The Frankenstein Variorum must support traditional, linear readings of individual editions along with nonlinear explorations of textual comparison through collation units and tunneling multi-edition annotations. To achieve this, we build our web interfaces directly on the TEI without transforming it to HTML first. We use CETEIcean, a JavaScript library for publishing TEI documents in the browser, to display the texts, visualize the linkages among Frankensteins sources, and scholarly annotations to the TEI-encoded texts. We aspire to the principles of Universal Designto increase access for those with visual limitations or who require a less cluttered and more responsive interface. We acknowledge the near-impossibility of addressing the variety of user needs and devices used to access web-based content; yet, if we cannot meet the ideal, we can nevertheless draw on UD principles to expand our audience and inform our creation of vision-sensitive color palettes and adaptive stylesheets for those with difficulty differentiating particular colors or low visual acuity. The UI will be optimized for those using tablets and larger displays, applying Responsive Design to prioritize tablet delivery, limit navigation clutter, and highlight primary features including the text reader, variations table, and annotation window. We are refining a method of signaling the level of heat or variance of a selection across editions, based on Levenshtein distance, the mathematical calculation of the minimum number of edits required to change one text string into another. We are aware of the limitations of Levenshtein for evaluating intensity of variation, since a change of a single letter or mark may totally change the meaning of a passage, such as the edit distance of 1 between the words went and wept. Nevertheless, as a convenient method for surveying intensity of edits on a macro scale, edit-distance values prove helpful. The tunneling annotations and overrides to the computed color codes will help to intervene where we discover the Levenshtein values to be less meaningful. Our color-coding system applies the maximum edit-distance based on pairwise comparison of each set of divergent witnesses at given variant locations in the novel, as represented in Figure 1 below. Fig. 1: Prototype view of the Frankenstein Variorum interface. Features Depicted in Fig 1: A single text reading window with identifying text and navigation elements for edition, volume, chapter, and page.Alternate reading frame to review collation sections with navigation by collation unit and source.Variations frame holds a table that displays the strings from each edition in a collation table. An annotation frame will display relevant annotations for the text selection, including those shared by multiple editions, Color intensity indicates intensity of the variance across a human-mediated ten-value range. Words and passages are encoded using this scale. At DH2019 we will share a more complete view of the interface, its annotation apparatus, and visualizations that will help guide the reader to sections where the most intensive editing took place. We look forward to conducting usability testing to enhance the interactivity and responsiveness of the digital interface in the coming months. We hope to provide a user experience that invites Frankenstein fans, new readers, and scholars alike to take pleasure not only in reading multiple Frankensteins but also in reading the story of how this novel changed over time. "
	},
	{
		"id": 40,
		"title": "Digital Curation For World-Literature Pedagogy At The Global Crossing Point of Singapore",
		"authors": [
			"Bhattacharyya, Sayan",
			"Gornall, Alastair"
		],
		"body": " We describe theoretical and practical issues raised by our experience of teaching a core course in world literature taken by all undergraduates at a technology-focused university in Singapore. Eric Hayot has argued that the loss of engagement with the allegorical and symbolic dimensions of the world, brought about by the geometrization of space and time wrought by Enlightenment rationality, needs to be addressed by attention to the complex world-making capacity of literature: how representations of space and time, and of their scales, operate within and across literary texts. Our technology-minded students experience this geometrization acutely. In response, we took world-making at face-value, having our students, literally, make things. Our students created group-based digital curation projects using the content-management platform Omeka. Our approach has some similarities with a use case for Omeka in pedagogy, but also many differences. Reflecting the history and cultural influences of the multicultural city that is Singapore, our curriculum draws upon a broad range of thinkers — from ancient Greece and China, the mediaeval Arab world, as well as early modern Europe and modern India. Our students focus on similarities and differences between the thematic contents of these texts from very different times and places. Each project functioned as an museum exhibit combining digital artifacts and curatorial statements, with students linking together concepts, ideas and/or objects within individual texts and/or across texts. Public-facing activities made the undergraduate classroom visibleand made pedagogy accountable by holding students to the expectation that they become reciprocal partners. We illustrate, below, how the allegorical and symbolic dimensions referred to by Hayotcan be accommodated in our use case. One of the texts that we use is Ḥayy ibn Yaqẓān, an allegorical novel in Arabic written by the well-known Arab Andalusian philosopher and polymath Ibn Tufayl in the 12th century. In the novel, the protagonist passes through a phase in his adolescence, during which he explores the natural world, taking apart things and dissecting animals — in what has been usually read as an allegory of a proto-scientific outlook and an anticipation of the scientific method. Another text that our students read is Descartess Meditations, where the question of radical skepticism and scientific inquiry is broached more directly. Thus, an allegorical dimension is implicated in a connection which a student might choose to show between Ibn Tufayls Ḥayy ibn Yaqẓān and Descartess Meditations — in the form, for example, of a labeled link in a network graph in which the nodes represent an artworkrepresenting specific contents from chapters or sections of these two texts. With regard to the symbolic dimension, our point of reference is C.S. Peirces notion of the symbol as a sign in which there is a conventional connection between sign and object. Thus, the symbolic dimension is involved when a student draws an analogy, representable as a link in the network graph between two nodes constituted by artworks representing the symbolic terms. For example, with respect to Red Oleanders, a 20th century Bengali play by Rabindranath Tagore that is part of our curriculum, a student could argue that darkness, as represented by the Kings dark cave in the play, is a sign for abstract knowledge, whereas light and color are signs for concrete objects. Likewise, a student could argue that in Platos theory of forms as elaborated in the Phaedo, an ideal form is a sign for abstract knowledge while instances of those forms are signs for concrete knowledge. Here, the twinned analogies that the student can establish between darkness and ideal form, and between light/color and physical instantiations of the ideal, operate in the symbolic dimension. Visual images from artifactsin that same production, could then be organized to provide a visible form to this analogy. The practice of making remains controversial in the humanities. Questions remain as to whether the emphasis on making distracts from the centrality of reading and writing in an introductory undergraduate class. For this reason, we are currently moving in the direction of a more hybrid approach in which students, while continuing to use Omeka, will focus more on textual rather than visual artifacts. "
	},
	{
		"id": 41,
		"title": "Disentangling the Hairball: Observing International Style in Kazuo Ishiguro’s Novels in Network Visualisations",
		"authors": [
			"Bieber, Jasmin"
		],
		"body": " When Rebecca Walkowitz proclaims that nothing [is] easier and nothing more contemporary than translation,she does not refer to computer-automated translation but to the recent phenomenon of numerous novels that are published in an international mindset. For this, she identifies three distinct types in which translation dictates literary production with one being written as translations, which Walkowitz describes as pretending to take place in language other than the one in which they have, in fact, been composed.In other words, these books are born translated. Multiple of Kazuo Ishiguros novels fit this definition, as they were written in one of two distinct cultural frameworks that reflect his personal national identification; British and Japanese. The acknowledged and self-announced international writerexplains, for instance, that the characters in his novel An Artist of the Floating World, were not only Japanese but [that] they were meant to be speaking in Japanese even though it was written in English.Another one such example would be his The Remains of the Day, which is contrary to his novels set in post-World War II Japan, narrated through the perspective of a British butler and engage[s] with recognisable English literary traditions.Among current international authors, Ishiguros oeuvre poses two distinct questions: Firstly, if style is dictated by the culture as well as the nationality of the country that is a novels focal point; and secondly whether it is possible to determine if they were written, as Walkowitz argues, as pre-translated. Digital stilometry offers the possibility to analyse his novels in a mode that goes beyond simply identifying culturally charged terms. Based on the assumption of Burrows Delta – while also considering variations of this formula by also applying for example Eders Delta and Cosine Delta – stylometric features generate a novels similarity in relation to a collection of texts of a corpus based on a variable amount of several hundred most frequent words. The difference between stylistic and stylometric features must also be stressed; where stylistics focus on what a text is saying among its meaningful words, stylometry would be more associated with how a text is written.The poster is aware of the polemic and generalised ascription of a novels style based on stylometric analysis, yet intends to explore the boundaries and possibilities of said approach to test the influence stylometric patterns have in differing cultural contexts. Beyond this methodological basis, it will be a central concern of the poster to represent this analysis in reflective, reproducible and flexible illustrations. For these means, this poster utilises the network program visone developed by Ulrik Brandes and Dorothea Wagner. Visone was designed to offer visual explorations of data through a seemingly simple interface that nonetheless allows for a variety of unique and advanced methods derived from social network analysis. Through means of disentangling, network visualisations can be obtained by filtering the original hairball – based on an adjacency matrix containing the measurements of the Delta formula – and be made increasingly more complex by adding visual labels and scaling representing the metadata of the relevant texts. In the case of Ishiguros oeuvre, information surrounding the cultural context of each novel – for instance the cultural setting and overarching cultural-sensitive themes – will be collected into a metadata table. This will be applied to the networks based on the novels stylometric similarities in order to discover either the potential correlations between differing stylometric habits and national as well as cultural context or their arbitrary nature. First results demonstrate a stable centrality of Ishiguros When We Were Orphans, a detective story, which narrates the protagonists endeavours to retrace and reconnect with his family that lead him from Britain back to China. Additionally, and contrary to previous assumptions, the two novels, which present a distinct Japanese setting, do not share a strong stylometric relation. Instead, further network plots modelled on different bases – like manually devised stop word lists and by dividing the novels in direct and indirect speech as distinct texts within the corpus – hint at a correlation of overall thematic contexts like homelessness, globalised identities and memory and the novels stylometricsimilarities. The poster intends to elaborate on these elements in order to disentangle the essential features of Ishiguros transnational oeuvre. "
	},
	{
		"id": 42,
		"title": "Nodes and Edges in Literary History. Modelling 19th Century Literary Landscapes",
		"authors": [
			"Bjerring-Hansen, Jens",
			"Sørensen, Nicolai Hartvig",
			"Jelsbak, Torben",
			"Fischer, Frank"
		],
		"body": " Who were the protagonists of 19th century European literature? And what are the promises and pitfalls when it comes to the modelling of the composition and dynamics of historiographical works with the means of network analysis? These are the central questions to be addressed and displayed in this poster. Data set The text, to which network analysis is applied, is Main Currents of 19th Century Literatureby the Danish literary critic Georg Brandes. Tracing the course of European thought in the first half of the nineteenth century by describing the most important movements in French, German, and English literature, it was a highly influential handbook with translations to several languages, whereby Brandes somewhat personal and idiosyncratic take on the subject to some extent came to represent a common European frame for the understanding of the literature of the recent past. In 1927, Thomas Mann, the German author and Nobel Prize winner, labelled the Main Currents… the bible of young, intellectual Europe. Data organization By a thorough, manual TEI-encoding through philologists establishing a digital scholarly edition of the some 3,000 pages work, different opportunities of digital explorations have been enabled and facilitated, not least, network analysis. All some 2000 historical persons featured in the work are tagged and, furthermore, identified to unique IDs, all linked to a relational database with metadata. For this investigation we automatically construct an edge between two nodes every time the entity occurs in the same paragraph of the text. Analysis On the basis of these highly curated and reliable data, the poster aims to show the results of our endeavours to analyse and visualise central and, hopefully, new aspects of Brandes vast and complex work. More specifically, and along the lines of modern historiographywe want to explore Brandes historical writings as texts reliant on strategies of narration and emplotment. Firstly, inspired by network studies of dominance relations in dramatic workswe want to identify the central characters in Brandes display of nineteenth century literature. We will investigate the different common measures for centrality: Which are the most appropriate for our data and why? What can they tell us that we didnt know beforehand? Secondly, and tentatively, following the authors own characterisation of his work as a drama in six parts, we want to study the dynamic composition of the six volumes with the help of network analysis, by extracting, visualising, analysing and comparing networks of each the six volumes comprising the work, whereby the problems of intertwining the static character of networks with the sequentiality of narrative will be addressed. Already suggested by Moretti 2011 and in recent time discussed and put to use in regards to computational methods, the application of network analysis as a tool for quantitative plot analysis has hitherto been restricted to works of fiction. Brandes essentially narrative approach to the writing of literary history as well as the biographical and sociological character of his criticism make such a combination of methods highly relevant "
	},
	{
		"id": 43,
		"title": "Discoverable Data Models and Extended Text Properties in the CITE Architecture",
		"authors": [
			"Blackwell, Christopher William",
			"Smith, David Neel"
		],
		"body": " The CITE Architecture is a generic framework for identification, retrieval, and alignment of information about things humanists study. The challenge of a generic framework lies in how it can handle theinnumerable specific kinds of data likely to appear in any non-trivial digital library. CITE allows abstraction of data from specific encodings of that data, while maintaining scholarly identity. This allows a digital library an open-ended ability to incorporate new formats or retrieval methods in a self-documenting plain-text serialization that maintains backwards compatibility. This paper will describe the implementation of Discoverable Data Models and Extended Text Property Types serialized in the CEX format and implemented in applications. Specific examples will begeo-spatial data, in which a place can be represented by a URI to a gazetteer, a latitude/longitude pair, or a geoJson string,textual data represented as plain-text, Markdown, or as a TEI-XML fragment, andimage collections, where the same image may be exposed as a JPG on a filesystem, via the IIIF-API, or as a DeepZoom file. CEX, the plain-text exchange format, can serialize collections and allow an application or service to discover these extended types or ignore them gracefully. All tools and data for these examples will be downloadable from GitHub. Citation of Versioned Collections The acronym in the title of the CITE Architecture stands for Collections, Indices, Texts, and Extensions. Texts are CTS-compliant texts, that is, texts canonically citable with machine-actionable CTS URNs because they implement the OHCO2 model, an ordered hierarchy of citation objects.Indices are simple URN to URN relationships, subject-verb-object relationships akin to RDF triples, with the proviso that subject, verb, and object are canonically citable by machine-actionable URNs. Collections are of data objects and may be ordered or unordered. The CITE Architecture allows collections of objects to be cited at many levels of abstraction and specificity. urn:cite2:botcar:catesbySpecimen: A citation to a notional collection of of botanical specimens collected by Mark Catesby. urn:cite2:botcar:catesbySpecimen.2018: A citation to a specific version of a collection of of botanical specimens collected by Mark Catesby. urn:cite2:botcar:catesbySpecimen.2018:223 A citation to a specific specimen in a specific version of the collection. urn:cite2:botcar:catesbySpecimen:223 A citation to a specific specimen in any version of the collection. This recognizes that the specimen is an object of study that might have different expressions in data. A version of a CITE Collection is defined by its properties and their values. Each property is citable by URN: urn:cite2:botcar:catesbySpecimen.2018.label: The label property in a specific version of a collection. urn:cite2:botcar:catesbySpecimen.2018.binomial: The binomial property in a specific version of a collection. An notional object is instantiated in a versioned collection by the sum of properties and their values: urn:cite2:botcar:catesbySpecimen.2018.label:223 The label property for object 223 in a specific version of a collection. urn:cite2:botcar:catesbySpecimen.2018.binomial:223 The binomial property for object 223 in a specific version of a collection. Two versions of a notional collection need not have the same properties. Properties are typed, at a very low level. CITE defines valid types as: StringBoolean Number Cite2Urn CtsUrn Compositions of Scholarly Primitives The CITE Architecture defines scholarly primitives: texts, or objects in versioned collections. Objects in versioned collections consist of a set of typed properties, with a very limited number of types. This makes the CITE Architecture flexible and relatively simple: libraries for working with two types of URN, libraries for manipulating corpora of texts, and libraries for dealing with objects in collections, libraries for managing relations of URN to URN. All CITE data—texts, objects, and relations—can be expressed in plain text, and CEX, the Cite Exchange format, can serialize a digital library, or a part of a digital library, as plain text. CITE-aware services or applications can load data from CEX. the Homer Multitexts interactive web-application , the Homer Multitexts microservice and more specific applications exposing digital libraries for teaching , all read CEX files as their input. Extensions I: Connecting to the Physical World The E in CITE is Extensions, additional discoverable information providing richer interaction with the basic scholarly primitives. A CITE Collection can describe a collection of images. A very basic image collection might have the properties label, license, and caption. Clearly, while we can serialize this information easily as plain-text in CEX, resolving a URN to binary image data requires a connection to the physical world. A notional image might be resolved to a JPG file, to data delivered by the IIIF API, to a DeepZoom file, or to any combination of these. CITE and CEX solve this problem by means of discoverable data models, additional datathat can identify specific collections of images as being served by one or more binary image services. In this case, an additional Binary Image Service collection associates a collection with: A type of image serviceA URL to a service hosting images from the collection Filepath information necessary to resolve an images URN to files on the server. A working example of this is the Homer Multitexts interactive web-application . The CEX of the HMTs data release identifies image collections as being expose both as DeepZoom files and via the IIIF-API. The web-application takes advantage of both of these to provide thumbnail views and interactive zooming views. Extensions II: Different Expressions of Textual Data An object in a version of a collection might have a property of type string, and that is easily discoverable with the basic CITE tools. But of course, a string might be plain text, Markdown, some form of XML, or some other encoding. It is easy to imagine a project publishing a version of a collection of comments as plain-text, and subsequently publishing a new version that adds some markup to those comments. Because the CITE2 URN allows identification of notional collections, versioned collections, individual properties in versioned collections, in each case across the collection or filtered by an objects identifier, we can expose additional information about the nature of a property of type string. By means of a discoverable data model, just as we associated whole collections of images with different binary image services, we can associate properties with different encodings, without losing scholarly identity. This paper will demonstrate a notional Collection of comments on the text of Herodotus, expressed in three different versions: one with comments in plain-text, one with comments formatted as Markdown, and one with comments formatted as TEI-XML fragements. These discoverable extended string property types are ignored by any application that is unaware of them, but exploited for human display by applications that are aware of them. As a working example of discoverable extended string property types, we can point to a CITE Library of lexicon data, based on the openly licensed XML versions of the Liddell-Scott-Jones Greek Lexicon , the Lewis & Short Latin Dictionary , and Strongs Hebrew Lexicon . Data for each of these serialized in a CEX file, and served through a Microservice. Querying the service shows that the lexicon entries, of type StringType can be read as plain-text: http://folio2.furman.edu/lex/objects/urn:cite2:hmt:lsj.chicago_md:n2389 . But because this collection identifies that property as extended by Markdown, an aware application can process the plain-text expression and apply formatting: http://folio2.furman.edu/lsj/index.html?urn=urn:cite2:hmt:lsj.chicago_md:n2389 . Extensions III: Different Expressions of Real World Data Digital Gazetteers such as the Pleiades project[^pleiades] have solved the problem of scholarly identity across historically diverse placenames, so Naulochon, Smyrna, and Izmir, different names for the same place, are canonically citable as https://pleiades.stoa.org/places/550771. But for very sound technological reasons, we might want to express the location of Izmir by 550771, by its full Pleiades URI, by 38.440912, 27.14781, by 27.14781, 38.440912, by {type: FeatureCollection, features: [{type: Feature, properties: {}, geometry: {type: Point, coordinates: [27.14781, 38.440912 ] } } ] }, or by <?xml version=1.0 encoding=UTF-8?><kml xmlns=http://www.opengis.net/kml/2.2><Document><Placemark><ExtendedData></ExtendedData><Point><coordinates>27.14781,38.440912</coordinates></Point></Placemark></Document></kml>. A collection of, for example, ancient places mentioned in Herodotus, as a publication should separate the notional scholarly objects—Sardis, Athens—from any particular technology for locating those objects on a map. The CITE extended string property types allows different versions of such a collection to record locations in any of a variety of formats, or to mix formats within a single expression, e.g. latitude and longitude for a simple point, with geoJson for describing regions. The presentation will include a demonstration dataset illustrating this. Conclusion CEX, as a line-based, plain-text serialization of diverse data—texts, collections, relations—is a convenient, future-proof, and open means of data exchange for small projects and large project.With discoverable-data-models and extended-text-property-types, CEX can serve data in a variety of current and future formats; these formats are discoverable by applications that, but degrade gracefully back to generic, plain-text in generic CITE applications. The talk will point to example CEX files and applications, on GitHub, demonstrating these capabilities with scholarly data. "
	},
	{
		"id": 44,
		"title": "Hyper Audio Linking – Generating Hybrids of Text and Video Content for Digital Publishing",
		"authors": [
			"Blaha, Agnes",
			"Findeisen, Andreas Leo"
		],
		"body": " The Digital Humanities as a field of research and publication face both new opportunities and challenges due to the increasing number of media sources relevant to researchers and students, no matter their age and origin. Aggregator platforms like YouTube or Vimeo let formerly unknown materials surface that are relevant for new research in the humanities. Meanwhile, large public and private collections held by historical organizations, broadcasting corporations and NGOs, or individuals like artists, scientists or politicians remain to be indexed, investigated and re-published. At the same time, researchers who are experimenting with new, creative ways to combine curating, scholarship and presentation draw attention to the immersive and narrative potential of sound-based media. Some of the use cases relevant to the field of digital humanities that will probably become even more frequent over the next years include: Support for oral history researchers who need to transcribe large amounts of recordings Using transcripts for lectures and online learning tools, both for students and the public at large Enhancing the usability of digital A/V archives via full text search Facilitating community contribution and citizen science projects Furthering the accessibility of A/V contentIntertwining multimedia material and text for research papers published in online journals Alternating between more theory-focused discussion and hands-on experience, we will different methods to integrate transcripts into multimedia formats that are useable for research and publications. After a brief introduction into the principles and current possibilities as well as the limitations of computerized speech to text transcription, particularly in comparison with tools for manual transcription such as WebAnno and OCTRA, we will introduce participants to a small number of different speech-to-text software packages that can be used to semi-automatically transcribe A/V content, while discussing their respective pros and cons. We will then demonstrate how to use the open source package ffmpeg to extract and convert various types of A/V recordings to formats suitable for further processing and try out automatic speech to text conversion with OH portal, an online transcription tool developed and maintained by the phonetics research team at the university of Munichand Watson, a powerful, trainable natural language processing tool developed by IBM. For many in the DH community, creating a good transcript is just the start. Presenting research to fellow academics and the public at large, garnering interest for archives and projects, creating lively and easy to use learning material - all while preserving the non-verbal aspects of raw sources - are often just as important. This is probably even more true for novel, hybrid formats which have been theorized to be result of an amalgamation of analog and digital publishing. As an exemplary solution, we will introduce Hyper Audio Linking, a technique for the presentation of digital content that allows to link transcripts to pre-defined jump marks in video or audio recordings via JavaScript. HAL augmentation deals with diverse content aspects of a source while letting the original untouched. It can be used for all kinds of sources, be it contemporary recordings of lectures and panel discussions, interviews, or historical footage, as well as some art genres like theatre plays, films, or audio dramas. By defining sections or chapters and linking them to timestamps, the transcript can be used to jump between those parts in the original media, allowing recipients to switch between reading or viewing/listening mode. The resulting multi-media hybrid can be further augmented by defining formatting options for different types of content and by including various metadata, images, annotations, keywords, and the like. The result is an elegant and easy to use frontend interface that allows for full-text search and easy navigation within A/V content. A simplified workflow of an oral history project that uses HAL both as support for researchers and to publish research results and multimedia documents can be seen in figure 1. Importantly, HAL-augmented files can not only be used for the final step of publishing and presenting results. Depending on the configuration of jump marks, they can also support members of the research team in navigating recordings or in tracing observations, selected utterances and topics over multiple sources. Meanwhile, the method is flexible enough to be introduced on the fly at any point in time. If desired, its use may also be restricted to a certain part of collected sources, e.g. only those that are selected for public access. Figure 1: Sample workflow with HAL integrated in an oral history project Using an existing interface to a database of videos on history and politics of digital culture, participants will experiment with different ways to use HAL links within transcripts. We will discuss and compare our Drupal-based implementation with other existing tools such as ELAN, an annotation tool for A/V content developed at the Max Planck Institute for Psycholinguistics in Nijmegen. We will also talk about some of the key differences between software designed to work as a standalone tool for researchers versus a method developed for usage either as the interface of an online archive or as an enrichment or additional feature of web publishing formats. To round off the workshop, we will start to build a web page featuring A/V content, a transcript, and additional material such as photographs and annotations from scratch. Using just a text editor and a few simple lines of Html and JavaScript code, participants will learn to apply HAL augmentation to their own publications. We will use a pre-configured installation of the open source content management system Drupal that can be customized to fit different use cases, including curated online collections, project documentation, event pages etc. For this second practice part of the workshop, participants should ideally bring their own A/V sources and start working on their personal project. Those who dont have a specific use case in mindwill be able to choose among different sources provided by the workshop team instead. The last workshop unit will be a discussion dealing with the editorial planning for publication. Whether you are an individual producer, a research team or an online cluster, you will have to make certain decisions in how to manage the publication: Who is the audience that will be primarily addressed? Which specific needs have to be taken into account with regards to usability, accessibility and technological literacy? How muchpre-existing knowledge on the topic can be assumed, and how much introductory information should be included? Target audience and requirements Previous workshops and presentations of Hyper-Audio-Linking have been held at BASICS, transmediale Berlin, at the Ludwig-Boltzmann-Institute for Media.Art.Research, Linz, and most recently as part of the EADH conference Galway 2018. Based on these earlier workshops on similar topics, we expect to work with a group of between ten and fifteen participants, which seems ideal for a more hands-on experience. There are no skill requirements for participation, all introductions into software usage and programming will be suitable for beginners but can be easily adapted to participants with more advanced levels of pre-existing knowledge. Participants should bring their own laptop. Participants who use a laptop provided by their employer should make sure to either have all software pre-installed or know that they are authorized to install software on their computer. All required software is either open access or free to use, there will be no additional costs. Pre-conference support and provision of material Participants who would like to familiarize themselves with HAL in advance will be given access to stubs of prepared A/V content on our platform Transforming Freedom.org four weeks before the conference start. Download links for other software packages and instruction material on how to install and use the respective software will be made available to the participants about two weeks in advance of the workshop. Intended length and format The duration of the workshop will be a half day, that is, approximately four hours including breaks. As far as the format is concerned, we will alternate between theory-focused presentation, group discussion, and practical tasks that let participants try out different techniques supported by the instructors. "
	},
	{
		"id": 45,
		"title": "Relational Perspectives as Situated Visualizations of Art Collections",
		"authors": [
			"Bludau, Mark-Jan",
			"Dörk, Marian",
			"Heidmann, Frank"
		],
		"body": " Fig. 1: A relational perspective on the Hausmann collection centered around a selection. Link to the prototype: https://uclab.fh-potsdam.de/hausmann Introduction Akin to digitization processes, cultural heritage institutions are digitizing their inventories and are looking for approaches to devise meaningful digital representations of their collections. In contrast to traditional systems that use keyword search as the main mode of accessing collection items, usually requiring familiarity with the collection, a growing body of work demonstrates how visual interfaces can support more exploratory and serendipitous modes of accessing collections. Interactivity is often used as a strategy to traverse the extent and complexity of information visualizations. Despite the growing variety of visualizations of cultural collections, researchers like Drucker argue that many of them are transferred uncritically to the humanities, ignoring fundamental aspects of humanistic research, such as interpretation, ambiguity or uncertainty, and the specificity and situatedness of a given point of view. While visualizations that focus on providing one overview are suitable for the display of patterns across entire datasets, they often fall short of visualizing relations and similarities at the level of individual items. Additionally, the limitation of using only one view implies a loss of information and may lead to distorted perceptions. Referring to the relational importance of an individuals social context, Latour et al.conceptualize the individual relations in large social networks as monads. Here, monad refers to a conceptual point of view that defines an entity through its many particular relations to all other entities of a dataset. Monadic exploration is a visualization technique for relational information spaces, in which each node can be selected as a navigation point to trigger changes in flexible layouts. Following the premise that for cultural data the individual relations can be just as important as an overview of the whole, we investigate the potential of relational perspectives in the visualization of cultural collections. We engaged in an iterative prototyping and research process in close collaboration with the Berlinische Galerie – museum of modern art, using their Raoul Hausmann collection as a case study. Based on the exchanges with our collaborators, a co-design workshop and prior work on collection visualizations, our research ambitions can be summarized by these design goals: 1. Provide multiple, flexible views on the data. 2. Reveal the individuality and diversity of the artifacts. 3. Promote open exploration and serendipitous discovery. 4. Expose the temporal context of artifacts. 5. Acknowledge uncertainties and gaps in the data. Taking a relational perspective on a collection To examine the potential of relational perspectives in comparison to overviews, the design of the visual interface contrasts two approaches: 1) an overview of the collection, and 2) multiple perspective views, each centered around one artifact. The overview serves as a landing page providing a comprehensive representation of all artifacts arranged by media type/genre and contextualized by visualizations of the temporal and social relations. Although we are particularly interested in relational perspectives, we acknowledge the benefits of an overview as an entrance point to a collection. The overview consists of three connected visualizations, a vertical timeline, artifacts displayed by genre, and a people/relations diagram. In the center, all artifacts are organized in small thumbnails by genre of the respective items. Hovering over an item displays a bigger thumbnail. Titles of the genres also function as a legend for the colors of the other visualization parts. Fig. 2: Overview: a) top-left: no selection, absolute timeline format; b) top-right: person selected, absolute timeline format, hovering contextual information; c) bottom-left: year and person selected, relative timeline format, hovering an element; d) bottom-right: keyword selected, relative timeline format In the timeline, which supports absolute and relative scales, each year inside the time frame of the collectionis visualized by a horizontally stacked bar consisting of small aligned rectangles, one for each artifact in the color of the corresponding genre, representing the total quantity of each genre per year. Small icons next to a year reveal additional contextual information. The arc diagram displays all persons and relations that are linked to artifacts within a selected time frame. Each person that is connected to one of the items is represented by a circle, whose size represents the total number of artifacts connected to the respective person in the current selection. The ordering, font size, and size of the nodes depend on the number of related artifacts. Selections of a year in the timeline, a person in the relations diagram, or a keyword via the search bartrigger highlights or activate filters. A click on an artifact triggers a switch to the perspective views, starting with the attribute-based perspective. The perspective views are primarily based on individual, yet relational viewpoints anchored by a selected artifact and its unique connections with other items. The prototype offers three views, two faceted perspectives and a temporal perspective, each enabling access to a detail view. The core functionality of the two faceted perspectives is to display related items in order to stimulate open-ended exploration and increase the likelihood of serendipitous discoveries. While the attribute-based perspectivedisplays general metadata of the object, the content-based perspectiveis based on manually authored and automatically extracted keywords. Fig. 3: Perspective views: a) top-left: attribute-based, b) top-right: content-based; c) bottom-left: temporal; d) bottom-right: detail view Both views are structured around a selected item, displayed in the center of the page along with an image and basic information. Other data attributes/keywords are displayed once above and once below the selected item; related artifacts sharing the same attribute/keyword are displayed above/below these labels, roughly ordered by their temporal distance to the selected object. All artifacts created in the same year as the selected item or in later years are displayed above the selected element, all artifacts that were created in the past are displayed below, followed by undated elements. In addition to the ordering, temporally close elements are larger than those that lie further in the past or future. Additional encodings help to identify genres, the number of shared attributes with the selection, duplicates in other attribute columns, and uncertainty. The selection of another artifact causes a rearrangement of the visualization based on the artifacts and attributes related to the new selection, resulting in many unique visual and semantic fingerprints. Fig. 4: Close up of a faceted perspective view Fig. 5: Close up of temporal perspective view In contrast to faceted perspectives, the aim of the temporal perspective is to induce iterative navigation from one collection item to the next along temporal vicinity by revealing precise dating of artifacts, including uncertainties. The selected element is positioned on the left side of the interface, marking the moment in time it was created. All remaining elements of the collection are represented in a scrollable vertical timeline around the selection. While a dot indicates a precise point in time and a bar represents an uncertain time interval, dating based on uncertain assumptions is marked by a texture. Conclusion Based on the observation that many common visualization types cannot represent the complexity and uniqueness of individual artifacts of cultural collections, we developed the idea of multiple visualizations acting as relational perspectives. Findings from a co-creation workshop, frequent exchanges with our collaborators, and a thorough literature review indicated that such individual perspectives on artifacts may offer a richer and more diverse approach to cultural collections. To examine the potential of such an approach, we carried out a case study using the Hausmann collection with the intention to put focus on the individuality of each collection element by providing a variety of visualizations, in particular an overview of the entire collection and three relational perspective centered around individual artifacts. Usage logs suggest that, while many visitors actively used the perspective views as exploration tools, the provision of an overview still proved to be useful for others. The present research suggests that a deliberate consideration of a diversity of situated views holds promise for the digital exploration of cultural collections. By displaying relational perspectives of each individual artifact and by acknowledging uncertainties we propose an approach to promote exploration, user-dependent interpretations, and furthermore we emphasize the consideration of perspective in future projects. The conceptual ideas are very much open for further development, but we hope that this research contributes to the exciting research carried out by a growing transdisciplinary community at the intersection between design and the humanities, where critical, aesthetic, and functional considerations converge. "
	},
	{
		"id": 46,
		"title": "The Complexity of Character-building: Speech, Portraits, Interactions in Leo Tolstoy's \"War and Peace",
		"authors": [
			"Bonch-Osmolovskaya, Anastasia",
			"Daniil, Skorinkin"
		],
		"body": " The introduction of computerized methods for philological analysis is compounded by the richness, complexity and multidimensional nature of literary texts. As it has been coined by Ju.M. Lotman, the arts should be considered as a secondary modelling system, whilst the natural language is a primary one.. Computational models allow to extract and analyse linguistic features of the text - POS tagging, syntactic structures, word frequency, and semantic domains of word classes. Meanwhile the most important elements of textual poetics remain outside the scope of application of these tools. As a result, the most rapid development of computational methods for literary study is observed in the field of computational stylistics and stylometry,,,, and thematic modelling,, which may be deployed using bag of words approach only without any specific textual mark-up. It seems that one of the most important reasons why distant reading methods are still not regarded as a mainstream in literary scholarships and are often looked at with suspicion by traditional philologists is their extremely difficult access to textual complexity, the layers of the secondary modeling system. In fact, even in the cases of modelling some complicated phenomena such as social networks of characters or plot elements extraction, the studies are more concerned on engineering but not on the research issues, which means elaboration of computational but not literary analysis methods. This paper aims to introduce a new approach to the task of capturing textual complexity. We use five methods to model the character system in a novel, each one is aimed to discover one of the layers of this system. The combination of these layers gives as a result a complex view on the novels composition enriched by computationally obtained data, quantitative and statistical metrics and graphical schemes and networks We apply slylometric and alternative non-lexical analysis to characters direct speech, two alternative methods of network analysis to model characters interactions and clustering method for comparison of portrait descriptions in Leo Tolstoys War and Peace. The Tolstoys great novel which counts hundreds of characters among which several dozen may be viewed as prominent, serves a perfect material for such a study. We claim that with the help of the complex layer analysis we can reveal some new structural constituents of the novel composition, that could not be captured by traditionalinterpretations of Tolstoys poetics. Preliminary preparations The complex layer analysis of the character system requires thorough and precise mark-up. All automatic or semi-automatic mark-up has been checked and corrected manually if needed. First of all, all the characters have been encoded with TEI labels. This procedure was also important as far as the characters may be referred to by several different namesand anaphorical pronouns. Secondly, all the dialogues in the novel have been identified and connected to speakersand their TEI labels. Finally, all the portrait descriptions have been elicited with the help of semantic mark-up, borrowed from the National Corpus of the Russian Language. Next stage of the portraits mark-up involved encoding one of the four types of Tolstoys way of designation of his characters appearances - metaphorical, emotional, portrait, and value expression. Each sentence of the description was also supported by the integral sentiment assessmentMethodology Three major traits of literary characters have been studied with the help of computational means: speech, portrait and social interactions. Two methods have been used for character speech analysis. The first method exploits basic stylometric analysis. It measures the distance between direct speech of different characters by calculating the distribution of top keywords for speech sentences of every prominent character. The stylometric method refers to the layer of topical connections between the characters in the novel. The main oppositions which are set by this method are the oppositions between men and women, and between the Moscow and Saint-Petersburg circles. The second method presents an alternative to the stylometric approach. It compares non-lexical characteristics of the direct speech sentences, such as ratio of words and punctuation in a speech sentence, exclamation and question marks, frequency of discourse words and readability score. These parameters differentiate the characters by their manner of speech, in particular distinguishing oral and written types of sayings as a principal opposition. Each considered character has been defined by a vector, which accumulates the mean value of all the parameters. The clustering model brings together the vectors with closest distance. The PCA analysis shows that this layer is sensitive to family similarity, as it opposes Natasha and Nikolay Rostovs to Andrey Bolkonsky and his sister Princess Mary. The analysis of portraits was also based on vector clustering. This method builds the vectors for each prominent character out of two metrics: the first one combined the normalized frequency values of each type of portrait description, the second referred to the frequency values of four types of sentiment assessment. The hierarchical tree of this layer brought together the vectors of the main pairs of the novel: Pierre and Natasha, and Nikolay and Marya. Surprisingly the analysis also revealed intrinsic similarity between the two bad guys of the novel: Napoleon and Dolokhov. This layer concerns concealed parallelism in Tolstoys way of thinking and describing his characters. The social interaction has been measured by two semantic networks built on different bases. The first method sets the connection between the characters that talk to each other. This network reveals the main communities of the novel, the characters that have intensive communications cluster together. One could suppose that the layer captured by this network reflects the family dramatical part of the plot. The second method sets as a connection the fact of two characters mentioned together. The two networks differ in a crucial way, in particular within the war parts, where the dialogue communications happen less often then in the peaceful parts. This layer stands for the epic part of the novel, thus Napoleon and Kutuzov form one cluster here, though they never communicate and are at utmost distance from each other on the previous network. Discussion The five layers of the character system in War and Peace are defined by the five different methods of computational analysis of literary text. All the methods are in no way new to digital humanities: stylometry, clustering, PCA, corpus and network analysis. The important conclusion needs to be emphasized: all the methods when applied to clean, well marked up data give different results and lead the researcher to different interpretations. At the same time the masterpiece contains all the interpretations within itself. To say more the inner architecture of the text supports and interconnects in a very sophisticated way all the interpretative layers. This point may be proven by the following example. If we consider the whole novel excluding the epilogue, we may see that the speech and network layers group the five main characters of the noveldifferently, but no grouping reflects the romantic relationships of the novel. Figure 1. Connections/proximity of the five main characters, revealed by the 4 layers of speech and interaction analyses of the novel without the epilogue: no romantic relations reflected on these layers Figure 2. Connections added by the analysis of the epiloguereveal the romantic connections of the main characters) It is in the epilogue where the whole construction gets the stability and the romantic attractions of the characters are openly demonstrated to the reader. The Prince Andrey, though not alive in the epilogue, is still mentioned a lot in the talks of Natasha and Pierre, thus the romantic triangle which has never been defined explicitly during the novel is resolved in the epilogue. Amazingly, the fifth layer not mentioned above, which reflects the way Tolstoy depicts and presents his characters, sets the connection between the members of the two happy families of the epilogue from the very beginning. "
	},
	{
		"id": 47,
		"title": "E Pluribus Unum: a Uniform DL Solution for Historical Data Management, Archiving and Exploitation of Opera",
		"authors": [
			"Bonora, Paolo",
			"Pompilio, Angelo"
		],
		"body": " Among the performing arts, opera is undoubtedly the one that presents the greatest complexity, since it is made up of a variety of texts that differ in nature, but which are closely coordinated. As a result, documents relating to opera come in multiple formsand cover the entire life cycle of an opera: the verbal texts, the musical score, materials for the operas performance in the theatre, and documents that relate to the performance itself. The conservation of these heterogeneous sources is usually entrusted to different institutionsaccording to the type of materialand catalogued using methodologies that vary according to the type of material. The documentation relating to an opera is thus to be found in different institutions and is described in different information systems which are often not compatible. The material is therefore not always easily accessible for consultation. Our initial approach was to adopt Functional Requirements for Bibliographic Recordsas a conceptual solution for a unified management of documents and their contents. The result is a highly specialized model in which information is collated and analysed in depth. But, to fulfil the functional perspective prescribed by FRBR, cataloguing must take account of knowledge of the domain, and the interests and expertise of different categories of users. One of the aims of the Corago LOD The project web site address is: http://corago.unibo.it/lod project was thus to demonstrate that by adopting DL it would be possible to express domain knowledge at different descriptive levels within a single coherent semantic model. The FRBRs conceptual hierarchy is defined by the three core classes: Work, Expression and Manifestation. In the domain of opera, these three classes are represented by the headings Opera, Event and Document, which become entry points for accessing information about repertoire, chronology and documentary sources. Diagram 1- From Works to Performances and Librettos The path that leads from the conceptual content of operas to its two expressions, namely performances and librettos, can be expressed using the FRBR Object Orientedmodel. Moreover, by transposing knowledge from the original relational model to RDF, using CIDOC Conceptual Reference Modeland FRBRoo as main reference ontologies, it was possible to overcome constraints deriving from the original implementation of FRBR by using a proprietary model such as data interoperability. Access to information contained in catalogues, archives or directories has often been limited by a lack of interoperability provided by the information system used. From the users point of view, a knowledge based facility such as Linked Open Datalowers the access threshold because alignment is at a basic semantic level. Increased interoperability also leads to a wider dissemination and circulation of information. As this information becomes part of the knowledge held in other systems, its availability would be guaranteed even beyond the capabilities or aims of the original project. As a result, LOD interoperability both facilitates universal access to digital resources and is a viable strategy for preserving information in the long term. Conversely, the adoption of formal ontologies often produces data structures that are complex and cannot be browsed in a user-friendly way. The project faced two major factors of complexity introduced by RDF data representation: the deeper level of analysis and the morphological intricacy of the graph. The analyticity is directly linked to the fine-grained accuracy of the ontology adopted for the definition of the dataset, while the complexity of the graph structure derives both from the fine-grained conceptual model and the amount of information that the repository contains. As a result, providing synthetic and effective representations of data to end users becomes challenging. The solution presented exploits some basic mechanisms of the DL such as generalization and inheritance. Through generalization it is possible to build progressive levels of abstraction on top of basic RDF triples. These intermediate layers act as semantic lenses which are capable of focusing the functional requirements expressed by users. While the traditional design of cataloguing applications based on RDBMS requires that data extraction and representation tasks follow a business logic, the transition to semantic models allows intermediation to take place on a conceptual level. An intermediate layer can be formally introduced within the semantic model of the repository to provide a user-friendly representation of the domain entities. This layer would be designed by the domain expert according to user needs. Diagram 2 - Multi layer semantic architecture The same approach has been adopted with regard to access to domain knowledge. The goal in this case is to make information from diverse sources accessible through search criteria which start go from plain text level up to criteria specific to the field of music. The aim of the project was also to identify semantic references between results and search criteria. We realised this goal by introducing a full-text semantic index within the dataset. Indexing is based on a specific class of properties that identifies the relationships between the core entities of the domain and terms that would be used to search each class of entity. The system can thus provide the reference context that produced the results displayed. This allows users to navigate the results in a more controlled and accurate way. This kind of semantic indexing becomes a tool for managing both the complexity of the domain, and for making with information more accessible in a distributed environment such as the LOD ecosystem. Finally, in our experience, identifying the layers of abstraction used when searching and navigating knowledge constitutes a kind of conceptual modelling which, since it is part of the semantic model of the knowledge base, also becomes part of the competence of the domain. In other words, modelling the way opera history is being accessed and represented becomes a possible way of reducing the intrinsic complexity of the domain as experienced by users. "
	},
	{
		"id": 48,
		"title": "VELUM : Towards Innovative Ways of Visualising, Exploring and Linking Resources for Medieval Latin",
		"authors": [
			"Bon, Bruno",
			"Alexandre, Renaud",
			"Nowak, Krzysztof",
			"Vangone, Laura"
		],
		"body": " The medieval civilization can only be investigated by means of the study of traces that have survived to our times. The best source of our knowledge is the texts, preserved in huge quantity and variety. Written mainly in Medieval Latin, they have not benefited from recent advances in computational linguistics and digital humanities in general. The paper briefly presents and reports on the early development of the project VELUM – Visualisation, Exploration et Liaison de ressources innovantes pour le latin médiéval. The ANR-funded projectis a first step towards an innovative digital environment for the study of the language and culture of medieval Europe. It has a challenging and ambitious goal: to build foundations for empirical research in medieval culture, language and history, by providing an appropriate environment for textual source analysis. We aim at building, firstly, a largebalanced corpus of Medieval Latin texts composed between 500 and 1500 AD all across Europe. In order to assess the population that we would like our corpus to reflect, we rely mainly on the most comprehensive list of Medieval Latin texts, namely the Index Scriptorum Mediae Latinitatis. A representative corpus should make it possible to avoid the bias that affect existing text collections, resulting from uneven and unjustified distribution of sources of different date, genre and place of composition. For example, medievalists have at their disposal big digital text collections, but none has been conceived in order to give an idea of the whole production in Medieval Latin in its variety. They tend to focus either on specific domainsor territories. As to the chronological variation, the texts written before 1200 are generally far better represented than the texts of the Later Middle Ages. The corpus will also attempt at remedying another shortcoming of existing text collections: since they usually are not balanced, they do not lend themselves to any sound statistical analysis. Apart from wide geographical and temporal coverage, the corpus will also reflect the variety of genres practised in the Middle Ages, as well as the functional richness of the medieval textual culture. In order to enable automatic processing, the texts will be annotated with part-of-speech, lemma, time and place labels. The compilation and annotation of the corpus, albeit extremely important, will be only a first step of the project. Secondly, a corpus search engine will be built with the help of the CQP-Web software. The users will be able to query the texts and benefit from their rich linguistic annotation through a user-friendly interface. Thirdly, the project aims at developing a set of efficient statistical analysis and data visualisation tools that researchers would embed in their own workflows. Written mostly in R, scripts, programs, wrapper functions will allow for advanced study of Medieval Latin vocabulary, but will be applicable to other languages as well. Both the texts and the tools will be made freely available to the scientific community through the projects website and public code repositories. After the brief presentation of the projects goals, we will report on the workflow and the first results of the project that has started in March 2018. During less than two years, we intend to cover every aspect of corpus compilation from digitizing paper edition to linguistic annotation of the machine-readable texts. The first stage started with intensive work of cleaning and structuring the texts; some of them already are available in interoperable formats, but many are not. Our first work consisted, then, in selecting and retrieving about 1500 scholarly editions of Medieval Latin texts, among others from the Web. Image files which drastically differed in quality needed to be cleaned and standardized. To that purpose we employed a set of optimization scripts and tools such as ScanTailor. The TIFF files were next sent to the OCR engine. We chose the Tesseract-4 as our OCR engine for both its accurateness and performance. The quality of the OCRed text was optimized through error profiling and selective manual proofreading with the latest version of the Post Correction Tool software. The PoCoTo software addresses the problem of recurrent OCR errors by making easier both their detection and batch correction by even less technically-oriented users. Figure : Using PoCoTo for OCR correction The next phase of this early development will consist mainly in separating Latin from non-Latin text. The text blocks will be automatically annotated with Latin or Non-Latin labels by using a simple classifier. We will next employ Transkribus to verify the results of the automatic classification and to manually remove redundant text blocks such as editorial introductions, footnotes, indexes etc. After that, the texts will be exported to the TEI-XML format and annotated with metadata. Once a rough representativeness and balance of this corpus is achieved, the texts will be lemmatized and PoS-tagged using the Treetagger with the parameters developed for Medieval Latin during the Omnia Project. "
	},
	{
		"id": 49,
		"title": "Diversity WorkshopDiversity Implicit Bias and Cultural Cloning PrivilegeIntersectionality ",
		"authors": [
			"Bordalejo, Barbara",
			"O'Donnell, Daniel Paul"
		],
		"body": " This workshop seeks to create awareness of diversity and cultural differences. This is especially important within the discipline of Digital Humanities because of its emphasis on collaborative work which brings together different individuals. Moreover, our work has shown that in Digital Humanities, where there could be many different voices, most of the power and prestige remain centralized in the Global-North, in Anglophone countries. This is not surprising considering the general state of academia as a mostly male, mostly white environment. For this reason, its essential to work with individuals to foster a richer environment, to change behaviours, and to challenge prejudices. Reflecting on these matters also allows us to expand the horizons of our own limited perspectives. An article published in the Scientific American, How Diversity Makes Us Smarter, states: Decades of research by organizational scientists, psychologists, sociologists, economists and demographers show that socially diverse groupsare more innovative than homogeneous groups.In DH, creativity and innovation are essential for the development of new approaches or new applications of traditional methodologies. ODonnell has argued that diversity is a core value in Digital Humanities: Diversity—in the sense of access to as wide a possible range of experiences, contexts, and purposes in the computational context of or application of computation to the study of problems in the Humanities, particularly as this is represented by the lived experiences of different demographic groups—is in fact more important than Quality, especially if Quality is determined using methods that encourage the reinscription of already dominant forms of research and experience.. This workshop was first commissioned by Karina Van Dalen-Oskan in 2016 to be delivered to the members of ADHOs steering committee. We delivered it first in Montreal 2017 and later in Mexico City 2018. These experiences have shown the workshop topic is a moving target as community norms adjust and change. In light of the last two workshops, we have modified and updated the content and activities for this new proposal. Contents: Different concepts of diversity, with particular emphasis on cultural and contextual differences. Notions of implicit biasand Cultural Cloning. The impact of Implicit Bias on teachers and educators, hiring committees and other bodies making decisions about others. We carry out exercises on implicit bias. To work on the concept of privilege we have developed a game which we use to illustrate the many factors shaping our lives. Particularly in the case of privileged individuals, it is not easy to locate and categorize their own instances of privilege. When two or more oppressive systems overlap and negatively impact an individual, we talk about intersectionality. This core concept allows us to explore how privilege or lack thereof impacts individuals directly. At the end of this workshop, we aim to guide participants to reflect on the benefits of a more diverse working environment by questioning our preconceived notions of sameness as an ideal. Discussions in the context of this workshop deal with various delicate subjects and, for this reason, individuals are likely to experience bonding within their working groups. We consider this a fortunate side effect of the workshop. The workshop is directed at anyone with an interest in understanding diversity in digital humanities and creating a welcoming and inclusive DH environment. Conference organizers, leaders in the field, and those who often form part of hiring committees are invited to participate. Everyone is welcome to attend, but we particularly encourage the participation of people who are in privileged positions in academia, GLAM, or similar environments. References Bibliography Crenshaw, K.. Mapping the margins: intersectionality, identity politics, and violence against women of color. Stanford Law Review, 43.6: 1241-1299. Essed, P.. Making and breaking ethnic boundaries: womens studies, diversity, and racism. Womens Studies Quarterly, 22.3/4: 232-249. Essed, P.. Dilemmas in leadership: women of colour in the Academy. Ethnic and Racial Studies, 23.5: 888-904. Essed, P.. Cloning amongst professors: normativities and imagined homogeneities. NORA - Nordic Journal of Feminist and Gender Research, 12.2: 113-122. Essed, P. and Goldberg, D. T.. Cloning cultures: the social injustices of sameness. Ethnic and Racial Studies, 25.6: 1066-1082. Gold, M.. Debates in the Digital Humanities. Minneapolis: University of Minnesota Press. Johnsrud, L. and Des Jarlais, C. D.. Barriers to tenures for women and minorities. The Review of Higher Education, 17.4: 335-353. ODonnell, D. P.. All along the watchtower: intersectional diversity as a core intellectual value in the Digital Humanities. In Bordalejo , B. and Risam , R., Intersectionality in Digital Humanities. Phillips, K. W.. How diversity makes us smarter. Scientific American. https://www.scientificamerican.com/article/how-diversity-makes-us-smarter/ 1014-42. Towsend, R. B.. Gender and success in academia: more from the historians career paths survey. In Perspectives in History. http://www.historians.org/publications-and-directories/perspectives-on-history/january-2013/gender-and-success-in-academia Risam, R.. Beyond the margins: intersectionality and Digital Humanities. DHQ, 9.4. http://www.digitalhumanities.org/dhq/vol/9/2/000208/000208.htm l "
	},
	{
		"id": 50,
		"title": "A Predictive Approach to Semantic Change Modelling",
		"authors": [
			"Boukhaled, Mohamed Amine",
			"Fagard, Benjamin",
			"Poibeau, Thierry"
		],
		"body": " Introduction Although it is well known that word meanings evolve over time, there is still much to discover concerning the causes and pace of semantic change . In this context, computational modelling can shed new light on the problem by considering at the same time a large number of variables that are supposed to interact in a complex manner. This field has already given birth to a large number of publications ranging from early work involving statistical and mathematical formalismto more recent work involving robotics and large-scale simulations. We consider that semantic change includes all kinds of change in the meanings of lexical items happening over the years. For example, the word awful has dramatically changed in meaning, moving away from a rather positive perspective equivalent to impressive or majestic at the beginning of the nineteenth century to a negative one equivalent to disgusting and messy nowadays. In this work, we address the question of semantic change from a computational point of view. Our aim is to capture the systemic change of word meanings in an empirical model that is also predictive, contrary to most previous approaches that meant to reproduce empirical observations. We will first describe our methodology, then the experiment and our results, before concluding. Proposed methodology Our goal is to train a model representing semantic change over a certain period and, from there, to predict potential future semantic changes. The evaluation will thus be based on the observation of the gap between actual data and predicted data. Our model is based on two main components: 1- Diachronic word embeddings representing the meaning of words over time-periods, following Turney and Pantel. Word embeddings are known to effectively represent the meaning of words by taking into account their surrounding contexts. The representation can be extended to include a diachronic perspective: word embeddings are first trained for each time-period and then aligned temporally, so as to be able to track semantic change over time, see Fig. 1. For our study, we used the pre-trained diachronic word embeddings released by Hamilton et al.: for each decade from 1800 to 1990, a specific word embedding is built using the word2vec skip gram algorithm. The training corpus used to produce these word embeddings was derived from the English Google Books N-gram datasets, which contain a large number of historical texts in many languages. Each word in the corpus appearing from 1800 to 1999 is represented by a set of twenty 300-dimensional vectors, with one vector per decade. Figure 1. Two-dimensional visualization of the semantic change in the English word  cell using diachronic word embedding. In the early 19th century the word cell was typically used to refer to a prison cell, hence the frequency of cage and dungeon in the context of cell in 1800, whereas in the late 19th century its meaning changed as it came to be frequently used in a scientific context, referring to a microscopic part of a living being. 2- Recurrent Neural Networksmodelling semantic change itself. RNNs are known to be powerful at recognizing dynamic temporal behaviours in diachronic data. In this experiment, we used the word embeddings representing the semantic space of each decade from 1800 to 1990 as input for the RNN, and from this we predicted the embedding corresponding to the 1990-1999 decade. Our RNNs have a long short-term memoryand are implemented through Tensorflow. To explore different scenarios, we ran several experiments with different vocabulary sizes. We used the stratified 10-fold cross-validation method to estimate the prediction error. The overall prediction accuracy is taken as the average performance over these 10 runs. Experiment, Results and Discussion To get an overall estimation of the prediction accuracy, we compare each predicted embedding to the ground truth obtained from real data. Though it is impossible to predict exactly the vector corresponding to a given word w, as we are working in a continuous 300-dimensional space, one can assess the accuracy of the predicted meaning by extracting the closest vectors, i.e. the closest neighbours of a given word over time. If the word w is actually the nearest semantic neighbour to the predicted vector, then it is considered to be a correct prediction. Otherwise, it is considered to be an error. The results are summarized in Table 1. Vocabulary Size Accuracy 1000 91.7% 5000 86.1% 10000 71.4% 20000 52.2% 50000 25% Table 1. Results of prediction accuracy measured for different vocabulary sizes. The training and the prediction using the RNNs model were performed on embeddings derived from the Google N-gram corpus. The results show that the model can be highly effective at capturing semantic change, and can achieve a high accuracy when predicting the evolution of word meaning through distributional semantics. As one can see from Table 1, the model was able to achieve 71.4% accuracy when trained and tested exclusively on embeddings based on the 10,000 most frequent words of the corpus. The model was even able to correctly predict word embeddings for words that have radically changed their meaning over time such as awful, nice, cell and record. The results also show better results when using smaller vocabulary sizes containing top frequent words. The decrease of performance with large vocabularies is due to the fact that infrequent words do not have enough occurrences to derive meaningful and stable enough contexts so as to observe reliable evolutions. It is thus fundamental to use large corpora for this kind of experiments, but also to adapt the size of the vocabulary to the size of the corpus. Conclusion We have proposed a new computational model of semantic change. Although this model issuccessful at representing this evolution, it can still appear to be too simple compared to the complexity of language change in general and semantic change in particular. For now, it may remain hard to understand precisely how this type of computational modelling can be combined with more traditional methods of linguistic analysis. However, we strongly believe that such empirical approaches based on diachronic vector-based representations can considerably help to refine and clarify theoretical insights on the foundations and mechanisms of semantic change, as well as provide an accurate empirical evaluation. Acknowledgements This work is supported by the project 2016-147 ANR OPLADYN TAP-DD2016. Thierry Poibeau is also supported by the CNRS International Research Network Cyclades. Our thanks go to the anonymous reviewers for their constructive comments. "
	},
	{
		"id": 51,
		"title": "Building a Diachronic and Contrastive Parallel Corpus - and an Intended Application in the Form of a Study of Germanic Complex Verb Constructions",
		"authors": [
			"Bouma, Gerlof",
			"Coussé, Evie",
			"de Kooter, Dirk-Jan",
			"van der Sijs, Nicoline"
		],
		"body": " Introduction Our project The rise of complex verb constructions in Germanic started in the autumn of 2018, and aims to investigate how the possibilities for combining several auxiliary verbs fold out over time in four Germanic languages. To aid in this, we are compiling a parallel corpus of Bible texts, with translations in different languages from around the same time, as well as translations from different stages of a language. Intended linguistic application Complex verb constructions combine two or more auxiliary verbs with a lexical verb. As an example of the kind of combinations involved, consider the following four sentences from contemporary Dutch, English, German and Swedish, respectively: a) Ik moet kunnen komen. b) I must be able to come. c) Ich muss kommen können. d) Jag måste kunna komma. Note that the English example stands out by the use of the periphrastic be able to in stead of a form of can. The earliest attestations of complex verb constructions in our languages of interest are from the 13th century. Double modal auxiliaries are initially only headed by a form of shall. The following two examples are from Middle English and Middle Dutch. e) þatt mannkinn shollde muȝhenn wel Upp cumenn inntill heoffne that mankind should .3sg may .inf well upp come .inf into heaven that mankind should be able to come into heaven.f) dat deen sonder den andren niet daer towe en sal moghen gaen that the one without the other not to there neg shall . 3sg may .inf go .inf [so] that the one shall not be able/allowed to go there without the other.Complex verb constructions in contemporary Germanic are well-studied, and there are dedicated studies on the earliest stages of the double modal construction in English and Dutch. Nevertheless, our knowledge of the historical development of these constructions to their present-day distributions in the Germanic languages is limited. A parallel corpus, with contrastively as well as diachronically parallel material, would put us in the position of being able to track a construction through time and in different languages. The corpus As the source for a parallel corpus, the Bible is presumably unique as a a relatively stable collection of texts, available in many translations across languages and ages. Its division into books, chapters and verses is a well-established paratext, which greatly aids alignment of the parallel material. The sizes of the translations, typically ½–1 million words, also speak in favour of their use as the source of a linguistic corpus. Pilot explorations of a 1 million word corpus of contemporary,written Dutch yield around 3000 attestations of three-verb complex verb constructions. We therefore need texts of this size to find attestations even for languages that are more restrictive with the construction. A methodological advantage of parallel material is that it provides negative evidence, when a passage does not contain the construction even though aligned passages do. Our selection principles include a preference for prose translations that reflect the language of their time – this excludes translations that are completely in verse, archaic, or that prioritize source-language characteristics. To maximize parallelism, we avoid fragments, paraphrases and in-line commentaries. Finally, we prefer widely disseminated translations. Our current selection contains around 40 Bibles. We have selected Bible translations from the 14th century to the present, and included at least one version per century for each of the languages except Swedish, as fewer translations exist for the latter. Although there are existing parallel corpora based on modern Bible translations from different languages, and parallel corpora with a diachronic dimension, to our knowledge, ours will be the first corpus that systematically contains parallel texts in two dimensions. The digital texts are collected from different sources: from existing corpora, earlier digitization projects, and through cooperation with Bible societies. In addition, a small number of Bibles will be digitized from print as part of our project. Depending on the suppliers, some parts of the corpus will be available under an open license and other parts under a restricted, login required license. Acknowledgements The project The rise of complex verbs constructions in Germanic is funded by the Swedish Research Council. For more information on the project, see https://complexverbconstructions.wordpress.com/. "
	},
	{
		"id": 52,
		"title": "Prendre en Compte le Contexte d’Usage et le Contexte Technique dans le Développement du Service d’Annotation Vidéo Celluloid",
		"authors": [
			"Bourgatte, Michael"
		],
		"body": " 1. Le projet de développement du service dannotation vidéo Celluloid Nous présenterons ici les résultats dun projet de recherche, sinscrivant dans une dynamique socio-constructiviste, intitulé Celluloid ayant pour objet les usages de lannotation vidéo en contexte déducation ou de recherche. Lannotation est lune des pratiques essentielles de toute activité denseignement et de recherche. Elle consiste à inscrire des commentaires, dans les marges ou entre les lignes dun texte, témoignant de la consultation active dun contenu. Elle peut avoir une simple fonction daide-mémoire, ou permettre de mettre en évidence ou de dévoiler la compréhension dun texte. Elle rend surtout possible le partage du fruit de cette lecture active avec un tiers, dès lors que le contenu annoté par un individu est porté à la connaissance dun autre. Définie en ce sens très large, lhistoire des pratiques dannotation est pour ainsi dire aussi ancienne que celle de la production intellectuelle. Cette pratique ancienne est prolongée et renouvelée avec lapparition des outils numériques. Si le principe reste le même, lannotation numérique se distingue de trois manières. Dun point de vue proprement technique, elle correspond dabord à un contenu pris entre deux balises informatiques qui le rendent matériellement indépendant et dissociable du contenu premier. Elle permet ensuite dinstaurer une relation intermédiatique avec les textes, car si lon peut toujours produire une annotation écrite, on peut aussi intégrer un hyperlien, une image, une vidéo ou du son. Enfin, le principe de lannotation ne se réfère plus seulement aux textes : on peut désormais annoter des documents visuels ou sonores. Lannotation devient alors multimédiatique. 2. Réaliser une ethnographie des usages et un état de lart technologique Dans le cadre du projet Celluloid, nous sommes partis de lobservation de pratiques de chercheurs et denseignants. Pour cela, nous avons conduit une campagne dentretiens semi-directifs auprès denseignants-chercheurs en SHS travaillant sur des matériaux vidéo. Cette ethnographie a révélé quils méconnaissaient très largement lexistence doutils dannotation ou quils préféraient sen détourner par crainte de voir leur travail disparaître. Ainsi, ils étaient tous entrés dans une dynamique de « bricolage »consistant à réaliser des photomontages à partir de captures décranou à utiliser un lecteur vidéo simple comme VLC. Certains détournaient des outils de montage professionnel comme Final Cut ou Premiere pour en faire un usage fort éloigné de ce qui a été imaginé par leurs concepteurs. Nous avons dans un second temps analysé et comparé les dispositifs proposés par plusieurs plateformes dannotations vidéo, devant permettre de dépasser ces situations de bricolage. Nous mentionnerons Lignes de temps qui est le premier outil dannotation vidéo utilisé de manière systématique en France en contexte éducatif. Imaginé en collaboration avec des professionnels du cinéma, il sinspire pour cela des interfaces de montage filmique autant dans son fonctionnement que dans son ergonomie. Il est centré sur le caractère temporel du film et fonctionne selon un principe de fabrication de lignes danalyse. Ce type dinterface et ces fonctionnalités se retrouvent dans de nombreux autres projets comme Advene, ou Mediascope. À linternational, une somme dautres technologies se distingue en proposant non plus des interfaces sous la forme du banc de montage, mais sous la forme dannotations marginales, telles que Vialogues, VideoAnt ou Pad.ma. Plus récemment, le projet Rekall / MemoRekall a tenté de faire la synthèse de ces deux approches ergonomiques. 3. Développer le service Cet état de lart a permis de mettre au jour les difficultés de travail que rencontrent les enseignants et les chercheurs pour mettre en place des projets éducatifs ou de recherche faisant appel à la vidéo. Il a également révélé que les choix ergonomiques et technologiques qui sont faits par les développeurs freinent les dynamiques collaboratives. À partir de ces analyses, nous avons donc développé un outil avec lobjectif de rendre la pratique dannotation vidéo plus efficientes et plus adaptées, mais aussi datteindre un certain nombre dobjectifs que nous nous étions fixés : naviguer dans un contenu, puis annoter des points ou des segments. Pour cela, nous avons associé une visionneuse et ses fonctionnalités standardsà une boîte à outils permettant la conduite dun travail dannotation : insertion dun texte, dune image, dune vidéo, dun son, dun hyperlien, de signes graphiques et stylisation de ces éléments. faciliter les interactions. Pour cela, un enseignant ou un formateur doit pouvoir donner des consignes, répondre à des questions ou générer des échanges, tandis que les apprenants doivent pouvoir annoter les vidéos au fur et à mesure de leur consultation, soit en formulant des réponses ou des commentaires, soit en adressant des questions au professeur. Loutil doit donc favoriser la collaboration de plusieurs acteurs ayant des statuts différents. désacraliser le film ou la vidéo, en prenant au sérieux la mutation des discours vidéographiques : sur le modèle des pratiques dannotation textuelle qui se sont démocratisées avec la naissance du livre de poche, nous promouvons une pratique dannotation réalisée directement sur la matière filmique. Techniquement, on insère donc des marques dannotation sur une trame transparente qui est posée sur limage. Au final, on obtient une constellation de marques sur la vidéo que lon peut faire apparaître ou masquer à sa guise. On entre ainsi en rupture avec les modèles dannotateur vidéo les plus répandus qui reposent sur une économie décran dissociant les annotations du contenu annoté. 4. Tester Celluloid et disséminer Une première version a été testée dans deux contextes différents en 2015 : dans le Playground du festival SXSWedu à Austinaux États-Unis, puis dans un atelier organisé pour le THATCamp Paris. Ces deux événements nous ont permis de réaliser une série dobservations participantes avec plus dune cinquantaine dindividus. À partir de ces premiers retours expérimentaux, une seconde version a été élaborée et finalisée en 2018. Cet outil, totalement libre et open source dont le code est partagé sur Github, peut aujourdhui être utilisé à des fins de recherche personnelle ou à grande échelle, avec des groupes dapprenants en situation réelle denseignement. Après sêtre identifié, lutilisateur peut se créer un compte, puis engager une activité en copiant/collant le lien dune vidéo Youtube libre de droits qui sera alors directement streamée. Il peut aussi repartir dune activité existante proposée par un collègue sur la plateforme. Ce projet pourra ensuite être partagé si on souhaite entrer dans une dynamique de collaboration. Lors de cette communication, nous présenterons donc les différentes étapes de cette recherche, les premiers retours dusages de notre solution, ainsi que les hypothèses que ces usages nous permettent de formuler quant à la place de lannotation vidéo dans le cadre de recherches en humanités numériques. "
	},
	{
		"id": 53,
		"title": "Adapting a system for Named Entity Recognition and Linking for 19th century French Novels",
		"authors": [
			"Soudani, Aicha",
			"Meherzi, Yosra",
			"Bouhafs, Asma",
			"Frontini, Francesca",
			"Brando, Carmen",
			"Dupont, Yoann",
			"Mélanie-Becquet, Frédérique"
		],
		"body": " The annotation and linking of Named Entities - be it People, Places, or other proper names - in novels is an important task both for the creation of quality Digital Editions as well as for Digital Literary Stylistics and Spatial Humanities, which largely rely on Distant Reading techniques involving among other things spatialisationand network analysis. Automatic NERCalgorithms can be crucial in this sense, but they are often developed and tested on contemporary newspaper corpora and need to be adapted. As to NEL, namely the task of referencing entities with links to external Knowledge Bases, the additional issue arises of finding the correct reference base containing knowledge from the past. Also, the question of formats is crucial; the Text Encoding Initiativeis the format of choice for most digital editions in the humanities, but the majority of existing Natural Language Processing tools are not suited to support this type of input and require specific formats such as CONLL, which werent built to preserve text structure. In this poster, we describe a pipeline combining two different tools - SEM for NERCand REDEN for NEL- and its adaptation for the annotation of 19th century French novels. This work is set within the context of a larger initiative aiming at creating a Time Machine project for the city of Paris, following the example of the Venice Time machine. The input of the pipeline is an XML-TEI edition and the output is an enriched version with tagged and identified Named Entities. REDEN provides also the necessary input for a dynamic cartography, based on information in the KB. Figure 1 - The overall pipeline. The corpus consists of the first two chapters of Le Ventre de Parisand the first chapter of Cesar Birotteau, the size is 30,889 words and both texts were annotated for places and fictional characters by two separate annotators; an inter-annotator agreement of 0,91indicates a strong overall agreement. An Internationalized Resource Identifierwas added for those place names for which a reference existed in the KB. SEM is a machine learning system based on conditional random fieldsmodels; its default French model for NER performs poorly on our corpus. We thus re-trained SEM using our gold standard, and evaluated the domain adaptation with two setups: Setup1: training on the first chapter of Zola and tested on the second one Setup2: training on the Zola subcorpus and tested on Balzac. Setup 1 Setup 2 Precision 1 0,7 Recall 0,69 0,26 F-measure 0,82 0,38 Results for Setup 1 are quite encouraging: they show that the model trained on one chapter performs very well on another. This means it could in principle suffice to manually annotateone chapter to obtain a system that can annotate the rest of the novel with an acceptable accuracy. Results for Setup 2 show that even two novels that are relatively close from a temporal, geographical and stylistic point of view are sufficiently different to cause an important drop in performance when the model trained on one is applied on the other, with important consequences for our ongoing work to constitute an adapted NERC model for 19th century French novels. REDEN relies on graph-based algorithms and on Semantic Web technologies, and like many NEL algorithms, is composed of two phases, candidate retrieval and disambiguation. It was designed for DH applications and is adaptable to various domains thanks to the possibility of using different KBs. Figure 2 - The REDEN model. For this experiment, REDEN was tested on the task of referencing placeNames , using three different Kbs, a complete description of the evaluation metrics used in this table, and which are commonly used for NEL, can be found in Brando et al.,. The best configuration in terms of overall accuracy is the one using Wikidata; however, BnF is a more accurate source of information for old place names. Disambiguation accuracy is an interesting measure when there is more than one candidates for a place mention, and for Wikidata the value is strong. Weak values on NIL precision tell us that REDEN, which uses exact string match for retrieving candidates, sometimes misses the correct referent, and needs to be improved in this respect. KB Candidate Precision Candidate recall NIL precision NIL recall Disambiguation accuracy Overall linking accuracy DBpedia 1 0,816 0,367 1 none 0,834 BNF 0,76 0,63 0,58 0,97 1 0,7 Wikidata 0,91 0,83 0,44 1 1 0,85 Geo-visualisation Once the digital edition is enriched with placeName tags, REDEN allows for the exploration of the spatial dimension of texts by retrieving structured information about places. For instance, in Wikidata, resources are described according to an ontology which contains properties for coordinate locations <https://www.wikidata.org/wiki/Property:P625>, images <https://www.wikidata.org/wiki/Property:P18>. By dereferencing IRIs, it is possible to access values for the aforementioned properties and use them in the context of a Web mapping application thereby to project places as points onto a map along with media or facts about these places. "
	},
	{
		"id": 54,
		"title": "A CLARIAH Environment for Linguistic Research",
		"authors": [
			"Broeder, Daan",
			"Ding, QiQing",
			"Leenknegt, Bas"
		],
		"body": " Introduction With the rise of digital humanities and research infrastructures many tools and services have become available for linguistic research. However, often these tools and services come with their own dedicated environments, e.g., a specific programming or user interface. In the CLARIAH projecta Virtual Research Environmentis constructed, which unifies the access to these dedicated environments and thus lowers the barier for linguists to apply the new methods coming available in the field of Digital Humanities to their own research. This VRE stands in a tradition to provide researchers with an environment that brings together different services and tools necessary for a researcher to perform her work. An important discussion platform is the RDA VRE Interest Group. The main features of this linguistics VRE will include: easy up- and download of a researchers own resources; automatic analysis for known resource types; automatic indexing of linguistic resources; profile matching of services and resources; hide invocation details of different types of services; storing the provenance track of the resources; gradually eliciting the resources metadata; persist a resource into a repository. The next sections will highlight some of these features and sketch their implementation. Resources The nextCloud open source file sharing serviceallows users to easily up- and load their resources using a web interface or a native application. The latter also provides good facilities for synchronization and transfer of large files, which can be problematic in a browser environment. The VRE uses nextClouds hooks to integate its own backend. Whenever a file is created or changed, events are broadcast to a Apache Kafka message queue. Other parts of the system listen to this queue and react appropiately. For example, the resources are examined using the File Information Tool Setextended with special probes for domain-specific resource types . When the resource type allows the resource is indexed using the Multi Tier Annotation Search indexer. This allows to search these resources for relevant linguistic patterns using the Corpus Query Language. It will also become possible to associate resources with component-based metadata. This is a very flexible metadata format easily adapted to specific researchers and projects need. Meta dating is elicited in a non obtrusive way and populated automatically where possible. However, when a resource is ready to be archived metadata has to be complete. Services The linguisticservices landscape is fragmented. The european GATE Cloud, CLAM, WebLichtfamilies of services form just a small example of the diversity. Each of these families have their own specification on how to invoke them for a specific resource and retrieve the results. For example, in a CLAM service one creates a project, uploads the input resources, triggers execution, polls asynchronously if execution has finished before downloading the results. The VRE captures such a specific interaction in a recipe, and hides away these details from the user. The service registry contains for each service not only the recipe but also its expected in- and output. This information is used to do profile matching between resources and services. In its simplest form this means a match on MIME type, but more advanced forms of profile matching will also need to look inside resources, e.g., which part-of-speech tag set is used. All events in the VRE are passed through a Kafka message queue and are thus stored in a persistent log. This log can be analysed to reconstruct the provenance of a resource. This means that its possible to provide information on what the primary data source was, which services, including versions and parameter values, were invoked to create the resource. Potentially, the VRE can also elicit information from the researcher when it detects updates outside of service invocations, e.g., a manual correction phase. Provenance information can help to semi-automatically complete the metadata, but it is also a valuable addition in reaching the goal of verifiable and reproducable science. Conclusions This VRE for linguistic research provides an increasing rich feature set with the aim to unify access to and lower the barrier to the use of the powerful, yet fragmented, landscape of services for many researchers. "
	},
	{
		"id": 55,
		"title": "CWRC-Writer Design and Survival Strategies: Observations from the Post-Launch Trenches",
		"authors": [
			"Ilovan, Mihaela",
			"Brown, Susan"
		],
		"body": " The CWRC-Writer XML/RDF editor is the centerpiece of the Canadian Writing Research Collaboratoryplatform for the production, hosting, and dissemination of digital humanities scholarship. In development since 2011 and launched with the platform in 2016, the browser-based editor has reached maturity and stability. Well prior to this, the team had begun strategizing towards sustainability. We outline this strategy while highlighting features of the editor. Compared to some outcomes of digital dumanities tool building – such as gaining new insights into ones own research – the effort of turning a tool into a sustainable, generalized service is less glamorous, more laborious, and less acknowledged. Tool-building is considered part and parcel of the scholarly work of DHand is beginning to be recognized by academic reward systems. Yet scant support and rewards accrue once software is up and running. This situation has changed little over the years, despite increasing concern regarding digital infrastructure sustainability generallyand attention to care and repair within DH. Like all software, DH tools require maintenance, enhancement and updates, which is to say, continued funding and expertise. Pursuing uptake seems like a natural approach to the sustainability dilemma, since: it is easier to demonstrate the success of a tool and to justify further resource allocation in light of increases in use; and adopters of a tool are invested in its survival and might put resources towards sustainability. However, uptake is no guarantee of sustainability. As observed by Cameron Neylon, many scholarly infrastructures are public goods, and Finding sustainability models to support them is a challenge due to free-loading, where someone who does not contribute to the support of an infrastructure nonetheless gains the benefit of it. Nevertheless, unused tools are poorly positioned to request continued funding or support. The uptake or adoption of existing DH software by new users is far from guaranteed, even if it fulfills a need that it is well-documented in the research community where it originates. Fred Gibbs and Trevor Owens crystallize the ways in which tool uptake is hindered by multiple factors. Significant problems include: managing expectations, while also scaling up functionality from local to more general needs; limited learning resources; unintuitive or complex user interfaces that discourage novice users; lack of support for standards and interoperability. community building Together with more mundane but important activities like code maintenance, stable hosting, and systems administration, these factors create challenges that can prove fatal to promising technologies. Some are proclaimed at digital humanities conferences but seldom heard from again, while others like Paper Machinesshow immense promise but do not develop into fully robust tools. Even mature tools with uptake from a wide range of users, such as Gephi, live quite precarious lives. In short, the challenges of sustaining tools are manifold. We use the above points as a rubric for reflecting on CWRC-Writers engagement with the challenges of uptake. Scaling features and expectations The modular CWRC-Writer exists in several types of installation to suit users from novices to technical experts: CWRC-Writer: available to researchers within the CWRC platform, where it is integrated with an Islandora repository, Git-Writer: uses GitHubs file storage, versioning, and authentication to allow anyone to edit GitHub-hosted XML documents. Installations by third parties in other software stacks. Fig. 1. Git-Writer document loading interface To support a wide variety of users, CWRC-Writer provides these core features: an interface that renders XML in a human-readable layout using CSS; XML tagging, with or without tags showing, with validation and error identification; raw/source XML editing for experts; entities tagging in XML and/or Web Annotation RDF with built-in authority lookups. Members of the DH community, as well as literary and cultural studies scholars using XML for their texts, were involved from the beginning in the design of the tool. The user group comprises both power-users – researchers with decades of experience in markup – and novice or occasional users with little familiarity with DH. CWRC-Writer was designed from the outset as a light-weight editor to allow novices to tag XML documents and link them to named entity authorities, such as the Virtual International Authority File, in a manner that would avoid the steep learning curve associated with other, more complex editing tools. This lightweight usage is our main use case. CWRC-Writer does not aim to replace a full-featured XML editor for heavy-duty markup or transformations. The complexity of managing XML through an HTML front-end mean that major restructuring, for instance, is very tricky. To ensure that available affordances are aligned with the needs of the users, CWRC-Writer offers three different editing modes: A default combined XML & RDF mode creates both XML tags and Web Annotations identifying entities in the same span of text; external named entity identifiers are mapped onto the equivalent tags within supported XML schemas - which include established TEI customizations and other schemas employed by CWRC-supported projects. RDF-only mode for Web Annotations that leave the body of the XML file untouched. XML-only mode for tagging without adding any Web Annotations. Fig. 2. CWRC-Writer document showing application of CSS In conjunction with particular user communities, we are extending CWRC-Writer functionality based on a documents schema declaration; for example, for EpiDoc files, a popup editor for translations will allow users to create or tag a translation while viewing it side-by-side with the original. To support transcription, side-by-side display of the XML and images allows transcribers to view the scanned manuscript within the tool. Fig. 3. Editing interfacewith side-by-side display of manuscript scan Learning resources We mitigate the challenge of a new interface by providing extensive, searchable user documentationand tutorial videos , as well as virtual office hours for real-time support. Learning to apply markup is a major challenge for the uninitiated, so there are sandbox templates for fooling around. Projects can create customized document templates that can be used to kickstart content creation and editing. These can provide highly detailed instructions, in order to promote consistency and best practices. A user-friendly interface From 2012 on, CWRC-Writer has undergone successive rounds of user testing, which have informed feature development and UX improvements. Two extensive rounds of survey-based user-testing were conducted before 2016, followed by numerous informal consultations and feedback from users and workshop participants. CWRC-Writer code is available in GitHub and a ticketing template allows adopters to submit both feature requests and bug reports. Formal announcement of the GitHub version in 2019 will be followed by another round of systematic user testing. Standards and interoperability CWRC-Writer editor adheres to the standards for both markup and Web Annotation. An integrated XML validator allows users to validate against the declared schema as they work on the document. TEI is supported in all version of the editor. RDF annotations adhere to the Web Annotation Data model, a W3C Recommendation that is being widely adopted within DH and in the scholarly publishing community as a standard for annotation data. Promoting a community of users In addition to passive adopters, who employ CWRC-Writer as made available through CWRC or GitHub, we have projects joining CWRC primarily thanks to its integration of the editor with other tools. There is growing interest from members of the DH community considering it for use in TEI editing projects, as components of library-based DH tool suites, or for teaching XML. The Center of Digital Humanities Research at Texas A&M has produced a containerized version and has installed it on top of Fedora 4 as part of a larger toolkit. Bucknell University is installing a version of the Git-Writer to support diverse local DH projects, and other institutional installations are planned. External partners were consulted for the development of Git-Writer, and the code is configurable, modular, and well documented in order to permit installation in a range of software environments. Users currently cohere around specific projects. We hope a broader CWRC-Writer community will develop as numbers grow, and be joined by a community of developers familiar with and willing to contribute to upkeep. However, the experience of other projects indicates that this is a major challenge. Future developments CWRC-Writer has for several years now, since its launch within CWRC, been thinking hard about how to promote uptake and long-term sustainability. Our development roadmap is constructed around current and oncoming user needs. We will continue to adapt our strategy in response to insights gained from further user testing and feedback from the community following the launch of the Git-Writer to the DH community. "
	},
	{
		"id": 56,
		"title": "Linked Literary History, or An Ontology of One’s Own: The Canadian Writing Research Collaboratory Ontology",
		"authors": [
			"Brown, Susan",
			"Cummings, Joel",
			"Drudge-Willson, Jasmine",
			"Lemak, Abigel",
			"Martin, Kim",
			"Mo, Alliyya",
			"Stacey, Deb"
		],
		"body": " Introduction Creating an ontology for feminist literary history with a view to its extensibility to other literary and cultural work involves significant decisions. The ontology described here supports the linked open datastrategy of the Canadian Writing Research Collaboratorys online platform. CWRC launched in 2016 as An online infrastructure for literary research in and about Canada designed to meet the challenges and embrace the opportunities of the digital turn. It pursues this mission in part through contributing LOD to the Semantic Web initiative to make Web resources more discoverable, shareable, and interoperable by making them machine-readable. It promotes linking amongst CWRC-hosted projects through use of LOD identifiers both in metadata and within XML documents through an online XML editor and Web Annotation creator, CWRC-Writer. The next step is to push its LOD extracted from CWRC content into the Web. The Orlando Projectis our pilot dataset for extracting more complex LOD. The Orlando Project has published since 2006 Orlando: Womens Writing in the British Isles from the Beginnings to the Present, an online textbase for the research and discovery of womens writing. Its 8 million words of richly-encoded text describe writers lives, careers, and contexts using a bespoke XML schema. The textbase has been acclaimed as initiating a new breed of information resource and for having changed the parameters of the scholarship and teaching of British womens writing. However, its utility is impeded by its data structure and paywall. LOD offers a means to make much of the knowledge embedded in Orlando more accessible and interoperable. The first requirement was an ontology. No single extant ontology covers the range of biographical and literary relationships that CWRC needs to represent. We therefore sought to adapt as much as we could from elsewhere while filling gaps through an ontology of our own. We embrace Grubers widely accepted definition of a computational ontology as A specification of a representational vocabulary for a shared domain of discourse; an explicit specification of a conceptualization, where a conceptualization is understood as an abstract, simplified view of the world that we wish to represent for some purpose. We outline here some key aspects of our ontological strategy, many of which are more fully described in the published preamble to the ontology itself. Ontology Design Process Our process was pragmatic: we aimed to mobilize the data as soon as feasible, adopting where we could in accordance with best practices and extending or creating where we could not. In using other ontologies we sometimes importing, sometimes cherry-pick terms or relationships, and sometimes base definitions on external ones and cite themso we can relate terms to each other within our structure. The ontologies from which CWRC draws include FOAF, Organization, BIBFRAME, TIME, Web Annotation, Dublin Core, PROV, and CIDOC-CRM, as well as the SKOS vocabulary. The ontology is designed to support open-world datasets like Orlandos: we develop terms on an as-needed basis rather than trying to be exhaustive. We are not trying to map the world but to represent what we have. The ontology is dynamic, expanding as needed, and the development process iterative. In some respects, this ontology strategy resembles lean startup and agile development processes. We here outline the major principles and decisions that have informed the work thus far. Antifoundationalism and representationality The ontology reflects what Alan Liu calls a lightly antifoundationalist epistemology; it emerges from feminist theory and science and technology studies, with an emphasis on situated knowing and a wariness of the consequences of classification. Ontologies can be understood as approximating the world and thus capturing some truth about it, without enjoying a one-to-one correspondence with categories of entities as they exist completely independently of human languages or human practices. Rather than viewing ontologies as involving reality representation, we understand the CWRC ontology as deeply representational. Structuring it with the Web Ontology Languageand devising Shape Constraint Languagerules for our data will provide our data with the tactical benefits of ontological rules including intelligibility, error detection, processability, inferencing, and interoperability. This work aims to intervene, with experimental models, in the knowledge structures of our time. We therefore need to accommodate some aspects of those structures in order to make an argument and to be intelligible, even if the aim is to move beyond some categories. So we take as our touchstone the core concerns of the datasets themselves. Some priorities were covered by existing ontologies, but sometimes there was nothing to approximate what we needed, so we invested significant resources there. Deferred upper-level ontology The team considered and debated adopting an upper-level ontology that lays out categories and relationships at the most general level. After reviewing leading candidates, we held off because none align closely with our approach. DOLCE Light is closest, but orphaned, and the CIDOC-CRM is very event-oriented, whereas much of our data is not. We are therefore deferring the question to see whether it seems necessary and until we have a wider range of use-cases, including ones related to space and time, so as to better evaluate the implications. Provenance and citation Every LOD assertion links back to the encoded source, connecting it to nuanced prose as well as the specific XML markup from which the relationship was extracted. The data is also designed to point to the specific scholarly sources from which claims within the source text are derived. Annotation The Web Annotation Data Modellinks identifications of entities to their source, as noted above. We also use the WA model to characterize the properties associated with a writer as descriptions of that person, refuting positivist claims. Framing claims as annotations motivated by description is important, given that the creators of the markup did not anticipate this use of the data, and given that only dates have any kind of certainty value attached to them. Ambiguity, diversity, and nuance Rather than being disambiguated, leaky cultural categories are generously documented and represented as mutually constitutive with specific discursive frameworks or Context classes of annotations. Textual labels group together related terms emerging from different discourses. Rather than mash complex and contested terms into a single identity, the ontology retains distinctions made between them within the encoding and groups them together with these labels – so, for instance, the jewishLabel instance and different constructions of Jewishness are linked using the relationship represents. Linking to legacy terms The labelling strategy and represents predicate help handle problematic legacy terms, along with another custom predicate called hasFunctionalRelation, which signifies that a term has served in a parallel way in other datasets, but has no semantic commensurability with our term. Gender and sex are definitively different, so the OWL sameAs relationship does not work to link terms for sex to womanLabel and manLabel. However, sex can mapto gender terms in existing datasets, which often employ the ISO 5218 categories for sex. The hasFunctionalRelation predicate creates a complex but still processable bridge between Orlando data and datasets of womens writing that employ ISO 5218 values. Challenges We conclude by demonstrating a couple of challenges arising from this work with visualizations of extracted data using the HuViz LOD explorer. First is the tension between complexity and nuance, on the one hand, and readability and processability on the other. As Rob Sanderson and David Newbury stress, we need Linked Open Usable Data. The Orlando LOD dataset is verbose and complex, raising concerns that it may be incomprehensible and unwieldy despite the ontology. Second is the tension between standards and bespoke terms. Staying close to the Orlando data has meant, for instance, devising a genre ontologyrather than adopting a partially suitable one, making CWRC data more of an island in the LOD stream than it might otherwise be. However, standards can also have unfortunate consequences: adopting the Web Annotation framework means that our assertions are now less direct and easily comprehensible by non-specialists than in our initial model for our data. The CWRC ontology aligns with the strain of critical DH that applies humanities epistemologies to create digital representations: the CWRC ontology works not with data but with capta. The millions of triples in the Orlando Projects British Womens Writing Dataset will advance feminist digital literary history, and experiments with the CWRC ontology will help refine strategies for writing feminist literary history, among other complicated stories, into the Web. Works Cited About CWRC. Canadian Writing Research Collaboratory. https://cwrc.ca/about. Alcoff, Linda Martín. Whos Afraid of Identity Politics? In Moya, Paula M. L. and Hames-García , Michael R., Reclaiming Identity: Realist Theory and the Predicament of Postmodernism. University of Califormia Press, pp. 312–44. BIBFRAME. Bibliographic Framework Initiative. Library of Congress. https://www.loc.gov/bibframe/. Bowers, Toni. Exploring the Richardson Circle Using the Orlando Database. The Scriblerian 44-45: 56–58. Bowker, Geoffrey C., and Susan Leigh Star. Sorting Things Out Classification and Its Consequences. MIT Press. Braidotti, Rosi. Posthuman, All Too Human: Towards a New Process Ontology. Theory, Culture & Society 23: 197–208. doi:10.1177/0263276406069232. Brown, Susan, et al.. CWRC Ontology Preamble. Canadian Writing Research Collaboratory. http://sparql.cwrc.ca/ontologies/cwrc-preamble-EN.html . Brown, Susan, James Chartrand, Mihaela Ilovan, Andrew McDonald, Jeffrey Antoniuk, and Michael Brundin. CWRC-Writer. Canadian Writing Research Collaboratory. https://cwrc-writer.cwrc.ca/. Brown, Susan, Patricia Clements, and Isobel Grundy. The Orlando Project: Feminist Literary History and Digital Humanities. http://www.artsrn.ualberta.ca/orlando/. Brown, Susan, Patricia Clements, and Isobel Grundy. Orlando: Womens Writing in the British Isles from the Beginnings to the Present. Cambridge University Press. http://orlando.cambridge.org/ Brown, Susan, Abigel Lemak, Colin Faulkner, Kim Martin, and Rob Warren. CulturalFormations: Structuring a Linked Data Ontology for Intersectional Identities. In Digital Humanities 2017: Conference Abstracts. McGill University. https://dh2017.adho.org/abstracts/580/580.pdf. CIDOC-CRM. Martin Doerr, Matthew Stiff, Nick Crofts, Stephen Stead, and Tony Gill. CIDOC Conceptual Reference Model V. 5.0.4. http://www.cidoc-crm.org/. Cummings, Joel, and Deborah Stacey. Lean Ontology Development: An Ontology Development Paradigm Based on Continuous Innovation. In Proceedings of the 10th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management - Volume 2: KEOD, pp. 365-372. DOI: 10.5220/0006963003670374 CWRC Ontology. Susan Brown et al. Canadian Writing Research Collaboratory. http://sparql.cwrc.ca/ontologies/cwrc.html. CWRC Literary Genre Ontology. Susan Brown et al. Canadian Writing Research Collaboratory. http://sparql.cwrc.ca/ontologies/genre.html. DBpedia. Leipzig, University, Univerity of Mannheim, and Openlink Software. DBPedia. Wikipedia2. https://wiki.dbpedia.org/. DCMI. Board, DCMI Usage. Dublin Core Metadata Initiative. Dublin Core Metadata Initiative. http://dublincore.org/documents/dcmi-terms/. DOLCE. Claudio Masolo, Stefano Borgo, Aldo Gangemi, Nicola Guarino, Alessandro Oltramari. Wonder Web. Laboratory for Applied Ontology. Internet Archive. https://web.archive.org/web/20190121172907/http://www.loa.istc.cnr.it/old/DOLCE.html Drucker, Johanna. Humanities Approaches to Graphical Display. Digital Humanities Quarterly 5. http://www.digitalhumanities.org/dhq/vol/5/1/000091/000091.html. FOAF. Dan Brickley and Libby Miller. FOAF Vocabulary Specification 0.99. Friend of a Friend. http://xmlns.com/foaf/spec/. Getty. Getty Vocabularies. The J. Paul Getty Trust. https://www.getty.edu/research/tools/vocabularies/index.html . Gruber, Thomas R.. A Translation Approach to Portable Ontology Specifications. Knowledge Acquisition 5: 199–220. Haraway, Donna. A Cyborg Manifesto : Science, Technology, and Socialist- Feminism in the Late Twentieth Century. In Simians, Cyborgs and Women: The Reinvention of Nature. Routledge, pp. 149–81. Liu, Alan. Drafts for Against the Cultural Singularity. Alan Liu. http://liu.english.ucsb.edu/drafts-for-against-the-cultural-singularity/. Oldman, Dominic, Martin Doerr, and Stefan Gradmann. Zen and the Art of Linked Data: New Strategies for a Semantic Web of Humanist Knowledge. In Schreibman, Susan, Siemens, Ray and Unsworth, John, A New Companion to Digital Humanities. Wiley-Blackwell, pp. 251-273. Organization. Dave Reynolds. Organization Ontology. W3C. https://www.w3.org/TR/vocab-org/. Orlando British Womens Writing Dataset. The Canadian Writing Research Collaboratory. http://sparql.cwrc.ca/releasenotes. Posner, Miriam. Whats Next: The Radical, Unrealized Potential of Digital Humanities. Miriam Posners Blog. http://miriamposner.com/blog/whats-next-the-radical-unrealized-potential-of-digital-humanities/. PROV. Timothy Lebo, Satya Sahoo, and Deborah McGuinness. PROV-O: The PROV Ontology. W3C. https://www.w3.org/TR/prov-o/. Sanderson, Robert, and David Newbury. Linked.Art & Vocabularies: Linked Open Usable Data. J. Paul Getty Trust. https://www.getty.edu/research/tools/vocabularies/newbury_sanderson_itwg_2017.pdf SKOS. SKOS Simple Knowledge Organization System Reference. Alistair Miles and Sean Bechhofer. W3C. https://www.w3.org/TR/skos-reference/. Smith, Barry. Beyond Concepts: Ontology as Reality Representation. In Varzi, Achille C. and Vieu, Laure, Formal Ontology in Information Systems. OIS Press, pp. 1-12. Smith, Barry. On Classifying Material Entities in Basic Formal Ontology. In Interdiscipinary Ontology. Proceedings of the Third Interdisciplinary Ontology Meeting. Keio University Press, pp. 1–13. Smithies, James. Digital Humanities, Postfoundationalism, Postindustrial Culture. Digital Humanities Quarterly 8. http://www.digitalhumanities.org/dhq/vol/8/1/000172/000172.html. Web Annotation. Robert Sanderson, Paolo Ciccarese, and Benjamin Young. Web Annotation Data Model. W3C. https://www.w3.org/TR/annotation-model/. Woolf, Virginia. A Rooms of Ones Own. Harcourt, Brace, and Company. "
	},
	{
		"id": 57,
		"title": "Data Beyond Vision",
		"authors": [
			"Sutton-Koeser, Rebecca",
			"Budak, Nicholas",
			"Li, Xinyi",
			"Doroudian, Gissoo"
		],
		"body": " Data visualization is frequently used in Digital Humanities for exploration, analysis, to make an argument, or to grapple with large-scale data. Increasing access to off-the-shelf data visualization tools is beneficial to the field, but it can lead to facile and homogenized visualizations. Data physicalization can be used to defamiliarize and refresh the insight that data visualizations initially brought to DH. Proliferation in 3D modeling software and relatively affordable 3D printing technology makes iterative, computer-generated data physicalization more feasible. Working in three dimensions gives additional affordances: parallel data series can be seen next to each other, rather than color-coded, overlapped, or staggered; and physical objects can be viewed from multiple angles, allowing for changing perspective. Data visualization necessarily privileges sight. Explorations into other senses, such as data sonification and physicalization provide new venues of experience. Touch is particularly significant, since it is the only meta-sense other than sight and because of the intimacy it affords, per the thought of feminist philosopher Luce Irigaray. Multimodal data explorations incorporating touch and sound can provide new possibilities of accessibility to those with low vision. Spatial, acoustic, and temporal dimensions of data representation can generate rich narratives, invite the audience to explore new relationships, and turn passive consumption into a sensory experience that encourages interpretation, and challenge the reductive nature of data visualization. In addition, creating data physicalizations can be a form of critical making; the iterative and reflective process requires more time to engage with the data, including the human aspects represented. As an alternative approach to the rhetoric of building or making that has become common in DH, we are inspired by the work of Lauren Klein and Catherine DIgnazio, who encourage a reorientation toward the emotional and affective qualities in our engagement with data. In employing physicalization as a technique to corporealize and re-humanize humanities data, we follow the ethical principles articulated by the Colored Conventions Project to contextualize and narrate the conditions of the people who appear as data and to name them when possible. We propose a multi-media installation consisting of data physicalization objects and dynamic displays to be exhibited at the conference poster session concurrently with an explanatory poster. Pieces in the installation will utilize space, time, and/or interaction to provide new ways of engaging with a dataset and the arguments and narratives behind it, in order to challenge the dominant paradigms of conventional screen-based data visualization. Each piece will have an accompanying statement on the poster documenting the humanities data and projects it draws on and the significance or insights of the design. Pieces will be designed as much as possible to accommodate display in a space alongside the poster. If individual pieces are unable to be transported to or exhibited at the poster session, the poster will still serve as a proxy for understanding the ways they subvert traditional paradigms of data visualization. Provisional list of pieces: experience a well-known work of philosophy through a threaded representation of citation networks and references woven on a loom. interact with a physical manifestation of membership data for a famous lending library by pulling apart its two main archival sources in three dimensions. discover patterns in borrowing activity of lesser-known members of the same library by folding printed data into a paper model. "
	},
	{
		"id": 58,
		"title": "The Complexities Of The Representation Of Xhosa Protagonists, Represented By Male And Female Authors In IsiXhosa Dramas Using Computational Methodologies",
		"authors": [
			"Bukula, Andiswa",
			"Steyn, Juan"
		],
		"body": " IsiXhosa is an Nguni language classified in the south-eastern geographical zone of South Africa. It is one of the official South African languages, and one of the most widely spokenwith approximately eight million mother-tongue speakers. In terms of natural language processing, particularly computational morphology, the Nguni languages including isiXhosa belong to the lesser-studied languages of the world and can be classified as under-resourced languages. Nguni languages are characterised by a rich agglutinating morphological structure, based on two principles: the nominal classification system and the concordial agreement system. The principal authors masters study focused on the representation of women protagonists by male and female authors in isiXhosa dramas. The whole analysis process was done manually, mainly because digitised isiXhosa literature books were not available. This limited the study to only four books. The analysis focused on gender inequality in the way women are represented by male authors, as opposed to the way in which women authors represent women protagonists, and also on patriarchal traces found in isiXhosa dramas. When examining the representation of female protagonists in isiXhosa dramas, similar works from other scholars are noteworthy. The first contribution that narrates the same viewpoint as the one investigated here is by Ngqase, in which she examines the representations of women in four isiXhosa drama books. The study highlights the interplay between culture and womens social space. The second contribution by Peterexpresses female character portrayal in various drama works written by males. He concludes that many male writers are unwilling to portray female characters in their totality and true complexity, which is evident in the way some writers have resorted to the use of stereotypes. As an isiXhosa language researcher at the South African Centre for Digital Language Resources, the principal author has been introduced to computational methods which could afford new ways to approach the research topic described. Assessing and reporting on the usability of computational tools when analysing isiXhosa texts This presentation reports on the same research topic, with the focus on computational methods to analyse the texts instead of manual approaches. The computational tools which were utilised include, Voyant Tools and regular expressionsas well as testing the feasibility of BookNLP when used conjunctively with written languages. The creators of Voyant Tools note that it supports analysis in any language since it mostly operates on character sequences; however, limited language-specific support is available. Capitalisation in isiXhosa is of special importance in the proposed study as the language follows a pattern where the second letter of a word is capitalised instead of the first. For instance, the Context tool in Voyant Tools produces search terms only in lower case. With BookNLP , which is specifically built for English texts, the authors will now focus on how successfully the sub-processes in its pipeline fare with a non-Western language and how it could be adapted and/or how a similar pipeline could be developed for isiXhosa using tools developed by SADiLaR. BookNLP was developed by Bamman, Underwood and Smith. The study follows similar approaches to those utilised by Algee-Hewitt, Porter and Walser. Finally, regular expressions will be used as well, as it allows to match patterns and search for very specific character sequences more effectively. Operationalisation of the research questions The study has two parts. First, by reporting on the performance of the computational tools on the isiXhosa drama corpus versus an English equivalent. The specific steps for Voyant Tools and differences when using regular expressions will be provided and compared. Second, in terms of research questions focusing on the representation of Xhosa protagonists by male and female authors, regular expressions was used as the main investigation tool. The paper reports on: How can computational tools used to analyse Western languages be used for conjunctively written South African languages? Are authors of isiXhosa literature influenced or led by their gender when writing? Do authors conceptualise their work with the intention to uplift one gender while diminishing the other? How can the gap caused by inequality between sexes be bridged through written literature? A practical example: If data from an English corpus is analysed using Voyant Tools, the tool would automatically be able to provide word frequencies and links between words. However, with a conjunctive language like isiXhosa, this would only be possible by making use of special search options, because the generic frequency table will be skewed owing to the difference in semantic properties of the words. For example, gender association in isiXhosa depends solely on the prefix. Only through the prefix will one be able to confirm whether a noun is referring to a single person or a group of people. Furthermore, only through contextualisation will one know whether that person is male or female, as isiXhosa prefixes are also unisex. E.g.: uPeter u sela amanzi Peteris drinking water uSammy u phunga iti. Sammyis drinking tea. This research also aims to provide a point of departure for new scholars interested in analysing isiXhosa literary works using computational approaches. Key words : Conjunctive language, Nguni language, computational methodologies, voyant tools, regular expressions, BookNLB "
	},
	{
		"id": 59,
		"title": "A Computational Approach to Analyzing Musical Complexity of the Beatles",
		"authors": [
			"Burghardt, Manuel",
			"Fuchs, Florian"
		],
		"body": " Introduction: The Beatles and musical complexity The Beatles are considered to be one of the most influential bands of the 20th century, who still shape and influence pop music today. In the course of their creative history, the band has proven an enormous range and variety of individual compositions. One reason for the unusually large musical diversity of the Beatles is that with Paul McCartney, John Lennon and George Harrison three persons were involved in composing the Beatles songs. In addition, the Beatles producer George Martin also had a considerable influence on the composition of many songs. In this paper we will investigate how this diversity in composition is reflected in musical complexity in the work of the Beatles by using computational methods. So far, related work studies can be found in Mason, who statistically analyzes the properties of Beatles songs in order to decipher what he calls the Beatles genome. The aspect of complexity has already been investigated by Eerola et al., who use a MIDI corpus to analyze the relationship between musical complexity in Beatles songs and its effect on chart placement. Eerola et al.also discover a highly significant increasing time-trend in melodic complexity, i.e. the Beatles songs became melodically more complex through the course of time. While Eerola et al.only looked at singular melodies and their complexity, we present a study that takes into account all of the available melodies and also the chords to compute the complexity of a song. We present an exploratory tool for the interactive visualization of musical complexity distributions in the work of the Beatles. The visualizations can be scaled from all of the Beatles albums to single songs and composers, to investigate complexity on a more detailed level. Corpus and Method Corpus – The corpus used for this study is based on guitar tablatures from the online platform Ultimate Guitar Ultimate Guitar portal: https://www.ultimate-guitar.com/,, which were downloaded in GuitarPro Guitar Pro Tool: https://www.guitar-pro.com/ format and converted to MusicXML MusicXML documentation: https://www.musicxml.com/for-developers/ for analysis. This platform has already been successfully used as a data source for other scientific studiesand includes 205 songs from the Beatles first single in 1962 to their last studio album in 1970. Normalization – To be able to compare songs with different scales to each other we normalize the note inventory according to Cuddy et al., who propose the Roman Numeral Analysis method. With this method all notes of a diatonic scale are represented by Roman numerals, starting from the basic note of the scale as step I. Tones that are not part of the scale are marked with a sharp. Example for the classification of tones according to the Roman Numerals Analysis using the scales of C Major /A Minor and D Major / B Minor. Complexity model – To operationalize the concept of musical complexity we rely on experiments by Krumhansl & Shepard, in which test persons were asked to evaluate how a certain tone completes the tone sequence of a scale. The results show that scale tonesare rated better than non-diatonic tones. Building on this previous work, we define four levels of complexity. We use this complexity model for both, the analysis of single notes as well as for chord progressions. Classification of tones into different levels of expectation-based complexity using the example of C Maj. / A Min. Computation of results – Our corpus of MusicXML files is analyzed by means of Python scripts that parse individual notes and chords from the data and count their frequencies. For the recognition of chords, the existing music21 Music 21 Toolkit: http://web.mit.edu/music21/ library is used, as it provides many useful functions for the analysis of transcribed music. Because both the tone material and the chord material of each song are to be analyzed on the basis of the previously described Roman Numeral categories, it is necessary to identify the scale of a song. In many songs the scale is explicitly annotated by means of global accidentals and therefore can be extracted directly from the MusicXML transcription. For those cases where global accidentals are missing, we apply an existing algorithm for scale detection. By assigning notes and chords to the previously introduced four complexity levels, a complexity distribution can be calculated for each song and album. Results and Discussion The results can be analyzed statistically, to detect general trends in the development of musical complexity in the work of the Beatles. We conducted a Pearson-Bravaiscorrelation test to investigate how musical complexity has developed through time. For each year, we calculate the frequency of tones and chords that belong to the complexity levels 1+2and also the frequency of tones that belong to the complexity 3+4 levels. For the higher complexity levels, we find a weak positive correlationfor both tonesand chords. These results indicate a gentle trend toward increased musical complexity over time, but cannot confirm the observation of a highly significant correlation as noted by Eerola et al.. Our results rather seem to correspond with existing research on the musical development of the Beatles, which does not describe a general complexity trend, but rather identifies different phasesand individual albums with increased complexity. This observation is also illustrated by the following graph, which does not show a clear trend in the development of complexity levels, but rather goes up and down over time. Development of tonal complexity levels The graphs for chord complexity levels largely correspond to the tonal complexity levels. The detailed graphs are available via the online visualization tool.for the Beatles albums over time. We can also show that a general complexity trend for albums is problematic, as even just a few outlier songs can substantially influence the overall complexity score of an album. This can be best illustrated for the album A Hard Days Night. While Fig. 1 suggests that tones on the complexity level 4 for the whole album increased as compared to the previousalbums, a closer lookat the individual songs shows that in fact only two of the 13 songs show high complexity on level 4. Overview of the distribution of tonal complexity levelsfor all songs of the Beatles album A Hard Days Night. These observations reflect the initial notion that the work of the Beatles is extremely heterogeneous, which is also a result of the bands different composers and their individual musical development. When we look at musical complexity from the perspective of individual composers, Paul McCartney seems to be the most stable composer, i.e. his complexity curves largely correspond to the overall complexity curves of the Beatles albums. George Harrison and John Lennon each have several albums where they contribute songs with higher complexity. Overview of the distribution of tonal complexity levelsfor the main composers of the Beatles. Note that George Harrison was not involved as a composer for the albums Please please me, A Hard Days Night and Beatles for Sale and overall only composed 24 songs. The absence of a general trend or temporal pattern in musical complexity lead us to present the results of our computational approach in a rather exploratory interface that can be used to look at complexity from different scales. The visualization tool is available online Visualization tool: https://fuchsflo90.github.io/beatles-analysis/# and can be used to explore complexity for both tones and chords, for any album of the Beatles filtered by the main composer. In addition to the complexity scores, it also provides information on the frequencies of different scales and rhythms. Conclusion In this paper we showcased the application of a computational approach to measure musical complexity in a corpus of user-generated transcriptions of Beatles songs. We were able to demonstrate that musical complexity did not consistently increase over timeand that high complexity seems to be a situational phenomenon that can occur for single songs rather than for a whole album. The approach presented in this paper can be considered as a case study for further computational studies on musical complexity, thus adding to the branch of computational musicology as part of the Digital Humanities. As next steps we plan to extend the approach to comparative complexity analyses for different bands and genres. "
	},
	{
		"id": 60,
		"title": "Tracing the Complex History of Manuscripts Using Linked Data",
		"authors": [
			"Burrows, Toby Nicolas",
			"Page, Kevin",
			"Lewis, David",
			"Cawlfield, Emma",
			"Cleaver, Laura",
			"Tuominen, Jouni"
		],
		"body": " Medieval and Renaissance manuscripts, like other cultural heritage objects, have lengthy and complex histories of production and ownership, involving events as varied as sales, gifts, and even outright theft. Research into these histories can reveal a great deal about the changing value placed on such objects over the centuries, not just in a monetary sense but also in terms of their cultural significance and meaning. Manuscript studies has been a very active interdisciplinary research area in recent decades, and researchers and curators have been intensely involved in the development of digital resources. This workshop will focus on methods for deploying Linked Data methodologies to aggregate complex data relating to the history and provenance of manuscripts, and to address large-scale research questions in this field through analysis and visualization. The presenters of the workshop have extensive expertise across manuscript provenance research, database development, and Linked Data technologies, and include members of a major international project in this field: Mapping Manuscript Migrations, one of the Digging into Data projects funded in 2017. The workshop will be of considerable interest to the DH community for its focus on Linked Data and on practical methods for transforming, aggregating and reconciling complex existing digital resources to a common data model, whilst preserving the integrity of the data sources. It will also be of particular value for DH researchers, curators, and librarians interested in new approaches to documenting, analysing and visualizing the histories of cultural heritage objects, especially manuscripts. The workshop will begin by reviewing the field of manuscript history and provenance, and identifying research questions which are representative of the types of research being carried out. It will also examine existing digital resources for this field, including the Schoenberg Database of Manuscripts, Bibale and Medium, and library databases like the Bodleian Librarys new manuscript catalogue. This introductory session will conclude with an overview of the place of Linked Data in the humanities, including the selection and use of ontologies for data modelling. The middle session of the workshop will focus on the practicalities of mapping a range of different data structures into compatible RDF, including pipelines for transforming standard relational database modelsand TEI-XML documents. The content will include discussion of the levels of complexity involved, the types of difficulties likely to be encountered, and suggested approaches to problem resolution. Participants will have the opportunity to carry out an introductory hands-on exercise in data analysis for transformation. The session will include an evaluation of suitable reconciliation processes used for matching vocabularies for persons and places across a variety of different source datasets. The final session of the workshop will provide participants with a hands-on opportunity to work with aggregated manuscript provenance data, through the user interface developed for the MMM project. This will include browsing and searching the aggregated data, and creating various different visualizations based on the data. Participants will be asked to test a variety of research questions against the data and to provide structured feedback on the functionality and usability of the interface and on the data structures. The workshop will end with an invited reflection and response from an eminent manuscript researcher coordinating a major new international investigation. Detailed programme of the workshop • Overview: Manuscript history and provenance: research questions, existing digital resources and projects Linked Data in the humanities: why and how, RDF Selection of ontologies: CIDOC-CRM, FRBR OO • Data mapping and transformation: Transformation pipeline: relational database to RDF [SDBM and Bibale] Transformation pipeline: bespoke XML to RDF [Medium] Transformation pipeline: TEI documents to RDF [Bodleian] Hands-on exercise - data analysis Reconciliation processes: vocabularies, identifiers • Aggregated data in action: hands-on exercises Browsing and searching the data Visualizing the data Testing – research questions Feedback on the interface and the data structures • Response and summing-up Workshop leaders Dr Toby Burrows is a Senior Researcher in the Oxford e-Research Centre at the University of Oxford who is coordinating a major international Digging into Data projectfocused on manuscript provenance data. He is a medieval studies researcher and librarian with considerable experience in designing and managing digital humanities projects. His current research interests and areas of expertise include: digital technology and the humanities; medieval manuscripts; the history of cultural heritage collecting, especially in the 19th and 20th centuries. Email: toby.burrows@oerc.ox.ac.uk Emma Cawlfield Thomson, University of Pennsylvania, is the Project Coordinator for the Schoenberg Database of Manuscripts. She has worked on the Schoenberg Database for over seven years, in roles evolving from data entry and quality control to instructional design and user support. In her training as a librarian, she specialized in digital librarianship and information architecture. Her current research interests revolve around the linked data environment and its application in authority metadata management. Email: emmacaw@upenn.edu Dr Laura Cleaver, School of Histories and Humanities, Trinity College Dublin, is an expert in the art and architecture in the High Middle Ages, concentrating particularly on medieval manuscripts. Her current interests include the illumination of histories, medieval diagrams, and the trade in medieval manuscripts in the early twentieth century. She is the recipient of a European Research Council Consolidator Grant, beginning in May 2019, for a major project to analyse the collecting of medieval manuscripts in the early twentieth century. Email: cleaver@tcd.ie David Lewis is Research Associate, Oxford e-Research Centre, University of Oxford. Having trained as a musicologist, he focuses his research on ways that computers and computational approaches can help musicology and other musical activities. He has worked on online resources for instrumental music, music theory and work catalogues. His current research at the Centre explores uses of Linked Data to support and extend the exploration and sharing of musical information and research. Email: david.lewis@oerc.ox.ac.uk Dr Kevin Page, University of Oxford, is a senior researcher and associate member of faculty at the University of Oxford e-Research Centre, where he applies Linked Data to the Digital Humanities through several research projects including Unlocking Musicology, Digital Delius, Mapping Manuscript Migrations, and Workset Creation for Scholarly Analysis. As Technical Director of Oxford Linked Open Datahe works with collections across the Gardens, Libraries, and Museums of the University, and has participated in standards activities including the W3C Linked Data Platformworking group and Linked.Art editorial board. Email: kevin.page@oerc.ox.ac.uk Dr Jouni Tuominen, University of Helsinki, is a coordinating researcher at Helsinki Centre for Digital Humanities, and a researcher at Department of Computer Science, Aalto University. His research interests include ontology repositories and services, linked data publishing methods, ontology models for legacy data, and tooling for digital humanities. He has collaborated with museums, libraries, and archives on their collection cataloging and cultural heritage data publishing processes. Email: jouni.tuominen@helsinki.fi "
	},
	{
		"id": 61,
		"title": "Tracing the History and Provenance of Medieval and Renaissance Manuscripts through Linked Data",
		"authors": [
			"Burrows, Toby Nicolas"
		],
		"body": " The study of medieval and Renaissance manuscripts is a very active research field, covering a wide range of disciplinary perspectives, and supported by a proliferation of digital resources in the form of catalogues, databases, digital editions and digital images. But there is little in the way of interoperable digital infrastructure to link these disparate sources together, and the evidence base for manuscript research is, for the most part, fragmented and scattered. As a result, large-scale research questions remain very difficult, if not impossible, to answer. The Mapping Manuscript Migrationsproject, funded by the Trans-Atlantic Platform under its Digging into Data Challenge for 2017-2019, aims to address these problems. It is led by the University of Oxford, in partnership with the University of Pennnsylvania, Aalto University in Helsinki, and the Institut de recherche et dhistoire des textes in Paris. The project is building a coherent framework to link manuscript data from various disparate sources, with the aim of enabling searchable and browsable semantic access to aggregated evidence about the history and provenance of medieval and Renaissance manuscripts. The four initial data sources include three relational databasesand a manuscripts catalogue consisting of XML documents encoded with the Text Encoding Initiatives Manuscript Description schema. They contain a total of 370,000 records. These have been transformed into RDF conforming to a unified data model based on CIDOC-CRM and FRBR OO. Key entity types have been reconciled with Linked Data vocabularies for persons, organizations, places, and works. A user interface has been built which combines faceted searching and browsing with visualizations for exploring the data geographically and chronologically. This framework is being used as the basis for a large-scale analysis of the history and movement of these manuscripts over the centuries. The broad research questions being addressed include: how many manuscripts have survived; where they are now; and which people and institutions have been involved in their history. More specific research focuses on particular collectors and types of manuscripts. The poster will present the results of the first eighteen months of this project. The topics covered will include the new digital platform which has been developed, the sources of data which have been combined, the data modeling which has been carried out to link disparate data sources, the Linked Data principles and techniques which have been deployed, and the ways in which the aggregated evidence has been presented and visualized. In particular, the poster will demonstrate the following aspects of the Linked Data environment developed by the project: The unified data model based on CIDOC-CRM and FRBRoo; The publishing process for transforming the source datasets into RDF suitable for ingest to the MMM triple store; The reconciliation processes implemented to link the aggregated data to Linked Data vocabularies; The user interface developed by the project, including geographical and network visualizations for exploring the aggregated data. "
	},
	{
		"id": 62,
		"title": "Scalable Exploration. Prototype Study For The Visualization Of An Author’s Library On The Example Of 'Theodor Fontane’s Library'.",
		"authors": [
			"Busch, Anna",
			"Bludau, Mark-Jan",
			"Brüggemann, Viktoria",
			"Genzel, Kristina",
			"Seifert, Sabine",
			"Trilcke, Peer"
		],
		"body": " State of research and problem outline In the course of the continuing upswing in writing process research, source types that document processes of authorial self-organization, notation, or reading have come into focus of literary and cultural studies. These include, among others, notebooks, card indexesand author libraries. While digital presentation techniques based on transcriptions and TEI editions are being developed for notebooks and card indexes, the digital presentation of author libraries is currently limited to the provision of either digital catalogs that make library metadata availableor of simple digital copies, which are offered in a viewer and/or as a PDF download. These forms of presentation, however, do not succeed in grasping the autographical character of these book collections, whose particularity lies in the reading traces that the authorhas left in them. At the same time, the aim of an online presentation of such libraries cannot be an edited text; rather, they are intended to enable the reproduction of creative reading and usage practices. Three objectives can be phrased for such a presentation, with reference to research needs: 1) the provision of the entire volumes of an authors library; 2) the implementation ofsearch opportunities; 3) the provision of novel applications with which the profile of the collection and reading patterns emerging in it can be discovered, recognised, and researched. For objective 1), common techniques for presenting digitized booksare already available; for objective 2), similarly, solutions are being offered. For objective 3), which constitutes the focus of our project, no solutions or conceptualizations have been presented so far, although there have been calls for the supplementation of search-based interfaces by concepts that enable serendipitous encounters. Even if the establishment of so-called explore modes has already partly contributed to the fulfilment of these goals, there is still a lack of flexible navigation along the relations between the objects in digital collections. Project, procedure and corpus We present a draft of an explorable and scalable representation of an authors library, developed in a research-based rapid-prototyping process. Our prototype study combines the presentation of authors libraries, which is usually addressed as a philological problem, with design-oriented research on the visualization of cultural collections. With this research we present a prototypically implemented approach of how to digitally represent authors libraries. The project takes up two impulses: On the one hand, it is committed to distant reading, which looks for possibilities of pattern recognition inan authors library; on the other hand the representations should be scalable, which raises questions about transitions between different granularities. The basis for the prototype is the library of Theodor Fontane, which comprises approximately 170 volumes. The significance of this authors library is primarily due to its provenance. It is irreplaceable because of the marginal notes and glosses written inside the books by Fontane and valuable because of numerous presentation copies. In addition to the complete digitization of the holdings according to archival standards, and the indexing of individual volumes according to bibliographic standards, data is collected at both page and corpus level. Access to the data is possible at four levels: overall corpus, object, page, and individual phenomenon. The visualization enables various access and analysis options, which ideally reveal the broad spectrum of thematic and personal clusters and the different uses of commentary within the authors library. Visualization concept The visualization places a special focus on continuous navigation through zoom and filters on several granularity levels. This allows for the exploration of individual objects as well as their comparison and broad overviews of the whole inventory. The interactive prototype offers three basic levels for navigating the collection: the level of the individual authors, books, and pages. Fig.1: Landing page/book level with overview of all books The starting point for the exploration of the visualization is the book level, which provides an overview of all books in the library, sorted by authors—starting with the author with the most books. Each book is represented by a vertical bar, in which each page of the respective book is represented by a segment, ordered from topto bottom. The page segments are color coded according to the category of occurring reading traces. While pages without traces are displayed in white, pages containing traces are colored, divided into four categories: 1) provenance, 2) markings, 3) marginalia, and 4) additional material. Hovering over a segment displays a preview of the respective page scan and the book title. Fig.2: Page level with a book selected, filtered by marginalia, and hovering over an element Giving an overview and resembling individual bar codes for each book, the book level visualizes the number of pages of a bookas well as the distribution and patterns of reading traces. Above the visualization, a filter navigation serves as a legend for the color coding and offers the possibility to focus on certain categories of reading traces. The selection of a filter triggers the unfolding of the corresponding subcategories in the filter bar. In addition, a search bar enables specific keyword search in all transcribed marginalia, highlighting respective occurrences in the visualization. By selecting a book, the other books are compressed and a detailed view of the selected book is unfolded, revealing additional metadata and details. The browsers scroll function can be used to reach the other two granularity levels of the visualization. Following the principles of semantic zoom, scrolling upwards leads to a higher level of abstraction and downwards to a higher level of detail—thus allowing a change between three main granularities: authors, books, and pages. Scrolling enables continuous, meaningful transitions between the views and offers the possibility to go back and forth at ones own speed with the aim of making transitions between views more comprehensible. Fig.3: Author level, filtered by marks, the colors of various mark types are unfolded In contrast to the book level, on the author levelall books of an author are combined by presenting the total distribution of the reading traces in form of an area chart. It is therefore possible to compare Fontanes reading traces, distributed over the works of different authors. However, the higher abstraction also allows insight into broader patterns. Navigating from the book level towards the other direction to the lower page level, the page fragments of books of a selected author are zoomed in, enabling interaction with individual reading traces. Furthermore, selecting a book on this level unfolds a visualization of the transcribed marginalia. All selections, filter options and the selected level of granularity are encoded via a fragment identifier in the URL, allowing the sharing and bookmarking of specifics views and the use of the browsers history functions. Reflection of results The novel visual approach to authors libraries developed within the framework of the prototype combines design-oriented approaches to the visualization of cultural collections with philological, archival, and library research questions. By means of visual filters, subsets can be identified and categories can be created that enable pattern recognition within the collection. Users should be enabled to grasp concepts and themes within dedicated categories and across their boundaries. The focus of the project lies on the integration of search functions, visibility and scalability in visualizations, and how further exploration possibilities can be opened up by different access options to a collection. The development of new research questions during the prototyping process and the associated readjustment in the indexing process illuminate the interactions between visual research, metadata management, and philology. It became clear that the collection is to be understood as a construct that is only generated retrospectively. In the process, the collection emerges from the unity of perspective and object. The resulting consequences of offering multiple views of the objectare reflected in the various granularity levels of the prototype and result in an observation-dependent visualization of the available source data. "
	},
	{
		"id": 63,
		"title": "Script Analysis In A World Of Anonymous WritersDigital Forensics for Historical Documents",
		"authors": [
			"Busch, Hannah"
		],
		"body": " In the medieval period script and handwriting were not very personal: they were a taught and strictly constructed set of characters. Yet scripts did develop over time, from the cursive scripts of antiquity to Caroline minuscule, Gothic textualis, humanist and early modern scripts. The discipline of studying those chronological and regional variances of script is called palaeography, and it requires a deep and detailed knowledge of historical script features. The work of palaeographers is characterized by their high level of expertise and their skill of minute manual comparison with the scope to date and localize handwritings. Therefore, there are and there have been only a few authorities in the field. To date script classification addresses the subjectivity of the human mind,but computational methods from the field of image analysis and pattern recognition could be able to objectify the study of historical scripts. The decisive reason for the application of computational approaches is not the distrust in the opinion of the few experts, but the fact that due to mass digitization thousands of medieval manuscripts have become digitally available. Beside the enormous amount of high quality image data, new standards are being established, such as IIIF , which allows to browse through and compare manuscripts kept at different institutions, and at different ends of the world with one mouse click. The increasing computing power enables scholars to process larger datasets and to analyze the images. There have been attempts in the past to develop quantitative approaches, which have experienced critique by those authorities, like Bischoff who already in 1979 suspected, that palaeography, which used to be an art of vision and empathy, becomes an art of measuring by technical means. The worries about computational methods might be justified, but as a matter of fact, the number of experts in the field has not grown in proportion to the material which has become available in recent years. It seems obvious that thinking about newapproaches to palaeography becomes more urgent than before. Therefore, the here presented project The project Digital Forensics for Historical Documents. Cracking cold cases with new technology is funded by the Research Fund of the Royal Netherlands Academy of Arts and Sciences from 2018 to 2021. attempts to create a digital tool, based on a deep learning system, in which the unique characteristics of a certain script sample will be matched with similar script samples by making use of digitized manuscript collections available in the world wide web and their IIIF APIs. While methods of feature extraction regarding the segmentation and comparison of single letters or groups of letters are time consuming when it comes to larger amounts of image data, deep learning is the technique of choice for processing a large number of historical written documents. Convolutional neural networks are able to learn the features of a script sample, to reproduce and memorize them, and to compare different samples of script and give a concrete statement about their similarity. Hence, the approach of deep learning is similar to the experienced eye of highly trained palaeographers, but the computer is able to compare even more and finer features than the human eye. Moreover, it can give a precise and objective output about the degree of similarity—or distinction between processed samples of scripts. And transform the subjective view of the palaeographer into an objective assessment. In addition to the state of the art of the technical developments, we also want to discuss the challenges the humanities scholar using computational methods encounters: The need of ground truth data in order to cluster the script samples in a way manuscript scholars can extract information from the results. The question that arises is whether the given information in the accompanying metadata can help us to add a ground-truth to the images available. Within the project we do not only want to build the technical framework, but also evaluate the deep learning approach for palaeographic research. Does the approach allow new insights on spatial and diachronic evolution of medieval book scripts by comparing different ground-truth data, and different neural networks: Which differences does the computer see? How do different settings change the classification? How similar are scripts that have been assigned to the same time and place of production, or script family? And, can the application of artificial neural networks help the manuscript scholars to answer their research questions, or the curator of digital collections to add more information to the catalog? "
	},
	{
		"id": 64,
		"title": "Complex Space-TimeModelling, Visualising and Performing Literary and Historical Chronotopes",
		"authors": [
			"Butterworth, Alex",
			"Wibberley, Simon",
			"Hay, Duncan",
			"Goudarouli, Eirini",
			"Liem, Johannes",
			"Hischorn, Steven",
			"Wood, Jo",
			"Perin, Charles",
			"Rasterhoff, Claartje",
			"Li, Weixuan",
			"van den Heuvel, Charles",
			"Speakman, Duncan"
		],
		"body": " Overview In seeking to understand human activity using computational methods, researchers in the Digital Humanities must inevitably wrestle with the complexities of space and time. Considered separately, these situating vectors of human experience pose challenges of perspective, direction, and, of course, uncertainty: both in how they are perceived and in the form of their documentation. When combined, and addressed as a cognitive matrix within which knowledge is constructed and narrative accounts are traced, that complexity is compounded. The challenges entailed are conceptual, theoretic and practical, and the panel aims to bring these diverse approaches into a productive dialogue. The panelists will offer prismatic perspectives on primarily historical human experience and knowledge construction in relation to varying notions space-time, in six short papers. These will afford a keyhole view into the research undertaken from each of the projects, as it is described in the abstracts, but specified for its particular resonance with the work of one of more of the other participants. Some participants will report on research in progress, others on work produced and evaluated, while one surveys the work of past pioneers with an argument for its reconsideration. Collectively, they will inform a lively dialogue bridging knowledge representation, data modelling, semantic annotation, machine learning, urbanism, social and cultural history, literary and courtroom chronotopes, and critical artistic and design practice. The studies are concerned with how space-time defines and is defined by human experiences: of pleasure and trauma, creativity and crime, security and insecurity; and how it is represented in textual, graphical, audiovisual and extended reality forms, empirically and imaginatively. They address questions of mobility, place-making, displacement and co-presence, as sites of complex space-time relations, in every century from the seventeenth to the present. All though, with one exception, confront common challenges around how best to identify, extract, model and compare the spatio-temporal data that inheres in the historical or contemporary record. And that one exception serves as a keystone, suggesting possible solutions to some of these questions by treating space-time as an interface metaphor that may render the multi-dimensionality of humanistic data legible in terms that are disciplinarily congenial. It is hoped from the resulting dialogue between panel members will produce some pointers to how organising concepts and frameworks from one approach - chronotopes, dimensionality reduction, thick-mapping, fuzziness and spatial syntax - may prove unexpectedly applicable across conventional boundaries, and how these may be usefully mediated by new or existing digital tools and techniques. Deep mapping cultural venues as a historical space-time interfaceClaartje Rasterhoff and Weixuan Li Why are some places and periods considered to be more innovative than others? In addressing this question, humanities scholars and social scientists have pointed to a complex interplay of historical circumstances, urban conditions, talented individuals, as well as local supporting institutions. Individual studies tend to favor the one factor over the other, but collectively they increasingly employ spatial concepts and methods, visible in the mapping of spatial clustering, social networks, and migration patterns of actors or objects. In these approaches, it is often assumed that urban space facilitated the creation and the transmission of innovation over time and place, but it is still unclear how this would have worked in practice. Where and how wereideas negotiated and challenged, adopted and adapted? Would this have differed across time and place? If so, how can we present the changes over time and space? Inspired by time-space geography, we propose a methodological framework for embedding complex spatial thinking and visualizations into existing research on the historical relationships between place and innovation, and create multi-layered deep maps with Geographical Information Systemto capture the dynamics and the uncertainties in all its complexity. And we test it by dynamic digital mapping of a particularly lively and well-researched time and place: seventeenth-century Amsterdam. At the heart of our approach is a visual representation of cultural production and consumption as dynamic and performed activities embedded inpublic life. We argue that we can observe and represent these activities in specific private and public urban spaces, and that, more broadly, a focus on events and venues contributes to the integration of temporal and spatial perspectives in the digital humanities. After all, it is in specific venues in specific urban environments that time and space cross paths, because historical activities relating to the cultural life of urban dwellers can only take place in a physical location on a particular time. The early modern period is, moreover, particularly interesting and challenging, because there were few formal cultural venues, as in the case of Amsterdam. Deep mapping culture venues in early modern Amsterdam forces us to explore available sources, and, more importantly, to rethink how we can analyze and visualize time and space while allowing for uncertainties and ambiguities. Historians have observed that many activities that we might now consider as cultural such as theatre, music, but also art auctions and dice games were taking place in taverns or inns, which in turn were often run by cultural entrepreneurs or their family members. We investigate the role of these venues in the citys various creative industries, and as spaces where entertainment and cultural consumption may have converged to give way to popular innovations. Extracting the historical experience of urban and domestic space-time from the Old Bailey corpus by a novel application of machine learningAlex Butterworth and Simon Wibberley The courtroom records of the Old Bailey teem with the everyday life of eighteenth and nineteenth century London, around those moments when mundane existence is more or less abruptly intersected by acts of criminal transgress. Dense sequences of human experience are described, often in multiple accounts, in an attempt to forge a consensus version of events. These are transcriptions of the legal process as it remediates the configuration of space-time that defined the original criminal event, through the formalising chronotopes of the courtroom. The temporal and spatial dimensions of the original events, already subject to uncertainty and contestation, become further layered and involuted through narrative testimony. These concatenated constructions of space-time can be disentangled by close reading. However, it is only when the corpus is viewed at scale that it reveals historically significant patterns of human behaviour within the urban and domestic environment: the lived experience of the historical actors, and the micro-incidents that comprise it. To access these insights, we are developing a novel methodology for the semantic annotation of the historical corpus of the Old Bailey that employs supervised machine/deep learning technologies: a process in which the humanist scholar is continuously involved in assessing and categorising outputs, to iteratively generate additional training data, in a virtuous cycle. The approach deploys a collection of advanced natural language processing methods to automatically annotate textual elements in the initial phase. The subsequent examination of these entities by their distribution, individually by type and in combination, within the texts and in relation to their discourse structure, make it possible to identify and code fragments of a semantic graph as representing events. These events, in turn, can be mapped or otherwise visualised in their physical context - of crime scene or courtroom reconstruction - and further analysed and validated using measures of coherence and plausibility that leverage the analysts intuitive knowledge of space-time relations. This method will generate a rich set of semantic annotations, enabling new kinds of research questions to be posed of the corpus, at three levels of granularity: macro: eg. what patterns can be discerned in the spatial distributions of event over time, or in the frequency of reference in specific types of courtroom discourse? meso: eg. what types of events, considered as assemblages of activity, place, time and person, characterise particular crimes? micro: eg. what events and locations characterise the itineraries of thieves and their victims, and what causes deviation from these norms? Visualising the uncertain: How abstract representations of spatio-temporal data can help us unlock large-scale heritage collectionsEirini Goudarouli, Johannes Liem, Steven Hischorn, Charles Perin Since 2014, a major archive has undertaken the digitisation at scale of analogue, hand-written War Diaries from the First World War, documenting the story of the British Army and its units on the Western Front, using an interactive crowd-sourcing platform. Crowd-workers have digitised more than a million diary pages. During this process, essential information about military units, including labels for casualties, unit strength, weather, everyday army life, soldiers names and grades, military activities at the front and non-military activities behind the line, have been tagged and annotated. However, the transcription process introduced uncertainty on many levels due to missing records or pages, misspellings, illegible passages or lost diaries. Some uncertainties derive from the particular type of data that has been gathered, the circumstances of its creation, and its post-processing. These may be the result of human errors: the British soldiers who wrote the diaries made misspellings, faithfully transcribed, while the crowd-workers also introduced new typos, leading to further ambiguity regarding names of people and locations. Or they may be intrinsic to the historical record, as where spatio-temporal uncertainty arises when several places are mentioned on one single day in the diary. Close reading is then required to determine in which order the places were visited by the troops or whether a place was mentioned as a troop location or other reasons. These types of uncertainty may lead to ambiguous geographic coordinates during the geo-referencing process. As part of the collaboration between Archives and its university partner, an approach was developed that we term GeoBlobs: an abstract representation of spatio-temporal data, which visualises uncertain spatio-temporal data derived from the handwritten military diaries. GeoBlobs offer one possible solution to these problems. They are an abstract representation of moving entities on a map with uncertain positions. Instead of showing a unit at a given point in time, GeoBlobs convey an unordered estimation of the possible locations over a temporal window using enclosed shapes, or blobs. To this end, we apply heuristics to weigh each location within the temporal window. At the representation stage, it is possible to specify dynamically whichsets of location are considered to form the GeoBlobs. For other visualization approaches of spatio-temporal uncertainty in a cultural heritage context please refer to Windhager et al.. Different form and style parameters can influence the visual appearance of GeoBlobs and their semantics. Uncertainty can be expressed through sketchy or blurry styles, while the use of multiple GeoBlobs allows us to compare units and show different probabilities. Animation or overlays communicate additional context like militaryor non-military activities. The GeoBlob project aims to reveal stories of the soldiers day-to-day life behind the lines, which will lead towards a narrative visualizationfor communicating the life behind the trenches that cannot be found in our history books. *As part of this paper, a short video of a prototype displaying the GeoBlobs in action will be presented. Spatial allusion, temporal recurrence and cognitive uncertainty: visualising chronotopic structure in a literary textDuncan Hay and Alex Butterworth The mapping of literary texts has become a commonplace of Digital Humanities research in recent years. Whilst many projects have interpreted this mapping as the accurate georesolution of textual name references, others have used computational methods to explore the place-making mediated by text. Often, these approaches have aimed to locate the text, using GIS methods. This study abstracts itself further from an indexical relationship to the physical world, focusing instead on the narratological role of chronotopic relationships within literary texts: how varying modes of time and space are deployed and inter-operate, curdling together to create and recall the textual experience of situated event. It explores this construction of narrative through the process of designing an interactive data visualisation interface, using semantic annotations of a literary text, manually produced during an initial process of close reading. The annotations encode semantic entities as types of place, time, person and event, along with textual cues and referential clues, more or less explicit and probabilistically interpreted by the reader. Conceived as an experimental exploration of the problems and potential analytical value of the computational modelling and visualisation of literary chronotopes, this phase of the project addresses texts that are manageable in length but pose significant challenges: allusion and anaphora, uncertainty and ambiguity, recurrence and duplication, cognitive lacunae. It embraces the notion of literary text as cognitive scaffolding, whose effect lies as much in the absence of description and specificity as its presence, while grounding its analysis through the identification of chronotopic anchor points within more fluid constructions of contextualising time and space. The case study is Coleridges The Rime of the Ancient Mariner. The poem adheres to the simple past tense in its progressive dynamic, yet operates through a temporal scheme of analepsis and recurrence between a cursed present and the framing device of a confessional future, with spatial allusion playing a complex role. The paper will offer critical reflections on designing exploratory visualisations of fragments of semantic graphs in fluid and dynamic configurations; on the most useful and appropriate schema for such semantic annotation; on the practicability of scaling such annotation using supervised deep learning methods; and on the insights gained by such analysis both for literary study and the authoring of novel digital literary forms. Only Expansion: Composing Temporal Structures For Augmented Audio Experiences In The AnthropoceneDuncan Speakman This paper will present the practice based research outcomes of an augmented audio urban artwork, offering an account of how merging realtime processing of the listeners immediate sonic environment with remote field recordings can offer new critical approaches to temporal perception and contemporary ecological thinking. The artwork under examination uses custom mobile technology to create an urban audio walk that both remixes the immediate sound environment of the audience and combines it with field recordings from remote locations. In the experience participants wear headphones that also contain binaural microphones, the signal from these microphones is fed through DSP software in bespoke handheld devices before being fed back to the headphones. In this way the voices of passing pedestrians might become a resonant choir, or a bus engine may form a rhythmic counterpoint. The field recordings are sourced from a series of international locations all undergoing major environmental shifts, so the sound of the wind in the city where the audience experiences the piece may become merged with wind recordings from the Tunisian Sahara. Through the combination of field recordings with processed and raw microphone signals, an interface is created between the listeners presence, the immediate space and remote locations. Within the arc of the composition the audience is invited to seek out types of location through textual prompts often as simple as single words such as drift or border. In this way the work offers a site responsive rather than site specific experience, and the absence of cardinal guidance forces the audience to navigate the urban space through direct physical and sensory engagement. Drawing on over a decade of international practice in the creation of locative audio walks by the author, the paper considers new compositional structures for works using augmented audio technologies, focusing on the layering of different temporalities within urban environments. The effect that is produced when the audiences lived experience of walking through the work are layered with the timescales represented within the field recordings speaks to Timothy Mortons idea that we are currently living with the uncanny sense of existing on two timescales simultaneously. Our everyday human actions feeding into processes that extend far beyond our lifetimes. This experience is considered within the context of Anja Kanngiesers proposal that sound can help to differentiate the sweeping universality - and hence the seeming unchangeability - that the Anthropocene poses, and that sound renders apparent that the world is not for humans. The world is rather with humans.. By situating the audience within the layered temporality of the work, not just physically, but as an active contributor to the soundscape, this inquiry offers new approaches to augmented audio as a way of inhabiting, communicating and knowing an entangled world. It begins not with distant stories being collected and delivered, but at the site of the audience experience, and expands outwards from there through the transversality of sound. Exploring how historical visual classifications can inform data modelling using semantic web technologies for the digital humanitiesCharles van den Heuvel Digital humanists who seek to subject complex realities to computational methods of analysis must first build and manage complex models, conceptualised in terms that embrace but exceed the familiar dimensions of space and time. This challenge has a long history that may be traced back at least to the early 20 th century and this paper will survey that tradition for how it may inform contemporary approaches in the digital humanities. At the turn of the twentieth century, scholars from diverse disciplines were concerned with knowledge organization at a global level, aiming to reconcile new insights from evolutionary theory, the emerging disciplines of psychology and physicsand traditional philosophical and humanist epistemology. Three-dimenstional objects and imagistic thinking played a crucial role in the classificatory systems and knowledge modelling developed by Paul Otlet, Patrick Geddes, Wilhelm Ostwald, H.G.Wells and others. Their approach was conceived to allow both automatic information retrieval and analysis, and a synthesising of knowledge that retained its dynamic complexity. Novel techniques of dimension reduction nevertheless challenged the multidimensional habits of humanistic thought - a tension that the mathematician Shiyali Ranganathan, who developed the multidimensional Colon classification, described in 1951: Thought is multi-dimensional. But we are one-dimensional beings – that is we still prefer all things to be handled to be arranged in one-dimension […] This means that classification is essentially a transformation of a many-dimensional universe into a uni-dimensional, uni-directional one. Machine tools are expected to perform this transformation. As the computer proved its capacity to analyse large quantities of scientific information and to make complex calculations, there was criticism of the efficient approaches developed by the information sciences. Gerard Cordonnierhad already tried to define intellectual space and advocated classification as a collection ordered by points of view. Joseph Lickliderwould reassert the value of spatial analogies in the face of linear methods, observing the relative absence from the contemporary debate of reflections on concepts such as information space, semantic space or the space of knowledge. Parallel to theseconceptualizations of space in library and information sciences, historians formulated theories of time and periodization, distinguishing types of duration. Their aim was to equip their field of conceptual modelling with rigorous defined spatial metaphors of multidimensionality: to imagine what Kubler described as the shape of time. It was, in some senses, the conjunction of these aspirations that Ted Nelson had sought to reify in Project Xanadu, espousing the use of viewable units of meaning: vunits whose contents are linkable and transclusible. By 2007, Nelson would become frustrated that the World Wide Wide had come to imitate paper under glass rather than a vision of true hypertexutality. This paper argues that a decade on, as the humanities increasingly embrace semantic web technologies, reflection on past attempts to explore multi-dimensional conceptualization of time and space can significantly inform current aspirations for the inclusive modelling of complexity in humanistic enquiry. "
	},
	{
		"id": 65,
		"title": "Feature Selection in Authorship Attribution: Ordering the Wordlist",
		"authors": [
			"Eder, Maciej",
			"Byszuk, Joanna"
		],
		"body": " Introduction Feature selection in machine learning is one of the best covered topics in general statistics literature, and, next to classification algorithm, the most important factor to consider. To name but a few, the techniques of identifying the most efficient features include dimension reduction, shrinkage, or penalization. However, in stylometric investigations the selection of best performing style-markers is usually narrowed down to the choice between word frequencies, character n-grams, POS-tag n-grams, etc., without devoting much attention to their statistical properties. The purpose of this study is somewhat different: apart from identifying meaningful features – or such that facilitate telling apart analyzed classes – we would like to provide some deeper linguistic understanding of the most distinctive features, i.e. discover if words efficient in classification share any linguistic properties. In stylometry, and particularly in authorship attribution, most frequent words, or more specifically, their mean frequencies in the corpus, are traditionally claimed to exhibit strong discriminative power. In a vast majority of studies following a seminal study by Mosteller and Wallace, the feature selection procedure starts with preparing a joint frequency list containing words ordered in a decreasing sequence of the number of their occurrences in the entire corpus, from the most to the least frequent. It has been shown that MFWs are mostly functionwords, bearing meaning only in the company of other words, which makes it a very plausible theoretical justification for simply taking a considerable number of top words from a frequency list as potential features. Even if the impact of contentwords is also considered important, and as claimed in a recent studycrucial for the Zipfian distribution of words in a dataset, these words are very sensitive to topic, theme, and other factors that might overshadow the authorial signal. However, there is no simple answer to the question how many of MFWs should be taken into consideration. Consequently, there is no consensus between scholars when the choice of MFW vector is concerned, ranging from a very limited number of top wordsto long vectors of 1,000 or so features. Rybicki and Edertested hundreds of combinations for both frequent and not-so-frequent words, and came to a conclusion that there is no universal number of features that would lead to satisfying results: this always depends on language and corpus, although a vector between 100 and 1000 MFWs usually reveals acceptable performance. Further studiescorroborate these findings using various measures of textual similarity. There are several ways to balance the influence of a certain words in the corpora and lessen the impact of less important ones. Undoubtedly, the most popular is tf-idf, commonly used in information retrieval systems. It assumes that a word which is attested in few documents, but is yet relatively frequent, contributes much more to the general knowledge of the text than a popular word evenly distributed across the corpus. This method of weighting by definition culls the most frequent words, and boosts the weights of keywords, or unusual words. A possibly unwanted effect is that the keywords also include several proper nouns, names, and so on. A different approach assumes that the use of some words – no matter how frequent they actually are – does not differ significantly across the corpus, whereas some others are over- and underused. The variability of a given word in a text collection might then be a good indication of its discriminative power. However, since the standard deviation of a given sequence of numbers strongly depends on their actual values, this also holds for word frequencies. E.g., the standard deviation for the set x= {1, 2, 3, 4} is as high as 1.290994, the standard deviation of the set y= {10, 20, 30, 40} is ten times bigger, which, going back to the realm of word frequencies, would mean that the variability of the word the would be orders of magnitude higher than that of the infrequent words. This can be corrected with the coefficient of variation, which is the standard deviation of a set of values divided by its arithmetic mean. First attempts to tune the word list according to the CoV were carried out by Hoover. In the following experiments, we explore the above three basic ways of ordering the features:according to their mean frequency in the corpus,according to their mean TF-IDF score, andaccording to their CoV. Machine learning is always burdened with the problem of over- or underfitting the model: using too few features usually makes it difficult to reveal the signal from the corpus, but relying on too many without any pruning might lead to overshadowing the signal with noise. Rather than looking for the optimal length of feature vector, we approach this problem with the intention of a rearrangement of features from the most to the least meaningful, which can help neutralize the issue of unimportant words landingin the number of considered features. While it is possible to balance the impact of particular, not necessarily very important, features with a proper choice of distance measure and classification method, these solutions require advanced knowledge of the existing practices and the ability to critically assess the method and results. Applying a better method of feature selection, more resistant to words that carry importance only for singular works or authors, makes stylometry more accessible to beginners and provides more objectivity, reducing the opportunity and urge for cherry-picking. Dataset We used 4 datasets: 100 Polish, 100 English, and 75 French novels for main analysis, and smaller set of 28 English for test purposes, as the computations were time-consuming. The datasets were retrieved from resources used as benchmark by Computational Stylistics Group and Evert et al. 2017 comparison of Delta measures. The texts in these corpora are balanced for the time period, number of texts per authorand to some extent style and topic – all canon literary texts, dealing with social issues, with some influence of history. TF z-scores TF-IDF z-scored TF-IDF mean TFx x, discussed in detail x x TF-IDF x x, discussed in detail x x CoV x x, discussed in detail x x Table 1: Considered scenarios, out of which z-score weighting discussed in this paper. Although we tested all above-mentioned scenarios, due to limited amount of space in this abstract, we focus on the behavior of various order of features that are next transformed to z-scores, which proved most interesting in terms of the results. Other weights and their combinations with different arrangements of the features will be discussed in the full-length version of this paper. Experiment design In our approach, we performed a series of controlled tests of attribution, using the above-mentioned corpora of known authorship. Over multiple iterations we recorded the number of texts that were correctly attributed by our chosen supervised classifier. To neutralize an impact of any local idiosyncrasies in our corpora, we applied leave-one-out cross-validation scenario, which meant that every single text from a corpus was checked against a slightly altered training set. As for chosen classification method, because of conceptual simplicity and intuitive interpretation of the results, we use k -NN supervised lazy learner, with k= 1. In stylometric community, this classifier is well known under the name Delta, and widely used. Since it is somewhat difficult to test multidimensional methods using one feature at a time, we apply a moving window approach, in which for every single feature to be tested, we perform a test in 10-dimensional space for the feature in question and its 9 subsequent features. In other words, in each iteration we test the combination of w i + w i+1 + … + w i+9 features. Results The evaluation of different rearrangements of the list of features starts with a classic MFW-centric approach, or ordering the features according to their mean term frequency. The results turned out to be surprisingly unsurprising: mere word frequencies outperform other approaches. Fig. 1: Attribution scores for the words ordered based on MFWs, weighted with z-scores. One circle represents one window of 10 subsequent features. TF-IDF reveals reasonable predictive power for the features ranked at the top of the list, as evidenced by the first few hundreds of words. However, the obtained values are significantly lower than for the regular MFWs. Worth noticing is the fact that the end of the feature list spikes up: very frequent function words excluded from the TF-IDF company and clustered at the end of the list actually outperform all the other features in this picture, once again highlighting the relevance of very common and very frequent words for the attribution. Fig. 2: Attribution scores for the words ordered based on mean TF-IDF, weighted with z-scores. Interesting and counter-intuitive are the results for the coefficient of variation: the most successful attribution scores alike, or even higher, than those obtained for TF occur only for the features grouped at the end of the considered wordlist. Moreover, the significant features are distributed more densely than in the other case. Essentially, an inverse version of CoV, computed as 1 / CoV, will serve as an efficient feature harvester. Fig. 3: Attribution scores for the words ordered based on coefficient of variation, weighed with z-scores. While general attributive success rates are higher for TF than for CoV, the tail of non-distinctive features is longer in the case of CoV than for the TF-weighted list, which suggests that the two methods can harvest meaningful features quite effectively, despite differences. This brings us to the question: What if we combine their potential to extract the right features? We propose to simply multiply TF and reverse CoV. Knowing that CoV is in fact the standard deviation of a given feature divided by its mean, we can denote the formula as follows: ω i = μ i ×) where ω i is the new weight of a feature i, μ i – its mean TF, and σ i – its standard deviation. With a little bit of algebra, we observe that: ω i = μ i 2 / σ i which, we believe, will further aggregate meaningful features at the beginning of the wordlist. We additionally tested a similar idea of boosting the meaningful features by simply magnifying TF by standard deviations: μ i × σ i. Fig. 4: Cumulative attribution scores for different scenarios discussed in the paper, for the first 500 features. Inset: an overview of the entire feature set. Results obtained on the test corpus of 28 English novels. The comparison of the above three plus two newly introduced scenarios is shown in Fig. 4, where cumulative sums of attribution accuracies are presented. If the features were spread randomly on the word list, the observed results would follow the grey dashed line. The higher a given trajectory, the better an order of features from most to least meaningful. An overview of the entire feature setexhibits a good performance of TF, but a closer look at the top few dozen featuresshows that our newly introduced weighting outperforms the time-proven MFW-centric approach. The suboptimal behavior of CoVis mirrored by its very low trajectory. Conclusions In this paper, we experimentally confirmed that the intuition of ordering a list of features according to their decreasing frequencies has solid grounds. An alternative approach – ordering features according to their keyness, i.e. TF-IDF scores – turned out to be questionable. The major observation formulated here, however, was that combining the usual MFW approach with an inverse CoV weighting arranges the features even more efficiently. Acknowledgments The study was conducted as part of Large-Scale Text Analysis and Methodological Foundations of Computational Stylistics projectfunded by the Polish National Science Center, for whose support we are very grateful. "
	},
	{
		"id": 66,
		"title": "A Digital Platform for the “Latin Silk Road”: Issues and Perspectives in Building a Multilingual Corpus for Textual Analysis",
		"authors": [
			"Carbe', Emmanuela",
			"Giannelli, Nicola"
		],
		"body": " Introduction In March 2018 the project Eurasian Latin Archive was started with the aim of building a digital platform for a large corpus of Latin texts and documents from medieval and early modern times. The corpus encompasses text from East Asia, including Latin-Chinese texts, on which a group of researchers began to work addressing specific multilingual issues. The corpus will be available within a digital library provided with tools for textual and thematic analysis. The goal of the project is the comparative linguistic analysis, both internal with other Latin texts from different eras and areas, and external with non-Latin texts on homogeneous subjects. The ultimate purpose of the project is to highlight relationships by extracting and investigating 1) historical-cultural data about religion, law, science, art, and customs; 2) linguistic data, to be compared with homologous values in European Latin literature, in order to determine the specificity of East Asian Latin and investigate overlappings and mismatches between Latin words and their localequivalents. The start-up phase of the project, which will end by February 2020, is co-financed by Regione Toscana and QuestIT, an IT company specialized in NLP and Artificial Intelligence. The working group is interdisciplinary and brings together medievalists, digital humanists, engineers and IT specialists. The platform is inspired by the digital archive ALIM, which acts as a starting point for the new archive. However, unlike ALIM, the project is geared towards providing textual analysis also with machine learning tools; ALIM focuses instead on digital representation of editionsand takes into account texts of Italian Latinity exclusively. Preliminary complexities: defining and creating the corpus To define the corpus, a first census of over three hundred texts has been made. The amount of documents is due to increase, but it is sufficiently large to collect some preliminary results in the main projects tasks. It seems useful to classify documents into four main categories: 1. Born-digital editions, freely available or to request for; 2) moderneditions that have entered the public domain; 3. editions with possible issues with OCR; 4. manuscripts. With the exception of the born-digital editions, the categories can a) be already digitized and made available by other Institutions; b) still need to be digitized. At this stage, besides the born-digital editions already acquired, seventy more documents of the type 2a and 2b have already been processed. In the case of critical editions, we chose to provide the text without encoding the critical apparatus. Items are being encoded in TEI P5, with a particular attention for the essentials metadata, such as VIAF and Wikidata ID when available, OCLC references, reliability both for external and internal processes of the item, possibly opening the way to integration with semantic models. As a non-conclusion: a structure that is modular and can grow more complex with time For such an ambitious project it is obviously necessary to proceed by ensuring a modular architecture from the very beginning, so that it can be implemented within this project progressively and can possibly merge, through its interoperability features, with open source projects from other Institutions. For this purpose, a requirement analysis has been carried out, and a list of specifications has been redacted in order to provide a basis of guidelines for the subsequent implementation. The aim of this paper is to display the architecture of the project and some of the early results: a prototype based on the ElasticSearch search engine has been developed, which includes a search moduleand a browse module, with the possibility to investigate the corpus by authors, dates and periods, collections, sources, places mentioned within the document, languages used in the text, keywords. A particular attention is being paid to the recognition of entities. For the moment being, we are working on recognition of places, dates and people. For the particular case of places, we also need to take into account that location names often differ significantly from the ones used nowadays, to the point that many researchers actually disagree on the current location of cities mentioned in these texts. More tools will be added to the preliminary ones, in order to pursue a text analysis. One of the most important issues remains however the encoding of multilingual documents, currently under examination by handling Prospero Intorcettas Sapientia Sinica, which contains chinese text, and transliterations in latin of the chinese terms. "
	},
	{
		"id": 67,
		"title": "Mise en Discours de l’Information et Parcours de Lecture dans l’Edition critique numérique",
		"authors": [
			"Casenave, Joana"
		],
		"body": " Introduction Les éditions critiques traditionnelles, imprimées sur papier et regroupées dans des collections spécialisées, répondent à des codes stricts de structuration du contenu. Cest un jeu complexe de normes de lisibilité, élaborées au fil des siècles, qui a donné au texte imprimé son maximum defficacité et en a permis une lecture facile et rapide. Lorsque lon passe au support numérique, ces conventions disparaissent en partie. A lécran, il sagit donc de reprendre les normes élaborées pour limprimé et de les faire évoluer afin quelles sadaptent aux particularités du format numérique. Il existe un standard dévolu à lencodage des documents textuels dont font partie les éditions critiques : il sagit bien entendu du langage XML/TEI. Mais si la structuration des données est régie par les règles de la TEI, la présentation finale de linformation na pas fait, jusquà ce jour, lobjet dune quelconque normalisation. Pour une étude sémiotique des éditions savantes numériques Dans notre présentation, nous nous concentrerons précisément sur les dispositifs de visualisation qui soutiennent une mise en discours de linformation dans les éditions critiques numériques. De fait, comment léditeur met-il en discours linformation quil souhaite présenter aux lecteurs ? Ce qui nous intéresse est de savoir quelles sont les règles empiriques de structuration, de disposition et de présentation de linformation qui peuvent être observées dans les éditions numériques. Nous ferons une étude sémiotique de lédition critique numérique et nous pourrons ainsi voir si lédition critique numérique actuelle est, ou non, en voie dinventer de nouveaux codes de structuration de linformation et de lecture du contenu. Pour ce faire, nous allons nous appuyer sur un corpus conséquent déditions critiques numériques et sur leurs données de description. Des chercheurs de l Austrian centre for digital humanities ont récemment mis en ligne un catalogue des éditions critiques numériques Dirigé par Greta Franzini, ce projet est consultable à ladresse suivante : https://dig-ed- cat.acdh.oeaw.ac.at/browsing/editions/ . Appelé à être enrichi au fil des années, ce catalogue recense à ce jour plus de 300 éditions, qui sont décrites au travers dune trentaine de critères. Plusieurs dentre eux concernent les outils de traitement et de visualisation de linformation. Pour mener notre étude, nous allons nous appuyer sur ces données de description spécifiques. Interrogeables directement sur la plateforme, elles sont également librement téléchargeables sur Github. Nous allons donc nous servir de ces données pour les analyser et répondre à nos questions de recherche. La communication que nous proposons sorganise en deux axes : - une analyse quantitative des données du catalogue nous permettra de dresser un tableau générique des dispositifs de visualisation qui portent la mise en discours de linformation ; - une analyse qualitative dun corpus resserré de ces éditions critiques numériques nous permettra détudier plus précisément les parcours de lecture élaborés dans les éditions. Analyse quantitative des données de description du Catalogue of Digital Editions Lanalyse quantitative de ces données nous permettra de répondre à des questions de recherche telles que les suivantes : - quels outils de visualisation sont utilisés dans ces éditions critiques numériques, et dans quelles proportions ? Observe-t-on des tendances générales ? Distingue-t-on une normalisation des pratiques ? - la définition dun type de lectorat particulier auquel sadresse les éditeursa-t-elle une influence sur les outils intégrés à lédition, et sur les formes de visualisation de linformation proposées ? Le cas échéant, quelles tendances peut-on relever ? - le type de documentation-sourceet la périodeconcernées ont-elles un impact sur les formes de visualisation privilégiées par les éditeurs ? Analyse qualitative des parcours de lecture proposés dans les éditions critiques Après avoir dressé un tableau général des outils de visualisation de linformation récurrents dans les éditions critiques numériques, nous allons nous intéresser à une forme de visualisation de linformation qui nous semble constituer un enjeu majeur dans les éditions de textes : il sagit de la mise en discours de linformation au moyen des parcours de lecture. Il sagit dun enjeu important dans lédition numérique, car celle-ci permet aux éditeurs de démultiplier les discours critiques. En effet, lorsque les corpus présentés dans les éditions sont de tailles importantes ou bien sont très spécialisés, il devient essentiel de guider le lecteur dans sa découverte des textes. Les parcours de lecture constituent alors des modes de visualisation transversaux de linformation. Mais que sont ces parcours de lecture ? Sur quels outils techniques sappuient-ils ? Comment assurent-ils la mise en discours de linformation ? Quelle part dinnovation les éditeurs détiennent-ils et dans quelle mesure sont-ils contraints par les structures éditoriales qui portent leurs travaux ? Lanalyse préalable des données de description du catalogue permet de cibler certaines éditions critiques numériques dans lesquelles les éditeurs attachent une importance particulière aux outils de visualisation et à la mise en discours de linformation. Nous allons donc mener notre étude qualitative des parcours de lecture sur ces éditions numériques repérées au préalable. Ces dernières sont au nombre de dix, et constituent ainsi notre corpus dobservation restreint, présenté ci-après. A lexamen de ces éditions, il nous est apparu clairement que les parcours de lecture peuvent prendre deux directions principales, suivant quils se polarisent sur le texte édité ou bien quils prennent lensemble de la documentation comme objet de discours. Ces parcours se partagent entre parcours savants dans le texte établiet les parcours transversaux dans lensemble de la documentation. Les observations que nous ferons dans cette communication nous permettront de dégager des tendances et caractéristiques des éditions critiques numériques afin de comprendre comment ces éditions sont organisées et comment sy créent des normes de design et de structuration de linformation. Corpus dobservation des éditions critiques numériques : Ainsworth, Peter et Croenen Godfried, « The Online Froissart ». < http://www.hrionline.ac.uk/onlinefroissart> Biet Christian et Ravel Jeffrey, « Les Registres de la Comédie française ». http://cfregisters.org/fr/ Burgio Eugenio, Buzzoni Marina et Ghersetti Antonella, « Dei Viaggi di Messer Marco Polo ». < http://virgo.unive.it/ecf-workflow/books/Ramusio/main/index.html> Carrasco Raphaël et Clavaud Florence, « Le Petit Thalamus de Montpellier ». <http://thalamus.huma-num.fr/> Fianu Kouky, Fortier Anne et Clavaud Florence, « Lannée 1437 dans la pratique de Pierre Christofle, notaire du Châtelet dOrléans ». < http://elec.enc.sorbonne.fr/christofle/> Ghelardi Maurizio, « Burckhardt Source. ». < http://burckhardtsource.org> Huber Alexander , « Thomas Gray Archive ». < http://www.thomasgray.org/> McGann Jerome, « The Complete Writings and Pictures of Dante Gabriel Rossetti, A Hypermedia Archive ». < http://www.rossettiarchive.org/> Nougaret Christine et Clavaud Florence, « Les carnets de prison dHenri Delescluze à Belle-Île ». < http://elec.enc.sorbonne.fr/delescluze/> Stinson Timothy L., Duggan Hoyt N. et Turville-Petre Thorlac, « Piers Plowman Electronic Archive ». < http://piers.iath.virginia.edu/> "
	},
	{
		"id": 68,
		"title": "A Database of Islamic Scientific Manuscripts — Challenges of Past and Future",
		"authors": [
			"Casties, Robert"
		],
		"body": " The ISMI project The Islamic Scientific Manuscript Initiativeproject was founded in 2005 to make accessible information on all Islamic manuscripts in the exact sciences, whether in Arabic, Persian, Turkish, or other languages from the 9 th to the 19 th century ISMI website: The ISMI project limits itself to scientific manuscripts but it tries to encompass all such manuscripts worldwide regardless of their current location and it tries to record as much information about these manuscripts as available, including reader and ownership marks, annotations and illustrations, making it possible to learn more about structures and practices of knowledge in the islamicate world. The database The database of the ISMI project is a cooperation project by the Max Planck Institute for the History of Science and the Institute of Islamic Studies at McGill University in Montreal. The database has been built up over more than ten years starting from an early personal database project by the involved scholars, extended by corrected information from catalog works like MAMSand personal research by the scholars in the project and outside. It currently contains information about over 4700 texts in 15000 witnesses in 8000 codices and 2500 persons and an accompanying secondary bibliography of 2700 titles and it is constantly being extended. The database development started in 2006 with a new data model based on the idea of a network of flexible objects and relations. Objects can have arbitrary attributes and the relations between objects are also like objects and can have attributes. Part of current ISMI data model showing relations between text, witness, person and codex objects. The basic objects in the data model are for example the TEXT which is abstract, the WITNESS which is a concrete material manuscript and the PERSON. These objects are connected by relations like is_exemplar_of which connects a text and its witnesses and was_created_by which connects a text and a person as its author. The same person can at the same time also be connected to other witnesses as a copyist or as a dedicatee. This very flexible data model was regularly modified and extended to accommodate changes and refinements that were developed in close cooperation with the scholars entering the data as their understanding of the source material and the technical possibilities of the system changed. Examples of theses unique additions are the possibility to record misattributions of authorship and misidentifications of witnesses in existing literature as well as documented reading events and changes of ownership. This concept of a network of objects with flexible relations, an attribute-graph, exists today in database products like Neo4J Neo4J: but those were not available in 2006 which led to the development of a custom database called OpenMind. The database software is Open Source, written in Java, uses a conventional SQL database backend and a Web-based frontend. A first version of a public website presenting a limited set of 130 codices by the Staatsbibliothek Berlin with digitalizations was published in 2015. Towards new standards The current database system OpenMind was a custom development which was necessary at the time of its creation but has not aged well and burdens the future development of the project with limited flexibility and high maintenance for software development. The data model was also not created based on existing ontologies due to a lack of usable tools at the time. Both features were acceptable during the development of the project but they pose a problem to the continued maintenance of the project and the reusability of its data. Currently both software and data are migrated to new standard tools in two phases: In the first phase data is still entered in the legacy OpenMind backend but there is a new public web frontend based on the Drupal CMS that is fed by an XML export from the legacy backend. The XML data is also fed into a Neo4J graph database for additional queries and visualisations. This is the architecture for the beta launch in September 2018 and the public launch end of November 2018. In the second phase the data model will be migrated to the CIDOC-CRM CIDOC-CRM: reference ontology using the FRBRoo FRBRoo: model and other extensions. All data is converted to RDF following the new data model and a frontend based on the ResearchSpace ResearchSpace: software and a triple store backend is created for data entry and specialized queries and visualisations. This process is currently under way. The new ISMI website The new public website presents data on 650 persons, 2300 texts, 6900 witnesses and related objects, representing authors from before 1350CE. The website will be public starting end of November 2018. Additional data publications are in preparation. The new web frontend provides browsable lists of all major object typesas well as a general search and searches for specific object types. All objects on the pages are linked which makes it easy to get from a person to all their works and their witnesses as well as to the commentaries on the titles and their supercommentaries. The search has a simple normalization for Arabic and a special normalization for romanized Arabic and is specially tuned to be very forgiving for differences in spelling especially for Arabic names. Feedback for the search and navigation during the beta test phase was very positive. The website also shows currently 104 freely available digitized codices using the IIIF IIIF: image standard and the Diva.js Diva.js: viewer. Most of the codices were scanned by the MPIWG in a cooperation with the Staatsbibliothek Berlin but some exemplars from the Gallica project of the Bibliothéque Nationale de France and the Qatar Digital Library are also present to demonstrate the potential of public IIIF image sources in an area that has been plagued in the past with proprietary data silos and restrictive access conditions making global electronic manuscript databases nearly impossible. We hope to expand the amount of scanned codices in the future. Display of scanned manuscriptThe experimental ISMI Lab section of the site offers access to the Query Builder tool which allows to construct custom queries to the database based on objects, attributes and relations and a full Neo4J graph database console with access to all published data. These additional tools are very powerful but require some technical expertise and familiarity with the ISMI data model. There is some documentation but this section is more of an experimental offer to also get in contact with interested scholars in the hope that interesting queries and research questions can be exchanged and new, easier to use, tools can be developed in the future. Experimental Neo4J console showing partial graph of commentary relations. A never ending project? The history of the project in the last ten years has shown the difficulties of developing and maintaining a project of this complexity – organisationally, in terms of hardware, software, and scholarly support. We think this project shows the potential for a unifying manuscript database that is not limited to singular collections and presents the continually updated and expanded current knowledge of scholars in the field. We hope that scholars in the future will not have to figure out errors in decades-old printed catalogues individually again and again but that they can participate in a common database and share and enhance their individual findings. The collaborative phase of the ISMI database is only beginning and we would like to start the discussion now. We think we have laid the technical foundations to make the database maintainable and adaptable and the data shareable and linkable but the long term value of a shared resource lies in its users and its contributors. "
	},
	{
		"id": 69,
		"title": "Her Hands On Her Hips: Body Language In Children’s Literature",
		"authors": [
			"Cermakova, Anna",
			"Mahlberg, Michaela"
		],
		"body": " This poster will present our digital reading approach to body language in fiction. We use the web application CLiC – Corpus Linguistics in Contextand a range of corpus linguistic methods to identify gendered patterns of body language in literature for children. We are particularly interested in how the presentation of body language has changed over time and how the changes we identify reflect socially structured and gendered patterns of behaviour. We therefore work with two different data sets: the ChiLit corpus - a corpus of 19th century childrens literatureand a corpus of contemporary childrens literature - i.e. texts published after 2000 drawn from the Oxford Childrens Corpus. We use 5-gramsto identify candidates for body language clusters. We specifically select clusters that contain nouns referring to body parts, bodily functionsor pieces and parts of clothing. Focusing on 43 clusters we identify trends of diachronic development in body language descriptions. We both find clusters that seem to no longer play a significant role in contemporary dataas well as emerging clusters, such as got to his feet and or a hand on his shoulder. We also compare differences between body language clusters that tend to occur with male or female characters. For this comparison, we draw on the occurrence of personal and possessive pronounsthat serve as an approximation to calculate the overall population of male and female characters in the texts under investigation. In line with other relevant literature confirming gender imbalance in fiction, we only found one typically female cluster: her hands on her hips. Our findings further show, for instance, that head as a body part seems to be typical of clusters that tend to occur more frequently, or even exclusively, with male characters, e.g. the back of his head and his head in his hands are such male clusters in both corpora. Examples like his head in his hands have corresponding female clusterswith similar frequencies in the 19th century texts, while in contemporary data the male variant is more frequent. Examples of typically male clusters in ChiLit contain the word pocket : his hands in his pockets and put it in his pocket. In the contemporary data, these clusters still do not figure often with female characters, but at least pockets do get mentioned: the cluster out of his/her pocket and is an example that is found for both genders. There are two other typically male clusters: his eyes fixed on theand the emerging cluster the corner of his eye. Other clusters that include the plural form eyes also start off occurring with both male and female characters in ChiLit but then undergo a shift towards maleness. We found one cluster, with tears in his/her eyes, for which a shift from female to more general – less gendered – usage can be identified. Importantly, a detailed analysis reveals how different usages of clusters are linked to changes in social contexts. This poster will not only show gendered patterns of body language, it also demonstrates how corpus linguistic methods contribute to wider concerns in digital humanities. Corpus linguistic methods link literary and linguistic analysis through the computer-assisted study of patterns in texts. The poster contributes to a broader area of research within corpus stylistics, cf. Ruano San Segundosinvestigation into gendered patterns of reporting verbs in Charles Dickens or Čermáková and Mahlbergsstudy of gendered speech in Carrolls Alice in Wonderland. "
	},
	{
		"id": 70,
		"title": "Designing the Database of Indigenous Slavery in the Americas",
		"authors": [
			"Fisher, Linford",
			"Sanford, Heather",
			"Champagne, Ashley",
			"Mylonas, Elli",
			"McCauley, Steven"
		],
		"body": " Scholars estimate that between 2.5 and 5 million Native people were enslaved in the Americas between 1492 and 1900. This is an astonishing number, even compared to the approximately 12.5 million Africans who were brought as slaves from Africa during the same period. Only in the past fifteen years, however, have researchers undertaken a sustained examination of the history of this nearly hidden form of slavery. Many projects already aggregate, catalog and transcribe materials relevant to African and African-descent slavery: The Trans-Atlantic Slave Trade Database; Freedom on the Move; Slave Biographies: The Atlantic Database Network; Burial Database Project of Enslaved Americans; and Slave Societies Digital Archive. These materials may include information about indigenous enslaved people, but do not necessarily allow them to be identified as such, and may not pertain to them as a population. The Database of Indigenous Slavery in the Americasis developing a database to document as many instances as possible of indigenous enslavement in the Americas between 1492 and 1900, consulting records such as runaway slave ads, probate records, records of individual colonies, journals, financial records, ship manifests, correspondence, and church records. Our work details how we have engaged with a variety of complexities in designing a database about enslaved people. Data can be static and limited; people are complex and fluid. DISA is built on a SQL database in which we record mentions of indigenous enslaved people. In designing the database, we had to grapple with a variety of issues - for example, we decided at the outset, since our sources could only be historical documents, that database records would represent instances of documents in which traces of individual people were found. However, we also wanted to ensure that we were documenting the existence of a individual, in as much detail as possible. We resolve this tension by designing our interface so that entering information is a document-centric practice, but searching and exploring the database is a person-centric activity. The demands of a database architecture also reveal the ambiguities in defining the nature of slavery and the boundaries of racial identity. Colonial records use a variety of words for the various unfreedoms that existed. Indians could be chattel slaves, but they could also be enslaved for a shorter period of time, or indentured either by the government or by parents, or they could be assigned to servitude due to having been convicted of a crime or being unable to pay debt. We have to represent the continuum of slave/free as more than a binary state. In cases of intermarriage, colonistsoften elevated African lineage while minimizing Native ancestry. But racial identity is far more complex than this. How should we categorize and representsomeone who the records calls a mulattobut other evidence suggests he or she has Indian parentage? Databases like DISA have to highlight regional and cultural differences as well as allow researchers with different expertise to search productively. In addition to compiling and disseminating information on thousands of enslaved Indian people, our prototype highlights some important questions within digital humanities. Miriam Posner points out that humanities data is a necessary contradiction. Our data are essentially about people—and people are not a static data point. When a 1706 Boston newspaper lists a runaway Indian servant girl named Hannah, it reveals a partial story, told through a slave owners words. Hannah may appear otherwise in later documents, or may not be documented beyond this one description. This poster demonstrates the approaches weve adopted in a database-driven project to structure and use data so that we can understand more about indigenous slavery, despite the sparsity of information. "
	},
	{
		"id": 71,
		"title": "Seen by Machine: Computational Spectatorship in the BBC television archive",
		"authors": [
			"Chavez Heras, Daniel Alberto",
			"Blanke, Tobias",
			"Cowlishaw, Tim",
			"Fiala, Jakub",
			"Herranz Donnan, Amaya",
			"Man, David"
		],
		"body": " Summary This paper is a report and critical account of research undertaken in the project Made by Machine: When AI Met the Archive. In this project we used three computational approaches to analyse and automatically browse the BBC television archive; we then proposed a novel way of combining these approaches through machine learning by fitting their outputs to a recurrent neural network as a time-series. Through these methods, we were able to generate new sequences out of a dataset of short clips of archive footage. These sequences were then edited and packaged into a television programme broadcast on BBC Four in September 2018. This is, to our knowledge, the first time machine learning has been used in this way to recast archive footage into original prime-time content for television. In this paper, we first frame Made by Machine as a project inscribed in the political economy of datafied cultural production. We then describe the technological approaches we used to traverse archive space, learn and extract features from video, and model their relations through time. And finally, we introduce the idea of computational spectatorship as a concept to analyse the objects and practices of automated seeing/editing of moving imagery through machine learning. The television archive as cultural big data In public discourse we are constantly reminded that how we learn, what we enjoy, and the ways we conceive and exercise power, are all consistently mediated by imagery. In various academic disciplines this pre-eminence of imagery in all spheres of human activity has been referred to as visual culture or the visual turn. It has been further recognised that digital and networked technologies have dramatically increased the role imagery plays in society. According to an estimate already 23.2% of files available online in 2003 were imagesand this is before the rise of YouTube and Netflix in the mid-2000s. A more recent report by CISCO predicts that by 2022 video will account for more than 80% of the worlds IP traffic. If this is the case, by then we will be collectively watching through Video-on-Demandsystems the equivalent to 10 billion DVDs per month. Arguably, out of all the cognitive-cultural production today, moving imagery is the most semantically ubiquitous, the larger in size, and one of the most semiotically complex insofar as it combines visual, linguistic, and aural modes of expression. In their abundance and complexity moving images are in todays networked societies one of the most challenging aspects of contemporary culture, and one that increasingly demands social and academic attention. In this context, while the volume of digital video grows, and its formats continue to diversify, the use of computational techniques to access, analyse, produce and reproduce vast collections of digital moving imagery becomes a pressing issue for organisations and archives that deal with audio-visual productionOne of such collections of is the BBC television archive, which amounts to approximately 700,000 hours of television distributed in about 400,000 programmes in various formats, including items originally stored in film and magnetic tapes that have now been digitised, and new programming which is preserved as born-digital data. A significant portion of this archive is now available through the internal Redux system, which allows on-demand access to high-resolution video stream files through an online interface. In the rapidly changing landscape of digital entertainment industries, where large corporations like Netflix or Amazon are producing original video content and are —in industry parlance— leveraging their own moving image collections as profitable datasets, the BBC is increasingly pushed to recognise the value of their archives as data. In 2017, BBC R&D signed a five-year partnership with eight UK universities to unlock the potential of data in the media by creating a framework to make BBC data available for research. To unlock such potential can be understood in this context as extracting value from this data, but unlike Amazon or Netflix, the BBC is a public corporation funded largely by tax-payers to fulfil public purposes. The British broadcaster therefore needs to grapple with the questions of what types of data and what kinds of value it extracts from its television archives. These questions require the broader intellectual frameworks of the humanities to place Made by Machine into context. Seen by Machine Made by Machine was a project commissioned under a simple premise: to create a TV programme out of archive footage using machine learning. The project was conceived, produced and delivered between May and August 2018, and it aired under the category of documentary on BBC Four in September as part of a two-night experimental programming block called AITV on BBC 4.1. Most of Made by Machine was developed internally by a small group of technologists at BBC R&D, in collaboration with an external researcher from Kings College London. This team designed the computational methods to analyse archive footage and produce new video sequences out of it; the sequences were then packaged and delivered as a documentary by a producer who commissioned a 3D-rendered talking head and a presenter whose commentary bookends the sequences. First, a Support Vector Machinewas trained on existing programme metadata to predict whether or not a given programme was broadcast on BBC Four. Using this, an ad hoc subset of programmes was selected, and then segmented into several thousand clips that ranged from tens of seconds to several minutes using an algorithm developed internally at BBC R&D. These clips were then used as a dataset to be explored and manipulated using four methods: 1. Object detection 2. Subtitle analysis 3. Visual energy 4. Mixedmodel Object detection The Densecap systemwas used to automatically annotate the dataset of clips and then these annotations were used as metadata to generate the sequences by similarity. Densecap is a computer vision system designed to localize and describe salient regions in images in natural language. It is trained on the visual genome dataset, a crowd-sourced set of densely annotated images. Subtitle analysis A pre-trained Word2Vec language modelwas used to create word embeddings of the subtitles of the programmes, and then term frequency–inverse document frequencyto retrieve clips and generate the sequences based on word salience and similarity. Visual energy MPEG-2 video encoding was used to extract a frame-to-frame motion vector of pixel colour difference. This signal was then used to rank every clip in the dataset according to this representation of video dynamics. Mixed mode The features learned/extracted from the previous three sections were then used as inputs to train a long short-term memorymachineto learn a model of the relations between the features across time. The model was later used to generate new similar sequences. Through these four techniques the team aimed to represent digital video in terms of three of its dimensions: depiction, dialogueand motion. One significant contribution in our approach is to try to model moving imagery as a multi-dimensional time-series problem: analysing video streams as a sequence of para-symbolic units whose ordering in time is integral to the creation of meaning. Computational spectatorship Finally, we conceptualise this family of machine-learning approaches as machine-seers. These machine-seers mediate our relationship with large audio-visual collections and provide novel ways of automatic browsing as a form of automated editing. We argue that machine-seers enable a new and specific modality of vision, one which depends heavily on machine-learned representations not only of digital video features but, crucially, of spectatorship. Machine-seers are built on top of existing computer vision and other machine learning techniques, but we conceive them as a second generation of systems designed to learn and replicate complex viewing practices in and across media; they activate in audiences new visual dispositions and capacities, and as such they enable particular ways of seeing. One significant advantage of framing a project like Made by Machine in this way is that theories of spectatorship enable us to think of the imagery produced through machine learning systems as images of the systems themselves; their aesthetics include the technological and social conditions for their existence. We therefore argue that Made by Machine can be understood as one early instance of a new aesthetic modality: the datamatic. Unlike the cinematic or the televisual before it, the datamatic is first and foremost a form of computational spectatorship: more than novel ways of creating imagery, machine-seers afford us with novel ways of enjoying imagery; they fetishise calculation and turn the datafication of society into its own form of spectacle. Through this spectaculum ex computatio, datamatic watching allows us, potentially, to enjoy sequencing without continuity, narrative without authorship and, ultimately, presence without subject. "
	},
	{
		"id": 72,
		"title": "A Scientific Network Serving The Uses Of 3D For The Digital Humanities3D consortium for Humanities",
		"authors": [
			"Chayani, Mehdi",
			"Laroche, Florent",
			"Granier, Xavier"
		],
		"body": " The 3D Consortium of the TGIR Huma-Num has been created based on the observation that there are many initiatives around 3D for the Digital Humanities without real coordination between them. The proliferation of initiatives makes the task difficult and only a consortium-type organization can bring together forces in order to define standardized solutions. The difficulty is increased by the fact that we are dealing with multiple domains, combining science and technology with the humanities. The aim of the consortium is to facilitate discussions by putting together a maximum of research groups that integrate the use of 3D digital data in their scientific practice, to develops tools for acquisition, visualization, interpretation and preservation of data for the Humanities. The consortium is therefore willing to develop synergies between national Humanities and 3D disciplines in order to:share experiences;edit recommendations Abergel, V. Benistant, Bergerot, L Cassen, S. and al.. Livre blanc du Consortium 3D SHS . France, HAL in terms of methodology, standards and formats;identify the specific needs of Digital Humanities and link them to the rapid evolution of 3D technologies;develop common tools and platforms;bring French experiences into international networks;disseminate all this information as widely as possible, among academics, industry and administrations. "
	},
	{
		"id": 73,
		"title": "Chinese Dunhuang Mural Vocabulary Construction Based on Human-Machine Cooperation",
		"authors": [
			"Wang, Xiaoguang",
			"Cheng, Hanghang",
			"Li, Huinan",
			"Tan, Xu",
			"Duan, Qingyu"
		],
		"body": " 1.Introduction Dunhuang grottoes is an internationally recognized cultural heritage which locates on the ancient Silk Road, in Gansu province, China. The well-known Mogao caves there, which meets all 6 evaluation criteria for the World Cultural Heritage, has 492 caves and the total size of the murals reaches more than 45,000 square meters. It is a gallery with 1,600 years of art history and is regarded as a grand Library on the wall. The numerous huge number of Dunhuang murals contain exceedingly rich content, making this site a significant academic treasure with an abundance of vivid materials depicting various aspects of medieval politics, economics, cultures, arts, religions, ethnic relations, and daily dress in Western China. However, the absence of Dunhuang mural vocabulary not only hinders the reasonable and efficient organization of relevant Dunhuang mural resources, but also limits the Dunhuang mural studies and its value exploration. We applied machine learning to the quick construction of Chinese vocabulary of Dunhuang murals with the hope to advance the retrieval efficiency and accuracy of Dunhuang mural information, and facilitate the process of mural annotation standardization. Besides, Dunhuang mural vocabulary could also further the development of digital humanities applications such as deep semantic annotation, knowledge extraction, and multi-modal data modeling. Figure 1. Human-machine Cooperation Mechanism of Vocabulary Construction 2.Top-down Process: Structure DesignPrimary word bank construction. The construction of vocabulary began with the entry collection from Dunhuang-mural-related Chinese dictionaries, from which 7,121 entries were obtained for the primary word bank.Structure development. The resulting entries were categorized referring to existing vocabularies in humanities, among which AATpresented a rather satisfying results in regards to its word categorizing accuracy. Therefore, the primary vocabulary structure was developed drawing on the category structure of AAT. Dunhuang mural tends to be very inclusive as the outgrowth of Eastern-Western cultural and art communications. The way we referred to AATs idea of facet plays a positive role in the realization of applying our vocabulary to tangible and intangible cultures that vary in time and location, which in turn would benefit the versatility and inclusiveness of the vocabulary.Structure adjustment and verification. Modifications had been carried out after the discussion with multiple experts from Dunhuang Academy China who study on Dunhuang murals, such as historians, artists, religious researchers, engineers for mural preservation etc., and then the primary vocabulary structure was verified. Current vocabulary consists of 5 first categories, 25 second categories and 103 facets, and presents a 8-level hierarchical structure. Figure 2. Dunhuang Mural Vocabulary Top-level Structure 3.Bottom-up Process: Vocabulary OptimizationLiterature collection. Through web crawling, we collected the fundamental dictionary Dunhuang Studies Dictionary and over 700 papers from two Chinese top-level Dunhuang journals.The corpus building. This step was carried out through the combination of OCR and manual checking to build the corpus for machine learning.Candidate words finding. Viterbi algorithm, a HMM-model-based algorithm commonly used in Chinese segmentation, was adopted with new word detection mechanism to perform words segmentation. The segmentation of Dunhuang Studies Dictionary resulted in 30,854 new words, which were classified into 3 categories, the candidate words, stop words and error words. According to our classification, the segmentation of Dunhuang Studies Dictionary proved an accuracy of 72.13%. The error rate was a bit high in that we classified numerals as error words, and also the literature contained ancient Chinese that was significantly different in grammar, semantics and pragmatics from modern Chinese. To improve this situation, we would continue the optimization of algorithm for better accuracy. The resulting candidate words and stop words had all been stored into the word bank to assist the new word detection and accuracy improvement. The segmentation of papers resulted in 122,991 new words which are still in the process of classification.Structure adjustment and content expansion. New candidate words are currently being added into the vocabulary by indexers, and the structure would be redeveloped if it isnt reasonable enough to fit new words. New words and structures would be audited by experts on a regular basis. The optimization could be iterative, as more resources would be processed to enrich the machine learning corpus, to further detect Dunhuang mural related words, and finally to enhance our vocabulary. The vocabulary that has been examined and confirmed by experts is being transferred into vocabulary management system which supports image format annotation, API call and vocabulary publishing in multiple formats such as SKOS, XML and JSON. Table 1. Statistical Results 4.About Future As for future work, it would be interesting to:process more literature to expand the vocabulary, and continue the improvement of word detection algorithm to increase the accuracy of segmentation, so that it could be better applied to the ancient Chinese and art expressions;explore the method that could realize the automatic creation of new entries in the vocabulary; andwork on the vocabulary translation through both machine translation and manual checking, and publish multi-language versions on LOD after the semantic processing. "
	},
	{
		"id": 74,
		"title": "Enumerating Gendered Bodies In Two Centuries of English-Language Fiction",
		"authors": [
			"Cheng, Jonathan"
		],
		"body": " Introduction Recent computational work has analyzed the significance of gender in characterization, investigating the extent that character descriptions are sorted along a feminine-masculine axis. Matthew Jockers and Gabi Kirilloff, for instance, tabulate pronoun-verb bigrams, exploring the connection between characters actions and their gendered representation in nineteenth-century novels. They show evidence of a stable relationship between gendered characters and the verbs they perform. Ted Underwood, David Bamman, and Sabrina Lee chart a broader range of character descriptions from the past two centuries, measuring the difference between the words describing fictional men and those describing fictional women. They demonstrate that the implicit differences between gendered characters becomes less and less clear as we move towards the twenty-first century. So while the former study argues that characters actions reveal genders steady prominence, the latter research suggests that those overarching gender divisions might actually be diminishing. But these seemingly disparate arguments should not be taken as contradictory. Rather, these varying conclusions should reinforce a more complicated sense of how some forms of gender differentiation…are declining while other forms…are on the rise. To further explore the varying degrees and modes of gender differentiation, I employ quantitative methods to investigate genders prominence in the configuration of characters bodies. This research contributes to that ongoing research, analyzing characters physical depiction throughout a collection of around 15,000 English-language novels. By producing a model of gender based solely on characters physical features, I explore the extent that literary embodiment is defined along a feminine-masculine axis. And I pursue two central claims that complicate existing models of character and gender. The first is that bodily description becomes an increasingly prominent aspect of characterization. In fact, an increasing proportion of character description is devoted to detailing the anatomical features of both fictional men and women. Secondly, those characteristics are increasingly deployed along gendered lines. As we move towards the twenty-first century, men and women are more and more embodied using different words. Even seemingly innocuous attributes such as blue and red function as consistent signs of gender. Those two patterns form a suggestive parallel: as the body became an expanding aspect of characterization, that dimension was increasingly organized along gender stereotypes. Methods and Tools In order to gather characters physical descriptions, I needed a way to separate characters from each other and tabulate the words embodying them. Bamman et al.s BookNLP pipeline has worked well for many similar problems, so I modified it using spaCy libraries. First, it uses coreference resolution to identify character names and cluster them with any synonymous markers in each text. The name Scout Finch, for instance, gets clustered with Scout and any associated pronouns, treating each of those markers as referring to a single character. Then it uses dependency parsing to tabulate a wide range of words connected to each character. By default, BookNLP extracts actions characters perform, actions that theyre the object of, adjectives modifying them, and nouns they govern. Taken altogether, we get several words used to describe fictional people. For my purposes, however, additional words are extracted to capture each characters physical description. Whenever a characters body part is mentioned, his hands for instance, I also gathered the verbs and adjectives modifying their bodily features, such as his hands grasped, took her wrist, and her blue eyes. As a result, for each novel in the corpus, we get a frequency table of words used to describe men and women. We can then subset that table to see which words pertain to the description of characters bodies. In effect, this process tabulates the same characterizing words as BookNLP, but it additionally procures and counts the words attributed to their physical features. Similar to previous research on genders significance in characterization, my method comes with a few methodological challenges. In order to separate characters from each other and assign them gender identities, spaCy will identify proper names, and I use Lincoln Mullens Gender package to label those names with a grammatical gender. Mullens package uses U.S and North Atlantic census data to accurately predict gender of first names, accounting for shifts in time and geographic location. The problem with using proper names to identify character, however, is that characters referred to by generic nouns are excluded, such as the baker. This pipeline attempts to account for this by including characters signaled by stereotypically gendered nouns, such as the queen or the father, but this does not comprehensively account for majority of generic nouns used to produce characters. Moreover, this study also does not provide any robust solutions to the first-person narrator problem. The pronoun I does not consistently connote a particular gender identity, so their bodily configurations are excluded from this essay. In effect, there are certain kinds of characters whose physical features will not be counted in this study. Analysis Taken altogether, the benefit of this approach is that it accurately assigns gender to named entities and consistently extracts their anatomical features, allowing us to explore the gendered distribution of bodily language. By examining how gender impacted this particular aspect of representation, we can then ask questions about forms of gender contingent upon specific registers of characterization. For example, proportionally speaking, how much space did authors allocate to the physical description of men and women? So to what extent is characterization composed of bodily description? Lets start by simply taking the number of words that physically describe female characters and divide that by the number of all words describing those characters. Then we perform that calculation for each year. In effect, were just plotting the proportion of words that describe womens bodily features out of all the words characterizing fictive women. The same calculation is done for the fictional men and we compare the proportions. When we perform this calculation for each year, two clear long-term patterns emerge. First, body language becomes a growing aspect of character as we get closer to the twenty-first century. For both female and male characters, more and more words describe bodily features and gestures. Second, physical description consistently tends to be a larger proportion of characterizing women than men. In fact, while womens bodies are regularly described more than mens, this gap gets wider the further we move back into the nineteenth century. So, on the one hand, this picture reflects a well-known story: the body becomes a growing aspect of producing characters. These two slopes provide further evidence of Heusers and Le-Khacs claim that the body becomes steadily more important in fiction, showing that it was specifically important to describing characters across the past two centuries. This isnt to say that the body was only becoming important during the twentieth century. Theres a lot of evidence to the contrary. Rather, the interest in characters physical features, appearances, and actions seems to continually grow over the past two centuries. On the other hand, there is another important trend, characterizing women involves a greater proportion of bodily description than characterizing men. In fact, as we move from the 1850s to the 2000s, that gap remains jarringly stable. The pattern remains intact even when physical characteristics were becoming more prominent for all characters. This suggests that the body has historically played a larger role in representing women. Thats an important facet of literary history, because it underscores the extent that characters physical descriptions are imbricated in gender discourses. Feminist scholars have, of course, already captured important parts of this story. Butlers argument about gendered bodies, for instance, hinges upon her claim that there are cultural associations of mind with masculinity and body with femininity. Figure 2 doesnt completely verify this alignment of women with the body. Rather, it is congruent with the latter part of Butlers claim, showing that the representation of women tends to rely more heavily on bodily language. But to what extent were fictive men and women embodied in different ways? In order explore to that question, binary classification methods have proven effective at modelling the weight of ideological categories. At its core, this method tests to see how well individual elements can be sorted into two related categories. In our case, we want to test whether bodily nouns, verbs, adjectives, etc. are consistently attributed to either fictive men or women. This means, first, taking multiple random samples of bodily words from each decade. Then each descriptor is labelled according to a characters grammatical gender. By showing a model a large number of these labelled words, we train it to develop a stereotypical sense of what attributes constitute a stereotypically feminine or masculine body. Finally, we instruct the model to use its sense of gendered bodies to make gender predictions about characters it hasnt seen yet. If, for instance, physical characteristics are predominantly distributed along gendered lines, this will allow the model to consistently make accurate gender predictions. On the other hand, if body language and gender are generally unrelated, then the model will be less capable of accurately inferring gender from that language. If we train a classifier to see how well it can predict character gender based on physical characteristics, as a proxy for the strength of gender stereotypes, we see two overarching patterns : First, up until the 1970s, the model gradually has an easier time inferring the gender of characters from their physical characteristics. During that period, the models percentage of correct gender predictions rises from about 76% to about 83%. What this suggests is that words describing the body are becoming increasingly bifurcated along normative gender lines. Second, after the 1970s, however, characters physical features appear less and less distributed along a feminine-masculine axis. The percentage of correct predictions drops back down to roughly 77% as we reach the 2000s. By contrast, this seems to indicate that the association between body language and gender is changing, and the bodily differences between fictive men and women has recently diminished. More quantitative evidence will, of course, be needed to feel confident about these figures, but this approach takes a few steps in order to test the strength of this pattern. This figure was produced by running fifteen different models of physical description within each decade. Each of those models is produced using the physical descriptions within 350 randomly sampled novels, selecting 450 characters at a time, balancing the sample to contain an equal number of men and womens features, classifying them using the top 330 most commonly occurring words. This winnowing strategy comes at the cost of ignoring less frequent physical descriptions. For example, the adjective emerald is sparingly used to describe eye-color, so it is often excluded from each model. These sparsely deployed features can certainly be significant signs of gender, but the benefit of this approach is that it analyzes genders prominence within pervasively deployed physical features, such as having a nose or being attributed with brown hair. "
	},
	{
		"id": 75,
		"title": "Maps Re-imagined: Digital, Informational, and Perceptional Experimentations in Progress",
		"authors": [
			"Chuang, Tyng-Ruey",
			"Hsu, Chih-Chuan",
			"Syu, Huang-Sin"
		],
		"body": " A map can be thought as the fixture of certain places in a time to an artifact of indefinite longevity. The fixture was completed at the moment when the artifact had been produced. The places depicted on the map, however, can always be re-interpreted. We are interested in technical arrangements about historical maps upon which new possibilities can be experimented. We report on our experience in re-basing and rendering of early 20th Century land survey maps upon the OpenStreetMap technical infrastructure. Successive layers of digitalization and transformation are performed so as to bring out new representations from the old maps. Populated places, road networks, administrative centers and boundaries, water systems and diverse landscaping areas – surveyed and mapped long ago on paper – become digital objects waiting to be examined and navigated online. These places, connecting us to the past as well as to the present, can now be perceived anew. Old Maps, New Tech Maps are rich sources of geospatial information. Old maps reveal how people saw the places in their world. Researchers take hints from historical maps to help them connect the places of today to those of the past. Change in place names, the vanished old trails and the emergence of new roads, or the shifting boundaries of settlements, all these details could well be observed when land surveys from different periods are compared for a particular region. There are stories to be told about how these differences come to exist. We distinguish three kinds of digitalization for historical maps. One is built on top of the other in stage. In the first stage, paper maps are scanned into digital images. The change from paper media to digital media makes the maps more accessible and open for other use. In the following stage, geospatial features in the images are identified and extracted into programmable objects. The shift from visual features to digital objects facilitates transformative and analytic use of the information from the maps. In the last stage, the maps, now exist as collections of digital objects, are reprogrammed for new functions and novel cognitions. The three stages can be short-termed as the digital, the informational, and the perceptionalstages. In this report we focus on experimentations in the perceptional stage. In particular, we map-out and re-style early 20th Century land surveys of Taiwan on top of the OpenStreetMap infrastructure. Work in the digital and informational stages, that is, image scanning and feature extraction, is performedbefore the work in this perceptional experimentation can start. Reuse the OpenStreetMap infrastructure has many benefits. The software is open source, so it can be freely modified for experimentations. The modifications to the software are also free to redistribute, so the experimentations and the results can be reproduced by others. The development of OpenStreetMap is by a grassroots community effort where innovations occur quite frequently. In this experiment we use tools from others, and the tools we have developed are also free for others to use. The data format used in OpenStreetMap is quite straightforwardand there are converters to and from other data formats. The styling of maps on OpenStreetMap is also customizable. Indeed we rely on this ability in OpenStreetMap in order to render datasets from old maps into new styles. Taiwan Baotu: Maps of Early 20th Century Land Survey The Taiwan Baotu was a collection of topographic maps published in 1906. It is the output of an island-wide land survey, and the collection includes in total 457 maps covering a major part of Formosa and the Pescadores Islands. The maps also incorporated many other types of geographic information. The maps illustrate administration areas with their detailed boundaries. The maps contain a wealth of place names in use in Taiwan at that time. It includes details about land use, transportation, as well as landmark and landscaping information about Taiwan in the early 1900s. An image from the Taiwan Baotu collection is shown in Figure 1. Figure 1. Map on the area around SinhuaTownshipRe-imagining Taiwan Baotu in OpenStreetMap Once the feature datasets have been extracted from the Taiwan Baotu, all kinds of visual styling can be applied to them in the OpenStreetMap to generate new maps. An identical dataset can be rendered into different maps of the same area. We present below a sampling of the styles available in our current implementation. Please note that we merge in all datasets extracted from the individual maps from the Taiwan Baotu collection. In our OpenStreetMap realization of the Taiwan Baotu, there are no longer 457 maps, but one dataset for the entire collection. Further, the map can now be zoomed in and out in different scales. For illustrative purposes, in the following we use the area roughly corresponding to the map in Figure 1. Figure 2 shows a new rendering of the area, following mostly the original Taiwan Baotu map style. However, new markers are used to re-style certain keys in the original map. For example, the Chinese characters for treesare now used to replace the symbols for trees. The markers are adaptive to different zoom levels which result in nice visualization. In Figure 3 , the dataset is rendered to emphasize different landscaping areas and their boundaries. Figure 4 is the rendering of the Baotu dataset with the default OpenStreetMap style to give the old map a modern look. Figure 5 is the current OpenStreetMap map of the area. Comparing details in Figure 4 to Figure 5 using the same mapping style help us better perceive changes to the landscape of the area. Figure 6 and Figure 7 are zoomed out views of Figure 4 and Figure 5 respectively. We experiment with other styles in rendering the Taiwan Baotu datasets. Figure 8 is the rendering of the Tainan and Anping area, using the Ink style. Figure 9 is the area rendered in the Green style and Figure 10, just for fun, is the area in the Ukiyo style. The rendering of Taiwan Baotu datasets into maps of different styles can be experimented online. Figure 2. New rendering of the area around Sinhua, original Baotu style mostly Figure 3. New rendering of the area around Sinhua, matching texture for landscaping Figure 4. New rendering of the area around Sinhua, the modern OpenStreetMap look Figure 5. The current OpenStreetMap map of the Sinhua areaFigure 6. The Sinhua area in Taiwan Baotu, in modern OpenStreetMap look, zoomed out to a different scale Figure 7. The current OpenStreetMap map of the Sinhua area, zoomed outFigure 8. Rendering of the Tainan and Anping area, in the Ink style Figure 9. Rendering of the Tainan and Anping area, in the Green style Figure 10. Rendering of the Tainan and Anping area, in the Ukiyo style Background and Workflow Our transformation of the Taiwan Baotu proceeds in stages. There are technical choices and styling decisions to be made along the re-imagination process. Our experimentations naturally are limited by the functionalities made available by the OpenStreetMap infrastructure. We depends thoroughly on existing technical tools and map datasets as well. The above factors however should not be considered just as constraints. Rather, upon these tools and datasets we are able to quickly test ideas and build systems. They are accessible and reusable, hence greatly help experimentations. We are very grateful. The initial experiment we set out for ourselves is to re-make the Tainan portion of the Taiwan Baotu. Previously we had access to a collection of Tainan region image files digitized from the Taiwan Baotu paper maps. We were provided with several datasets of vectorized features already extracted from the images. The datasets are in the form of ArcGIS shapefiles. The shapefiles were converted into OpenStreetMap datasets with the help of available tools. They were rendered, in the default OpenStreetMap style, and put online by an OpenStreetMap server hosted by us. In the Summer of 2017, we began to re-do the Tainan region with the goals of achieving better data quality and gaining more flexibility in map styling. Wedigitalized the entire Tainan region by vectorizing map features drawn from the Taiwan Baotu online imagery service provided by the Center for GIS, Academia Sinica, Taiwan. The features not only are recorded for their geometriesbut also for their semantics. These features are annotated with the tags used in the OpenStreetMap datasets. OpenStreetMap tags are pre-defined key-value pairs describing specific features of map elements. In a way, we use OpenStreetMap tags to re-model the semantics of Taiwan Baotu map symbols. At the same time, we keep an image catalogue of all the Taiwan Baotu map symbols. We then use the TileMill editor, with the help of the image catalogue above, to define a Taiwan Baotu map style for use by the OpenStreetMap tile server, so that map features extracted from Taiwan Baotu can be rendered online in a way similar to the original paper map style. This is the Ink style shown in Figure 8. We produce two more map styles: a Green style mainly for illustrating different types of land use, and a Ukiyo style mostly for style experimentation and for fun. Map styles produced by TileMill are provided as style configurations to the Mapnik toolkitin OpenStreetMap to render map datasets into map tiles of the prescribed styles. Note that, no additional style configuration is required if datasets are to be rendered with the default OpenStreetMap map style. For easy comparisons, we use OpenLayersto overlap Taiwan Baotu, in various styles and all drawn from the same OpenStreetMap server, with other online maps. This results in an experimental Taiwan Baotumap service. The overall workflow is shown in Figure 11 which is itself derived from the component diagram in the OpenStreetMap wiki. The parts marked in red are new or modified for our experimentations. Figure 11. Workflow diagram, the parts marked in red are new or modified Discussion We have rendered the Taiwan Baotu datasets, which are extracted from maps produced more than one hundred years ago, using the OpenStreetMap infrastructure. The experimentations are both technical and humanistic. We try out to see how difficult the task is and how the maps look like. We are satisfied with the outcome. Our experimentations are humanistic in the sense that we see such transformative use of historical maps beneficial to researchers, teachers, and students in the humanities. Re-imagining the past with accurate datasets rooted in historical maps, we feel, could well generate interesting ideas for more research. There is a wealth of literature on cultural cartography and visual cognition which we have not touched upon. We wish to draw lessons from these research areas and to continue our experimentations. "
	},
	{
		"id": 76,
		"title": "DH for All: Towards an Inclusive, Usable, and Accessible Digital Humanities",
		"authors": [
			"Clark, Jasmine Lelis",
			"Wermer-Colan, Alex",
			"Hample, Jordan"
		],
		"body": " Within the digital humanities, principles of accessibility remain marginal to project development, pedagogy, and research. The body of literature on accessibilityis very limited, with George H. Williams Disability, Universal Design, and the Digital Humanities being the most cited American chapter on the subject. Within emerging technologies for digital methods, accessibility features are often non-existent, at best an afterthought. Yet established and innovative methods of accommodation for computational technology and media are absolutely necessary to take into consideration from the very onset of a project. As digital methods become more widely used, it is vital that familiarity with web accessibility standards, like W3Cs Web Content and Accessibility Guidelines, along with concepts like universal design and multimodality be explicated and propagated to ensure that the wide-array of digitally produced scholarship and pedagogic practices reach users with a diverse range of learning abilities. This workshop will open with an introduction to designing digital projects for a wide range of applications, from research to teaching. We will explore applications of digital technology in academic contexts, before discussing the obstacles and opportunities present in diverse situations. Using a few exemplary case studies of digital projects in major sectors of digital humanities research and pedagogy, we will assess diverging strategies for project design and implementation. During this exercise, while examining these case studies, we will also discuss strategies for setting minimum goals, scaffolding phases of the project, and taking into consideration not only user design, but also project workflow that enables collaborative, active-learning for assistant researchers and students alike. After considering standards for research and pedagogy, we will consider obstacles within different institutions, as well as the role of digital scholarship centers and media services for fostering experimentation with emerging technology. While assessing the obstacles for developing digital practices at scale, we will conclude the first section of our workshop by discussing what principles were not accounted for within our case study activity, with the goal of examining how complex designing projects for research and pedagogy can become when attempting to consider accessibility, inclusion, and usability. Accessibility is often used as a catch all term for multiple concepts. Often, when used, it is referring to increased availability to members of underrepresented groups, or some other form of inclusion, as opposed to the elimination of the barriers that exclude users with disabilities. The second part of this workshop is built around clarifying and differentiating the core concepts of accessibility, inclusion, and usability as defined by W3Cs Web Accessibility Initiative. Attendees will be provided with definitions of these concepts before being given an overview of the structure and documentation underlying WCAG. Universal Design and multimodal systems will be discussed as design frameworks that enable and support these core concepts. We will explore seven criteria that can guide us through the activity of assessing digital projects. These criteria will be presented in the form of a rubric to guide attendees in analyzing for whom projects are designed and whether or not that they are appropriately integrating accessible, inclusive, and usable practices at the foundation. Emphasis will be placed on integration and avoiding costly and inefficient remediation, or legal action, later on. Finally, through the lens of the case studies, we will explore the challenges developers would encounter implementing these concepts, as well as development strategies for overcoming these obstacles in a wide range of media, from web-based resources to immersive technology. Attendees will get a more in-depth look at some WCAG guidelines as we step through the intent, benefits, and examples provided by W3C. We will explore common failures to integrate accessible design in our case studies, while focusing on how to evaluate the success criteria to determine if the guidelines are being followed. There will also be discussions around the reality of available resources, including not only technology but also labor, such as development staff, digital scholarship consultants, and accessibility specialists. Assessing available resources will allow project managers to properly scale the recommendations we make to better suit individual projects. At the end of this workshop attendees should understand the broad range of digital projects in existence within the digital humanities and their obstacles, grasp the difference between accessibility, inclusion, and usability, appreciate the principles of universal design and multimodal system design, and have familiarity with the WCAG documents, guidelines, and available resources. After discussing common failures and success criteria for implementing accessible design in the digital humanities, we will open the floor for questions and discussion around the challenges and strategies for ensuring that participants can bring what they learn during our workshop about accessible, inclusive, and usable digital projects back to their home institutions, integrating these strategies into their curriculum and research protocols. "
	},
	{
		"id": 77,
		"title": "Transatlantic Collaboration in Digital HumanitiesThe Global Georgians",
		"authors": [
			"Cornell, Deborah A",
			"Callaghan, Samantha"
		],
		"body": " Collaboration is fundamental to digital humanities work and DH researchers and practitioners spend significant effort, time and resources on collaborative processes. Additionally collaboration is frequently necessary and actively encouraged by fundersand yet little formal discourse and attention is given to this topic in DH publications and project reports. In an attempt to address this lack of dialogue, this short paper introduces a project that aims to map and document the collaboration of multiple diverse partners, in a large-scale distributed digital humanities project. The Georgian Papers Programmeis a ten-year interdisciplinary, transatlantic project to digitize, conserve, catalogue, transcribe, interpret, and disseminate 65,000 manuscript items from the British Royal Archives and Royal Library relating to the Georgian period and its monarchs, 1714-1837. The corpus contains letters, diaries, account books, inventories, household records and more. The Programme is a partnership between the Royal Collection Trust and Kings College London and is joined by primary United States partners the Omohundro Institute of Early American History & Culture and William & Mary Libraries. It is a collaborative enterprise in almost all facets. This is by design and on occasion, especially with regards to the cataloguing, metadata enhancement and data exchange, by necessity. The whole process is very much a learning curve for all parties concerned and significant teamwork and cooperation is absolutely key. Project & framework The Programme is a distributed project which involves partners in the areas of archivists, librarians, academics/researchers in a variety of disciplines, and research software engineers. Partners are responsible for development of specific areas and products that ultimately are expected to be integrated into the wider Programme ecosystem. Kings Digital Laband William & Mary Libraries, both technical partners, each lead the development and production of core portions of the ecosystem: KDL is producing a digital collaborative workspace and enhancing metadata; W&M is responsible for full-text transcription. KDL and W&M teams work independently but rely on digitized archival materials from the Royal Archives and on the academic and scholarly community that is being built around the Programme. Coordination among the partners, and support from colleagues outside of the Programme on development, standards, data exchange, timelines, and product delivery is a continuous process. The leads for transcription at W&M, and at KDL for metadata and documentation have set about to study and capture the Programmes collaboration activities and efforts. The study began in the first half of 2019, and is scheduled to continue for the life of the Programme. We are using a mixed methods research approach, that includes an online survey and a number of video interviews asking our Programme partners to reflect upon each year of their Programme work. Online survey questions are based partially on the IDEA Partnership Success Rating Scale, with questions specifically about communication tools and digital humanities added. The interview questions are derived from the INKE e-research collaboration study, and tailored to this project. Analysis of the data is conducted using grounded theory methodology. The study focuses on collaboration in two areas: 1) Group to group Exchange amongst digital projects and research groups is commonplace for DH practitioners. Not always intentional, the interaction with a fellow DH group easily becomes lost over the course of a large project, and in a field that often requires interdisciplinary teamwork and expertise, capturing this aspect of collaboration is vital for documenting the evolution of DH work, and building on a record of impact, alongside more traditional records such as reference citations. Using information visualization techniques, we map groups and projects that we consider, are influenced by, and actively engage with Programme collaborative workspace development, metadata enhancement and transcription work. A timeline illustrates the time frames of each collaboration, and the mapped collaborations are categorized based on the level of impact, if any, the group has on the development and processes of our Programme work. Transkribus, a Handwritten Text Recognition tool developed as part of the Recognition and Enrichment of Archival Documents Project, is one example of a DH project that is an essential tool for the Programme and consequently has a large impact in terms of its direct contribution of full-text transcriptions to the collaborative workspace. 2) Human to human The human coordination in DH work, especially on large distributed projects, can be the most stressful and rewarding part of the project. Dialogue on human to human collaborationin DH projects often happens in informal conversations, is reflective and more about the challenges of the work as much as it is directly about what steps to take next. This research also aims to capture and chart the human relationships side of collaboration. Conclusion Our presentation will present the visual mapping of group to group collaboration from the initial two years of the Programme, and report on the human to human reflection work. Through analyzing and mapping collaborative activities in a transatlantic collaboration, we believe this work will contribute significantly to the discussion on the topic of documentation of collaboration in the Digital Humanities. "
	},
	{
		"id": 78,
		"title": "Confronting Complexity of Babel in a Global and Digital Age",
		"authors": [
			"Crane, Gregory Ralph",
			"Jovanovic, Neven",
			"Sklaviadis, Sophia",
			"de Luca, Margherita",
			"Šoštarić, Petra",
			"Foradi, Maryam",
			"Cottrell, Kate",
			"Tauber, James",
			"Shamsian, Farnoosh",
			"Palladino, Chiara"
		],
		"body": " This panel describes work that has been and is being done to address the complexities of working with a historical record that contains far more languages than any individual could study, much less master. Individuals can realistically develop proficiency in no more than a handful of the languages, contemporary and premodern, from the current and surviving human record. DH2019, for example, despite its international community, warmly invites submissions in languages other than English but can only offer a sufficient pool of reviews for papers in English, French, German, Italian, and Spanish. Difficult as it is to support such a multilingual culture of five modern European languages, it is not practical for most researchers to learn, in any serious way, the additional nineteen official languages of the European Union, much less the twenty-two official languages of India, and/or Chinese, Arabic and other languages with tens of millions of speakers. Students of the historical record also need to consider how to think about Classical languages more broadlysuch asClassical Chinese, the Cuneiform languages, the various pre-modern forms of Egyptian, Syriac, Classical Arabic, Old and Middle Persian, or the various pre-modern forms of contemporary European languages -- India alone officially recognizes six classical languages. The papers in this panel review research and development of digital tools to explore new forms of digitally enhanced reading and language learning. How can new digital services allow you to work with languages that you do not know? This paper reports work done on and off over many years and particularly over the past five years to help students work directly with source texts in historical languages of which they have little or no knowledge. The paper also introduces general questions that the remaining papers explore in detail. What can you produce and what can you learn when aligning a translation to a language that you have not studied? This paper looks at a different historical and modern language pair than those in the first paper and provides data on two topics: the quality of data that non-experts can produce and what the non-experts actually learn while producing the data. What are the limits of translation? How well can we actually translate emotions from an ancient language? This paper explores ways that modern readers can get at core meanings for a complex semantic field such as emotions for historical languages. How do you build on initial interactions to develop a systematic understanding of a language? How do you bootstrap study of one language in a new language with few, if any, resources? This paper reports on complementary efforts to bootstrap, and on potential models of collaboration for, the study of the same European classical language in a less commonly spoken European language and in a non-European language. The final paper examines Schlegels Latin translation of the Bhagavad Gita and explores how the digital tools discussed in this panel are being applied to making the relationship between Sanskrit source text and Latin translation visible to fundamentally new audiences. 1. How can you work with a language that you do not know? This paper describes how readers have worked with source texts in a language that they did not know. The experiences in this and similar classes have not only motivated work in the other papers on the panel, while the results presented in the subsequent panel papers are already shaping plans for future classes. The particular context was a course on ancient literature in a non-Roman alphabet via modern language translation. This use case is typical not only of many pedagogical contexts but also of much academic research -- World Literature, for example, regularly focuses on translationsin order to engage with texts in many different languages while many researchers in fields such as the History of Science -- and History as a whole -- must rely upon translations to examine primary sources in a wide range of languages. The digital tools are relatively simple and have not yet incorporated more advanced methods: students had side-by-side source text and translations and the ability to call up dictionary entries for most words in the source text. The experience of undergraduates with relatively simple digital tools and no advanced training in the Humanities provides a useful demonstration of what is possible. We present here initial impressions from assessing the student performance. The course has been taught four times with enrollment ranging from c. 12 to 25. It includes three projects:a comparison of two modern language translations against each other and against the source text in the unknown original language:analysis of the semantics of words in the original language that lay behind terms such as love and holy in the modern language translation;intensive analysis of a single passage in the original, mastering as much of the linguistic, metrical and stylistic content as possible. A number of students found the premise initially daunting, if not implausible, but the vast majority of students discovered that they could indeed execute all three assignments. Students regularly recognized which translation was freer but often made critical assessments that favored the freer translation on literary grounds. The students -- especially during oral presentations -- conveyed both an acute sense of how limited their understanding was as well as a critical -- and sometimes astonished -- recognition of how much they could indeed understand. Likewise, they developed a tangible sense for the fluidity of translation when they saw how the same words were translated and explored cultural semantics on their own. Many, if not most, came away with a fundamentally different view of what translations did and did not offer and a recognition that they could push beyond the surface of the translation and understand the source text in ways that they had not thought possible. This paper presents examples of work from student projects and our assessments of what did and did not work. Subsequent papers will advance the questions and challenges raised here. 2. What can you produce and what can you learn when aligning a translation to a language that you have not studied? This paper reports the results of two experiments investigating a) whether users with no knowledge in Persian are able to align the source text with its translation at word level and b) what the users actually learned about the language in performing the alignments. This effort frames alignment as a Citizen Science project, in that participants, not only contribute data but also develop their own skills. This paper builds upon the results from the course described in paper 1 but applies the alignment task to two different languages. In this experiment, German students used an English translation aligned to a Classical Persian as scaffolding to align the source text with a translation in their native language. The first part of the study measured accuracy of alignments between source text and translation. First, two automatic alignment systems, Giza++ and the Berkeley Aligner, provided a baseline alignment between the classical source text and the German translation. Second, graduate students who were expert in both Persian and German performed the alignment manually. Third, graduate students who were expert German but had no knowledge of the Classical Persian aligned the texts. Both groups of human annotators outperformed the automated systems. To our surprise, participants with no knowledge of Classical Persian produced alignments of the same accuracy as experts in that language. Both groups thus provided data that was useful in itself and as training data for improving automated alignment. The study thus provides data about the potential quality of such contributions by non-experts. The second part of this investigation explored whatof Classical Persian the non-expert contributors learned in creating the alignments. In particular, we compared incidental learning during alignment and vocabulary learning with flashcards. The experiment monitored the learning success of each method by an immediate post-test and two delayed post-tests after two weeks and two months. Evaluating the test results reveal that the typical forgetting curve occurs for the vocabularies learned by digital flashcards, whereas despite the lower level of immediate learning of vocabularies using translation alignment, the recall rate has a minimal decrease after two months. Ultimately, we found that participants remembered words best when they combined explicit practice through flashcards and incidental exposure using translation alignment. The non-expert contributors developed a more general but very concrete sense about the limitations of working with translations of the classical language, especially when they were able to see, with considerable precision, where and how translations deviated from the source texts. In addition to the impact of the translation alignment on the process of learning vocabularies, it helps readers to compare the translation with another translation and with the source text. 3. Ancient emotions: semantic alignment and translatability The variation in conceptions of categories of emotions across cultures and languages undermines both universal and innate theories of emotion concepts. Emotion words, as labels for emotion concepts, communicate culturally held notions about values and goals. Thus, the organization of the language of emotions plays a key role in decoding affective experiences and understanding deeper social structures. The domain of emotion concepts shows poor semantic alignment across languages, demonstrating how languages reflect the organization of the external world and carve up conceptual space. Given the importance of formal characterizations of meaning to theories of emotions, the aim of our project is to evaluate the extent to which different modern languages have access to emotion concepts conveyed by texts in historical languages. Our approach focuses on the analysis of a corpus of translations to assess how the domain of emotion words is linguistically structured and how it behaves across languages. We use two canonical works in a historical language and their translations in eight Indo-European languages [references removed for anonymity but to be replaced if the panel is accepted]. We model cross linguistic semantic alignment via word-embeddings, using skipgramstrained on the multilingual corpus described above. We expect semantic alignment of emotion words to be predicted by a measure of the phylogenetic distance of languages: languages that are historically more closely related will show higher semantic alignment in the domain of emotions compared to phylogenetically more distant languages. Phylogenetic distance between language pairs is estimated using a Bayesian spatial diffusion model. A new aspect in our model is that we use as a second explanatory variable for predicting semantic alignment the frequency of repeating n-grams: in this way we control for repetition of word sequences and balance the poetic structure of the texts. Our model of multilingual semantic alignment of emotion words in the ancient text requires a significant amount of alignment data that can be used by digital dictionaries and textbooks for the historical language. Moreover, the work on fine grained syntactic dependency, and POS annotation that is available in XML for the historical corpus may be generalized semi-automatically to aligned translations. 4. How do you build on initial interactions to develop systematic understanding of a language? What linguistic knowledge is necessary to understand a text in a pre-modern, historical language and what knowledge of a language has a student gained once they understand a text? How can this knowledge then be used in selecting other appropriate texts for the student and in providing the necessary scaffolding to fill the gaps? This paper looks at issues in overall language modeling and the linguistic annotation of texts for this purpose with a particular focus on vocabulary and inflection but with some consideration given to syntactic constructions as well. Typically vocabulary ordering in historical language instruction is driven by frequency, although in many traditional grammar-translation approaches, inflectional class is also a key factor. For example, a particular noun declension might be introduced and then high-frequency vocabulary from that declension taught. As will be shown, this leads to a far from optimal ordering in terms of being able to read real text early on. Instead the paper explores allowing the texts, chunked to appropriate sizes, to algorithmically drive the ordering of teaching vocabulary and inflection. And by tracking what the student has already seen and what they need to see more of, a structured sequence through new passages of text can be developed, guiding them through specific texts that reinforce what they have already seen while introducing new vocabulary and grammar in appropriate increments. We will discuss, using the Greek texts of the New Testament and of Homer, how inflectional morphology, lexical relatedness and the choice of target unit larger than an individual word impact the optimal ordering of the introduction of new vocabulary. Finally we demonstrate an online adaptive reading environment prototype that, backed by both annotation of the text being read and a model of student knowledge, provides assistance in vocabulary, morphology, and syntax while both explicitly assessing knowledge and also implicitly tracking their inquiries when seeking additional help. 5. Bootstrapping the study of an ancient language This paper describes two different approaches to a shared problem. Each speaker seeks to teach Ancient Greek language but each needs to teach that language in a modern language with few resources. One of the modern languages has long and deep ties to the historical language but traditionally depends upon translations of grammars and reference works from the larger academic languages. The second language, Persian, comes from a very different cultural sphere but many of the resources about the history of Iran and Iranian cultural heritage are originally in Greek. There are virtually no resources in for Ancient Greek in Modern Persian-- even the translations that do exist are indirect, being derived from translations of the historical sources and not from the sources themselves. Each of the speakers has exploited digital methods to bootstrap the study of the classical language in their national language and both explore the possibilities for speakers of less well-supported modern languages to develop a more localizable infrastructure for the study of Ancient Greek and other historical languages. The first case study focuses upon teaching the European classical language in a less widely spoken European language. Although the educational system in the European nation makes it possible to learn the classical language from primary school to MA degree, the few textbooks and reference works available in the local language are seriously outdated. Producing and publishing a new one would be a long and complicated process, therefore using available digital tools is the next best thing when it comes to teaching the classical language in this modern language. In the last few years, several courses at the local university included experimental use of morphological annotation, treebanking and text alignment. Neither treebanking nor text alignment can be done properly without a careful lexical and syntactical text analysis, so this has helped identify some of the problems students face, especially when it comes to syntax. The second case study focuses on teaching Ancient Greek to Persian speakers in Iran, a non-Western country with very different cultural traditions. This paper describes bootstrapping the study of Ancient Greek with similar open educational resources and digital libraries with complex metadata to those exploited in the European context. Like its European counterpart, this effort includes comparisons between Greek and Persian grammar to give the learners a clearer understanding and facilitate the learning process. Also, it uses translation alignment of simple sentences to gradually prepare the learners to face the original texts and help them with basics of the syntax. The paper concludes with a discussion of how such originally separate efforts, in very different cultural contexts, can leverage linked open data and common guidelines to serve each national audience more fully, establish intellectual ties between their students and hopefully spur additional study of historical languages in other modern languages. 6. How have users of one classical language engaged with another? Schlegel, Sanskrit, and Latin In 1823 August Wilhelm von Schlegelpublished a Latin translation of Bhagavad Gita, a philosophically important Sanskrit text, part of the epic poem Mahabharata. Schlegel, a German by birth, chose Latin to translate into because he considered it not encumbered by particles, articles, pronouns, and auxiliary verbs, and therefore closer to Sanskrit in structure and in style, and also esthetically. At the same time -- although he does not say so -- the translator chose Latin because it was a language familiar to all Western scholars. Traces of Latin as a European cultural language remain even today: in many European countries it is still taught in schools. Building on that remaining presence of Latin, and intending to demonstrate how one historical language -- structurally, stylistically, culturally different from modern modes of expression -- can serve as a bridge to another, even further removed historical language, a team of undergraduate students and their teachersdeveloped a course around a digital Sanskrit-Latin edition of the Bhagavad Gita. The task also gave us an opportunity to assess the state of digital philology of Sanskrit and Latin: which resources are available, which arereusable, how to connect what we have, how to supply what we lack, do we have to be IT experts to do it? Our digital Bhagavad Gita consists of a Sanskrit version in Devanagari, aligned to a romanized version, which is again aligned to Schlegels Latin version. A canonical reference system has been added. In both Sanskrit and Latin versions words were morphologically annotated and lemmatized, using existing digital resources. Links to existing digital dictionaries were provided as well. Linguistic annotations, together with a concordance, served as the basis for producing vocabulary exercises, presented as flashcards. A standard open-source learning platform was used to connect all resources and to guide and assess students progress. A digital, linguistically annotated, connected, and pedagogically structured Sanskrit-Latin Bhagavad Gita offers students of Latin an opportunity to use what they already know to interact with another historical language and, through it, with another cultural tradition. The dialogue about the two-thousand-year old Hindu and Roman worlds is multilateral - 19th century scholarly culture and Schlegel took part in it while the 21st century digital philology builds on this earlier work as it forges its own views. Additionally, work on the course has shown that many of the resources we needed were already available - but also that, in the field of historical languages, two of the hardest challenges for digital humanities - and two significant obstacles to its widespread cultural and social acceptance - are connecting resources and tools, and making sure that we are allowed to reuse them. "
	},
	{
		"id": 79,
		"title": "New Approaches to Women’s Writing Virtual Research Environment",
		"authors": [
			"Kirkley, Laura",
			"Sousa Garcia, Tiago",
			"Cummings, James",
			"Turner, Mark"
		],
		"body": " The New Approaches to Womens WritingNetwork brings together scholars from across the globe to research women writers transnational collaborations and reception histories from the early modern era to the twentieth century. The aim is not only to recuperate national histories of womens writing but also to establish how feminist ideas were disseminated as texts crossed national and linguistic borders. This short paper seeks to introduce the NEWW network and its pilot virtual research environment as it seeks to develop this further. In part the creation of this VRE is born out of frustration with more traditional forms of the conference paper and article which tend to lend themselves to single-author case studies. These research outputs often point towards broader patterns of transnational networking and influence, but corroborating and interpreting these patterns demands an overview of the significant amount of data now stored in the VRE. We have therefore created a number of outputs in an effort to visualise the reception trajectories of key feminist writers texts as they crossed national borders, often appearing in translation in other countries. The NEWW VRE was created to answer some of these questions, originally as a standalone database, later it was adopted as part of an overarching internally-funded project at Newcastle University called ATNU. This project has initiated a number of digital humanities projects looking at text in various forms, and this was an important early pilot to map the appearance of translations of feminist literature called Mapping translations of feminist literature in Europe 1750-1930. This was initially developed with ATNU using fairly simple methodologies and then later as a full pilot expanded in order to identify the mechanisms and contexts for the transnational development of feminism before the so-called First Wave. The technological implementation has since been significantly expanded through the use of custom maps built on top of a D3 visualisation library. This allowed for the use of vector images detailing the changing borders of Europe to be used instead of the modern borders available via Google Maps. The initial interface was replaced by a full relational database to support more advanced queries and store more metadata, particularly around the evidence sources. The pilot project selected key early modern feminist writers and plotted their reception data on a European map. The data is categorised by reception type and includes: translations; adaptations; reviews and articles; evidence of reading; and texts where demonstrable influence has been established. This data can be visualised either as distinct markers on the map or as a heat map, whereby the densest clusters of reception data appear in a range of colours depending on the amount of data attached to them. Hot spots highlight areas of particular interest and the connections between the different texts. The map also has a timeline which allows the user to travel through time and view the developing reception of a given text or writer. The map is dynamic, changing with the date on the timeline so that the user sees the national borders in Europe shift over the course of the eighteenth and nineteenth centuries. This is important because the significance of particular clusters of reception data may be illuminated by political and cultural connections between nations would not be readily apparent on a modern map. Given the relatively small size of the testing dataset, any conclusions drawn from the distribution of results are naturally skewed. In selecting the test dataset, however, questions about the criteria for inclusion in the complete resource have surfaced: what constitutes feminist writing, how to compensate for spotty data in the sources, and how to weigh the different types of reception to more accurately represent the spread of early feminist ideas -- these aspects will be refined and expanded in the course of the larger project which will follow. In addition to introducing the NEWW network and its pilot project this short paper will discuss a number of issues more directly of interest to a DH2019 audience. These include the problems of mapping with historical datasets: although we wanted to provide digital maps, the dataset is historical so merely plotting these points on a modern Google Map would be obviously misleading. Sourcing historical maps in a useful form in itself is a problem – since the date range covers some of the most turbulent years in European history, it has constant border changes throughout the time period of the data. The provision of historic open-source borders in a useful format proved difficult but eventually a series of over twenty maps of Europe after major border changes were sourced from the Leibniz Institute of European History, provided by Andreas Kunz. Each map contained a number of artefacts not relevant to this project, so each was edited using Adobe Illustrator to resize and remove unneeded sections before saving into SVG. The NEWW project also faced additional difficulties, for example, in the categorisation of evidence types into a hierarchy that members of the network could agree upon. For example, a translation was judged to be a more significant reception text than a brief mention in a letter. Moreover, evidence for the existence of translations comes from a number of different sources. Each source type needs to be ranked against all other types to build up a score that represents how certain the members of the network are that any given translation existed at that time and place. As much of the data is fragmentary or has degrees of uncertainty, there were a number of issues with the visualization of this uncertainty. This short paper will introduce the network, the ATNU pilot project that led to the revamped NEWW VRE, and the resulting website itself. It will look for feedback and seek to open dialogue among DH scholars working on related topics, not only on feminist writers but those encountering similar challenges on the technical development of historical datasets. "
	},
	{
		"id": 80,
		"title": "Deciphering Lyrical Topics in Music and Their Connection to Audio Feature Dimensions Based on a Corpus of Over 100.000 Metal Songs",
		"authors": [
			"Czedik-Eysenberg, Isabella",
			"Wieczorek, Oliver"
		],
		"body": " As audio and text features provide complementary layers of information on songs, a combination of both data types has been shown to improve automatic classification of high-level attributes in music such as genre, mood and emotion. Multi-modal approaches interlinking these features offer insights into possible relations between lyrical and musical information. Therefore, we examine the connection between audio features and the lyrical content of metal music by combining automated extraction of high-level music properties and quantitative text analysis on a large corpus of music from this genre. Sound dimensions like loudness, distortion and particularly hardness/heaviness play an essential role in defining the sound of metal music. Topics typically ascribed to metal lyrics contain sadness, death, freedom, nature, occultism or unpleasant/disgusting objects and are often labeled as brutal, dystopian or satanistic. By combining both audio feature and text analysis, weoffer a comprehensive overview of the lyrical topics present within the metal genre andare able to address whether or not levels of hardness and other music dimensions are associated with the occurrence of brutaltextual topics. Methodically, our research builds on a previous examination, in which ratings were obtained for 212 music stimuli by 40 raters each. Based on this music perception study, prediction models for the automatic extraction of high-level audio feature dimensions like hardness and darkness had been trained via machine learning methods. Now, in our most recent complementary step, we programmed a web crawler to automatically retrieve metal music lyrics from www.darklyrics.com, resulting in a sample of 152.916 song texts. After cleaning procedures, our subsample included 124.288 texts. We applied latent Dirichlet allocationon the remaining subsample to construct a probabilistic topic model. Log-perplexity and log UMass coherence were used as goodness-of-fit measures evaluating topic models ranging from 10 to 100 topics. We then examined the most salient and most typical words for each topic. Additionally, within the topic configuration, we identified a latent dimension where philosophical and brutal topics oppose texts with shallow content. Results of the investigation for relations between high-level audio features and lyrical topics will be presented and the topic models will be displayed to visitors during the poster session for interactive exploration. "
	},
	{
		"id": 81,
		"title": "Linked Data for the Humanities: methods and techniques",
		"authors": [
			"Daga, Enrico",
			"Gangemi, Aldo"
		],
		"body": " Background and Motivation The primary goal of the digital humanities vision is the possibility to develop scholar studies at a large scale, where the sources of investigation can be reused among studies and arguments and methods to be developed and compared in a rigorous way. However, the underlying assumption is that sufficient data are available and that these data are properly described and distributed to the research community. The Linked Open Datamovement started within the Semantic Webresearch community with the objective of building a Web of Data. So far, the impact of Linked Data in the Library and Cultural Heritage domain has been significant and testified by large scale efforts such as the one of Europeana. Related to that, LD has proven to be the framework for Open Science, linking knowledge with FAIR principleselecting the Web URI as the natural method to cite everything. Indeed, Linked Dataseems to match the Digital Humanitiesagenda perfectly. However, at a closer look, the impact of Semantic Web research on the Humanities has been discontinuous. On one hand, digital humanities projects emerged having data as focal point of the research and as enabler of novel approaches to humanistic enquiry. Notable examples are the LED Project, drawing on LOD published by The Open University, and Pelagios, aiming at building a hub of geospatial data about the ancient world. However, the potential for data reuse seems to remain largely unexploited. In many DH projects, data is linked through user interfaces as hypertexts with minimal support for RDF and SPARQL. Too often, the data side of LD is left out as future work. On the other hand, Semantic Web researchers contribute tools to support scholars in dealing with LD. The variety of use cases and endeavours is well testified in the proceedings of the WHiSe workshop, reaching the third edition this year. Workshop on Humanities in the Semantic Web: http://whise.kmi.open.ac.uk/ These span from cultural heritage, historical datasets, biographical data, the ancient world, musicand musicology, on reflecting about methodological aspects. Indeed, Humanities can be a land of opportunities for SW researchers, a space where the Semantic Web vision can be tested by challenging requirements coming from sophisticated, highly specialised domains of enquiry. However, the interaction between the two communities has been occasional and as a result foundational techniques and methods developed by the SW community are still perceived as esoteric by many DH practitioners. In addition, more recent approaches have not been disseminated yet in the DH community and could contribute at enriching the toolkit available for leveraging Linked Data, from supporting the knowledge extraction phase, to effectively build applications on top of SPARQL endpoints. Adding semantics to DH has multiple effects: as an enabling technology, as a vehicle to grow DH data as sharable and interoperable assets, and as an experimental platform to make innovative research in computer science by reverse engineering humanistic methods. A plethora of use cases are emerging that can benefit of practical semantic platforms such as LOD and Knowledge Graphs. A cartography of such cases can be built within the following areas: use or adaptation of computational tools for the collection, display or use of databases, corpora, collections; creation of new exploration and discovery methods in humanities; implementation of software components supporting humanities; creation of new knowledge; creation of new hybrid methods; creative use of hybridisation to generate new works or collections. This classification has been built out of a survey of DH projects from 40 academic centers, and shows the variety of data-oriented problems, approaches, and creative solutions: a happy chaos that can benefit from semantic methods. With the above in mind, we propose a half-day tutorial on LD methods and techniques, having the following objectives: To present the theoretical and technical foundations of Linked Data and introduce basic methods for data production and publishing to students, researchers, and practitioners. To provide a reference collection of reusable tools to boost an effective adoption of LD in DH projects. To showcase a set of innovative methods for extracting and linking data from texts. Contents and target audience The tutorial will be organised in three sessions: 1) Linked Data in a nutshell; 2) Producing and consuming Linked Data; 3) Hybrid methods for working with texts and LD. The content will be tuned to accommodate a wide range of participants, spanning from the student that is curious to hear more about LD to the humanist hacker that looks for a robust toolkit to apply to her research. One output of the tutorial will be an openly accessible and persistent registry of reusable resources for developing linked humanities applications. http://purl.org /ld4dh/ dh 2019. Linked Data in a nutshell This first session offers an overview of the basic notions and technologies behind Linked Data, from the adoption of URIs, the Resource Description Framework, Semantic Web ontologies, and SPARQL, to their use for representing data as a KG. A catalogue of exemplary LOD sources will be used in the hands-on session. Producing and Consuming Linked Data The second session will be dedicated to illustrate a number of case studies from humanities domains such as library science, art, and musicology. The objective of this session is to show how Linked Data works Under the hood and to enable participants to orient themselves when deciding to start a project based on Linked Data. The content includes basic recipes for generating RDF from pre-existing data sources such as Excel/CSV files or Relational Databases, for reusing existing vocabularies and ontology design patterns, for discovering links with pre-existing third-party datasets, and for publishing the result as 5-star Linked Data. We will showcase the standard toolkit for consuming linked data as well as more advanced approaches that make it easier for developers to interact with SPARQL endpoints - e.g. BASIL. Hybrid methods for working with texts and LD The last session is dedicated to hybrid methods for the identification, extraction, and linkage of data from texts. We will showcase methods and tools that leverage Semantic Technologies for discovering entities in texts, and for automatically creating knowledge graphs from texts, and how LD can be exploited for characterizing the content of texts for similarity analysis and content recommendation. Off-the-shelf tools such as DBpedia Spotlight, Tagmeand FREDwill be introduced to showcase the potential of hybrid methods. About the Organisers Enrico Daga Enrico Daga has a PhD in Artificial Intelligence and has carried out research on Web Semantics and Ontology Engineering since 2006, first at the Italian National Research Counciland then at the Knowledge Media Institute of The Open University in the UK, where he leads the OU Linked Data initiative. He has played key roles in R&D projects related to the development of intelligent systems for Ontology Engineeringand Smart Cities. Currently, he is exploring the application of knowledge-based methods to support scholarship in the humanities. A former student of Music and Performing Arts, Enrico is founder and chair of the WHiSe Workshop on Humanities in the Semantic Web . Aldo Gangemi Aldo Gangemi is full professor at University of Bologna, and associate researcher at Italian National Research Council, Rome. He has co-founded the Semantic Technology Labat ISTC-CNR. His research focuses on Semantic Technologies as an integration of methods from Knowledge Engineering, the Semantic Web, Linked Data, Cognitive Science, and Natural Language Processing. He has worked in many different domains, including Cultural Heritage, where he has designed ontologies and linked datafor the Italian Ministry of Cultural Heritage. He has published more than 250 papers in international peer-reviewed journals, conferences and books, and seats as EB member of international journals. He has worked in several EU projects related to LOD such as WonderWeb, Metokis, NeOn, and IKS. "
	},
	{
		"id": 82,
		"title": "(Re)connecting Complex Lexical Data: Updating the Historical Thesaurus of English",
		"authors": [
			"Aitken, Brian",
			"Alexander, Marc",
			"Dallachy, Fraser"
		],
		"body": " The University of Glasgows Historical Thesaurus of Englisharranges all the recorded words in the last millennium of English into almost a quarter of a million concepts. The work of half a century, it is available onlineand a second edition is underway. This edition draws upon editorial work conducted by the Oxford English Dictionaryin its ongoing third edition, and thus a crucial activity in creating the new edition of the Thesaurus is the meshing of the Glasgow database with the separate data held by the OED. This paper describes the processes developed by the HT editorial team to tackle the complex task of linking these datasets, allowing rapid updates to be made to the HT and the OED. Through these means, it illustrates challenges associated with the linking of complex, structured data comprised of a mixture of text and numerical information, discusses methods developed by the team, and evaluates the effectiveness of the different linking techniques. Background The HTs database evolved over forty years of digital humanities work at Glasgow, pre-dating even the concept of a relational database or a primary key. At the time the data was sent to the OED for publication of the first edition, there were no unique IDs yet assigned to pieces of Thesaurus data. As a result, the production of the online HT and the Historical Thesaurus feature of the OED Online involved the respective teams creating separate key-indexed versions of the database. The linking of HT word senses to OED word senses was a painstaking process which at times required case-by-case analysis by OED editorial staff which occasionally resulted in minor alterations to the thesaurus structure or reassignment of word senses to different categories. On rare instances in which a headword has more or fewer senses in the HT than in the OED, the OED editors had to make decisions about how best to combine the relevant data. This work allowed a one-way connection from the original HT to OED data, but not back to the online HT produced at Glasgow. However, while the OEDs data is more up-to-date, in places the HTs data is richer, such as in its 800,000 manually-created complex statements of usage dates. A reciprocal arrangement between the OED and the HT means that the editorial teams now share data, with the result that the OED has access to the classification experience of the HT editors, whereas the HT team receives updates on word senses and their dates of activity as established by the OED team. These data updates form the basis of the Thesaurus second edition, and so in order to bring such updates into the HT database, the editorial team must create a data linkage between the HT and the OED which had not previously been attempted. The Linking Process A multi-stage linking process was developed. The first stage has involved aligning the hierarchies of the HT and OED datasets and verifying this alignment; the second stage consists in finding matches for the categories which appear not to have been successfully aligned during the first stage. A third stage will involve linking lexemes within categories. The initial alignment stage was achieved using category code numbers which exist in both the HT and the OED data. The OED editorial team made adjustments to some areas of the HT hierarchy. These known changes were accounted for by adjusting the HT category numbers accordingly when aligning them to the OED categories. An initial script checked for matches in the category code, part of speech, and text heading; this process confirmed over 200,000 category matches, leaving around 27,000 for which matches either could not be confirmed in this manner or which had no provisional match between the two datasets. The number of confirmed matches between the aligned hierarchies was increased using a suite of techniques allowing for known variation between the HT category and OED category headings. For quality assurance, matches confirmed in this way were required to meet additional criteria, including a minimum number of shared lexemes within the category and matches between elements of lexeme metadata. Following these processes approximately 5,000 categories containing lexemes remained either with unconfirmed matches or entirely without a potential match. Figure 1 Sample view of category lexeme and metadata used in linking and QA processes In the second phase, new potential matches for these 5,000 categories were sought. Methods used included looking for matches with sibling categories at the same tier in the HT hierarchy. Further methods used lexemes which only appeared once in each of the unmatched datasets, as well as the creation of date profiles for categories based on first citation dates for their constituent words. An intractable residue of approximately 1,000 categories were manually matched by project assistants. The linking between the datasets is work in progress and, at the time of abstract preparation, work has concentrated on matching at the category level. The next phase will match lexemes within the now linked categories, accounting for the knowledge that some lexemes may have been moved between categories as the result of editorial work conducted by OED staff in preparing the OEDs 3rd edition. By July 2019 this lexeme matching phase should be complete, and the final paper will also address the methods and challenges involved in this part of the matching process. When both categories and their lexemes are confidently matched, the HT team can begin the important work of using OED3 data to both update the dates for which words can be evidenced as active, and introduce words which have been added to the OED but which are not, as yet, represented in the HT. Accurate linking between the two datasets will allow automation of the update process in future as OED3 updates are periodically released, thus ensuring the continued accuracy and utility of the resource for academic and public use. "
	},
	{
		"id": 83,
		"title": "I’m the One Building the Tool!",
		"authors": [
			"Damerow, Julia",
			"Thiel, Carsten",
			"Vogl, Malte",
			"Druskat, Stephan",
			"Casties, Robert",
			"Czmiel, Alexander"
		],
		"body": " In the modern world of publish or perish, the pressure to present polished results can lead to a focus on outcomes and immediately understandable forms of presentation. Sometimes, this results in an underrepresentation of the technologies used to produce these and an omission of its specific challenges. While awareness for the relevance of the research data used to produce a published article has been growing for many years, its acceptance as scholarly output is still contested. The same holds true for the actual software used to process data and create results. At DH 2017 Montreal, the workshop Building an Infrastructure for Historical Research Tools, organized and/or attended by several of the authors, proved that a need for technical exchange exists. The workshop brought together developers, scholars with a programming or technical background, and generally people involved in the development of tools to support digital humanities research. A result of this workshop was the formation of DHTech, an international grass-roots community of Digital Humanities software engineersthat holds regular virtual workshops for the discussion of technical questions, technologies, and ideas. At the same time, the nationally organized Research Software EngineeringCommunity has been growing worldwide), while at DHd 2018 Cologne, some 100 attendees joined to form the DH-RSE working group, as a way to bring together the RSE and the DH communities at their intersection. DH-RSE is a platform for exchange and communication for German-speaking software developers in the digital humanities. Both groups, DHTech and DH-RSE, aim to support dialogue and collaboration among Digital Humanities tool developers and provide a forum for collaboration as well as to increase the visibility of the people behind Digital Humanities research software and their work. We propose a joint half-day workshop to discuss shared concerns and to explore ideas for closer collaboration among technology-oriented DH researchers. If you ever had a moment of how did they use that technology to do this, what software did they use and can I adapt it, or where is that data from and how did they process it during a conference talk, this is your chance to meet like-minded people! Workshop Format This half-day workshop will start out with short presentations about DHTech and DH-RSE, and related initiatives, describing the formats of the groups, topics they are concerned with, and their experiences gathered since their formations. Afterwards we invite workshop participants to describe their expectations and needs in regards to a DH specific research software community, as well as possibly existing communities and groups that addressthose problems. On that basis we will then discuss how DHTech and DH-RSE could address these needs, where they could join forces, and other relevant questions. Depending on the number of participants this might include breakout sessions that focus on particular issues. The desired outcome of the workshop is a joint white paper, describing topics of concern and relevant steps for solving them. Possible topics for breakout sessions include: ● Software development and academic careers ● Open Reputation Systems ● Collaboration vs. data-protection ● Publication of preliminary results / open source first ● Visibility of the technological side of DH "
	},
	{
		"id": 84,
		"title": "MAuth - Mining Authoritativeness In Art History",
		"authors": [
			"Daquino, Marilena",
			"Daga, Enrico",
			"Tomasi, Francesca"
		],
		"body": " Introduction Nowadays, historians gather art historical data from secondary sources, such as cataloguing records issued by cultural institutions and multipurpose sources. Online catalogues of art historical photo archives may include detailed information about authorship attributions. However, sources may provide contradictory information. For instance, according to the Zeris Foundation the artwork Three Graces is ascribed to Peruzzi Baldassarre. Several bibliographic references support the attribution. A discarded attribution to Bernardino Luinis workshop is also recorded, supported by Christies auction firm. The Berenson library records the same attribution supported by one bibliographic reference. The Frick Art Reference Library records an anonymous artist and the attribution has not been updated since 1952. Connoisseurs, i.e. art historians that ascribe artworks to artists, require plenty of documentation for supporting their statements. However, collecting sources is time-consuming and not all of those are relevant. While photo archives often provide details on discarded attributions, museum and gallery catalogues generally do not, and the motivation supporting the accepted attribution is not always evident to final users, who have to compare several sources in order to assess the most authoritative attribution. Furthermore, publishing curated information is expensive for cultural institutions. Lastly, cataloguing records may be affected by information quality issues, such as being incomplete, not up-to-date, or incorrect. In this paper we argue that we can rely on quantitative methods and Semantic Web technologies to support users and cataloguers tasks, such asto retrieve relevant sources of attributions,to compare sources on the basis of Information Quality metrics, andto support users decision-making process by leveraging a documentary, evidence-based approach. The result is mAuth, a framework for harvesting art historical Linked Open Data and return the ranked list of authorship attributions related to artworks. mAuth supports scholars inquiries such as  what is the most documented artwork attribution?. The proposed methodology and tool make authorships argumentations first-class citizen in catalogues and facilitate metadata quality management. Related work Aggregators of art historical data, such as Europeanaand Pharos, support users in gathering resources. However, available data modelsdo not take into account questionable and potentially contradictory information. In addition, aggregators do not support the assessment of reliability of sources. Despite several Information Quality measures existand can be applied to Linked Open Data as well, no bespoke studies on how to measure authoritativeness in the Arts field exist yet. So far, methods modelling and reasoning on argumentshavent been considered as part of cataloguing processes. In fact, existing metadata quality assessment approaches mainly focus on functional aspects of cataloguing data, and no effective solutions for supporting the decision-making process in assessing reliability of statements are available. Ontologies for the Arts domain Existing vocabulariesand thesaurifor the Cultural heritage naturally cover a number of aspects that are peculiar of the Arts. Since most of the statements in the Arts domain are questionable, recording provenance of information is fundamental. The PROV Ontologyexpresses provenance in terms of agentsand derivation. Nonetheless, supporting users in assessing reliability of questionable information in the Arts field is an open problem. Aspects such as adopted criteria, the date of the attribution, authority of primary sources and information providers may affect the validity of a statement. To fill this gap, the HiCO Ontologywas proposed as an extension of PROV-O to represent the aspects required to assess reliability of a statement. In the following picture is illustrated an overview of the HiCO ontology. The main class is the hico:InterpretationAct, which is linked to pieces of information required to validate a statement, e.g. the creation of an artwork performed by a certain artist. Overview of the HiCO ontology We developed an ontology-based ranking model and a proof-of-concept web application that leverages the HiCO Ontology for recommending the most authoritative authorship attributions. So doing, we aim at evaluating its expressivity in a real scenario. The ranking model of authorship attributions We distinguish two types of authoritativeness. Textual authorityrefers to the extent to which information is useful, good, current, and accurate. Such features can be quantified and measured by means of a number of Information Qualitymetrics. Cognitive authorityrefers to the extent to which a source is deemed trustworthy. The latter aspect is strongly domain-dependent and can be addressed by using third-party opinions or citation indexes. In this study we rank attributions on the basis of textual authority, and we present citation indexes of scholars cited as primary sources to support users evaluation of their credibility. We performed a comparative study on three representative datasets, i.e. the dataset of the Federico Zeri photo archive, the Berenson Library collection called homeless, and the Frick Art Reference Library collection of Italian anonymous paintings. The aim is to identify and validate dimensions characterising art historical data providers hermeneutic approach and obtain a ranking model. The argumentation around attributions is a peculiarity of historical photo archives, that usually record motivations, documentation and annotations. When photo archives document art historians work they provide details on discarded attributions. Museum and gallery catalogues generally do not. Therefore, photo archives are the focus of our case study. In detail, our approach includes the following steps. Review of cataloguing standards for collecting requirements. Extraction of a controlled vocabulary of criteria supporting attributions from three datasets. Rating of criteria based on experts consultancy. Validation of the rating by means of a comparative data analysis over three datasets. Selection of IQ measurestaken fromthat may affect the rating of criteria, namely: reputation, completeness, timeliness, relevance. Development of bespoke metrics that represent scholars impact in the communityand with regard to a certain artist. Such indexes do not affect the final ranking model, instead they are provided as informative source for the end-user. Overview of mAuth framework mAuthis a framework based on a semantic crawler that harvests authorship attributions in the Web of Data. The following picture shows an architecture overview. Overview of the mAuth framework Since the web application is a proof-of-concept, harvested data sources currently include only the three aforementioned datasets, VIAF, Dbpedia, and Wikidata. Data fetched are stored in a triplestore and statistics are produced to include citation indexes. The web application allows users to input the URL of an online record from the Zeri photo archive and retrieve the ranked list of attributions. Results includemotivations,dates of attributions,number of sources in agreement with the same artist, and, eventually,bibliographic references,names of cited scholars, andcitation indexes. The ranking model and the HiCO ontology were evaluated by means of a user study. Twenty domain experts performed searches by means of mAuth and similar services. We recorded their user satisfaction by using a Likert scale. In detail, they were asked to agree/disagree with the sorting of attributions, and to state whether the highest ranked attribution was deemed correct in three scenarios. Results show that the tool was able to emulate domain experts behaviour two out of three times, ranking the most authoritative attribution on top of the list. In one case users were not able to judge the proposed ranking, namely when only contradictory attributions were provided and these were supported by scholars opinions only. The scenario shows that reliable citation indexes would be required when other evidences are not available to support the decision. Conclusion and future work In this paper we presented an ontology for describing questionable information, a ranking model for addressing textual authority of art historical data, and web solutions for supporting historians and cataloguers decision-making process. We address features of authoritativeness that can be measured by means of an evidence-based approach, hence our approach is potentially applicable to similar informationand fields. However, sources that can be deemed authoritativebut do not provide motivations are penalised in the final ranking. Future works include the harvesting of such Linked Data providers, e.g. members of the PHAROS consortium, and the analysis of citation networks in the Arts and Humanities field to address other ways to assess cognitive authority. "
	},
	{
		"id": 85,
		"title": "\"The Mold Thats Branded On M Soul\": A Computational Approach to Racialized Voice in Jean Toomer's \"Kabnis\"",
		"authors": [
			"Dick, Jonathan",
			"Hammond, Adam"
		],
		"body": " Introduction Scholars of the Harlem Renaissance have long struggled with the generic status of Jean Toomers Cane, particularly as a result of the hybrid forms found in the forty-nine-page closing story, Kabnis. In order to narrow the possible field of inquiry, critics have opted to read Cane as an experiment in autobiography, one that either avows or disavows African American identity. What strikes us about such criticism is the way these readings of Kabnis inadvertentlyinsist on indexing the text along binary oppositions of black versus white despite Kabnis itself resisting such easy racial classification: not only is its eponymous character described in mixed racial epithets, but his speech, while occasionally dialectal, toes the line between an African-American Vernacular Englishregister and the register of a standardized English. With the extent of Kabniss racialization therefore dependent on how a reader interprets his features and his voice, literature scholars need to establish new methods through which to analyze this texts phonemical registers. Methodology Computational analysis provides us with one method to measure the reception of race in Toomers work. In particular, literature scholars can turn to the developing field of sound studies, using visualization tools such as SonicVisualizeralongside quantitative software like Gentle and Driftto ascertain how readers embody or avoid doing what scholars have variously called black voice in performances of Kabnis. SonicVisualiser was developed at the University of London and provides musicologists with a flexible apparatus in which to annotate, track, and edit audio recordings. Gentle, a forced aligner, and Drift, a pitch trace exploration tool, are under development by Robert Ochshorn with support from Marit MacArthurs ACLS Digital Innovation Fellowship; taken together, they cultivate nonsemantic data that, when run through a Matlab script, can calculate prosodic measures that reflect the salient features of performative speech. Though neither tool can directly map dialect, what they can do is analyze the nonsemantic aspects of speech that AAVE adopts by virtue of its phonology—aspects of speech such as irregular metrical patterns, higher pitch ranges, and a greater overall tonal expressivity. Applying them to recordings of Kabnis then presents critics with the opportunity to witness first hand not only what a reader might modify when approaching Toomers language, but also how they might modify it. To demonstrate the efficacy of sonic analysis to literary studies, we use as our case study the 2013 audiobook of Cane produced by Dreamscape Media and performed by Sean Crisden. Crisden, a self-identified African American, seems, on first listen, to embody a distinctly racial character. Yet as we progressed through his performance, it became clear that Crisden ultimately avoids the phonemical features distinct to AAVE, resulting in a portrayal of Kabnis that eludes clear raciality. It is difficult to describe the terms of this avoidance without qualitative measures. What prosodic features indicate the affectation of a racially charged voice? What prosodic features indicate the avoidance of a racially charged voice? How can variations in pitch and rhythm be used to advance arguments about identity performance? These ambiguous and deeply difficult questions can begin to be parsed through the literary application of sound studies software. Results and discussion To establish a baseline for our analysis, it is important to register subjective responses to Crisdens performance. When we listened to the Dreamscape audiobook, we recognized that Crisden adopts two distinct registers for its characterization: the first is what might be identified as a Northern voice while the second is a Southern voice. These two voices are distinct on the page, with Toomer representing the former in a standardized English and the latter in an ambiguous vernacular. Yet Sean Crisden does not fully abide by this textual difference, opting, in his performance, to enunciate Southern Kabniss words in a standardized English register, even when certain letters are elided. The distinction he makes between the Northern and Southern voice therefore occurs instead at the level of the nonsemantic. For instance, in our subjective response to the audiobook, we noted Crisdens Southern voice as more expressive than his Northern voice; it seems to have a wider pitch range, very few long pauses, and a conversational cadence which sees him playing more with assonance and internal rhyme. Likewise, his Southern voice exhibits upspeak tendencies, which is a trait many dialectologists associate with vernacular languages. Qualifying these responses computationally, however, makes clear that such subjective claims are only partially true. As shown in Figure 1, although Southern Kabnis has a greater intonation range than Northern Kabnis, his expressivity metrics are quite low. While his average pause length of 0.30 seconds might indicate a conversational approach to diction, his words per minute and rhythmic complexity suggest an attention to formality and enunciation at odds with the varied metrical patterns inherent in AAVE, AAVE being characterized by phoneme cluster reduction: certain sounds, most particularly consonants, are dropped from the ends or beginnings of words, resulting in a vocalization in which sentence components are elided. If Crisden abided by this phoneme cluster reduction rule, then his performance would have a higher rhythmic complexity factor overall, as these elisions would result in an unpredictable cadence. Likewise, his WPM would also be higher, since phoneme cluster reduction necessitates quicker speech. While we might therefore subjectively interpret Crisdens performance of Southern Kabnis as demonstrating the phonological features of AAVE, computational analysis reveals that the opposite is in fact true: Crisden, at the nonsemantic level, avoids the phonology of black English by regulating his WPM and by enunciating his words despite their being written without certain end sounds. We can further locate this de-vernacularization at the metrical level by turning to visualization software like SonicVisualizer. Figure 2 demonstrates a wave-form annotation of stress patterns found in brief excerpts of speech from Northern Kabnis and Southern Kabnis respectively, whereby orange lines indicate unstressed syllables, and red lines indicate stressed syllables. In both cases, Crisden alters the metric pattern inherent in Toomers writing to iambicize it, thereby emphasizing syllabic regularity despite the texts very irregular rhythms. A scansion of the former excerpt as it exists in Cane would, for instance, view bull-neck as a spondaic foot and and a as pyrrhic; Crisden, however, alters this rhythm by reading neck as an unstressed syllable and and as a stressed one, giving way to a cadence that is ordered and controlled. He continues this same metric regulation in the second excerpt by placing a stress on on, which leads, in turn, to a line that is forcefully iambicized despite its initial two trochaic substitutions. As with his previous modulation of WPM and articulatory patterns, the consequence of Crisdens metrical regulation is a sort of phonological standardization, a taming which forces Southern Kabnis to speak in no distinctly vernacular fashion. The result is a negotiation of Kabniss identity that de-vernacularizes the racial potential encoded into his speech. And what are the implications of such a choice? Prosodic analyses of three excerpts of Kabnis through Gentle and Drift: one passage which is distinctly Northern, one that is distinctly Southern, and one in a neutral Narrators voice. Millers script calculates twelve prosodic measures, though only six are salient for our purposes; these six measures are listed in the top row. All numbers are rounded to the nearest thousandth for concisions sake. Excerpt of wave-form visualizations from Kabniss speeches, the former from act 1, the latter from act 5. This excerpt has been annotated through SonicVisualizer to demonstrate Crisdens stress patterns. Conclusion Though it is impossible, of course, to claim with any authority Crisdens intentions, the effect of his de-vernacularization reminds scholars of the value judgements that are placed on languages and on the people who speak them. For there really is little if anything that […] distinguishes racial prejudice and linguistic prejudice, writes sociolinguist Sonja L. Lanehart: The people are not separate from the language. Exporting the sentiment to Kabnis, how individuals choose to interpret Toomers eponymous character—and how we, as scholars, choose to interpret portrayals of this character—therefore depends on how they view the construct of race as it operates not only within the early 20 th-century, but also during the period of interpretation. Perhaps, by rendering Kabnis without certain vernacular traits, Crisden is demonstrating a sort of racial prejudice that manifests, covertly, in his linguistic prejudice. Or perhaps, instead, his de-vernacularization is an attempt at equalization—an attempt, in other words, to reduce stigma by demonstrating that the people society often views as deviant are more similar than might once have been thought. That in our subjective analyses, we saw in the Northern and Southern voice a distinct, dialectal difference further bespeaks a culturally codified perception of racial speech: that African-American Vernacular is socially thought to sound a certain way, compared with standard Englishes, these sounds made clear through Marit et. als prosodic measures. Whatever angle a literary critic chooses to take in their argument, computational analysis provides then with both the necessary quantitative data to analyze text-in-performance, and a means of reflexively considering their own stance on linguistic difference "
	},
	{
		"id": 86,
		"title": "IIIFarm. Teaching Image Interoperability on a Raspberry Pi Network of IIIF-Compliant Image Servers.",
		"authors": [
			"Dillen, Wout",
			"Schäuble, Joshua"
		],
		"body": " This paper will discuss a two-and-a-half-day workshop on IIIF, the International Image Interoperability Framework that the authors designed and tutored as part of a summer school on analyzing and processing images in Digital Humanities. Across GLAM-institutionsand image-based Digital Humanities projects, IIIFs set of shared APIspecifications is quickly becoming a standard for enabling interoperability functionality in digital image repositories around the world. Hosting images on a IIIF-compliant image server makes it much easier to parse and share digital image data, migrate them across technology systems, and provide enhanced access to them for scholars, researchers, and general users. Essentially, IIIF allows for the possibility of opening up image collections for extensive sharing and reuse, and all without losing web-traffic on the host institutions web servers. Although the benefits of using IIIF for research and reuse purposes should not be underestimated, the concept behind IIIF is abstract and its implementation can be technically daunting. As such, the complexity of the technology may lead to some hesitancy on the part of representatives of cultural heritage institutions to make the change for their digital image collections. That is further complicated by the fact that IIIF seminars and workshops usuallydo not have the time for anything but scratching the surface of the technologys possibilities: such workshops try to explain the basic concept behind IIIF, show some of the APIs powerful parameters, and let participants play around with IIIF Image Viewers such as Mirador and UniversalViewer. Since we believe that convincing potential users of the real benefits of IIIF requires more of an engagement on the part of the user, we designed a hands-on workshop in order to immerse our participants in the topic: instead of just using the technology, we would take our time and teach the students how the technology works, and how they could install and configure it themselves. The aims of the tutorial were straightforward: first, we wanted to help the students set up their own IIIF compliant image servers; and then, taking it one step further, we wanted to let them collect and reuse each others images. This would introduce students to the technology behind IIIF and allow them to unlock its full potential in a classroom setting where they had ready access to help from the tutors. In the course of the workshop, we distributed images of random pages of a single document for the students to host and share on their image servers. In the end, we would teach them how to create a manifest that would link the documents distributed pages together, and that they could feed into a IIIF Image Viewer to bring their images together in a nice presentation environment. For the teaching materials, we used the draft manuscript of Mary Shelleys Frankenstein that the Bodleian had recently released on its IIIF-compatible Digital Library. Using the Frankenstein metaphor as a didactic instrument, the class was told they would be assembling their own monster from a series of unconnected parts. To get there, however, we needed to introduce some basic related technologies to our students, particularly because the course was open to students with no prior knowledge of IIIF or command line computing. But before we could do that, we first needed the hardware for the students to put their image servers on. That hardware consisted of a gigabit router, further networking hardware, and 15 monitors, keyboards, and Raspberry Pi 3 mini computers. Thanks to the generous support of DARIAH-BE, we were able to acquire enough of these devices to provide every group of two to three students with a credit card sized mini server for the duration of the course. As inexpensive and low-power computers, Raspberry Pis are especially helpful in areas where people face significant constraints on power, network capacity, finances, etc. Using Raspberry Pis also provided us with a useful segue to introduce the students to minimal computing – a subfield of Digital Humanities that aims to rethink DH work for areas in the world where these factors are not a given, or where we want to make less of an impact on our environment. Moreover, Connecting these Raspberry Pis over a local network to turn them into a mini server farm also offered the students a practical way to learn more about and play with fundamental computing concepts. In particular, the students were exposed to issues in physical computational and networking infrastructures; operating a computer without an interface; and communicating with and controlling other computers. After setting up their Raspberry Pis, linking them together in an offline network, and hacking a solution for connecting their server to the internet as well, the students disconnected them from their monitors and keyboards, and used SSHon their own laptops to configure their image servers remotely over WiFi. Practicing some basic commands that they had learned at the summer school only the day before, they now downloaded, created and edited new files on their Raspberry Pis; wrote a web-page in HTML; installed an Apache web server to make it accessible to the rest of the class; converted their high-resolution images to Pyramid TIFFs; and installed a IIIF-compatible image server to share those images with each other as well. Moreover, they performed these tasks via their own laptops CLI, and turned their Raspberry Pi monitors on as little as possible. When the students achieved these steps, we gave them a short introduction to JSON, which allowed them to read and edit IIIF manifests, and mix together each others images using a Mirador viewer that we had installed on a separate Raspberry Pi on the network, where they could behold the little monsters they had created. Throughout the course, we applied the didactic concept of experiential learning. Every step of the configuration process had to be executed individually by the participants, giving them enough opportunities to make mistakes and to learn from them. This was also true for the instructors: this was the first time we had taught this course, and we were fine-tuning it, incorporating the students problem solutions and fixing bugs as we were teaching it. Since we had never tested building a network with fifteen Raspberry Pis, we were as happy as the students when they succeeded just in time for the end of the workshop. In this paper, we will start with situating the workshop in the context of the summer school. After introducing the setting, concept, setup, and structure of the course in a little more detail, we will present some of the key lessons we learned organizing and teaching this course. To achieve this we will focus on some of the infrastructure, network, hardware, and software issues we encountered, and disclose how we tried to solve or circumvent those problems. In addition, we will report on the feedback we received from our studentswhich they submitted to us anonymously. We will then reflect on how a hands-on and in-depth treatment of a concept as complex and relevant as IIIF can be as rewarding for the teacher as it is for the student. In that spirit, we will end by presenting the tutorial we are currently developing on the basis of this workshop, and which will be available for reuse at the time of the conference. "
	},
	{
		"id": 87,
		"title": "Web Accessibility in Digital Scholarly Editing: Considerations from a Survey on Inclusive Design and Dissemination.",
		"authors": [
			"Dillen, Wout"
		],
		"body": " Introduction Answering the call for this years edition of the annual ADHO conference in Utrecht, the digital scholarly edition seems like a perfect example of those complex models of complex realities that humanists analys[e] with computational methods and communicat[e] to a broader public. While trying to convey the complexity of textual development, transmission, and transmutation over time, an editions editor often faces the challenge of making this wealth of information understandable and accessible to its diverse target audience. Acknowledging that much of this effort depends on how weanswer questions such as what do we make accessible?, how?, and to whom?, my colleagues and I organized a panel at DH2017to explore the layered conceptions of access and accessibility as they relate to the theory and praxis of digital scholarly editing. Access, we argued, in all its iterations, continues to shape the discourse of digital scholarly editing as the field grapples with new models and methods. Therefore, the panel would frame a discussion around a broader definition of the concept in relation to the field of digital textual scholarship, by critically reflecting on its meaning for digital scholarly editions and theorizing how the term relates to issues of accessibility, usability, pedagogy, collaboration, community, and diversity. To gauge the communitys perspectives on these matters in preparation of our panel, we released a qualitative survey on inclusive design and dissemination. Since we were still receiving rich, nuanced data from the community, and because we wanted to use the momentum of the conference to attract even more responses, we decided to leave the survey open for some time after the conference. Of course, this meant that we could not make any firm statements about the survey data at the conference; so instead we used the results we had received so far as a way to open up the discussion, and present the audience with a series of questions rather than answers. Since then, we have closed the survey and analyzed its results, and are now ready to present them to the community. Survey Description The survey was distributed through a series of relevant mailing lists, social media portals, and via personal emails to practitioners in the field in our own networks. In total we received 219 responses, 109 of which completed every required question in the survey – resulting in a completion rate of 49.7%. Given the length of the survey, this was a healthy completion rate. Taking into account that 65respondents expressed their willingness to participate in a follow-up interview, it is clear that the issues raised in the survey are of considerable interest to the community – or, at least, to that portion of the community that we were able to reach with our survey. The survey was structured around a series of themes relating to aspects of access and/or accessibility. After a demographic sectionand a section designed to assess the respondents involvement or role in the development or publication of digital scholarly editions, the survey first focused on Open Access and licensing issues; access to the underlying code and software of the edition; cataloging and dissemination of digital scholarly editions; web accessibility; and inclusivity; before ending with a general question about digital scholarly editions, and an inquiry whether the respondent had any additional comments, or was open to the possibility of a follow-up interview. This paper will zoom in on the responses we received in relation to one of the themes that were broached in the survey: namely, web accessibility. Scope As I suggested above, one of the main challenges the editor of a digital scholarly edition faces when it comes to the presentation of their research is to walk a fine line between complexity and simplicity. On the one hand, the editor will want to present the user with as much relevant information as possible, but on the other hand the editor will want to present this information in such a way that it does not overwhelm or distract the user as they are browsing through the edition. Since this turns interface design into a key aspect of the digital scholarly edition and a necessary tool to convey the editors interpretation to the editions users, we felt the need to examine the concept of access through the lens of web and software development in our survey as well. In this context of interfaces, the term accessibility has a very specific meaning, where it refers to the adoption of strategies that make the web application accessible to all users – including those withvisible disabilities. George H. Williams lamented the fact that although [o]ver the last decades, scholars have developed standards for how best to create, organize, present, and preserve digital information for future generations, the needs of people with disabilities have largely been neglected in this pursuit. And indeed, following Williams, a strong case can be made that while it may not be possible for one edition to cater to all of its potential users, editors should at least try to cater to the broadest possible interpretation of the target audience we have in mind for the edition – which will inevitably include people with disabilities. Especially in the field of digital scholarly editing, As opposed to, for example, digital collections hosted by GLAM-institutions, which are typically much more aware ofweb accessibility issues. discussions regarding different user needs typically refer to those with non-academic backgroundsrather than to users withvisible disabilities. In addition, as two majorpoints of reference in the field, neither Sahlesnor Franziniscatalogues mention accessibility in their respective lists of criteria for digital scholarly editions. This suggests that otherwise widely adopted standards such as @alt texts for links and images, consistent use of header tags, legibility of fonts, attentive use of colors and contrast, etc. are not sufficiently acknowledged or adopted in the field. In the web accessibility section of our survey, we wanted to test this hypothesis, while also gauging the communitys perspective about making web accessibility a prevailing concern for digital scholarly editions. Objectives of the Paper After an introduction to the survey, its methodology, reach, and some of our more general survey results, this paper will zoom in on its web accessibility section. Going over this sections results in more detail, this paper will map our respondents awareness of relevant accessibility guidelines, as well as their position towards implementing them; illustrate what kind of accessibility features they offer; delineate how web accessibility issues are tested, resolved, and incorporated in the editions workflow; and discuss in which cases the survey data suggests a regional divide in the answers we received to these questions. Taking some of the possible biases in the surveys data into account, this paper will then draw its conclusions from our survey results, review their implications for the field of digital textual scholarship, and suggest a way forward. In general, however, we can already say that while the survey suggests an overwhelmingly positive attitude towards making digital scholarly editions web accessible, the community also conversely indicates some marked resistance towards its implementation. This implies that six years after Williams essay, there is still a marked lack of field consensus about how or why to practically implement web accessibility features. That is why this paperalso aims to raise awareness in an attempt to give these issues a more central place in our discussions of digital scholarly editing. "
	},
	{
		"id": 88,
		"title": "How we designed galassia Ariosto",
		"authors": [
			"Di Donato, Francesca",
			"Andreini, Giulio",
			"Pezzini, Serena"
		],
		"body": " In the poster we will present the User Experience work we did for the digital library Galassia Ariosto. Galassia Ariosto is one of the main outputs of the ERC AdG Looking at Words through imagesleaded by Lina Bolzoni of the CTL research lab at Scuola Normale Superiore di Pisa . The CTL team has always been interested in studying the relations between text and images and thats what Galassia Ariosto is about: an in-depth study of the relations between the text of Orlando furiosoand the illustrations sets that were produced to enrich the text. The project dealt with Ariostos Orlando Furioso editions, starting from the 1516 first printed edition which soon became a best seller. In the following years venetian editors wanted to create some more appealing editions and started adding illustrations to the text: those illustrations were engraved in wooden blocks and placed alongside the moveable types to layout the pages. These were a visual description of whats written in the text and were the first books with images ever produced. Soon this became an editorial standard and the illustrations of the Orlando furioso became very complex and detailed, and this type of illustrations were introduced in other chivalric poems like La Gerusalemme Liberata, Orlando Innamorato, Tredici canti del Floridoro and many more. Net7 was involved in the design and implementation of the digital archive where all data and information are stored and made accessible on the web: A back-end system that allows the team to do their research, inserting images, texts, comments, entities and building relations between all those elements, including portions of images and texts. A front-end user interface to make all this work public and accessible to the world. The main challenges of the project were: to make all this complex data and relations easy to browse and understand; to create an attractive UI that involves the user; to create different ways to access the content of the archive targeted to different groups of users. In the project, we adopted a UX based methodology and approach. After the first user testing, the discovery process has been conceived considering 3 main macro-typologies of target users: the domain expert. She knows what the DL contains, and needs to directly access single leafs. She uses the advanced search feature, which filters results helping them through the autocomplete. a skilled scholar, who has not precise knowledge of what the DL contains. She enters the platform from the full index of Works. and the culture enthusiast, who browse the highly specialized content of the archive through stories which connect texts and images through storytelling. The result is a rich platform strongly oriented to different user needs, with different access points. "
	},
	{
		"id": 89,
		"title": "Repopulating Paris: massive extraction of 4 Million addresses from city directories between 1839 and 1922.",
		"authors": [
			"di Lenardo, Isabella",
			"Barman, Raphaël",
			"Descombes, Albane",
			"Kaplan, Frédéric"
		],
		"body": " Introduction In 1839, in Paris, the Maison Didot bought the Bottin company. Sébastien Bottin trained as a statistician was the initiator of a high impact yearly publication, called Almanachs containing the listing of residents, businesses and institutions, arranged geographically, alphabetically and by activity typologies. These regular publications encountered a great success. In 1820, the Parisian Bottin Almanach contained more than 50 000 addresses and until the end of the 20th century the word Bottin was the colloquial term to designate a city directory in France. The publication of the Didot-Bottin continued at an annual rhythm, mapping the evolution of the active population of Paris and other cities in France. Figure 1: Example of the cover page and alphabetical list for the year 1856 The relevance of automatically mining city directories for historical reconstruction has already been argued by several authors. This article reports on the extraction and analysis of the data contained in Didot-Bottin covering the period 1839-1922 for Paris, digitized by the Bibliotheque nationale de France. We process more than 27 500 pages to create a database of 4,2 Million entries linking addresses, person mention and activities. The quality of the document analysis process is assessed diachronically and a conservative strategy was chosen in order to populate the database with only information of high confidence. An initial analysis of the data is presented, reporting on the overall statistics of the distribution of professions in Paris and their evolution during more than 80 years, as well a general overview of the diversity of family names through time. Seven case studies corresponding to different streets are briefly compared, showing how information in city directories capture statistically the dynamics of segmentation of the city into functionality differentiated neighborhoods. Figure 2: Number of pages per city directory year by year. 29 years are missing in the digital collection used for this study. Method The document analysed in this article were digitized by Bibliotheque nationale de France and made available online through the Gallica portal. The dataset corresponds to three different published series which are homogeneous in their structure and aims: Annuaire général du commerce; Annuaire-Didot-Bottin; Annuaire du commerce Didot-Bottin. The documents were associated with an ALTO description containing a structural decomposition of each page into text blocks and lines, associated with a transcription obtained by an Optical Character Recognitionprocess. We designed a parsing process converting each line/entry into a record in a database documenting the name, the activity, the place and when relevant the street number. Only a subset of the entries was successfully parsedand included in the database. Figure 3: Structure of an entry in the directories A general discussion on quality assessment methods of the OCR for the BnF digital collection can be found in​ . In order to assess specifically the OCR quality of the inserted data, 14 pages were randomly picked for the years 1839, 1848, 1856, 1857, 1881, 1907, 1921 and manually controlled. Figure 4: Character error rate, Word error rate and Line error rate for 7 years of the corpus. On well parsed entries, the mean of the character error is of 2.6% with a standard deviation of 0.1%. The error per line is of 21% with a standard deviation of 2.9%. The overall transcription quality tends to decrease as sources are more recent. This is essentially due to three factors:the insertion at the beginning of the 20th century of special symbols used for saving textual space but unparsed by most OCR system,the increasing thickness of the volumes leading to the curvature issue during the scanning process making line detection and word identification more difficultthe use of continuously thinner paper sheets leading to problems of transparency between the verso and the recto of a page. Figure 5: Examples of problems of scanningsymbols,page straighteningtransparency and corresponding OCR The entries of the database for seven cases studies were realigned on the Vasserot cadastre digitized and analysed during the ALPAGE projectand available online. The Vasserot cadastre is giving a full director of addresses in Paris for the period 1810-1837. Paris Open Data covers the structural change due to the Haussmann period and the evolution of the 20th century. Using these two sources, 89,2% of the addresses were successfully realigned for the seven case studies considered. Results In total 4.2M person mentions were extracted for the period 1839 to 1922. This database could be the starting point of numerous studies, we are only giving here a broad illustration of the content of this dataset and discussing their potential relevance for future research. For instance, the diversity of family names, an indirect proxy of the social effects of urbanization, clearly increases during the 19th century and then stabilizes at the beginning of the 20th century. Figure 6: Number of unique family names by year during the entire period. Name diversity keeps increasing during the 19th century and then stabilizes. Figure 7: Family names frequency on the period 1839-1922 Figure 8: Frequency of trade type by family names Dubois ; Martin ; Petit on the period 1839-1922 If one compares the three most frequent family names Martin, Petit, Duboiswith the typologies of trading businesses, the sale of wine appears to be, by far, the most commonly practiced activity. If you encounter a Martin, a Petit or a Dubois at the end of the 19th century, there is a certain chance, hell be a wine seller. This result corresponds to a general trend, at the urban scale. In the figures 9 and 10 the dominance of the wine business is 2.5 times more important than the one of grocery stores, the second activity on the list. As confirmed by other studies, the relative proportion of the wine activity keeps increasing during the 19th century. This is in line with other figures like the wine consumptionand the construction of the Bercy wine hall in 1869and their extension in 1910. Grocery professions, tailor, hairdressers, bakers are equally represented, alongside liberal professions like doctors. Architects, cabinet makers and carpenter are also well represented, a sign of an important building activity on a city scale. Figure 9: most common trades over the whole period Figure 10: most common trades by decade The seven chosen case studies correspond to a specific analysis crossing historical information and urban space, focusing on very ancient streetsorganising a variety of activities over a deep urban palimpsestand others, much more recent, with a strong characterization by trade businessesor residential vocation. Fig 12 compares the situation of seven cases studies showing the diversification of activities by neighborhood. The ancient roman streetsare characterized by the equally distributed presence of services for the population, reflecting a residential activity and a mix of activities acquired over time. Only the mention of the bookbinders on the Rue Saint Jacques, highlights the presence of the university district strongly linked to book production. Rue Montmartre, mentioned since the 13th century, includes many more activities than other cases, without clear differentiators, except for the presence of the tailors. Indeed, this axis is the interface to the Richelieu District considered as a place of production of fabrics and clothes. Rue Richelieu and Rue Saint Anne have a concentration of activities linked with fashion. This denotes the original vocation not residential but specific to these trades. The distribution of activities on the Boulevard Saint-Michel constructed after the transformation from Baron Haussmann is significantly different from all the others with higher level of owners and liberal professions. Fig. 11: The seven case studies: Rue Montmartre; Rue Saint-Denis; Rue Saint-Antoine; Rue Saint-Jacques; Rue Richelieu; Rue Saint Anne; Boulevard Saint-Michel Fig. 12: Street tradesThese preliminary results on Paris hope to demonstrate the potential of city directories to conduct large-scale urban analysis at different level of granularity. The automatic extraction process designed for this article permits to envision to easily conduct similar studies on the population of many other important cities in the world. Provided that the quality of the extraction process is monitored, such kind of massive datasets will open new avenues to study the transformations of the urban structure at different geographical and temporal scales during the ongoing industrialization and other significant societal transformations of the 19th century, connecting these large datasets from the past with the ones of the present. "
	},
	{
		"id": 90,
		"title": "Changing Lanes: A Reanimation Of Shell Oil’s Carol Lane",
		"authors": [
			"Dollman, Melissa"
		],
		"body": " From 1947 to 1974 Shell Oil Company sponsored a public relations program that engaged single and married women drivers. They especially targeted married women who helped plan leisurely road trips for their families, and single gals who wanted to see the country. Over twenty different women portrayed its figurehead, the pseudonymous Carol Lane. For most of the programs twenty-seven years, two or more Carol Lanes split the United States and Canada geographically and lectured concurrently, while a third or more, at times, played her on television or radio. No single face represented the one. Each Lane had her own biography for press releases in what seems to have been an effort to individualize the new girl and highlight what special areas of expertise she brought to the character. At the same time traits essential to the composite biography carried over. What I aim to discover is who Carol Lane was to the women who played her, the PR department who created her, and her audiences. Precedent I situate Lane in the company of two other better-known American living trademarks, Aunt Jemima and Betty Crocker, who were also both played by multiple women, sometimes simultaneously. Aunt Jemima and Crocker were the face of domestic products, and Lane an attempt to domesticate products—automobiles and gasoline—not necessarily associated with the home or women. All three had radio and television presences, well-defined skill sets or expert knowledge, and public personas that shifted over time. Caroline Iverson, who developed the Lane persona while employed on Shells public relations team, cited Crocker as a direct influence in correspondence with her departmental directors. In crafting biographies for each of the women who portrayed Lane, I am following Marilyn Kern-Foxworths lead on her history of Aunt Jemima. In short paragraphs, she honors each woman who played Aunt Jemima, who embodied a living brand that was so racially charged yet represented employment and perhaps fleeting celebrity. I also turn to a number of authors who collocate an incomplete collective biography of Crocker including Carolyn M Goldstein, Susan Marks, and Laura Shapiro. I have yet to discover another scholarly endeavor that uses prosopography as a method to study a phenomenon like Carol Lane or Betty Crocker, let alone a digital humanities project focused on collective biography about women in the public relations field. Methodologies My digital dissertation combines textual and visual data, all displayed, searchable, and readable on the ArcGIS-based website. The combination of prosopography, mapping/graphing, close reading, and display of primary documents is methodologically appropriate and necessary to advancing my analysis of the Carol Lane phenomenon on macro and micro scales. On the macro level I develop how Shell public relations visually, rhetorically, strategically, and physically positioned Lane in relationship to her audiences and as part of companys larger marketing efforts. I also place these relationships in the context of the larger petroleum and public relations industries. Mapping her routes and grouping her audiences thematically and locationally also falls under the macro level. On the micro level, I assess richer, more granular details about audiences socioeconomic standing and their race- and class-orientations, as well the individual women who played the role of Carol Lane. While Lane received considerable press compared to similar women-fronted PR programs, the women who portrayed her, Lanes audiences, and the people in the PR department who created her -- collectively Lanes known associates and network -- did not. Information about them is tucked away in newspaper articles, photographs, corporate and personal papers, genealogical databases, yearbooks, obituaries, television footage, census records, and more. The remarkable amount of labor already put into scanning and otherwise facilitating the discoverability of newspapers and other resources – long before I began my research -- make prosopography, or collective biography, tenable. I bring together disparate bits of information from a large-scale database into a composite view of Carol Lane. Prosopography can shed light on lives whose footprints may be documented in only ephemeral and fragmentary ways. On the ArcGIS platform, I present, for others to use, a subset of the total dataI have collected to date, and input myself. The assembled ephemera are displayed in an online archive exhibition space, as will be textual analysis and video essays. Thus far I have created a fully articulated website with biographical sketches, print advertisements, complete copies of Carol Lane publications and films, a crowdsourcing form for sourcing information on unidentified Lanes, methodologies, video essays, how-to information, etc. I have input data for over 1500 sources that underpin the sites visualizations and maps, and will add a selection of approximately 500 more before I submit my final project to my committee. I am also in the early stages of creating the contextual and analytical narrative for each chapter on the site. In future, the site will be available at http://carollaneproject.com. My hybrid method takes a more feminist approach in visualizing related data in more ways than a straightforward relational database. Because I cannot anticipate who myaudiences may be, my project presents my analysis, and the raw data itself so visitors to the site may come to different conclusions than I. Of course with that I will include an explanation of the decisions behind how the data is structured, visualized, and categorized. "
	},
	{
		"id": 91,
		"title": "Mapping the Indefinable: Designing a Social Network Analysis Shiny App to Explore the Influence of East-West Exchanges on Poland’s Political Transformation",
		"authors": [
			"Domber, Gregory Frank",
			"Bodwin, Kelly"
		],
		"body": " Beginning in the 1950s Americans sponsored international exchange programs for Polish scientists and professionals, believing that exposure to the West would undermine Communism. In 1989, Poles underwent a negotiated revolution. Were these extensive American public diplomacy efforts successful? Can pathways of influence and shifts in perception within specific epistemic communities be measured, mapped, and visualized longitudinally to better understand exogenous influences on Eastern Europes democratization process? Based on an approach to quantifying individuals lives based on their institutional affiliations, our interdisciplinary team has designed an interactive social network analysis visualization app, built in R Statistical Software using the Shiny package. The app allows users to interactively explore the overlapping networks of political revolution and international exchange, and illustrate how these connections shifted over time. This provides insights into Polands specific experience, as well as a model for studies of other complex, longitudinal networks. "
	},
	{
		"id": 92,
		"title": "DARIAH Beyond Europe",
		"authors": [
			"Dombrowski, Quinn",
			"Fischer, Frank",
			"Edmond, Jennifer",
			"Tasovac, Toma",
			"Raciti, Marco",
			"Chambers, Sally",
			"Daems, Joke",
			"Hacigüzeller, Piraye",
			"Smith, Kathleen",
			"Worthey, Glen",
			"Potter, Abigail",
			"Ferriter, Meghan",
			"Brass, Kylie",
			"Brownlee, Rowan",
			"Tindall, Alexis"
		],
		"body": " DARIAH, the digital humanities infrastructure with origins and an organisational home in Europe, is nearing the completion of its implementation phase. The significant investment from the European Commission and member countries has yielded a robust set of technical and social infrastructures, ranging from working groups, various registries, pedagogical materials, and software to support diverse approaches to digital humanities scholarship. While the funding and leadership of DARIAH to date has come from countries in, or contiguous with, Europe, the needs that drive its technical and social development are widely shared within the international digital humanities community beyond Europe. Scholars on every continent would benefit from well-supported technical tools and platforms, directories for facilitating access to information and resources, and support for working groups. The DARIAH Beyond Europe workshop series, organised and financed under the umbrella of the DESIR project, convened three meetings between September 2018 and March 2019 in the United States and Australia. These workshops served as fora for cross-cultural exchange, and introduced many non-European DH scholars to DARIAH; each of the workshops included a significant delegation from various DARIAH bodies, together with a larger number of local presenters and participants. The local contexts for these workshops were significantly different in their embodiment of research infrastructures: on the one hand, in the U.S., a private research universityand the de facto national library, both in a country with a history of unsuccessful national-scale infrastructure efforts; and in Australia, a system which has invested substantially more in coordinated national research infrastructure in science and technology, but very little on a national scale in the humanities and arts. Europe is in many respects ahead of both host countries in terms of its research infrastructure ecosystem both at the national and pan-European levels. The Stanford workshop had four main topics of focus: corpus management; text and image analysis; geohumanities; and music, theatre, and sound studies. As the first of the workshops, the Stanford group also took the lead in proposing next steps toward exploring actionable DARIAH beyond Europe initiatives, including the beginnings of a blog shared among participants from all the workshops, extra-European use of DARIAHs DH Course Registry, and non-European participation in DARIAH Working Groups. The overall theme of the Library of Congress workshop was Collections as Data, building on a number of U.S.-based initiatives exploring how to enhance researcher engagement with digital collections through computationally-driven research. In Washington, D.C., the knowledge exchange sessions focussed on digitised newspapers and text analysis, infrastructural challenges for public humanities, and the use of web-archives in DH research. As at Stanford, interconnecting with DARIAH Working Groups was of core interest to participants, and a new Working Group was proposed to explore global access and use of digitised historical newspapers. A further important outcome was the agreement to explore collaboration between the U.S.-based Collections as Data initiatives and the Heritage Data Reuse Charter in Europe. The third and final workshop in the series took place in March 2019 in Australia, hosted by the National Library of Australia in Canberra. Convened by the Australian Academy of the Humanities, together with the Australian Research Data Commonsand DARIAH, this event was co-located with the Academys second annual Humanities, Arts and Culture Data Summit. The first day of the event, targeted at research leadership and policy makers, was intended to explore new horizons for data-driven humanities and arts research, digital cultural collections and research infrastructure. The two subsequent days focused on engaging with a wide variety of communities, includinghumanities researchers and cultural heritage professionals. Organised around a series of Knowledge Exchange Sessions, combined with research-led lightning talks, the participants spoke in detail about how big ideas can be implemented practically on the ground. This poster reflects on the key outcomes and future directions arising from these three workshops, and considers what it might look like for DARIAH to be adopted as a fundamental DH infrastructure in a complex variety of international, national, and regional contexts, with diverse funding models, resources, needs, and expectations. One major outcome of all workshops was the shared recognition that, in spite of extensive funding, planning, and goodwill, these workshops were not nearly global enough in their reach: most importantly they were not inclusive of the Global South. Our new DARIAH Beyond Europe community has a strong shared commitment to address this gap. "
	},
	{
		"id": 93,
		"title": "Gender and Intersectional Identities in the Digital Humanities",
		"authors": [
			"Hendery, Rachel",
			"McDonough, Katie",
			"Dombrowski, Quinn",
			"Gniady, Tassie"
		],
		"body": " The role of gender and intersectional identities in digital humanities remains an urgent topic of conversation. Despite this, precious few spaces exist for open, safe, and inclusive discussions around intersectional gender. Digital spaces like the Crunk Feminist Collective, FemTechNet, and FemBot Collectiveprovide blogs, resources, and opportunities for public writing on issues that matter to female-identified researchers. Perhaps despite these spaces, the expression of gender issues within digital humanities in conferences, publications, and projects struggles with striking a balance between public and private discourse. The narratives of digital spaces and the blogosphere prioritize sharing and making visible the labor of feminist activism within academia. Nevertheless, despite the emphasis on visibility, individuals in precarious, contingent labor conditions need protective shielding, as speaking about gendered experiences in DH can result in personal and professional consequences. Safety is even less assured in the conference venues and the purview of anonymous peer review of proposals, papers, and grant applications, where institutional affiliation and long-established projects and reputations regularly prevail. During fall 2018, a loosely organized working group formed around lived experiences of gender in the digital humanities. This group aims to provide a space for an open discussion about embodied experiences and intersectional gender identities in digital humanities. The working group aims not only to raise awareness, but pragmatically to enact change in the larger digital humanities community in its interests in strategies of resistance and survival for women and gender non-conforming digital humanists. Between January and June 2019, individual volunteers are organizing a series of monthly virtual meetings, each around a specific topicinfrastructure, emotional and invisible labor, gender equity at panels, gender disparities in technical work, gender and leadership in digital humanities initiatives, etc). We anticipate that these discussions will lead to the production of documents such as white papers that will be made available for anyone to use when advocating for change at their institution, for conferences they are organizing, etc. We expect at least two of these documents to be ready for dissemination by the DH 2019 conference. We intend to release these in advance of the conference to facilitate translation and encourage multilingual engagement with these issues. Presenting a poster at DH 2019 would provide an opportunity to expand the community and consider the groups next organizational steps. While this working group has included participants from a variety of countries since its inception, the community has been primarily centered in the US and Canada. Time zones and linguistic diversity pose logistical challenges for organizing synchronous discussions, both can be mitigated if a larger community wanted to take that approach. Regional clusters of synchronous discussions, with discussion leaders from each region collaborating on a white paper or other document that incorporates all perspectives, could be another approach to scaling the current model. The poster session would also provide a venue for brainstorming alternative, or supplemental, venues for surfacing issues, discussing them from intersectional and international perspectives, and collectively advocating for change within digital humanities. Finally, we hope to use the poster session as an opportunity to solicit input on a discussion draft of a Gender and DH SIG proposal, and find additional volunteers to help shepherd forward revision and submission of the proposal, given sufficient interest in engaging in organization and advocacy in that form. "
	},
	{
		"id": 94,
		"title": "Methodology as Community: Fostering Collaboration Beyond Scholarly Societies",
		"authors": [
			"Dombrowski, Quinn",
			"Haslinger, Peter",
			"Puchkovskaia, Antonina",
			"Bernstein, Seth",
			"Hill Reischl, Katherine",
			"Keenan, Thomas",
			"Ermolaev, Natalia",
			"Ilchuk, Yuliya"
		],
		"body": " Literature and history, writ large, are fields with prominent voices in the digital humanities community. Art history, film studies, archaeology, and anthropology form recognizable disciplinary clusters within the international discourse of digital humanities as well. Since the mid-20th century, area studies have offered their own interdisciplinary organizing principle for drawing together scholars who work with a variety of materials and approaches. While on one hand, this arrangement would appear to be naturally conducive to international collaboration, geopolitical tensions and restrictions have at times formed a barrier between area studies scholars in the United States, and their colleagues working in and around the areas in question. This has been especially true in the field of Slavic and East European studies, which have been knit together in the United States through two major professional organizations that have -- through a combination of circumstance, necessity, and then habit -- cultivated a distinct and somewhat insular scholarly community in Slavic studies. The fall of the Iron Curtain, the fluctuations in the perceived threat of Russia to the United States, and the spread of the Internet as a medium for communication have provided scholars worldwide with opportunities to engage with Slavic area studies through a global and collaborative lens in ways that were not conceivable earlier in the history of the field. While many U.S. scholars have taken advantage of the possibility to travel to Slavic and East European countries, the scholarly networks of citation and discourse remain centered within the communities formed by the U.S.-based professional organizations. The emergence of digital humanities tools and methodologies has provided an opportunity for rethinking the collaborative landscape for Slavic and East European studies. Much as in the humanities as a whole, the percentage of scholars in the field actively using digital tools and methodologies is fairly small. For Slavic and East European studies, however, any effort to develop a community around digital humanities is compounded by the comparatively small overall size of the field. Indeed, while there have been efforts to run a digital humanities interest group through one of the major professional organizations, the work of sustaining the group has fallen on one or two people, as there is not enough of a critical mass to spread around the necessary but time-consuming work of cultivating and growing the group. Rather than working within existing scholarly organizational frameworks, some U.S.-based scholars have turned towards international collaboration to further their engagement with digital tools and methodologies. This panel brings together Slavists and East Europeanists from the United States, Western Europe, and Russia whose research, teaching, and infrastructure development is shaped by engagement with colleagues who share similar materials and methods despite widely varying national and institutional contexts. In addition to presenting highlights of their own work, panel participants will reflect on the ways in which digital humanities provides a different organizing principle for their scholarly networks and community. Cultural Heritage and Critical Multiperspectivity: Building Research Infrastructures on Eastern Europe Due to the constantly increasing number of digitally available sources, those working in the fields of History and Cultural Studies are faced with a significant challenge, namely, to develop new practices and procedures around the verification of sources and the provision of digital material. Of central importance here are questions relating to the provenance, validity, and critical analysis of sources. These matters are particularly pertinent in the case of East Europe, where transnational conflicts around shared historical and cultural heritage have given rise to a special need for research-based information, digital source criticism, and questions of research ethics. As a leading European research institute on the history of Eastern Europe, we continue to develop our shared research infrastructure in collaboration with other partners, and we need to keep these goals in mind. This presentation aims at addressing the challenges and opportunities of building up a multi-national infrastructure for a digital and critical documentation of cultural heritage in Eastern Europe. This process involves not only developing research software and a standardized vocabulary, but also mapping out multi-perspective approaches relating to the exegesis of digital sources within metadata structures and, last but not least, providing dialogue-based formats for reflecting on historical sources in the digital age. Being an Englishman in New York, or How to Launch an International DH Lab in St. Petersburg DHlab, the international digital humanities lab launched in at our technical university in St. Petersburgin October 2018, is dedicated to exploring how technology facilitates new possibilities in understanding society and global culture. This is a collaboration between our institution and a private university in the United States, and intends to draw in partners from DH communities all over the world. In this presentation, we will discuss our priorities when starting a humanities-focused initiative at a technical university. These include 1) the importance of humanistic disciplines in the discourse of Information Technologies, Computer Science and Engineering; 2) motivating women in science; 3) applying STEM methods to Arts and Humanities research problems; 4) managing different approaches while working on digital humanities projects; 5) involving students of different majors and interests in interdisciplinary projects; and 6) highlighting digital humanities among wide audience using social networks. We will share our experience in training the next generation of DH scholars, developing innovative DH projects, contributing to creating useful software, providing women within Humanities and STEM with equal opportunities to conduct interdisciplinary research etc. We will also discuss one of our most significant projects, a web application that maps historical and cultural heritage data about key landmarks of St. Petersburg, Russia. With input from scholars of history, library science, cultural studies and information technologies, the project team has conducted semantic analysis on a large, multilingual textual corpus that includes memoirs, documentaries and periodicals, and uses Text Encoding Initiativeto encode information about people, relationships, and events, and Uniform Resource Identifiersto identify locations. All landmarks are being mapped onto an interactive city map of St. Petersburg with a user-friendly interface to facilitate easy navigation and filtering. Edges of Slavic Studies: Network Analysis of an Areas Studies Field from the Cold War to the Present This presentation uses DH tools to turn a lens to the Slavic Studies professional community. In 2018, the Association for Slavic, East European and Eurasian Studiesthe main North American-based international academic professional organization in the field, celebrates its 70 th anniversary. As part of the anniversary, the association published the programs for thirty-four of its conventions. These programs are a valuable guide not only to individual contributions but to the institutional networks embedded in the Slavic Studies field. This paper uses network analysis to analyze the shifting intellectual motivations and professional ties in Slavic Studies. Before the fall of the USSR, Slavic Studies was not only an academic field but a front in the Cold War. US government funding motivated by fear of communism also produced works in the humanities with no immediate political value and created a new field of area studies. Network analysis can assess Engermans qualitative research and go beyond it. After the Cold War ended, Slavic Studies undertook a post-mortem of the USSR. How did the constellation of intellectual and professional networks shift? After the financial crisis of 2007-08 that brought cuts in research spending, how did Slavic Studies change? What institutions remained in the field and with what institutional concerns? Similarly, network analysis can help understand early academic reactions to Russias more aggressive stance in the world since 2011. This study can also contribute to understanding how digital humanities is growing in fields outside of Western Europe and North America. By isolating the networks in panels whose papers use relevant keywordsthe paper will ask how the impact of digital humanities has and has not impacted the Slavic field. International DH Collaboration as Pedagogy Pedagogy has long been a major topic of interest in the DH community. An opportunity for engaging students in DH that has not been addressed is through involvement in international partnerships. In this presentation, we discuss our institutions emerging Slavic DH Working Group, our decision to prioritize collaboration with international partners, and our efforts to include both graduate and undergraduate students in our work. As an alternative - or supplement - to the curricular or project-based learning models that are most common in many DH programs, involvement in international partnerships provides not only skills, competencies and exposure to professional practice, but opportunities for cross-cultural knowledge exchange that are key for the advancement of both field-specific and DH scholarship. The Slavic DH Working Group at our research university is the only one of its kind in the United States. Established in 2017 through the joint effort of faculty, graduate students, librarians and DH staff, the Slavic DH Working Group is a community of scholars at all levels, from various disciplines, and engaged in a variety of professional fields. Promoting the exploration of digital humanities in Slavic, East European and Eurasian Studies, the group meets monthly during the academic year for events, trainings, and mentorship . In this presentation, we will discuss our collaboration with a research institute in Germany whose focus is Eastern European history. In the summer of 2018, we took eight members of the Slavic DH Working Group - including two graduate students and two undergraduates - to Germany for a 5-day DH workshop called Digital Mapping Eastern Europe. Besides our US group, the workshop brought in early-career scholars from Germany, Hungary, Poland, Czech Republic, Ukraine, Belarus, Russia and Israel. We will discuss how the format of the workshop allowed the graduate students to make significant strides in their dissertation work. The workshop was a unique opportunity for what Geoffrey Rockwell and Stefan Sinclair call acculturation- not just academic professionalization, but broader exposure to the culture of a field that includes different types of jobs, and in various national and international contexts. We will also discuss the opportunities this experience afforded the undergraduate participants, and the kind of both collaborative and independent work that the workshop generated. By contrast to the more conventional contexts in which undergraduates learn DH - either in the classroom setting or as paid laborers on faculty-led DH projects - participation in the international exchange allowed students to become involved in the community as scholars in their own right. We will suggest that international collaborations in DH are particularly beneficial for smaller fields like Slavic where use of digital methods are relatively new, and where formal and fruitful international exchange by groups of scholars can difficult to sustain. Bringing DH into the graduate and undergraduate Slavic Studies experience exposes students to new scholarly approaches while animating Russian and East European cultural heritage and collections for young scholars. Conversational Versus Co-occurrence Models in Networking the Russian Novel The Russian novel has been traditionally regarded as the novel of ideas, in which the conflicting views on the national identity and Russias relationship with the other are presented in the dramatized narrative. Especially after Mikhail Bakhitns pioneering ideas of dialogism and polyphony of Fyodor Dostoevskys novels, it became common to view the novel as a balanced dramatization of conflicts among polar opposites. In my ongoing research project on the network novel in the late imperial Russian culture, I approach Tolstoys and Dostoevskys novels as models of an emerging liberal society. These narratives are not merely depictions of individual experiences manifested in text, but also represent complex societies whose imaginary social forms can be quantified and analyzed. In British and American nineteenth-century novels, a shift occurred between the novel of domesticity and the network novel which exaggerated contrast within texts to expose the search for a resolution in the world outside. This study aims to examine a corpus of Russian novels to see whether they have the same clear dichotomy between a nuclear family type and a social panorama type. This study takes the approach of comparing the conversational and with co-occurrence methods for collecting data from the novels. The conversational method is based on the participation of the characters in a speech act, defined as a continuous span of narrative time featuring a set of characters co-located in space and time, where they take turns speaking, are mutually aware of each other, and each characters speech is intended for the other to hear. The co-occurrence method uses the automatic extraction of interactions based on the appearance of two characters in one sentence or short paragraph. This study compares networks created in Gephi using each method and argues that the co-occurrence of the characters in one scene does not yield significantly different results compared to measuring associations via speech acts. The relative simplicity of identifying speech acts, versus identifying all instances of speaking and non-speaking characters co-location in space and time, makes an approach based on speech acts even more attractive. Examining the effectiveness of different methods for generating networks from novels has implications for technical developments that can further research in Russian literature. If speech act-based networks reliably capture the nodes and edges of the Russian realist novels, that presents an opportunity to develop algorithms that can identify speech acts with a reasonable degree of accuracy. This, in turn, has implications for corpus development, and ensuring that those corpora are formatted in machine-readable ways. This talk will reflect on experiences, challenges, and successes in sharing corpora and tools with an international group of colleagues who use similar digital approaches in their research on Russian literature. Rethinking Scholarly Networks Through the Lens of Digital Humanities Scholarly societies are core social and community infrastructure in the humanities. Presenting at conferences sponsored by these societies is a crucial component for gaining visibility in a field, and bolsters the legitimacy of ones candidacy for jobs, awards, and other forms of recognition. In North America, these societies are typically scoped nationally, and attending a major annual conference for a society is an expensive endeavor — a situation only compounded for scholars who work interdisciplinarily. The professional necessity of directing significant amounts of ones travel budget towards these conferences, combined with the limits of their national scope, impede international collaboration, particularly for early-career scholars. Digital humanities provides a different axis for finding and engaging with colleagues across national boundaries. Particularly for the text-centric scholarship commonly found in literature and history, building corpora is time-consuming and expensive endeavor. Once completed, such corpora can be a transformative resource for colleagues pursuing a wide range of research questions. For scholars whose work involves applying natural-language processing tools to languages other than modern English, there are resources like stopword lists, word embeddings, and pre- and post-processing scripts that can meaningfully be shared between projects. Collaborative development of tools, resources, taxonomies, and infrastructure that are directly related to Slavic studies is itself a significant step forward for the field -- but in the long run, perhaps the most significant impact of DH-centered collaboration is the downstream effects of the relationships established between scholars. Scholarly networks are valuable not only for research collaboration, but also as a source of advice when dealing with institutional challenges, and connections and opportunities for ones students. This paper will draw together unifying themes from the presentations given by other panelists and consider the ways in which digital humanities could reshape the current insular, nationally-centered landscape of Slavic and East European area studies in the U.S., and foster the emergence of new international scholarly networks that have an impact beyond the field of Slavic studies itself. It will also reflect on the ways that existing disciplinary scholarly societies could take a more proactive role in breaking down the national boundaries that have emerged through their current structure, for instance, by fully engaging with digital tools and platforms with roots in the digital humanities / scholarly communications communities, such as Humanities Commons. "
	},
	{
		"id": 95,
		"title": "CO-OPERAS IN: Integration And Cooperation To Face Fragmentation And Address Complexity In The SSH",
		"authors": [
			"Dumouchel, Suzanne",
			"Giglia, Elena"
		],
		"body": " Complexity in the Social Sciences and Humanitiescan take the shape of the fragmentation of research fields, across many disciplines and subdisciplines, usually grounded in regional, national and linguistic specific communities. SSH data are fragmented themselves across different types, formats, languages, disciplines, resulting in a major impediment to their discovery and to their reuse from outside the specific and often small communities where they were produced. Big data does not apply to SSH, where data can often need to be very precisely qualified, described, curated and managed: they are smart and small data, which means they have to be specifically managed, all the more so in the perspective of being integrated in the European Open Science Cloudlandscape, as a major component of the IFDS. At the same time, SSH disciplines have undergone a major change in their communication practices, driven by the development of digital technologies and the Open Science paradigm: the boundaries of the scholarly record are now blurring, and the research monograph - the traditional primary form of research dissemination - is being challenged by technical innovations such as text and data mining, open annotations, data embedding, and collaborative writing. This short paper will present CO-OPERAS - Open Access in the European Research Area Through Scholarly Communication -, which is an Implementation Networkwithin the GoFAIR initiative. CO-OPERAS IN aims to build a bridge between SSH data and the EOSC, widening the concept of research data to all types of digital research outputs linked to scholarly communication that are, in SSH, part of the research process, whereas the concept of continuous communication underpinning the SSH research lifecycle holds an immense potential as an inspiring model of Open Science with direct societal impact. CO-OPERAS IN is based upon a solid international framework, grown through strong collaboration, and years of consensus building between more than 36 partners from 13 countries, representing diverse stakeholders and service providers encompassing the entire cycle of scholarly communication in SSH. CO-OPERAS IN aims to bring the FAIR principles into the SSH research environment, leveraging on existing scholarly communication services and platforms to connect them as components of an emerging EOSC, and more broadly to the global SSH communities. This short paper will try to explain how integration is a way to face fragmentation. This is the keyword of CO-OPERAS IN, while its core strategy is coordination rather than competition, nurturing existing realities. The first part of the paper will be dedicated to highlight this CO-OPERAS unique approach based on synergies between researchers, publishers, and libraries and the complex organization of the IN, with special regard to the number and varieties of partners through the presentation of its work plan and scheme of governance. The second, more techinal part will present the CO-OPERAS IN building blocks for the Discovery, Certification, Research for Society services and the tools to support the FAIRification of the research process and resources in the SSH, which is of utterly importance to bring Humanities and Social Sciences into the European Open Science Cloud. The paper wil provide some insights on identification and certification processes, metadata enrichment, interoperable standars, discovery tools based on a multilingual approach, and licensing practices. "
	},
	{
		"id": 96,
		"title": "Quantifying Complexity in Multimodal Media: Alan Moore and the “Density” of the Graphic Novel",
		"authors": [
			"Dunst, Alexander",
			"Hartel, Rita"
		],
		"body": " Introduction In an interview he gave in the year 2000, the well-known comics author Alan Moore made a remarkable observation about the graphic novel. Although he was critical of the term, which is commonly used to refer to book-length comics narratives, Moore acknowledged that canonical titles such as Art Spiegelmans Maus and his own Watchmen could legitimately be described as novelistic on the basis of their higher density. Moore thus implicitly hypothesized that critical appreciation may have a formal basis. As we understand it, Moores brief reference to density—which he does not expand on in the interview—may be reformulated as complexity: the visual and textual cues that make it comparatively easy or difficult to comprehend and interpret a given narrative. Yet, this notion of complexity introduces further complexities for a scholarly understanding: On the one hand, Moores hypothesis accords with recent attempts in DH to find a new middle ground between the older formalisms and a cultural studies emphasis on the discursive construction of literary concepts. In practice, this reorientation necessitates a combination of computation and cultural sociology. On the other hand, anyone who attempts to operationalize a concept such as density in multimodal media also faces technical challenges, in our case the automatic recognition of handwritten comics fonts. In this paper, we describe the operationalization of Moores concept of density with the help of six textual and visual measures. We then present a pilot study of 40 graphic novels and memoirs, which are taken from the first representative corpus of English graphic narrative, or GNC. Six of these can be described as canonical given their frequent discussion in academic scholarship. The relatively small number of titles can be traced to the aforementioned technical hurdles. DH research on comics and visual media more generally has made significant progress in recent years. Yet, existing computer vision methods still need to be adapted to the structural features of comics, such as individual panels, speech bubbles, and non-perspectival drawn images. Existing OCR software based on static and adaptive character classifiers leads to poor results in recognizing highly individualized and frequently handwritten comics fonts. This paper builds on early results of applying neural network-based automatic text recognitionto graphic narratives and may constitute the first computational analysis of comics text. Dataset & Methodology The brevity of Moores reference to density does not give any indication of his precise understanding of the term. However, our previous research has shown that Shannon entropy, a measure for the visual agitation of a page, and the number of shapes are useful indicators of style in comics. These measures also capture central elements of basic visual processing, which distinguishes variations in color and brightness and establishes discontinuities between shapes. In addition, we include the number of individual panels in our formalization. Most comics pages consist of several individual images that are framed by drawn borders or white space to suggest a sequence. Therefore, the number of panels per page indicates whether a page consists of one single image, or is constructed from the complex arrangement of many. We currently achieve the most promising results recognizing comics text with the open-source Tesseract 4 software, which is based on a long short-term memoryrecurrent neural network. As described in earlier work, we use the similarity measure of the Bag Error Rateto decide whether the ATR software produces plausible results for text analysis based on a Bag-of-Wordsapproach. For each graphic novel, we manually annotated around 10% of its pages and compared this gold standard to the results of the ATR. Only if the BOW of the gold standard and the recognized texts are similar enough, do we consider the graphic novel appropriate for text analysis. Research on the complexity of written texts often uses simple word-based measures. Standardized reading tests such as the Gunning fog index or Flesch-Kincaid count the length of individual words and sentence lengths. While our ATR does not reliably recognize punctuation at this point and is thus unable to count sentence length, we include the number of overall words on a page, word length by number of characters, and normalized type-token ratio in our calculation of textual complexity. In order to weigh all six textual and visual measures equally, we normalized each measure by computing the quotient of the value for each graphic novel and the maximum value for all graphic novels. The designation of certain graphic novels and memoirs in the GNC as canonical is based on the frequency with which they are mentioned in the Bonn Online Bibliography of Comics Research. Figure 1 gives an overview of the 20 titles with the highest number of mentions and includes all six titles that were part of our study. Fig. 1: 20 titles in GNC with the most mentions in Bonn Bibliography of Comics Research Results & Discussion Our pilot study provides empirical evidence that supports Moores hypothesis that critically esteemed, or canonical, graphic novels are characterized by higher density. Despite the comparatively small number of titles analyzed, figure 2 shows that the results are highly significant, with p<2*10 -16. Fig. 2.: Distinction in density between canonical and non-canonical graphic narratives The results introduce a number of finer distinctions that are, by necessity, absent from Moores brief mention of density. Figures 3 and 4 compare four genres: the umbrella category graphic fantasy, which includes science fiction, fantasy, horror, and superhero narratives; graphic memoirs; graphic novels in the narrower sense of the word as fictional, literary narratives; and graphic non-fiction. Both information channels present in graphic novels show significant differences for canonical and less celebrated titles. If we look at different genres, we see that graphic memoirs are less complex visually than other titles but show the highest score for textual density. Graphic fantasy emerges as the most visually complex genre. Fig. 3: Genre comparison for visual density. All categories show statistically significant distinction, with p < 0,05. Fig. 4: Genre comparison for textual density. The pairings graphic novel–graphic fantasy, graphic novel–graphic memoir, and graphic novel–graphic non-fiction are statistically significant, with p < 0,05. The difference between textual and visual density contributes to our empirical knowledge about narrative. The visual density of graphic fantasy is due to higher entropy and number of shapes. Work in progress indicates that these titles also tend to be more colorful and more irregular in their layout. Titles such as Moores Watchmen and V for Vendetta are thus visually highly complex, possibly because of the emphasis on spectacle and entertainment in these genres. In contrast, the textual density of graphic memoirs might contribute to their frequent discussion in academic scholarship, with Maus amassing 15% of all mentions in our corpus. Textual complexity arguably appeals to literary and cultural critics who have been schooled to appreciate titles that allow for ambiguity and subtle interpretations. However, many graphic memoirsare published in black and white—a feature that leads to lower entropy and, in our current operationalization, to somewhat lower visual density. Finally, a combination of high visual and textual density seems to augur well for the success of graphic narratives. As figure 5 shows, four of the six canonical examples included in our study can be found among the highest scoring titles on both counts. Fig. 5: Scores for overall density, with canonical titles marked green Conclusion & Outlook Weve detailed the operationalization of a concept, that of density or complexity, that was anecdotally connected to social processes of canonization by a leading comics author. Similar processes might be at work in multimodal media, including film and television. Generally speaking, higher-level concepts that combine information channels may provide useful research hypotheses for multimodal analysis. In contrast to more exploratory analyses of correlation between verbal and visual measures, these concepts can easily be connected to qualitative scholarship and sociological metadata. In a next step, we will increase the number of titles to the total of 250 included in the GNC. This will allow for a representative overview of graphic narrative. In addition to Tesseract 4, we are currently training Transkribus ATR softwareon comics fonts and are working on enhanced text spotting, so that we will likely be able to present a more comprehensive version of this study in time for DH 2019. "
	},
	{
		"id": 97,
		"title": "A Survey On LDA Topic Modeling In Digital Humanities",
		"authors": [
			"Du, Keli"
		],
		"body": " Introduction Latent Dirichlet Allocationtopic modelingis a statistical method that discovers hidden themes and topics from a text corpus, and it has been widely applied in digital humanities over the past several years. My survey on applications of topic modeling have found 53 studies from the books of abstracts of the annual international conference of the Alliance of Digital Humanities Organizations between 2011 and 2018 Collection of abstracts from the last decade was initially planned. Unfortunately, due to a broken link, the abstracts of DH2009 could not be obtained, and studies related to topic modeling could not be found in the abstracts of DH2010. . Topic modeling-related approaches are increasingand most of them are related to historical studiesand literary studies. It has also been used for religious studies, digital archaeology, etc. Figure 1. Distribution of studies over time. The Problem The standard use of LDA topic modeling is to browse corpora through topics and data visualizations, while it is more complex than just training and visualizing topics in practice. Results of topic modeling can be influenced by several factors such as the LDA hyperparameters, topic number, chunk-length of documents, number of iterations of model-updating as well as hyperparameter optimization. As far as I know, a common understanding on handling these factors seems yet to be established. Using text chunking as an example, Jockersdivided novels into chunks in order to capture transient themes which only appear at certain points of a novel. In contrast, Nichols et al.writes: Following common practice using LDA on texts, we did not chunk or split the texts in our corpus for analysis. Stop words removal is another example, where more coherent topics are able to be obtained in general by removing stop words with no contents. Most approaches remove stop words before training the model, however discussions regarding the effectiveness of removing stop words after the modeling process have surfaced. In order to provide a comprehensive overview of how the majority of humanities scholars understands and uses topic modeling, a survey on above mentioned 53 approaches in detail has been done. In this paper I therefore propose to look at these approaches in the following aspects: Preprocessing: text processing procedures before topic modeling. What kinds of preprocessing is used and why? Modeling: what are the parameters, which control a topic modeling process. How can they influence the results and how are they been used in different approaches? Postprocessing: What method has been used for the interpretation of topics? How were the quality of a topic model and the topics evaluated? Preprocessing: The common preprocessing procedures include lemmatization, part-of-speechtagging and document chunking. By transforming words to their base form, the topic model can become more concentrated on the semantic structure. Through POS tagging, words with less contents could be identified and removed from corpus, in order to get more coherent topics. Chunking allows us to capture topics which only appear at certain points. My survey pays particular attention to the reasons of applyinga preprocessing procedure in practice. For example, lemmatization is often applied when the corpora are in highly inflected languages like German or French. Document chunking is very diverse: the chunk-size could be several hundred or several thousand words, or a page of a book, or to split a book into ten equal segments. But almost no approach explained the reason of their chunking choices. Modeling: The LDA hyperparameters, the number of topics, the number of iterations of model-updating, the hyperparameter optimization control the modeling process. As a matter of fact, no approaches have been reported on setting the hyperparameters, while two approaches reported their number of iterationsand two approaches applied the hyperparameter optimization. Other approaches are more focused on the visualization and analysis on topic models. In 23 approaches, the choice of topic number has been reported, but only 8 of them explained how the numbers were determined. Postprocessing: After the modeling process, it is important to evaluate the topic model and the topics. Only 5 approaches reported their evaluation methods. Although the topic quality can be evaluated by measuring topic coherenceand topics can also be automatically labeled. Only one approach used topic coherence for the evaluation of the trained topics. It is more common to label topics manually and to highlight the correlation between interesting topics and metadata. More than half of all approaches applied data visualization for exploration purposes. Conclusion This survey intends to provide an overview regarding the common use of topic modeling in digital humanities. It presents the situation that DH-community not always report how topic modeling was applied: Within 53 approaches, around 74% didnt report how their corpora were prepared; more than 70% didnt report which tool was used to train their topic models; almost 57% didnt report how many topics were trained and about 90.5% didnt report how their topic model were evaluated. Without reporting the technical details, the scientific reproducibility and the stability of their research could be questionable. In addition, the lack of intereston the complexity of topic modeling itself may indicate non-optimal application of this method. The standard parametrization of the topic modeling tool could be used, but because texts in humanitiesare more complex, it is not always clear how much previous understanding of topic modeling from computer science can be beneficial. One good example of the variation is the subgenre classification using topic modeling in Schöch, 2017. The accuracy difference between the worst and the best models is 17%. To obtain a better understanding of topic modeling, series of systematic investigations into the impact of factors on topic modeling will be my next steps to proceed. "
	},
	{
		"id": 98,
		"title": "An Online Corpus For The Study Of Historical DialectologyOralia diacrónica del español (ODE)",
		"authors": [
			"Calderón Campos, Miguel",
			"Díaz Bravo, Rocío"
		],
		"body": " The proposed paper aims to present the development of Oralia diacrónica del español Funded by MINECO/AEI/FEDER, UE., a new digital resource for the study of historical dialectology, thanks to TEITOK, a web-based framework for corpus creation, annotation, and distribution, that combines textual and linguistic annotation within a single TEI based XML document. The digital resource consists of a diachronic corpus of Spanish texts from the south of Spainwritten between 1492 and 1833. These texts, characterised by communicative immediacy or conceptional orality, include inventories of goods, witnesses testimonies in criminal trials and surgeons reports on the state of an injured or dead person, where doctors and surgeons use both colloquialisms and learned words. Furthermore, there are texts from different archives in the south of Spain, which makes the corpus an excellent source for historical dialectology studies. The corpus follows the successful model of the ERC-funded project Post Scriptum: A Digital Archive of Ordinary Writing, based on TEITOK. This model allows the combination of two methodological approaches, which represent two subsequent stages in the creation of the corpus: 1) A philological approach that involves the digital edition of the manuscripts. The texts have been encoded following the TEI P5 Guidelines. Furthermore, as proposed by CHARTA, the texts in the corpus can be visualised in three different formats: images of the manuscripts, diplomatic transcriptions and critical editions. Each text is presented with metadata, such as date, place and text type. 2) A corpus linguistics approach, in which texts are tokenized, normalized and annotated by PoS, based on the international standard for European languages EAGLES, although the tagset has been adapted. NeoTag, a PoS tagger, has been trained with another corpus of early modern Spanish: Post Scriptum. When a considerable amount of data has been annotated and manually corrected in ODE, this will be used as the training corpus to automatically annotate new texts, improving this way the accuracy of the PoS tagger. Thanks to the user-friendly interface offered by TEITOK, it is possible to revise and modify the following information online: TEI tags, metadata, lemmas and PoS. Encoded information can be retrieved and visualised in different ways, such as KWIC, indexes and maps. It is possible to search and browse by different filters, which can be applied simultaneously, and combined with other filters like lemma and PoS. A friendly-interface query builder allows the exploration of the corpus by a general audience with no background in computational linguistics. Finally, we would like to emphasize that the new online corpus has successfully overcome the following difficulties: a) It combines digital textual scholarshipand corpus linguistics. b) It allows working in a single edition that can be visualised in different formats by the end user in the digital resource. c) Furthermore, it permits independent management, since scholars can upload and edit their work, having control over their own research without the need for an external person in charge of the digital resource. "
	},
	{
		"id": 99,
		"title": "Five years Later: Looking Back on the 'Parcours Numériques' open access series",
		"authors": [
			"Eberle-Sinatra, Michael",
			"Vitali-Rosati, Marcello"
		],
		"body": " In the spring of 2014, les Presse de lUniversité de Montréal launched an innovative new series entitled Parcours numériques. The goal of the series is to develop in-depth, scholarly theoretical ideas on the world of the digital by producing reference-quality texts in French which will add to the discussion and serve to guide practices while at the same time experimenting with new forms of content management. The series was also an attempt to offer an alternative publishing model that combined print and electronic publishing, as well as promoting open access while still maintaining a range of funding revenues to be economically sustainable. Every text in the series is published in paper, digital and enhanced digital formats: the enhanced digital edition is open access on the Parcours numériques platform; the paper edition and matching digital editionsare sold in bookstores and online. The economic model of the series is based on a differentiation between kinds of reading practice. The series offers its content in two forms in order to make possible two kinds of reading of the same original text: A linear reading The paper version and the matching digital versions, identical to the paper edition but in digital format, present the text in a linear and compact manner. They let the reader follow only an authors discourse and the development of a complex argument, with the sustained and linear concentration that reading on paper affords. These versions are offered with a minimum of critical apparatusin order to foreground this unique thread. A non-linear reading The enhanced digital version makes possible a non-linear reading. Around the original text in-depth exploration of certain subjects is encouraged and facilitated through the presence of additional content: references, notes, biographies, videos, diagrams, illustrations, etc. The non-linear reading is documented and makes it possible to explore particular aspects of the text and to linger on it. This version is open access, as it enables readers to create their own parcours numériqueout of the authors work and the hyperlinks that link to other content, produced and shared by other authors. Out of the free circulation of content there arises a knowledge network, a dialogue, a virtuous circle in which one wishes to take part. The two versions are thus complementary: two kinds of reading, two approaches to sharing knowledge and two ways of absorbing the topics addressed. The goal of the series Parcours numériques is also to offer a new editorial model, one that goes beyond the dichotomies of paper versus digital, open access versus paidand public subsidies versus private contributions by proposing other forms: innovative hybrid structures which make use of the possibilities of the Internet with the goal of disseminating knowledge. This is the role of the Parcours numériques platform, which was placed on line at the same time as the series was launched in March 2014. This platform, which complements the paper and matching digital versions available in the collection, is dynamic and living. Not only is the content kept up to date, but it is constantly evolving, expanding and mulling it over, thanks to the way in which the experience of all the actors involved in the projectis returned to it; to the work carried outon the manuscripts in the process of being published; and to technological progress. With a structure and environment which provide optimal reading comfort, the platform provides open access to the enhanced digital version of all of our volumes, including the text as a whole and the additional content provided to augment the readers experience. This content shines additional light on, explores in greater depth, and links to other content, other platforms and other supports. These resources added to the main text are the fruit of the joint efforts of the authors and the platforms editorial team, making available to readers an abundant and coherent ensemble of information and links which enables them to undertake different paths while reading the work according to the kind of in-depth additional knowledge they are interested in. The notes and additional content will evolve over time and in response to suggestions made by authors but also from readers. Because the collection is conceived in three formats, authors were required to take this fact into account when proposing and later writing a project. Indeed, the creation of enriched contentfor the augmented digital version is an integral part of the volume, which must be conceived, presented and designed with this in mind. Thus, because the augmented digital version makes possible a non-linear reading, authors are encouraged to provide additional, more in-depth information about some aspects of the original text. This in-depth information will be made up, for example, of references, notes, biographies, videos, diagrams, illustrations and any other document in any medium or tool capable of adding to the possible readings offered by this augmented digital version of the text. As could be expected with the open access nature of our series, it has had an excellent visibility for books published in French by a small Francophone university press in Quebec. [The latest usage statistics will be shared during the presentation.] The ongoing business model and next steps are however more of a mixed bag, as will be discussed in conclusion. "
	},
	{
		"id": 100,
		"title": "Digital Humanities, Knowledge Complexity and the Six ‘Aporias’ of Digital Research",
		"authors": [
			"Edmond, Jennifer C",
			"Lehmann, Jörg",
			"Priddy, Mike"
		],
		"body": " The idea that Digital Humanities practitioners might provide a translational capacity within and between the arts, humanities, information and computer science, easing collaboration between these disciplines and enhancing shared results, is not a new one: in fact, there is a long tradition of conceptualising at least some digital humanists as intermediaries,translatorsor hybrid people. As the long-predicted mainstreaming of digital humanities and digital methods into arts and humanities research advances, we might expect this transformation of the digital humanities from a disruptive to a supportive force to continue. Furthermore, while some within the academy certainly view the potential industrial relevance of the digital humanities with suspicion, there are also many voices from industry itself calling for the development of a more humanistic, critical dimension in the work of the ICT industry. While it may therefore seem timely to explore, as Liuhas called for, how the digital humanities might deliver a linchpin set of critical competencies for and reflections on the techno-social interface, how this cultural intervention into technology development might resonate with of the core tenets of DH remains unclear. This paper will introduce such a frame of reference by exploring the implications for digital humanities to be found in a corpus of 38 linked interviews about big data research. The project that developed this material, an EU-funded collaboration known as Knowledge Complexity, or KPLEX for short, explored in depth the perspectives of and attitudes toward big data found among computer scientists, collections holding institutions, and an interdisciplinary research community reaching from philosophy to fMRI studies. The project originally focussed on understanding unconscious bias in such research, but they also expose the depth of the misalignment between approaches to how knowledge is generated and validated across contributing disciplines. The data the project produced therefore offers much food for thought to those of us who identify as digital humanists, as it points toward a number of key barriers commonly faced and ideally negotiated within our hybrid research space. When viewed from the perspective of the KPLEX projects data, six distinct points of aporia arise, places where the interviewees explicitly or tacitly exposed gulfs in epistemic culture that are clearly at the heart of the tensions between disciplines as they seek to collaborate. These gulfs in goals and understanding echo the work of digital humanists, but also expand upon and throw into relief the underlying tensions in their research. While none of these findings presents, strictly speaking, an insoluble problem, the KPLEX interviews clearly illustrate the embeddedness of these challenges in the foundations of the contributing disciplines. This entanglement with professional identities and values raises them above the level of mere barriers, to a status where a more fundamental reconsideration of the scholarship produced within such collaborations may be required. In these fundamentals we may find future avenues for DH to grow in its own right, but also to expand and reconsider its potential impact. This paper will focus its exposition on the nature of and evidence for these gaps given in the interviews, which can be briefly described as follows: Language matters. In particular the interviews with computer scientists showed a resistance to discussing what certain key terms might mean or imply, a lack of precision that would draw criticism in a purely humanities context. This impulse weakens the potential for self-reflection in computer science but also greatly impedes successful interdisciplinary work, which may progress for extended periods on a falsely constructed sense of common understanding. While this obscurity had already been observed by Borgman, the KPLEX project results provide not only empirical evidence of the phenomenon, but also of its eventual negative consequences. Context matters. Datafication implies decontextualization, and this data/context-tradeoff is only rarely reflected in data-driven methodologies. But in humanistic enterprises context is indispensable: for an historian, for example, provenance is an all-important facet in the understanding of any source. But that which is a potentially harmful data modification for one community is a neutral, or in fact positive, process of data cleansing for another. Tools and standards are pharmaka , giving much but taking as well. In particular, information scientists can see how the availability of certain dominant toolsare both liberating and limiting in equal measure. Data and metadata standards can be perceived by humanists as handcuffs, limiting possible iterative adaptation of parameters, but the resulting variability and complexity stand in opposition to interoperability, aggregation, and scaling. Data without theory is as problematic as theories without evidence. A popular notion has been proposed that big data may have delivered us to the end of theory, but researchers actively working at the edges of big data can see clearly that this is not the case. That said, the lack of a critical frame merely pushes much of the transparency around complex phenomena into a black box with an authority based on a potentially flawed algorithm. The power structures of technology inhibit accommodation of analogue or hybrid narratives. Much of the humanistic source landscape is still measured in kilometres of shelving rather than terabytes of data. Because of this, digital humanities practices must be well-adapted to resisting the Matthew Effect, by which research becomes concentrated on the limited, potentially flawed data - this is not always the case outside of the humanities, however. Moreover, the struggle between archival thinking and computational thinking evidenced in the interviews and the conceit of routinisation raises questions of who will control cultural heritage knowledge in the future. Humanistic competences are not taught in conjunction with digital approaches. Critical, speculative, and hermeneutic thinking - the hallmarks of the humanities - are not taught alongside empirical methodologies, and critical approaches are not systematically implemented in computational studies -- Jonathon Morgans analysis of the Alt-Right Movement on Twitter, and the Digital Humanities Now Editors Choice Project Torn Apart / Separadosare two rare and enlightening exceptions. The paper will conclude with a series of reflections on how digital humanities researchers could move within their disciplines and beyond to become uniquely able to negotiate some of these critical conversations. It will also address crucial points DH can share with all interdisciplinary collaboration, such as shared data formats and structuring approaches, how misconceptions are surfaced and resolved, the place of self-reflection and methodological discussions, and the incommensurability of research questions and methodologies. In conclusion, it will offer recommendations for how each of the six aporias might be met and used to create a stronger digital humanities community and culture, fulfilling its potential as both a disruptive and productive force. "
	},
	{
		"id": 101,
		"title": "Named Entity Processing for Digital Humanities",
		"authors": [
			"Ehrmann, Maud",
			"Romanello, Matteo",
			"Clematide, Simon"
		],
		"body": " Context and Motivation Recognition and identification of real-world entities is at the core of virtually any text mining application. As a matter of fact, referential units such as names of persons, locations and organizations underlie the semantics of texts and guide their interpretation. Around since the seminal Message Understanding Conferenceevaluation cycle in the 1990s, named entity-related tasks have undergone major evolutions until now, from entity recognition and classification to entity disambiguation and linking. Besides the general domain of well-written newswire data, named entityprocessing is also applied on specific domains, particularly bio-medical, and on more noisy inputs such as speech transcriptionsand tweets. More recently, NE processing has also been called upon to contribute to the domain of digital humanities, where massive digitization of historical documents is producing huge amounts of texts. In the last few years, many cultural institutions have indeed engaged in large-scale digitization projects. Millions of images are being acquired and, when it comes to text, their content is transcribed, either manually via dedicated interfaces, or automatically via Optical Character Recognition. Beyond this great achievement in terms of document preservation and accessibility, the next crucial step is to provide an extensive and sophisticated access to the content of these textual digital resources. In this regard, information extraction techniques, and particularly NE extraction and linking, can certainly be regarded as among the first steps. De facto, NE processing tools are increasingly being used in the context of historical documents. Research activities in this domain target texts of different natureand different tasks. Experiments involve different time periods, focus on different domains, and use different typologies. This great diversity demonstrates how many and varied needs are, but makes performance comparison difficult. As per language technologies in general, it appears that the application of NE processing on historical texts poses new challenges. First, inputs can be extremely noisy, with errors which do not resemble tweet misspellings or speech transcription hesitations, for which adapted approaches have already been devised. Second, the language under study is mostly of earlier stage, which renders usual external and internal evidences ineffective. Further, beside historical VIPs, texts from the past contains rare entities which have undergone significant changesor do no longer exist, and for which adequate linguistic resources and knowledge bases are missing. Finally, archives and texts from the past are not as anglophone as in todays information society, making multilingual resources and processing capacities even more essential. Overall, and as already demonstrated by Vilain et al., the transfer of NE tools from one domain to another is not straightforward and performances of NE tools, initially developed for homogeneous texts of the immediate past, are affected when applied on historical material. Technically speaking, two strategies are usually followed: given an already existing system, application as it stands, or after adaptation/tuning, generally by training on new material. Besides, the recent development and availability of deep learning architectures for NE recognition opens up new promises, but these new approaches still need to be validated on cultural heritage texts. In this context, we are pleased to offer a tutorial on named entity processing on historical data which we hope will be beneficial for the DH community. Objective The objective of the tutorial is to provide the participants with essential knowledge with respect to a) NE processing in general and in DH, and b) how to apply NE recognition approaches. To this end, the session will be organized in two parts, as detailed in the synopsis below. Throughout the sessions, the audience will learn about the origins of named entity processing, the resources needed for their processing, the evaluation protocols, and the tools and algorithms used for their recognition, classification and disambiguation. Participants will also learn how to run an existing NER system and, more interestingly, how to build or adapt a system, by training it on historical materials. Material In the hands-on session we will make use of two datasets consisting of historical texts: 1. Quaero Datasetand 2. impresso dataset. Additionally, we will provide a list of alternative datasets, both historical and contemporary, that participants can decide to work with, in full respect of copyrights. Finally, participants are welcome to bring to the workshop their own datasets in order to apply the code and tools we will present to them. Technical set-up Hands-on material will be shared on GitHub and will include: Jupyter notebooks with explanations and code examples; if relevant, we will set up a multi user environmentin order to reduce system setup time during the tutorial; a bibliography on the topic; a list of of available open source academic and industrial tools; slides of the tutorial. Tutorial website: https://impresso.github.io/named-entity-tutorial-dh2019/ GitHub repository: https://github.com/impresso/named-entity-tutorial-dh2019 Impresso project is supported by the Swiss National Science Foundation under grant CR- SII5_173719. "
	},
	{
		"id": 102,
		"title": "Digital Documentation of Abandoned Heritage. The Case of Château de Noisy",
		"authors": [
			"Eisazadeh, Negin",
			"Bordalejo, Barbara"
		],
		"body": " Urban exploration or urbex is the exploration of human-made spaces that are generally inaccessible and hidden away from the general public. Recording the visit of these forgotten spaces through photography is a main component of this phenomenon which has resulted in a wealth of urban exploration photos and videos of abandoned sites. Urbex destinations are located worldwide and include a wide range of abandoned sites. Belgium has been a very popular destination for urban explorers and Château de Noisy, a neo-gothic castle in Belgium dating back to the 19 th century was a very famous destination which was demolished in 2017. There is a rich collection of urbex materials on this building which urban explorers have shared through various online platforms, such as personal websites, Facebook, YouTube, and Flickr. The latter has become a significant repository of urban exploration photographs. Regardless of the social and political complexities of this phenomenon, urban exploration is intertwined with abandoned historic sites and in recent years the potential of urban exploration for preservation of heritage has been brought to the attention of academia. Considering that urban exploration is becoming increasingly popular, the importance and possible contribution of this activity and its records for research on abandoned heritage sites cannot be neglected. This research focuses on the documentation and information management of abandoned heritage sites and looks into the potentials of the rich collection of existing digital urbex resources for their preservation by exploring their content and new means of representation and engagement. The unique value of such iconographic data can be attributed to the fact that normally these abandoned sites are inaccessible to the general public. Hence these photos and videos can shed light on these unknown places, and with the right utilization can not only document and digitally preserve some aspects of the valuable heritage but also can bring public attention to heritage sites that may still be saved from deterioration and revived. To explore the potentials of urbex produced materials for heritage preservation, concentrating on the rich collection of urbex data of numerous abandoned sites, this research aims to gain insights into the urbex scene and its evolution. Moreover, focusing on Château de Noisy, considering that the prevalent methods of documentation of a historic site which require physical access and presence are not applicable, it aims to explore the potential of distant documentation by investigating the application of existing tools and software to create a new approach for the preservation of abandoned and even demolished heritage sites and their story. To reach these objectives, focusing on Flickr and using the Flickr API service, two Flickr Dataset are collected: One of general photos related to urban exploration on Flickrand another which includes the specific photos of Château de Noisy on Flickr. To collect, prepare, visualize, analyse, create and present the data for this study multiple tools and methods are employed: Python Scripting Language, Tableau Desktop, Voyant, ContextCaptureand WebStorm. Terminology of the urbex Flickr photo titles and visualizing the distribution of the urbex Flickr photos, led to interesting insights into the urbex scene. Furthermore, the collected and downloaded images of Château de Noisy from Flickr offer insights into this abandoned building carrying information on diverse aspects such as its function, materials, structure and context over the course of many years. Château de Noisy was demolished without being given the chance for detailed documentation through advanced in situ techniques. Using the ContextCapture software and a selection of the images and videos of the castle that were identified through retrieving the images of the Château de Noisy Flickr Dataset, a 3D mesh model of the building and its immediate context is created. This scalable 3D reconstructed model can allow a flexible interactive experience of the site and can be used to curate and create an immersive experience of the exterior of the building and its immediate context while providing additional heritage information. A digital reconstruction of the building and subsequent narration that builds upon this vessel can create an engaging and immersive experience for the public and digitally preserve the fairytale castle building that once stood in Celles. The experience of such distant documentation of Château de Noisy can also be implemented in other heritage sites which are demolished or inaccessible. For buildings that still exist, raising awareness of its current state and heritage values can lead to their potential preservation and revival. Figure 1. Image extract from the reconstructed 3D model of Château de Noisy "
	},
	{
		"id": 103,
		"title": "An “Open Lab?” The Electronic Textual Cultures Lab in the Evolving Digital Humanities Landscape",
		"authors": [
			"El Khatib, Randa",
			"Arbuckle, Alyssa",
			"Siemens, Ray",
			"Meneses, Luis"
		],
		"body": " The evolution of the digital, and its intersection with the traditional role of the humanities, has impacted academic and non-academic modes of communication, as well as research practices including collaboration, knowledge dissemination, and engagement. As the scholarly landscape evolves, so does the nature of the institutions, labs, centers, and other places and spaces of research, including those of digital humanities. Engaging with these transformations in knowledge creation, but also continuously expanding and evolving with them, is the Electronic Textual Cultures Laboratoryat the University of Victoria, Canada. This paper is based on the premise that there is a correlation between the developing knowledge landscape and the structure of an intellectual center, especially when it is committed to open values; the development of the former necessarily affects the structure of the latter, especially over time. The ETCL operates in both physical places and virtual spaces; on campus, the ETCL serves as an intellectual research center that facilitates on-campus DH community building and off-campus engagements and networking; it simultaneously operates in physical and virtual spaces through research, skills training, and community-oriented initiatives. While existing in both modes, however, we are conscious not to perpetuate the general criticism that often identifies enclosed places of knowledge production, especially in a university setting and located on campus, as mechanisms of exclusion of the public. While digital humanities has existed in some form or other under various titles such as humanities computing for some decades now, Matthew Kirschenbaum in What Is Digital Humanities and Whats It Doing in English Departments? identifies the 2001 debate about the title for A Companion to Digital Humanitiesas the inception of the term that amalgamated different modes of scholarly inquiry under digital humanities. The ETCL, founded in 2004, falls among the earliest waves of digital humanities labs, and has evolved and expanded in multiple directions in the 14 years since its inception. Earlier this year, select current and past members of the lab came together to formally reflect upon the developments and infrastructural changes of the ETCL. The models that we considered to define the infrastructure of the lab, as well as the mission, mandate, and all ETCL-related work, tie to our values—namely our dedication to: community-driven scholarship that recognizes collaborative models of knowledge sharing; open practices in digital research, production, and dissemination; the intellectual development and well-being of our communities; shared mentorship, accountability, and support, across multiple disciplines, professions, and groups; and inclusive and ethical practices, as outlined in the DHSI Statement on Ethics and Inclusion. These values reflect the ETCLs main direction over the last years — to understand and practice open social scholarship, which builds on evolving modes of knowledge creation and communication, and seeks to create and disseminate research and research technologies to a broad audience of specialists and active non-specialists in ways that are accessible and significant to a broad audience. These are expressed in the three main constituents of the lab: Implementing New Knowledge EnvironmentsPartnership, the Digital Humanities Summer Institute, and the Canadian Social Knowledge Institute, that correspond to research, skills training, and community-oriented initiatives, respectively. In our paper, we consider the evolving landscape at the intersection of the digital and the humanities, specifically with reference to the lab place and space of the ETCL. Our research questions are the following: how does a DH lab that is primarily meant as a place and space for creating, enabling, and exploring open social scholarship, put into practice this goal? What type of lab infrastructure model, such as lab as incubator versus lab as branching tree structure, can best facilitate open social scholarship and support ETCLs projects, present and future? In this paper, our engagement with DH lab infrastructure operates on two levels: 1) lab infrastructure in physical and virtual settings to correspond to and reflect the evolving knowledge landscape and 2) lab practices that engage open social scholarship in physical place and virtual space, across research, teaching and service. The ETCL is a digital humanities research lab currently led by Dr. Ray Siemens as Director, Alyssa Arbuckle as Associate Director, Randa El Khatib as Assistant Director, and Luis Meneses as Assistant Director. The lab serves as an intellectual centre for the activities of ~20 local faculty, staff, students, and visiting scholars. Through a series of highly collaborative relationships, the ETCLs international community comprises over 300 researchers. The ETCL welcomes more than 800 students per year through their organization of the DHSI, and will be hosting its 19th annual training institute in the summer of 2019. The lab also supports the activities of the multidisciplinary INKE Partnership, which has involved over 42 researchers and consultants, 53 graduate research assistants, 4 staff members, 19 postdoctoral fellows, and 30 partners and associates. C-SKI actively engages issues related to networked open social scholarship. Representing, coordinating, and supporting the work of INKE, C-SKI activities include awareness raising, knowledge mobilization, training, public engagement, scholarly communication, and pertinent research and development on local, national, and international levels. The two main models that are pertinent to the ETCL structure are lab as incubator and lab as tree — to reflect a structure that is at once expanding and dynamic, but that stands on solid ground to provide steady support for expansion. Lab as incubator reflects one of our central values: a nourishing environment. The incubator metaphor also reflects the idea that a lab should be a positive space for growth — in our case, one that has facilitated INKE, DHSI, and C-SKI to develop into their current forms. The second model — lab as tree — represents something living, that grows, and at once serves as foundation and support for new growth. A tree also depends on communication between all of its constituent parts, and more accurately reflects the sub-branches that grow out of, and are the fruits of, the separate branches. Additionally, the tree as a strategy to aid thinking exists in many disciplines, and is often used in computing. In terms of practicing open social scholarship in the lab place and space, we launched a number of initiatives over recent years. With respect to place, the ETCL is in its third year of hosting the Open Knowledge Practicum Program, consisting of four-month fellowships that support projects proposed by university affiliates and members of the community to be carried out in the lab, which offers some local support. Fellows contribute to Wikipedia and publish their projects in online, public venues, and are also involved in the day-to-day lab life and events on and off campus. The practicum is meant to open ETCL doors to the rest of the university and the larger local community. A global community is also involved in the place of the lab annually at DHSI by spending two weeks on campus for the duration of the institute. Many attendees are awarded significant tuition scholarships by DHSI in order to offset the cost of participating in a training intensive. In addition, an open social scholarship course stream seeks to facilitate further knowledge development in this area; so far, DHSI has hosted ten courses in this stream annually, including courses related to open access, public humanities, feminist DH, accessibility in digital environments, queer DH, and other pertinent topics in the contemporary DH landscape. Other forms of community building include speaker series that span practical and theoretical topics related to digital scholarship in interdisciplinary settings, such as the Nuts & Bolts and the Digital Scholarship on Tap speaker series that are open to university and community members. The space of the lab has been engaging open social scholarship in numerous ways as well. For example, the annual Open Scholarship Awards for emerging and established scholars in any institution globally recognize contributions to open scholarship through projects or publications. This award is also meant to more appropriately recognize work related to open scholarship in a present scholarly framework where many institutions are yet to formally acknowledge this type of scholarly work. Additionally, the ETCL has authored three bibliographies that engage with the topic of social knowledge creation and open social scholarship. A virtual space for Canadian humanities and social science researchers to connect is the Canadian HSS Commons, an INKE / ETCL project inspired by the Modern Language Association Humanities Commons platform. The Canadian HSS Commons provides a platform that encourages a culture of sharing, accessing, re-purposing, and developing scholarly data, tools, and resources, thereby aiming to provide an open platform for virtual collaboration for scholars working in different institutions. Through its structure and initiatives the ETCL engages, facilitates, and promotes cross-community digital initiatives in local and virtual contexts by cultivating the practices and values of open scholarship. Arbuckle, A., Belojevic, N., Hiebert, M., and Siemens, R. G., with Wong, S., Siemens, D., Christie, A., Saklofske, J., Sayers, J., and the INKE & ETCL Research Groups.. Social knowledge creation: three annotated bibliographies. Scholarly and Research Communication, 5: n.p. doi:10.22230/src.2014v5n2a150. Arbuckle, A., Belojevic, N., El Hajj, T., El Khatib, R., Seatter, L., and Siemens, R. G., with Christie, A., Hiebert, M., Saklofske, J., Sayers, J., Siemens, D., Wong, S., and the INKE and ETCL Research Groups.. An annotated bibliography of social knowledge creation. In A. Arbuckle, A. Mauro, and D. Powell, Social Knowledge Creation in the Humanities Volume I, Arizona: Iter Press, pp. 29–264. Digital Humanities Summer Institute. 2016. DHSI statement of ethics and inclusion. http://www.dhsi.org/content/2016Content%20/events/00-2016EthicsInclusion.php. Electronic Textual Cultures Laboratory. n.d. Open knowledge practicum program. Accessed May 1, 2019. https://etcl.uvic.ca/okp/. El Khatib, R., Seatter, L., El-Hajj, T., and Leibel,C., with Arbuckle, A., Siemens, R.G., and the ETCL Research Group. Under development. Open social scholarship annotated bibliography. Kirschenbaum, M. 2010. What is digital humanities and whats it doing in English departments? ADE Bulletin, 150: 55-61. Schreibman, S., Siemens, R.G., and Unsworth, J.. 2004. A companion to digital humanities. Oxford: Blackwell. "
	},
	{
		"id": 104,
		"title": "Sublime Complexities and Extensive Possibilities: Strategies for Building an Academic Virtual Reality System",
		"authors": [
			"Endres, Bill",
			"Cook, Matthew",
			"Grime, John"
		],
		"body": " Dynamic encounters with three-dimensional assets make virtual realityan attractive medium for innovations in teaching and research. One of VRs most compelling advantages is an immersive experience: researchers and the public are no longer separated from digital artifacts; instead, they share a space with them. Such experiences engage the whole body. They provide a 360° event, eliminating the restrictions of computer screens. For the classroom and individual researchers, cost has previously hindered widespread adoption of VR in classrooms and by individual researchers. Fortunately, prices have fallen as technical solutions have evolved. Another obstacle, perhaps more complex and profound, is building useful academic VR systems. VR platforms are not equally capable; different systems tend to facilitate radically different actions and engagements. Tools, features, and types of engagements need identified, imagined, and built. Our presentation explores approaches to these system-design challenges while building the Oklahoma Virtual Academic Laboratory, a scholar-oriented, human-centered VR system. OVAL is the first generation of general-purpose, multiuser academic VR systems, and is free to download and use. Our first speaker, Matt Cook, explains the importance of collaborators across campus and institutions when designing and testing VR systems like OVAL. External to the University of Oklahoma, OVAL has hosted a range of multi-campus virtual tours, including early archaic caves in Arizona, Syrian ruins at Palmyra, and 3D-scannedsea turtles from around the world. Internally, OVAL has been used across a range of academic disciplines to test the pedagogical impact of immersive visualization. Each of these unique implementations represents an iterative hardware and software design process, ultimately providing todays students the means to quickly engage with complex 3D data in a way that preserves embodied interfacing and naturalistic visual depth-cues. Matt discusses the motivations, hardware & software considerations, usability testing, and documented pedagogical impact of OVAL across a range of associated academic disciplines. Importantly, specific disciplinary needs preclude a definitive version of the OVAL software and make designing a VR system an intricate puzzle. Different disciplines require different types of engagements and functionality, and some applications demand stringent preservation practices. For architecture, as part of the creative process, student needs to move through the structures they design, the experience revealing previously overlooked design flaws associated with accessibility, layout, and scale. For biology, students studying various protein molecules need to analyze VR content from the outside, turning and magnifying it, examining its features, and recording their analysis as a distributable video output. Beyond presenting the history of this open access VR system, then, Matt discusses how sorting through these divergent disciplinary considerations results in a more robust VR system with numerous academic applications. He also discusses human subjects testing and its need to refine and validate design features, as well as metrics and instrumentation useful to understand and document the value of VR in the classroom. Our second speaker, Bill Endres, from the English department, specializes in medieval manuscripts. With funding from the OU Humanities Forum, Bill built a 2-person travelling VR workstation. He has loaded 3D models of the eighth-century St Chad Gospels into it, including multispectral and post-processed renderings for recovered content. For comparing artistic techniques, he includes metal work, Pictish stone carvings, and related pages from other illuminated manuscripts. Bill discusses how the VR workstation allows him to engage experts and the public to test features and imagine new possibilities for VR. During the presentation, Bill will quickly demonstrate the traveling VR workstation and make it available throughout the conference. Before DH 2019, Bill will have presented the workstation at the Medieval Academys Annual Meeting, International Congress on Medieval Studies, and at the University of Glasgow. Medieval studies has numerous artifacts that benefits from encounters in VR, including manuscripts, mappae mundi , swords, tapestries, cathedrals, and stone crosses. Bill discusses the simple benefit of having VRs 360° field of visual. For example, mappae mundi are quite large. On a computer screen, it is impossible to explore details without losing track of the whole. VRs 360° visual field eliminates this problem. Because such possibilities regularly hinge on the human element rather than the technical, Bill discusses the complexities of human sensory experience. Human sensory experience is anything but simple. Rather than the popular notion of the Aristotelian five senses, neuroscientists have determined that humans have twenty-two to thirty-three senses, depending on how specific each sense is defined. For example, the sense of proprioception allows someone to sense where the parts of their body are, necessary input for performing simple tasks. It is the sense evaluated in the common sobriety test: with eyes closed, touch the tip of your nose. Proprioception provides significant information when encountering physical objects, such as a sense of size, as it relates to the human body. Size offers clues about artifacts like a manuscript, signifying whether it was meant for display, study, or travel. When examining high-resolution photographs on a computer, proprioception provides information about the body in relations to mouse and screen, not to the manuscript. But human sensory experience has a further complexity: it is constructed. For example, when sitting on an airplane, if you glimpse the walkway being withdrawn, you might experience the airplane backing away. In reality, the plane is still. The constuctedness of sensory experience allows for two possibilities. One, to translate sensory experience into VR, ratios and ratios of different sensory data can be used. For example, cold makes an object feel more rigid. This reflects learned experiences: cold generally makes an object stiffen. Therefore, to reproduce the experience of stiffness when turning a page of parchment, rather than manipulating pressure, manipulating temperature might prove more advantageous. Two, sensory experience can be generated in alternative ways. Researchers at the University of Bristol have invented a device that uses ultrasound to generate geometric shapes mid-air, which can be felt. Ultrasound shows promise for providing haptic experiences of parchment, such as feeling its contours and layered pigments. Bill discusses work with Julie Williamson at the University of Glasgow on this possibility. Our final speaker, John Grime, provides further disciplinary range to the team. John has a PhD in physics and chemistry. His background includes developing algorithms to explore biomedical phenomenon. John develops the platform architecture for OVAL, and he discusses some of the technical underpinnings for building different multidisciplinary tools from the same base code of the program. But thinking across disciplines is likewise highly productive. For example, in OVAL we are designing a feature that allows someone to experience the sky and its effects on lighting at Neolithic sites; such control over lighting is also important for an advanced imaging technique called reflectance transformation imaging. RTI software generates a single file from a series of photographs taken with different directional lighting. The file allows control over lighting to reveal surface details. Whether for a Neolithic site or RTI, code to control lighting for one contributes to code for the other. Furthermore, digitally grounded sensory experiences are not limited by the rules of sensory input in the physical world. John discusses one of the latest features in OVAL: Janus, which dynamically warps 360° of visual content to fit within the field-of-view of VR headsets. This allows users to analyze their environment or collection of artifacts as if their vision extends behind their heads. From its earliest days, the digital humanitieshave been collaborative. Nowhere is this more evident or important than designing VR. Sorting out how humans construct experience and how to reconstruct it in dynamic ways in VR is complex and challenging. However, as with all things complex and challenging, there are extensive rewards. VR provides unprecedented ways to engage in research and teaching by presenting content in a 360° environment that enables body-centered interactions and expanded representational characteristics for objects. It teaches us about ourselves, our artifacts, and our world. As with other research in the digital humanities, designing and building VR invites us to understand human experience more fully. "
	},
	{
		"id": 105,
		"title": "Visualizing A Prosopographical Study Of The Young Turk Elites: Using Data Mining, Network Clusters And Spatial Mapping",
		"authors": [
			"Erol, Emre",
			"Arın, İnanç",
			"Öztürk, Selman Bilgehan",
			"Ulusoy, Meryem Nagehan"
		],
		"body": " This poster presentation aims to visualize the output of a research project that seeks to analyze biographic data about the members of a distinct group of late-Ottoman / early-Republican elites, the Young Turks, who served in the parliament, in order to better understand patterns of relationship and activity among the various networks of these political elites whose roles were very significant in the making of modern Turkey. It seeks to discover the significance of certain biographic detailsin the political careers of this historically significant generation of elites. The poster is based on the primary authors collaborative research project that aims to create a digital database and employ digital humanities tools to interpret the biographic data in question, which would then constitute a basis for a prosopographical research. The project brings together three humanities scholars and a computer scientist who is consulted for the uses of data mining and digital visualization techniques throughout the project. The group under scrutiny in this project consists of the members of the Turkish Great National Assemblysfive legislative periodswhose extensive biographical information is published as a collected volume in 2010. The MPs in this period, most of whom also belong to the group referred to as the Young Turks, are of great significance in the process of nation-state building in modern Turkey. Most of them were initially members of the constitutionalist opposition against Abdulhamit II, the last absolutist Ottoman sultan. They successfully forced him to re-introduce the Ottoman constitution in 1908 and ushered an era of constitutionalism that proved to be very fragile and lasted only until the collapse of the Ottoman Empire at the end of the Great War. They played pivotal roles in the Second Constitutional era, the Great War, the Greco-Turkish War of 1919-1922 and the eventual creation of the Republic of Turkey. Finally, many ended up being the members of the GNA. Therefore, which particular network among the larger group referred to as the Young Turks repeatedly remained as decision makers and why throughout this process of transformation is an important question in the scholarship. This research brings in a new angle to this debate. This study focuses on the very first years of the new republic and the predominant roles of this elite network in the parliament of the new regime. Although there are strong biographical works on some well-known individuals of this prominent network, and some work on the collective biographical information of these elites, there is no large-scale prosopographical research about all of the parliamentarian elites of this period employing this novel approach informed by the use of digital humanities. As clearly demonstrated by previous scholarship, better understanding the patterns of relationship among the members of the network would help analyze its evolution to a smaller and closely-knit single-party network and also their nation-state building policies that uninterruptedly shaped modern Turkey until 1950. The idea behind the prosopographical approach is to first identify common biographic elements / characteristics / features among the members of the elite networks in question, categorize them and then to build hypotheses about the meaning of commonalities and patterns. The research builds its analysis on the data of some 2200 individuals over five legislative periods of the GNA. Each individuals data is classified under 17 categories. These categories include things like place of birth, marital status, educational institutions, total days of service, location of representation, professions and membership to various organizations of political significance etc. Following the conversion of these categories into a digital dataset, the relationship between various data are studied with the consultation of the expert on data mining techniques in order to make assessments about existence of clusters, nature of correlation/relation between features in different categories, the strengths of certain relationshipsand the correlation between certain features and the length of the political careers. This is achieved through the results of association rule mining that aims to measure confidence metric and support metric for each prosopographical feature in the data set. The results are then considered to see if they comply with generalizations in the existing literature / state of the art with regards to the study of this political elite network. The goal of the poster presentation is to present the patterns found in the prosopographical research both visuallyand statistically for peer feedback and sharing of the results. The poster presentation aims to make a point about the potential of digital humanities tools in the field of Ottoman / Turkish history and in the study of political networks in world history. The structure of the supervisors project complements this aim and also explicitly discusses and presents its conclusions. "
	},
	{
		"id": 106,
		"title": "Scholarly Multimedia Editions for Theatre Studies",
		"authors": [
			"Escobar Varela, Miguel",
			"Arps, Bernard"
		],
		"body": " Theatre constitutes itself through disappearanceand its ephemerality poses methodological problems for researches. Before the twentieth century, most theatre scholarship focused exclusively on texts, whether they were produced before or after a performance. This is true for many theatre traditions around the world. A textual model of theatre has serious limitations, as many of the social and improvised aspects of performance are rarely reflected in the texts. Audiovisual documents constitute betterrecords, but they have yet to be adopted as authoritative critical editions in theatre studies. Several online platforms offer full length recordings of key theatre performances, but they dont usually include the level of detailed annotation found in literary editions, where individual words or phrases are annotated to report their genesis, elucidate interpretations and trace variations across versions. Audiovisual theatre resources are often accompanied by interviews with performers or introductory notes, but there are no standard formats to annotate specific moments in a performanceand explain their significance within larger historical and cultural contexts. A scholarly infrastructure for the critical annotation of audiovisual documents has yet to emerge, even though relevant resources and technologies exist. We suggest that a digital philology of performance can be used to imagine new formats for scholarly analysis and communication, at the intersection of theatre studies and digital humanities. Why philology? Conventionally, philology has been associated with the study of literary material and the production of textual editions. However, the principles of philology can be used to interpret all aspects of a theatre performance: the audiovisual, social, and kinesthetic aspects of a performance can all benefit from a philological perspective. Theatre studies tends to be presentist, placing emphasis on novelty rather than tradition. A philological perspective offers a principled method to study the historical layering of a performance, countering this narrow focus on the present. There are many ways in which a text-based philological edition of a performance can document the emergent, interactive and multimedia aspects of a performance, by using notational conventions to represent vocal parameters, tinkering with the spatial arrangement of text on a page, and using extensive notes to describe emergent and interactive aspects of a performance. However, the potential of philological editions can be more fully realized in digital editions that can combine audiovisual sources with careful philological attention. There are calls for born-digital scholarship in performance studiesin response to impressive growth of digital archives that offer full-length recordings of performances around the world. However, the authoring platforms suggested by Meeare not sufficiently malleable to accomplish the level of critical attention required by a scholarly, multimedia edition of performance. For example, it is important for scholars to link specific sections of audiovisual media to textual transcription and translations, in ways that transcend subtitles. These different media should all be amenable to meticulous cross-reference and annotation in ways that are sustainable, findable and reusable. There is no straightforward way to achieve these objectives with most available tools. How does a digital edition of a theatre performance look? What should it seek to achieve? Textual editions are standard critical objects that have benefited from a long history of continuous experimentation in both print and born-digital formats. There is an extensive corpus of influential digital editions and an extensive literature that explores how digital editions modify and continue traditions of textual editing. But this level of experimentation and theoretical discussion has yet to be extended to multimedia editions in theatre studies. To sketch a prototype for such scholarly, multimedia editions, the present authors embarked on a collaborative journey of creativity and discussion. Both authors have an interest in the Javanese tradition of wayang kulit. A has worked as a scholar of Javanese language and culture for more than three decades; B is an early-career digital humanities scholar and web developer. In 2016, A published a philological, annotated translation of the work of an influential wayang kulit artist, based on the recording of a performance. The first version of this translation was published in book format. A and B are currently collaborating on an interactive, multimedia version of this translation. The development of a digital portal for this purpose is not just a matter of adding audiovisual materials but a dialogical experimentation with the format and possibilities of a digital philology of performance. Conceptualizing multimedia editions Spatzsuggests that video can document several aspects of performance, such as training. Although he refers to these videos as editions, it is unclear how they constitute scholarly interventions. As Sahlenotes, an edition without additional material that makes the document understandable or accessible is just a facsimile or an item in an archive. A critical attitude is required to determine what additional materials are required, and how they should be included. Their inclusion should follow rules derived from the relevant scholarly context, and these rules should be transparently and rigorously applied. An example from As print edition is that the symbol • indicates that the dhalangknocks a mallet against a wooden box. The specific sequence of such knocks is of great significance to a performance: it might constitute a cue to the gamelan musicians or indicate that a different personage is speaking, while also contributing to the aural aesthetic of the performance. The transcription of these sounds is surrounded by explanations, and linked to detailed notes. For example: [T]he dhalang raps the puppet chest to signal an accelerando and sforzando in the gamelan. ⓐ At the appropriate point in the structure of the piece he raps the pattern •• • as a cue to the gamelan to play slowly and pianissimo. In the print version, the symbols substitute for the experience of listening to the actual sounds of these rhythmical pattern. In the multimedia version, the passage above is time-linked to the recording. The user can play the recording, and the appropriate segment of the transcript will be highlighted in a different color. The user can also click on any portion of the transcript to navigate trough the audiovisual recording. This description is no longer a stand-in for an absent sound, but an interpretive scholarly layer. People who are not familiar with the tradition might not be able to identify the •• • pattern just by listening to the recording. Thus, the co-presence of audio and annotation, linked through time-based playback directs the attention of the users, making the material more accessible, understandable and usable for future research. This example shows that even the simplest inclusion of audiovisual material is never just an appendage. The audiovisual material changes the function and potential of scholarly annotation. We are at the early stages of discovering the full implications of linked transcripts, annotations and audiovisual documents. Besides producing a specific web portal for this wayang kulit performance, we are documenting our process and producing an open-source software package that can be adapted by other scholars to tackle the problems a performance philology poses for other theatrical traditions. We aim to develop tools that are usable by theatre scholarsin ways that are citable, reusable and sustainable. The transcripts, translations and annotations of our edition are all TEI-complaint and we are working with both an academic publisher and a digital archive to preserve our edition and to manage its metadata records. We are also committed to making our materials available as data: this will enable the perusal of the materials online through customizable portals, as well as their eventual integration within computational, data-driven research projects. We believe that more collaborative work on this area will open new avenues for the digital transformation of theatre scholarship. Figure 1. As the recording plays, the appropriate section of the transcript is highlighted. "
	},
	{
		"id": 107,
		"title": "Towards a Critical Approach to Digitally-Mediated Discursive Practices of Gender-Based Hostility",
		"authors": [
			"Esposito, Eleonora"
		],
		"body": " As an interactive, pluri-directional and multimodal realm, the cybersphere is characterized by the incessant production and sharing of information content, with an ever-growing number of bottom-up discourse formations and disseminations. One of the most significant and complex drawbacks of this unprecedented proliferation of user-generated content, and the so-called democratization of access to symbolic recourses, is the acutely increasing incidence of online hate or cyberhate. Hostility is a complex social, cultural and psychological phenomenon: motives behind peoples hate are various, different and often obscure, and the fluid and widely unregulated nature of the cybersphere seems to have added to further complicate an already thorny matter. One of the key scholarly assumptions on the issue is that social media affordances seem to act as a force multiplier, both in terms of sheer quantity and vitriolic quality of interactions. Both social psychologists and criminologists have attempted at sketching haters underlying motivations and strategies by drawing on psychological theories and research. These studies have provided useful insights into how some features inherent to Computer Mediated Communication - e.g. perceived anonymityand physical separation- contribute to trigger social practices online - e.g. dishinibition and de-individuation, polarizationand mob dynamics. As a result, the recognition of strong psychological features in antisocial behaviours like hate speech is basically entrenched in the differences between face-to-face communication and online interactions. One of the dangers of relying on these scholarly interpretations is the relatively straightforward establishment of a cause-effect relation between the affordances of the participatory web and practices of hostility online, highlighting the role of the digital medium and downplaying socio-political structures and power hierarchies. This paper advocates a Social Media Critical Discourse Studiesapproach to online hostility. As a socially committed, problem-oriented, textually based, critical analysis of discourse, SM-CDS deals with discourse as its central object of analysis: it is not only interested in what happens in the media per se as a closed loop but also in how it may shape and influence the social and political sphere of our life worlds and vice versa. Such an approach would deliberately steer away from media determinist accounts as well as from universalist understandings of social media effect: communication is to be regarded as a human endeavour, irrespective of the sophistication of the medium used. Despite difficulties in demographic and geographic accounting of online communities, macro-contextual aspects, including materialities of sociocultural categorizationsare to be carefully taken into account and not to be distilled into a bland cybernetic metaphor. In particular, this paper focuses on gender as a source of hate in its own right which has not received sufficient institutional and academic attention. While the dangers and risks of the digital world are well acknowledged, we still lack a clear grasp of what it actually entails being a woman navigating the cyberspace, and which specific threats and troubles this journey can bring about. In approaching online gender-based hostility, we would always start from the assumption that any online form of gendered violence replicates and extends the gender and power relations that pre-exist digital communications technologies and vice versa. Digital misogyny is to be regarded as a purposeful discursive strategy to maintain a gendered asymmetry of power by threatening, discrediting and ultimately silencing women in a way that it has historically regimented. The domain of online misogyny as a digital discursive practice would be, therefore, conceptualized at the intersection of digital media scholarship, discourse theorization and critical feminist explication, with audacious interdisciplinarityand substantial intersectionalityrepresenting the epistemic way forward. This paper presents a number of epistemological considerations in relation to digital media, discourses of hostility and critique, grounded in the results of a multi-lingual pilot study conducted in the context of a project funded by the European Commission. The study investigates phenomena of online misogyny, against highly visible, political and institutional female figures in Europe. More specifically, it maps the multimodal discursive strategies of online hate against women in the public sphere by collecting and analysing a corpus of user-generated comments on Social Networking Sitesfrom three different linguistic landscapes and political cultures in Europe, namely Italy, Spain, and the U.K. In order to locate abundant relevant foci of data, the preliminary phase has been characterized by a digital ethnographic stance. Criteria of inclusion encompassed: degree of digital presence, critical moments or events of particular relevance or visibility, as well as overall number of views, likes and comments,, to include instantiations of: a) gender-based hate speech; b) rape threats; and c) image-based harassment. The multimodal nature of data has called for an integrated methodology, encompassing: 1) Corpus Linguistics tools, for a quantitative identification of linguistic patterns; 2) Critical Discourse Analysis, for a close qualitative and critical analysis mapping the vast number of discursive strategies and rhetorical devices through which online misogyny is realized, in four different heuristic levels of context. 3) Visual Content Analysis, for a multimodal analysis of the videos containing image-based harassment by means of the four-layered framework proposed by Rodriguez and Dimitrova. Emerging results in all the three linguistic and socio-political contexts in exam are showing: 1) From a more micro linguistic perspective, the different degrees of formulaicity and creativity of the multimodal discursive strategies of online hate. This is in line with the already demonstrated algebraic and tediously predictable qualityof digital misogyny, but also highlights digital creativity as an integral part of mob dynamics and mentality online, often resulting in an ever-escalating competition to produce the most abusive content. 2) From a more macro social perspective, the profoundly intersectional nature of online gender-based hostility. In particular, the analysis points toward the interaction of mutual and intertwined factors both triggering and stoking hate such as: class, race, gender identity or behaviour, age as well as feminist activism. These results contribute to a more in-depth understanding of gender-based hostility against women in politics as an extremely multi-faceted and multi-layered phenomenon, where gender is not the only factor at play. They also call for the further integration and development of the concept of Digital Intersectionality, which would allow to further question the organization of social relations embedded in digital technologies and foster a clearer understanding of how power relations are organized through them. "
	},
	{
		"id": 108,
		"title": "From Reductionism to Complexity: A Digital Corpus for SufismUsing a NoSQL database for studying the emergence of complexity",
		"authors": [
			"Farrell, Jeremy"
		],
		"body": " Efforts to measure the organization of human activity in terms of complexity have a long history. The recent proliferation of computational tools has accelerated the creation of rigorous means of modeling and assessing the development of human collectivities , with particular reference to the emergence of social complexity. This paper argues that r ecent studies of historical social complexity have neglected to account for the emergence of social complexity because of a reliance on a reductionist approach. In place of the reductionist approach , we put forward a prototype of a NoSQL database architecture for studying the emergence of populated with data from historical Arabo-Islamic sources. In the face of pervasive critiques of narrativist historiographical techniques in historical scholarship, computationally-oriented approaches to human collectivities have turned to more scientific approaches. Frequently, this involves the use of a reductionist method, breaking down various non-narrative data related to a collective into various constituent parts under the belief that each of these can be scientifically described and analyzed. R ecent studies by Preiser-Kappelerand Turchin et alexemplify this approach. The former paper applies statistical tests to a single, node-level variableto model the long-term resiliency of transport networks in the pre-modern Roman and Chinese empires, whereas the latter proposes nine complexity characteristics that are coded into an RDFdatabase, the analysis of which reveals a single dimension that accounts for measures of complexity across a variety of civilizations. E ach of these studies makes a successful, independent case for the evolution of complex social structures; the use of a RDF database by Turchin et al moreover significantly increases the explanatory value of the convincing statistical analyses put forward by Preiser-Kappeler. However, the reductionist approach adopted in each assumes , rather than demonstrat es , the prior emergence of these complex features. Furthermore, both studies show how the limitations of some datasets limit the explanatory power of the reductionist approach in the first place. For instance, Preiser-Kappelers study also assumes long term stability of core elements of the Roman and Chinese empires, which precludes an investigation into the emergence of such core elements in the first place. Similarly, T urchin et al s limitation of analysis to the appearance of politically centralized societies, stands in stark contrast to a central tent of theories of complex systems which states that the are composed of networks of components with no [mechanism of] central control. The task of describing the actual emergence of complexity within human collectives thus seems to require the following elements :time-sensitive data related toentities that are substantially independent from a central control mechanism, anda sufficiently robust data storage system, such as a RDF database. We propose that the above-mentioned conditions can be met using data from a historical Islamic religious movement known as Sufism, which first appeared in Iraq during the 9 th century. No study of this movement has given a conclusive answer as to the emergence of Sufism, largely because previous research has failed to analyze a key feature of early literature written by Sufis: the pathway of transmissionused to introduce narrative content. An example of this form would appear as follows: I heard [X] say he heard [Y] say that he heard [Z] say […]. Although there have been numerous attempts to undertake the analysis of abundant relational data found in non-Sufi Islamic sources, these data are either not publicly available, or exclude the narrative component of these data because of the limitations of relationaldatabases in which they were compiled. In this paper, I outline the architecture needed to leverage the complexity of the relational data in early isnad-based Sufi literature with as network data NoSQL database framework. Within this network, the breadth and depth of node level datacan vary widely with regards to: names, occupations, birth and death dates, destinations of travels, teachers, students, intellectual specialties, and matrices that record other figures opinions about the figure, to name only a few. On the edge level, features are more standard, and can include: place of transmission, date of transmission, method of transmission, as well as the narrative content of the saying that is transmitted via the isnad. At present, the architecture of these data is being prototyped using the data from one early Sufi workon the MongoDB NoSQL database. This decision was based on both the flexibility of the JSON scheme that is native to MongoDB, as well as the ability to easily transfer this data onto a graph database scheme, such as Neo4J. "
	},
	{
		"id": 109,
		"title": "Programmable Corpora: Introducing DraCor, an Infrastructure for the Research on European Drama",
		"authors": [
			"Fischer, Frank",
			"Börner, Ingo",
			"Göbel, Mathias",
			"Hechtl, Angelika",
			"Kittel, Christopher",
			"Milling, Carsten",
			"Trilcke, Peer"
		],
		"body": " Introduction Although there have been some infrastructural developments of late, the main modus operandi in digital literary studies is still to apply a certain research method to an ephemeral corpus. In a best-case scenario, the results are somehow reproducible, in a worst-case scenario they are not reproducible at all. At best, there is an openly accessible corpus in a standard format such as TEI, another markup language, or at least TXT. At worst, the corpus is not even accessible, i.e., the research results cannot be questioned. However, there are signs that this is slowly changing. Some projects provide interfaces that allow for multiple ways of access to corpora. One of these projects is DraCor, an open platform for research ondrama, which will be introduced in this paper. DraCor transforms existing text collections into Programmable Corpora – a new term we bring into play with this talk. Building Blocks Vanilla Corpora Similar to the COST Action on European novels, the DraCor project seeks to establish a bundle of multilingual drama corpora encoded in basic TEI as basis for digital comparative studies. To date, the platform enables access to a Russian-languageand a German-language corpus of plays. Similar to Paul Fièvres collection Théâtre classique, these corpora are designed as vanilla corpora, which initially contain hardly any special markup beyond the necessary, but are freely available and can therefore be forked, enriched and expanded. To demonstrate that other corpora can be easily linked to the platform, we forked the Shakespeare Folger Corpus and the Swedish Dramawebben corpus and connected it, and all existing extraction and visualisation methods of the platform are readily applicable to the newly added corpora. Other corpora of dramatic texts are to follow; the only prerequisite is that they are encoded in TEI. The advantages of a freely available corpus hosted on GitHub are obvious. Not only can the corpus be cloned and loaded directly into an XML database like eXist-db. Using the SVN wrapper from GitHub, the entire corpus can also be downloaded directly, in its current state and without version history if this is not needed: svn export https://github.com/dracor-org/rusdracor/trunk/tei An openly accessible GitHub repository also means that pull requests for error correction are possible and welcome. XML Databaseand Frontend DraCor relies on eXist as XML database to process TEI files and to provide functions for researching the corpora. The frontend is built with React, it is responsive and easily extensible. However, the focus is not on the GUI, but on the API. API To come close to the ideal and the possibility of applying all methods to all texts in a simple manner, it takes more than open corpora. The article by Frank/Ivanovic advocates SPARQL endpoints. DraCoroffers such endpoint, but also features a rich general API documented and explained via Swagger. In a subarea of corpus philology, the digital scholarly editions, discussions about more proactive use of APIs have already begun, the Folger Digital Texts API may serve as an example. The advantage of a more modern solution like Swagger is that API queries can be executed live and directly and that the output can be controlled more precisely. A simple use-case scenario would look like this: using RStudio you can throw a quick glance into a corpus with just a few lines of code, maybe regarding the development of the number of characters in Russian drama between 1740 and 1940, stored in the metadata table. This table, which can be obtained in JSON or CSV format, is read into a Data.Table, whereupon the values of two columnscan simply be visualised via ggplot. Figure 1: Number of characters per play in chronological order. This very simple example is able to show the starting point of a decisive structural diversification of the Russian drama landscape. Pushkins historical drama Boris Godunov, result of his reading of Shakespeare, features speech acts of 79 characters, a number previously unthinkable in Russia drama. However, the possibilities are not limited to using ready-made API functions. New research ideas always create new needs for easily obtainable and reproducible data and metrics; the API can be extended accordingly, i.e., new research ideas can be implemented centrally in the API layer. This is made even easier by the fact that Apache Ant can be used to rebuild the entire development environment on your own system. In addition to structural data and metadata, full texts without markup can also be obtained, e.g., if methods such as stylometry or topic modelling are the purpose, i.e., methods that need a bag of words and do not require markup. All in all, the structure and documentation of open APIs makes it much easier to reproduce research results, which up to now has often been a time-consumingprocess. Shiny App An example of the versatility of the DraCor API is the Shiny App created by Ivan Pozdniakov. Shiny is a framework based on R, which makes it possible to display interactive visualisations in the browser. The DraCor Shiny App does just that, relying entirely on the DraCor API for data retrieval. Thus, visualisations of the current database can be used for teaching and research purposes, but also for easier data correction. Didactics The formalisation of literary texts, for example via markup, is not self-explanatory. Although the community can rely on some standards, every operationalisation depends on the actual research question. To give an example: if you would like to extract network data based on character interactions in a literary text, you would have dozens of different ways of doing this. This also applies to plays. In order to sharpen the senses for this in teaching, we developed the tool ezlinavisand integrated it into the DraCor toolchain. Network data can be extracted from literary texts manually, also to raise the awareness for the contingency of this process, an important preliminary step to the eventual step of operationalisation. In addition to an approach to the gamification of the process of correcting TEI-encoded corpora, we also developed a card game for teaching purposes in order to playfully train the understanding of network values. These didactic tools wrapped around the platform are an integral part of the whole project as they are based on the project data and operationalisations. While building the platform, it was important to recognise that data can take several forms and be equally important for research and teaching. Linked Open Data The TEI files contain GND [Integrated Authority File of the German National Library] and Wikidata identifiers for both authors and works. In this way, various data and facts that lie beyond ones own corpus work can be included. Something like an automatically created gallery of authors has a more illustrative character. But using the same identifiers, we can also determine if a corpus has a regional bias. Via Wikidata, we can easily display the distribution of the authors places of birth and death on a map. Similarly, the Wikidata ID of the plays can be used to find out where they were first performed, i.e., aspects of the performance history can be switched on, even though they are not the focus of the core project and based on data curated elsewhere. Infrastructure Instead of Rapid Prototyping Projects like DraCor seek to provide the digital literary studies with a reliable and extensible infrastructure so that the research community can focus on research questions. An important conclusion for us was that we would give up the further development of our all-in-one Python script collection dramavis, which we have been developing for four years now. From here on, we would rather devote our time to the API. Dramavis followed the idea of rapid prototyping and had to do all by itself, including the preprocessing of data, which is not untypical in the Digital Humanities. The code base has grown quite a bit in the meantime and its maintenance has become difficult and often enough led away from actual research questions. Outlook In allusion to the project ProgrammableWeb – which maintains a database of open APIs and whose slogan is: APIs, Mashups and the Web as Platform– we propose the term Programmable Corpora for research-oriented corpora providing an API. Programmable Corpora facilitate the implemention of research questions around corpora. It is to be expected that infrastructural efforts of this kind will pay off for the entire community with effects such as those listed by John Womersley in his presentation at the ICRI2018 conference in Vienna: a) dramatically increase scientific reach; b) address research questions of long duration requiring pooled effort; c) promote collaboration, interdisciplinarity, interaction. The are numerous ways to connect to Programmable Corpora, no matter if you dont want to code at all and only need a CSV file for Excel or LibreOffice Calc or a GEXF file for Gephi, if you want to research a corpus via its connections to the Linked Open Data cloud or just want to get specific data for your R or Python script without having to worry about the corpus and its maintenance and reproducibility. Programmable Corpora make it easier to decide on which level of the platform your own research process starts. "
	},
	{
		"id": 110,
		"title": "Towards An Epistemology Of “Gambiarra”: Technical Resignification In Brazil",
		"authors": [
			"Foletto, Leonardo Feltrin"
		],
		"body": " The techno-political manifestations that seek to breach the established order abound in the north or south of the globe. Tactics and strategiesare used by subaltern groups to express their opinions or deviant practices. In challenging socio-economic environments such practices can assume a wide variety of forms: for instance, by using the digital as both discourse and practice of subversion to established orders. In this article, we choose a single expression of such subversions to represent one of the ways through which the digital take part in everyday lives. The practice we discuss in this paper is what we call Gambiarra: a term applied to a myriad of improvisations, usually material and technical ones as a result of scarcities of all sorts. Gambiarras are normally the technical expression through which people overcome everyday obstacles from the most ordinary ones to the outmost complex environments. Rosasand Cliniodefine Gambiarra as a do it yourself a la brasileira, in which the technical limitations are overtaken through creativity in proposing innovative solutions. As a synonym for improvising in a popular culture realm, this is a inventive process of repossession, adaptation and transformation of available materials in an alternative design form, which allows the creation of improvised solutions for real demands. Widely used in the Brazilian quotidian, Gambiarra has a meaning in the daily lives of people that tactically adapt itsapparatuses in order to resist to daily-life problems of all sorts. As such, the paper is supported on two case studies. The first is Gambiarra Favela Tech Website: http://www.gambiarrafavelatech.org/ , an artistic residence held in July and December 2015 in the Maré Complex, in the northern area of Rio de Janeiro, through a partnership of the Olabi makerspace and the Favelas Observatory. The initiative brought together 12 young people from the local community to propose new usages for obsolete materials. Taking as its motto the improvisation and the inventiveness that transform realities, around 40 hours of workshops were provided in order to practice the usage of materials in line with gambiarra rationale: to take something that is used in a traditional way and to use it in another way, in a way that nobody would imagine as one of the young participants commented The first testimonial of the project summary video, available at: https://www.youtube.com/watch?time_continue=18&v=h54_A5fXk0o . Gambiarra Favela Techs proposal was anchored in three main aspects:developing environmental awareness, which indicates that better than producing something new and discarding another product in the environment would be to recycle non used products;losing the fear of opening the black boxes, by discovering how technical objects work through a playful � unable to handle picture here, no embed or link manner; andsevirismo, a brazilian expression that represents the science of dealing with what one has, - a term that can be seen as a synonym for gambiarra. Such project interacts with the history of the word gambiarra as a concrete example of abadly done improvisation – also typical of favela environments as already mentioned. The improvisations in this case used few resources, trash and unused objects in order to produce works of art that could relate to the favela community. By rearranging materials and giving alternative means to waste, bricolages and gambiarras are put forward as a way to produce art in a scarce context. The success of the project in that local context was, thus, a product of symbolic re-ordering of materials. The second case is Hacker Clubs activities in Raul Hacker Club,a group of people interested in using, re-using and sharing technology, learning, fun and culture in a collaborative and indiscriminate manner. Located in the northeast Brazilian coastal area, this hackerspace defines itself as an assemblage of different people managing a non-hierarchical space, without influences of public or private institutions. The name honors a well-known Brazilian singer-songwriter born in Bahia, Raul Seixas, whose songs praised, among other themes, the alternative ways of living. Some of the projects carried out by this group are the Criança Hacker Infant Hacker, in English , series of activities of education free of informatics and basic electronics for children; Data Laboratory for Citizenship Hacker, a collaborative space for research, work and discussion on scraping public data for journalistic purposes; among other diverse activities of learning and production of projects in the areas of electronics, free software, open data and hacker culture. The approach of the hacker culture - of which hackerspaces like Raul are some of the main representatives – has for many years included the idea of a gambiarra such as the creation of a network called Meta-Reciclagem Meta-Recycling, in English. : a movement that brings together hackers, students and artists who propose the deconstruction of technology for social transformation, which relates to a gambiarra rationale as an ideological practice of resistance to the dominant order. One of his projects, no longer in activity, was called the mutirão gambiarra Mutirão means a group of people that join efforts towards a specific goal in the form of a task-force. Also described as a communal work. and constituted as an editorial collective that articulates collaborative publications on themes such as creative appropriation of technologies, experimental digital culture and collaborative networks. The network has helped to popularize the term gambiology, a fusion of gambiarra with ideology in Portuguese, which can also be interpreted as science of gambiarra, a term that shifts the expression out of its pejorative meaning by entailing gambiarra as a localized practice of technological innovation with few resources. In hackerspaces, gambiarra is a quintessential form of a hacker culture We support our claims on the notion of hacker based on Colemanswritings about the term.  as if the term would entail the brazilian form of hacking materials, codes and ideas. As a typical hackerspace suggests, learning is based on collaboration in a hand-on modus operandi. Teaching, living and working with computers and electronics is an example of hackerspace that can be better understood by framing such practices as gambiarras. Every technology represents a cultural invention in the sense that it produces a world. It was our intention to shed light on practices that produce a world and, therefore, entail a form of getting to know the reality though the improvisation. It would be possible to argue that improvisation is not only related to materials and artifacts, but also ideas. We draw on Le Bretonas well to argue that the mind-body division that such assumption would suggest is not feasible in gambiarra terms - solely because even though our examples of gambiarras are technical and material applications of digital humanities, they are a result of ideation and creative improvisation that cannot properly work without planning. As such, we argue that applying gambiarras might evolve into an epistemology, a way of reaching reality through its manipulation. Gambiarra, therefore, is a combination of practices tailored to solve practical problems of an everyday life. Although not typically restrained to global-south areas, we argue that typically brazilian gambiarras resignify the material usage into new symbolic realms. Even though overcoming the myth of modernityis not a new argument in itself, the combination of such endeavour with the practices that shape everyday life in global-south regions was also the goal of our essay. Finally, as means to propose empirical continuations of this paper, the ethnographic and anthropological investigation of gambiarras in all sorts of forms and places is likely to make emerge a myriad of practices from the outskirts of the world, where the digital finds a way to exist despite its possible restrictions – socio-economic ones mostly. By showing the richness of gambiarras, we argue that it is possible to theorize digital humanities as mostly applied to the people that make use of it in its daily applications. "
	},
	{
		"id": 111,
		"title": "To Sign or not to Sign: Automated Generation of Annotation Slots for Sign Language Videos using Machine Learning",
		"authors": [
			"Fragkiadakis, Manolis"
		],
		"body": " Introduction Over the last years various corpus projects have started all over the world documenting sign languages. The purposes of such corpora focus primarily on the linguistic study of the languages as well as the preservation of the languages themselves. As Drew and Ney deduct, the processing and storing phase of these corpora require a textual representation of the signs. Although different notation systems have been created over the years, gloss notation seems the prevalent one. Instead of using an annotation system with components representing the main formal components of a sign, ID glosses are typically used. These consist of a uniquely identifying spoken language wordthat by definition refers to a particular sign form. During the annotation process the researcher has to determine the precise time a sign occurs and properly identify and gloss it. As a result, the annotation process is extremely labor intensive, but it is a condition for a reliable quantitative analysis of the sign language corpora. The focus of this project is the development of a tool for automatic annotation of sign occurrences in video corpora as a first step towards fully automatic annotation. This study presents a new approach to automatic annotation for sign languages using as little data for training as possible and taking advantage of a state-of-the-art pose estimation framework for a robust and unbiased tracking. Literature review Recent developments in the field of sign language recognition depict the advantages of machine and deep learning for tasks related to recognition and classification. However, they require a vast amount of data to be trained and they are bounded in the sign language they have been trained onAdditionally, approaches in automatic annotation for sign languages require manual annotation of the hands and body joints prior to the training process of the recognizer models. Furthermore, most studies apply skin color and motion detection algorithmsthat are prone to errors and possibly skin color bias. It is also often the case that in order to assist the hand tracking model, corpora are compiled using colored gloves for the subjectsor captured using Kinectmaking the result of such studies unusable in real-life conditions in the corpora. Pose estimation, as a technique to detect human figures in images and video, showed enormous improvement over the last years. OpenPoseis the state-of-the-art framework when it comes to accurately detect human body and hands keypoints. The model takes as input a color image or video and through a 2-branch multi-stage Convolutional Neural Network predicts the 2D locations of keypoints for each person in the image. This framework was chosen to be used in this study as it has been trained on the Multi-Personand COCO datasets making it exceptionally robust and fast. Methodology A data-set of 7805 frames in totalhas been compiled and labeled as signing or not signing. The dimensions of the frames were 352 by 288 pixels and were extracted from the Adamorobe and Berbey sign language corpora. These corpora portray an additional challenge as they are extremely noisy and low quality. Furthermore, they contain signing from one and two people at the same time. The original data-set was split into a training and testing set of 6150 and 1655 frames respectively. Using OpenPose, the positions of the hands, elbows, shoulders and head were extracted from each frame. The positions of the rest of the body joints were disregarded as most of the time they were out of the frame bounds. It is important to compare the performance of multiple different machine learning algorithms consistently. Thus, four different classification methods were used and optimized, namely: Support Vector Machines, Random Forests, Artificial Neural Networks and XGBoost. The majority of these algorithms have been extensively used in machine learning studies as well as in sign language applications. Performance was measured using the metric of Area Under the Receiver Operating Characteristics. Results All classifiers performed adequately well. However, the best AUC score was found in XGBoost. Figure 1 presents the AUROC curve after a 10-fold cross-validation. The Artificial Neural Network was found to perform sufficiently well. While the performance of the model is satisfactory, it is important to explore the features that contribute to the classification task. Figure 2 shows the importance of each feature as measured by the classifier. The result is reasonable as the position of the dominant handhas the highest importance on how the classifier weights the features. To account for multiple people signing in one frame, an extra module was added. The module creates bounding boxes around each person recognized by OpenPose, normalizes the positions of the body joints and runs the classifier. This process makes it possible to classify sign occurrences for multiple people in a frame irrespective of their positions. Once all the frames have been classified, the cleaning up and annotation phase starts. A sign occurrence is annotated only if at least 12 consecutive frames have been classified as signing frames. This way I account for the false positive errors. This sets the stage for the annotation step. By using the PyMpi python librarythe classifications are translated into annotations that can be imported directly to ELAN. Figure 4 shows the result of the overall output. AUROC curve of XGBoost after a 10-fold cross-validation. The importance of each feature based on XGBoost classifier. Recognition module in multiple people. Final output of the tool as seen in ELAN. Conclusion This is the first step towards fully automated sign language annotation. The results show that a frame-to-frame classification using XGBoost is a promising tool for the annotation of sign occurrences in a video. The significance of this study lies on the fact that the tool created can be easily adjusted and used in any kind of sign language corpus regardless of its quality, the sign language presented or the number of people in the video. Furthermore, one needs approximately only 4 minutes of annotated video in order to retrain the model making the process as easy as possible. Finally, the tool has the potential to be extended and used in gestural corpora as well. "
	},
	{
		"id": 112,
		"title": "Diagramming the Complexities of Historical Processes: From Ontology-based Modeling to Diagrammatic Reasoning",
		"authors": [
			"Frank, Ingo"
		],
		"body": " Introduction This paper presents ongoing foundational theoretical and practical work on the application of ontology-based modeling to represent and visualize the complexity of knowledge disseminated in historical narratives. In short, the new approach combines modeling informed by philosophical ontology and philosophy of history with semiotically founded visualization of historical processes in order to support historical understanding. The following quote from Munslowlends itself as an appealing summary of the character of historical narratives: In writing a history for the past we create a semiotic representation that encompasses reference to it, an explanation of it and a meaning for it. What role could information visualization tools play in this context? As Champagneremarks, [h]istorians occasionally use timelines, but many seem to regard such signs merely as ways of visually summarizing results that are presumably better expressed in prose. He challenges this view and argues that timelines could support the historian in gaining novel historical insights. The main cognitive funtion of timelines is according to Champagnethe logical conjunction by visual juxtaposition. Furthermore there is also the potential of abductive reasoning: Timelines, however, are more likely to surprise us, by showing us past events that we would have never otherwise considered chunking. Hence, in addition to historical scholarship expressed in regular prose, consulting diagrammatic signs can foster the discovery of patterns essential to a fuller understanding of the past. This is especially more likely if there are synchronoptic timelines showing historical events of different categories—i.e. not only political events, but also economic or cultural events etc.in their Synchronoptische Weltgeschichte.) For example, such parallel timelines could possibly be used as a tool to support periodization). Problem statement and objectives A visual historiographycan quite easily be done via the temporal, spatial, and thematic context of information about historical events, but without explicitly stated relations between events it is questionable how useful that could be in supporting historical research. The big advantage of that approach is of course, that one is able to represent the complexity of a historical subject, without having to fill out the gaps, or having to choose between different interpretations, but using an [information integration] architecture that places the subject in its contextas cited in Sabharwal). The problem with such a visual historiography is that it cannot support visual contextualization done by the historian during conceptualizing complex interrelations of historical events—including not only temporal, spatial, and thematic relations, but also causal relations, mereological, and constitutive relations of complexevents. Anyhow, explicit modeling of event structure and relations is necessary because without a more fine-grained representation of the structure and interrelations of events visualization tools are indeed limited to bare juxtaposition. Digital history demands information visualizations beyond simple timelines. Diagrammatic approaches for multiperspectival analysis and synopsis of historical sources are needed. There are rarely technical implementations and just a few theoretical approaches to the development of such tools for multiperspectival exploration of historical sources. See for example Druckerin Humanities Approaches to Graphical Display: At best, we need to take on the challenge of developing graphical expressions rooted in and appropriate to interpretative activity. Jänickeargues that the reason for this slow progress is the still deficient collaboration between researchers in information visualization and digital humanities. This brings us back to the announced modeling approach, because conceptualizing can be considered as modeling: the more schematic the conceptualization in a discipline, the more its practitoners are likely to engage with models rather than concepts. Historians construct concepts in order to understand historical events. The method for this kind of explanation is known as colligation—the construction of colligatory concepts—in philosophy of history. Walshdefines colligation as the procedure of explaining an event by tracing its intrinsic relations to other events and locating it in its historical contextas cited in Shaw). An adequate modeling of colligatory concepts and the relations between the concepts—the colligatory relations—is the precondition for semantic toolsbased on such an explicit representation of the past. Related work The digital editionof PetersSynchronoptische Weltgeschichtevisualizes parallel timelines showing historical events from different categories. However, it does not show the inner structure of complex events or processes and does not provide typed relations between events. Interestingly the tool provides visual contextualization of events based on the knowledge organization in its event database: related events are retrieved based on their common index terms. Fig. 1 Screenshot of Der Digitale Petersshowing contextualized historical events The older tool SemTimeprovides a solution to these typical shortcomings of timeline visualizations by introducing Semantic Timelines to visualize complex timelines with sub-timelines and different types of relations between historical events. Newer projects concentrate not only on the granularity of events but also on the the details of biographies, i.e. the modeling and visualization of the roles ofpersons in events. The VICODIprojectused semantic web ontologies as basis for visual contextualization. Based on the top-level ontology DOLCEthe SHOwas developed by Grossnerto overcome the shortcomings of exisiting ontologies in the modeling of spatial information in event ontologies. HEROis also founded on DOLCE and focuses on the modeling of different types of roles. DOLCE and DOLCE-DnS Ultraliterespectively contains the Descriptions and SituationsOntology Design Pattern. DnS allows the modeling of different perspectives on entities. A CRMbased alternative to model perspectives or interpretations is the MIDM. A much simpler modeling approach for different perspectives is SEM. Approach A crucial requirement for the approach presented here is the representation of different perspectives on historical events. Perspectival explanation or synoptic judgementis a main task of the historian. I have chosen DUL as top-level ontology for the modeling examples described in the following paragraphs because of its constructivist design principles and especially because its DnS ODP fits very well to our requirement of modeling colligations and the different perspectives or interpretations of historical events. Thus, a Description represents the conceptual relations which were grasped by the historian in a synoptic judgement. In biomedical ontologiesDescriptions are used to represent medical diagnoses. There is indeed an interesting similarity between the synoptic judgements of a historian and the medical diagnoses of a physician: The best analogy I can suggest for the way in which synoptic judgments are reached is that of a physicians diagnosis—a combination of broad medical knowledge, relevant evidence drawn from various tests, a knowledge of various theoretical possibilities for explanation, and skill in seeing which interpretation of the evidence works best in a particular case—the difference being, of course, that the physician deals primarily with law-bound physiological processes, the historian primarily with human conduct and purposive action. Fig. 2 shows a screenshot from an experimental tool that draws diagrams of causal narratives. The example is from Theda Skocpols States and Social Revolutions: A Comparative Analysis of France, Russia, and China. Skocpoldescribes the historical process which led to the French revolution in narrative form. In order to visualize the historical process consisting of three sub-processes, the mereological and causal relations had to be represented in a knowledge base according to the reconstruction of Skocpols narrative done by Mahoney. I am planning to adapt Semantic MediaWikiextensionsfor the creation of ontological hypertext as base for visualization using constraint-based layout algorithms. Fig. 2 Diagram of the causal narrative based on the reconstruction and visualization by MahoneySkocpolcombines macrosocial and idiographic historical research. Fewer idiographic detail is represented in the example from the CEWSproject. CEWS focuses on the phase sequencesin conflict processes. The CEWS Explorer was developed as a tool to visualize and compare conflict phase sequences and different perspectives on them as described in causal narratives about conflicts. In my talk I argue that a remake of this approach can benefit from an ontology-based representation of conflict phases and different perspectives on phases and causes for change of phases as seen from different conflict parties or other actors involved in the conflict. Fig. 3 Diagram of the first two episodes of the Transnistria conflictAccording to Peirces diagrammatic reasoning approach a diagram should be constructed under the rules of a system of representation. The ontology-based knowledge representation of historical events provides as well a framework for the construction of such representation systems. Similar to the so-called two-level theory of social revolutions in the first example) the so-called visual grammar of possible conflict phase sequences in Fig. 4 is a representation system that pretends all possible sequences of different types of conflict phases within conflict episodes. The system is used as system of diagrammatizationin order to construct diagrams for specific conflict trajectories. Fig. 4 Visual grammar for possible sequences of conflict phasesAs there is less granularity, i.e. no complex composite events, a simpler ODP can be used to represent conflict phase transitions. Fig. 5 shows an exemplary phase transition modeled with the ODP Transition . Based on the system of diagrammatization and some empirical data about conflicts diagrams of possible conflict sequences similar to the famous diagram of the Cuban missile crisis could be created. The CEWS Explorer was also built for counterfactual analysis of conflicts. Fig. 5 Modeling the change of a conflicts state with the Transition ODP and classification of the conflict phase types and the transitory event type with knowledge organization systems Fig. 6 Diagram for counterfactual exploration of possible decisions of the conflict parties and resulting outcomes of the Cuban crisis according to the game tree based counterfactual analysis by JoxeConclusion There are two feasible use cases for the presented modeling and visualization approach—followed by a more demanding one: preparation and communication of research results, knowledge representation and knowledge visualization for public history, and generation of new insights with information visualization tools for digital history. The added value for the diagrammatic communication of research findings originates in the explicit representation of historical knowledge: A complex historical process can be visualized for better communication of research results on the base of the explicitly represented entities and their relations. As Langenotes in his textbook on comparative-historical methods, it is recommended to use diagrams to represent clearly the argument of causal narration, to make the causal claim more explicit. Supported by a system of diagrammatization, ahistorian is enabled to represent and visualize the essential event relations of a basic story—excluding more granular expert knowledge about the historical process in focus. In knowledge visualization projects for public history additional diagram types could be used in order to provide the collateral knowledgenecessary for the public audienceto better understand the historical events—e.g. via concept maps or knowledge mapping in general. It has been shown in public historys neighboring discipline history didactics, that concept maps can help learners to analyse and synthesise knowledge. This is especially valuable to provide insight into the different perspectives on an issue. The third use case lies beyond knowledge visualization. It is aimed at interactive information visualization tools in the sense of a knowledge generator: using diagrams to support historians during research in order to create new knowledge. Formalization of the complexities and subtleties of expert knowledge is the necessary precondition for building diagrammatic reasoning tools for historical understanding. See Frankfor more details about the semiotic foundations of this systematic diagrammatic reasoning procedure by means of systematically experimenting on a knowledge basein order to infer new knowledge. Hence, the requirements of total explicitness and absolute consistencyfor formal knowledge modeling are large obstacles towards such thinking tools. Case studies in digital history have to reveal if the proposed diagrammatic reasoning approach is suitable. Besides historical conflicts the ontology-based modeling approach is planned to be used to prepare a collection of historical travelogues to be published as digital editions enhanced with interactive map-based visualizations. We will apply ODPs to model journeys and historical routes described in the travel narratives. See also the Linked Places projects conceptual model of historical geographic movement: "
	},
	{
		"id": 113,
		"title": "LiLa: Linking Latin. Building a Knowledge Base of Linguistic Resources for Latin",
		"authors": [
			"Passarotti, Marco",
			"Cecchini, Flavio M.",
			"Franzini, Greta",
			"Litta, Eleonora",
			"Mambrini, Francesco",
			"Ruffolo, Paolo",
			"Sprugnoli, Rachele"
		],
		"body": " Introduction The spread of information technology has led to a substantial growth in the quantity, diversity and complexity of linguistic data available on the web. Today, although a large variety of linguistic resourcesexist for a number of languages, these are often not interoperable. Linking linguistic resources to one another would maximise their contribution to, and use in, linguistic analysis at multiple levels, be those lexical, morphological, syntactic, semantic or pragmatic. In an ideal virtuous cycle, linguistic resources benefit from the Natural Language Processingtools used to build them, and NLP tools benefit from the linguistic resources that provide the empirical evidence upon which they are built. The project The objective of the LiLa: Linking Latin projectis to connect and, ultimately, exploit the wealth of linguistic resources and NLP tools for Latin developed thus far, in order to bridge the gap between raw language data, NLP and knowledge description. LiLa, which has received funding from the European Research CouncilEuropean Unions Horizon 2020 Research and Innovation programme, is building a Linked Data Knowledge Base of linguistic resources and NLP tools for Latin. The Knowledge Base consists of different kinds of objects connected through edges labelled with a restricted set of values taken from a vocabulary of knowledge description. LiLa collects and connects both existing and newly-generateddata. The former are mostly linguistic resourcesand NLP toolsfor Latin. These are currently available from different providers under different licences. As for the latter, LiLa assesses a set of selected linguistic resources by expanding their lexical and/or textual coverage. In particular, itenhances the Latin texts made available by existing digital libraries and resources with PoS-tagging and lemmatisation,harmonises the annotation of the three Universal Dependencies treebanks for Latin http://universaldependencies.org/ ,improves the lexical coverage of the Latin WordNet http://www.cyllenius.net/labium/index.php?option=com_content&task=view&id=21&Itemid=49 and the valency lexicon Latin-Vallex https://itreebank.marginalia.it/view/lvl.php , andexpands the textual coverage of the Index Thomisticus Treebank https://itreebank.marginalia.it/view/ittb.php . Furthermore, LiLa builds a set of newly-trained models for PoS-tagging and lemmatisation, and works on developing and testing the best performing NLP pipeline for such a task. Conceptual model of LiLa. As can be observed from the simplified conceptual model illustrated in Figure 1, the LiLa Knowledge Base is highly lexically-based. Lemmas are the key node type in the Knowledge Base. Lemmas occur in Lexical Resources, but may have one or moreForms. For instance, the lemma puella, girl has forms like puellam, puellis and puellas. Forms, too, can occur in lexical resources; for instance, in a lexicon containing all the word forms of a language. Both Lemmas and Forms can have one or more graphical variants. The occurrences of Forms in real texts are Tokens, which are provided by Textual Resources. Texts in Textual Resources can be different editions/versions of the same Work. Finally, NLP tools process either Formsor Tokens. LiLa develops across the following five Work Packages: WP1: Selecting and Improving Linguistic Resources for Latin. This WP assesses resources eligible to enter the Knowledge Base, i.e., linguistic data-sets. WP2: Building the Knowledge Base. This WP represents the core of the project, as it aims to model the linguistic resources for Latin collected and selected in WP1 to build the Knowledge Base. Furthermore, this WP aims to make NLP tools for Latin interoperable and to connect them with linguistic resources in order to exploit the empirical evidence these provide for different NLP purposes. WP3: Querying the Knowledge Base. This WP intends to build a user-friendly interface to allow users to write and run SPARQL queries on interconnected linguistic resources. WP4: Testing and Evaluating the Knowledge Base. This WP tests the Knowledge Base by conducting research on itsdata. WP5: Disseminating the Results. This WP is devoted to the dissemination of the results of the project through publications, conference presentations, tutorials and workshops. This poster contribution presents the detailed structure of LiLa, describes how it meets the so-called FAIR Guiding Principles for scientific data management and stewardship, which state that scholarly data must be Findable, Accessible, Interoperable and Reusable, and elaborates on the progress made in WP1. Acknowledgements The LiLa project has received funding from the European Research CouncilEuropean Unions Horizon 2020 Research and Innovation programme under grant agreement No. 769994. "
	},
	{
		"id": 114,
		"title": "The Quotable Musical Text in a Digital Age: Modeling Complexity in the Renaissance and Today",
		"authors": [
			"Freedman, Richard",
			"Fiala, David",
			"Walter, Micah"
		],
		"body": " Presentation Materials: https://sites.google.com/haverford.edu/crim-project/crim-dh-utrecht The allusiveness of musical discourse is so fundamental to the Western tradition that it is hard to imagine a work that does not in some way make reference to some other composition, type or topic. Indeed, music that refers to other music has been a constant in the European tradition of the last 1000 years, from the layered polyphony of 12th-century Notre Dame de Paris to the rampant borrowingof George Frideric Handel, and from the topical allusions of film music to looped sampling heard in rap. Thanks to the advent of new technologies for encoding and addressing symbolic music scores, we can now begin to explore these complex cultures of citation with both new scope and precision. Citations: The Renaissance Imitation Mass[Freedman and Fiala, 2017] focuses on one important but neglected part of this allusive tradition: the so-called Imitation or Parody Mass of the sixteenth century, in which a composer transformed a short sacred or secular piece into a long five-movement cyclic setting of the Ordinary of the Catholic Mass: Kyrie, Gloria, Credo, Sanctus, and Agnus Dei. The resulting works are far more than collections of quotations. The sheer scope of the transformation–in which a work that lasted perhaps five minutes was recast as a cycle lasting thirty minutes or more–required the composer to thoroughly re-think the model, adapting pre-existent melodies to fit new words, and shifting, extending, or compressing them to new musical contexts and expressive purposes. Indeed, if counterpoint is a craft of combinations, then the Imitation Mass involves the art of recombination on a massive scale. Musicologists have considered the intertextual relationships of these Masses from a number of vantage points. At a cultural level, for instance, they have been read in the context of debates about whether secular sounds of models might be elevated by the sacred lyrics and contexts of the Mass, or conversely whether the sacred words and purposes of the Mass were corrupted through secular sounds. But the chief challenge of measuring the genre has been dampened by two basic factors: the sheer number of possibilities for contrapuntal elaboration, and the idiosyncratic ways in which individual scholars have sought to explain and exemplify them. The CRIM Project, with its digital capacities for managing citations, claims, and counter-claims in a collaborative environment, answers both of these key challenges in ways that will transform our understanding of the repertory, and set the stage for the investigation of related corpora as well. CRIM builds upon recent developments in the digital domain for music scholarship, implementing for the first time a new kind of quotable text for music. XML encodings of symbolic music scores are the foundation of this work. Built according to the open-source Music Encoding Initiativestandard, these texts provide dynamic scholarly editions that are readable by musicians and computational systems alike. Scores of related works are presented in a novel citation engine, from which analysts can directly select any combination of notes, in any combination of staves or measures. These selections are stored as durable addresses following the Music Addressability API [Viglianti, 2016], which in turn can be used to return MEI representing the selected notesto any subsequent user. Such digital citations inaugurate a new kind of durable, quotable text for musical scores that we invite others to use. CRIM citations are also critical assertions: statements made by particular analysts about relationships between musical patterns. For these, too, we are proposing durable ontologies of various sorts. Within the confines of the CRIM project itself we have defined various controlled vocabularies that describe the workings of Renaissance counterpoint as it migrates from one work to the next. Participants record not only a collection of notes, but also a range of metadata that detail the specific kind of patterns, and the particular kind of transformation that has been applied to the model as source material is compressed, prolonged or recombined in imaginative ways. The specifically musical portions of the observations, moreover, are surrounded with information about the person responsible for the assertion, and other relevant data about its motivation and status. All of these data are exposed using Open Annotationand Linked Open Data standards. These complex, structured annotationsare assembled in a database and Django web application that permits users to discover related patterns in disparate works, and to deploy dozens of citations in collaborative discussions and narrative arguments about style and practice. We expect that digital publications of this sort–joining durable citations with open annotation–will have important implications for scholarly discourse about music. This in turn points towards the wider perspectives of the project: enhancing music digital edition and analytical annotation tools; improving the understanding of citation, imitation and, more generally, charting the processes by which pre-existing materials are transformed in the course of creative musical expression. We view the Digital Humanities community as the ideal forum in which to expand the reach of these technologies, and to continue the collaborative spirit of our work. "
	},
	{
		"id": 115,
		"title": "Anatomy of a Lick: Structure & Variants, History & Transmission",
		"authors": [
			"Frieler, Klaus",
			"Frank, Höger",
			"Martin, Pfleiderer"
		],
		"body": " Introduction Oral transmission of musical material plays an important role in many African-American music cultures, such as in blues and in jazz. This does not only pertain to entire songs but also to smaller musical units which are often called licks, formulas, or patterns. Due to the importance of improvisation in jazz, there is a certain need to command a personal vocabulary of patterns, which are musical snippets of a few tones. This greatly facilitates the improvisation process, particularly in fast tempos, by reducing the cognitive load. An overall heightened level of virtuosity became common in jazz with bebop in the 1940s. In this context, characteristic bebop lines were invented by players like Charlie Parker and Dizzy Gillespie, amongst others. Since the musical features those long lines are rarely found in other Western musical styles, they became a token of jazz. Certain patterns and licks are building blocks of those bebop lines and, hence, have become important components of jazz improvisation. Therefore, they deserve closer scrutiny, lending itself to the use of computers as it is not easy to discern patterns by listening or analyzing transcriptions manually. Pattern mining and search The international Dig That Lick projectis dedicated to investigating the usage of patterns and licks in monophonic jazz solos using search algorithms on a large database of jazz solo transcriptions. These transcriptions are created automatically using state-of-the-art melody extraction algorithms based on neural networks and advanced signal processing techniques.The transcriptions are equipped with extensive metadata based on a specifically designed semantic model. N-grams, i.e., melodic sub-sequences, are extracted from the transcriptions using pitch and interval representations and stored in a database. Similarity algorithms, which are grounded in music psychological research, are used to retrieve patterns instances for a given query and similarity threshold. Additionally, exact patterns can be extracted using regular expressions. This system allows tracing patterns and its variants across the whole database while combining it with the available metadata to make further inferences. A case study To demonstrate the viability of this approach, we present in the following a small case study exploring a typical bebop pattern. The pattern was found with the help of the Pattern History Explorer, which contains over 600 interval patterns in over 11,000 instances pre-mined from the Weimar Jazz Database Publicly available from http://jazzomat.hfm-weimar.defor exploration. Procedure The chosen interval pattern [‑1,‑2,‑1,3,3,3,‑1,‑2]can be considered a typical bebop pattern with a distinctive recognizable structure. The pattern can be found as patterns M20 and M40 in Owens work on Charlie Parker. The pointwise self-informationof this pattern is about 11 bits, which means it occurs about 3000 times more often than it could be expected based on a 0th order Markov model, which shows its significance. In order to find variants of the pattern, it was submitted as a query to the DTL similarity search system currently working with the Weimar Jazz Database which contains 456 solo transcriptions by 78 soloists. A similarity threshold of .7 and a maximum length difference of 2 was used. This resulted in a set of 768 similarpattern instances. Next, consecutive stretches of instance locations were filtered by using maximum similarity first, length matching and left-most precedence. This filtering left 184 patterns in total. Aural control of sample instances showed, however, that patterns without the ascending seventh chordin the center are usually not perceived to be similar to the query. After filtering these out, a final set of 100 instances was left. The pattern nuclei were classified by the seventh chord they represent, and prefixes and suffixes of the nuclei were frequency ranked. This allowed to construct unique tags of the form nn-X-mm, where nnis the frequency rank of the prefix, and X is the nucleus code: D for a diminished chord [3,3,3], D for its first inversion [-9,3,3], H for half diminished chord [3, 3, 4,], 7 for a dominant seventh chord [4,3,3] and 7 for its first inversion [-8,3,3], and m7 for a minor seventh chord [3,4,3]. Pattern structure and variants Out of 4 3 = 64 possible ascending seventh chords with inversions, only six occurred in our result set as nuclei, with the original [3,3,3] being the most common with 70 instances, followed by its first inversion [-9,3,3] with 14 instances. No sixth was found on any other than the first position. Together, this is a first indication for the stability and specificity of the pattern. For the prefixes, 21 different versions could be found with the original [‑1,‑2,‑1] the far most common with 63 instances. The suffixes are more varied with 26 forms and the original [-1,‑2] the most common with 35 instances. A pattern network using Edit Distance-based similarity of all patterns can be found in Fig. 1. Here, a similarity cut-off of .8 was used and node size is chosen proportional to Freeman centrality. The original patternis in the center, as expected. Figure 1. Similarity network the patterns with similarity cut-off of 0.8 for adjacency. For further structural analysis, we extracted chord contexts, metrical positions, absolute pitch values and chordal diatonic pitch class informationfor the first tones of the nuclei. This showed a remarkable consistency. 55% of all nuclei start on a beat, most frequently on the third and the first beat of a 4/4 bar. The most common chordal diatonic pitch class is the third of the chord, whereas the most common chord is a C 7 chord, followed by G 7, D 7 and F 7. Generally, the dominant seventh chord was the most common chord type with 65% of all instances. From these most common traits, a prototypical version of the pattern can be constructed, which, however, cannot be found as such in our results. The closest to such a prototype is an instance of [‑1,‑2,‑1,‑9,3,3,‑1,‑2] depicted in Fig. 1. The only difference to the virtual prototype is the nucleus D instead of D. Figure 2: Nearly prototypical pattern instance [-1,-2,-1,-9,3,3,-1,-2], found in m. 6 of Charlie Parkers solo on My Little Suede Shoes. Oral transmission In Fig. 3 a timeline plot of all instances of the pattern variants found in the WJD is depicted. Striking is the number of instances by Charlie Parker, nearly exclusively with a D or D nucleus. Dizzy Gillespie is another heavy and early user, with four instances along in one solo. Sonny Rollins, Dexter Gordon, Sonny Stitt and Phil Woods are also fond of this pattern. Interestingly, more recent post-bop players such as Michael Brecker, Chris Potter and Wynton Marsalis have this pattern in their repertoire. However, the pattern variants are not equally popular across the main jazz styles as annotated in the WJD= 91.8 p < 0.001), as it is much more likely to be found in bebop and hard-bop solos than in any other styles. This justifies post-hoc the denomination of the pattern as a bebop lick. However, the earliest instance can be found with swing tenor sax player Chu Berry. Figure 3: Timeline plot of all instances of all pattern variants according to the recording year of the containing solo, sorted by performer on the y-axis. Labels and colours correspond to nucleus type, point size represents number of instances. Discussion In this case study, we found several interesting results. Firstly, Charlie Parker and Dizzy Gillespie seem to have been, not unexpectedly, the main users and popularisers of this pattern and its variants, even though they themselves might have taken inspiration for it from the earlier swing players. Secondly, many other players from the bebop era, known to be influenced by Parker and Gillespie, also used the pattern quite frequently, indicating a direct transmission. Thirdly, modern post-bop players used it also quite often, which is indicative of their mastery of the bebop tradition, though it might also be a direct and deliberate reference to their bebop forebears. Fourthly, the pattern variants nearly always appear in a specific metrical configuration and certain harmonic contexts, which indicates that metrical and harmonic aspects might be stored along with the pattern in a players memory. However, it can also be adapted to different harmonic context without losing its musical shape which opens further questions about pattern construction and memorisation. Conclusion This case study demonstrates that computer-based methods are useful to address research questions at the interface of historical, cultural and psychological aspects, leading to new results which could not have been be gained without the help of digital tools. While the case study builds on the rather small sample of 456 solos contained in the WJD the much larger database of jazz improvisations which is currently under development by the DTL project will very likely corroborate the results and provide further insights. "
	},
	{
		"id": 116,
		"title": "Indexing and Linking Text in a Large Body of Family Writings",
		"authors": [
			"Dal Bo, Beatrice",
			"Frontini, Francesca",
			"Luxardo, Giancarlo",
			"Steuckardt, Agnès"
		],
		"body": " Corpus 14, a corpus of correspondences from World War I The processing of correspondences for a digital edition presents similarities with other textual genres, and therefore common methodologies and tools can be applied. However, as correspondence corpora display a reticular presentational organisation and since the related projects often require to progressively gather various sources, publishing through digital media has several advantages with respect to print publishing. As opposed to most correspondence projects, dedicated to well-known authors or scholarly writings For a list of projects see Stadler et al., 2016. , the Corpus 14 project deals with correspondences from ordinary people Another project dealing with the digitisation of this kind of correspondences is Digitising experiences of migrations: The development of interconnected letter collections, by Moreton and Nesi. For further information, see also Moreton, 2016; Moreton et al., 2014. . In conjunction with the commemoration of the centenary of WWI, the project coincides with the French initiative to appeal to families in order to gather documents or objects related to the war. The project allowed for the digitisation and the publication of a vast amount of dataaimed at specialists in two fields: the study of the Great War and its legacy on social memory, including the cultural transformations that occurred during and because of the war, the evolution of the linguistic uses that happened at that time, in particular among less-literate people, the propagation of a slangor the influence of regional variations, especially in areas characterised by diglossia situations. As of August 2018, the corpus is comprised of almost 1800 correspondences written by 37 writers in 11 areas, for a total of almost 500,000 tokens The current size of the corpus is limited, but future additions are planned; the effort was so far concentrated on establishing a methodology, selecting relevant texts that have been accurately transcribed. . The construction of the corpus has been carried out with the following requirements: the selection of exclusively less-literate writers, the preference for writings that can be followed up over time for a network of related writers, the accuracy of the transcription in terms of the encountered spelling, punctuation or syntax as well as characteristics of the sources. The transcripts were encoded in conformance with the TEI guidelines See http://www.tei-c.org/activities/sig/correspondence/ , which preserves the alignment between the text and the facsimiles, to mark up the logical structure of the text as well as features of readability. Two versions of the text are kept: one accurate and one with a normalised orthography. The metadata, described by the TEIheader, includes the correspDesc element, allowing for the identification of sender, addressee, relationship, date and place of sending. In addition to the methodological approach followed in the transcription process, Corpus 14 relies on the data representation model provided by the TXM platform You can access the Corpus 14 from http://textometrie.univ-montp3.fr/ , which is used for text querying and analysis. TXM distinguishes properties for structural unitsand lexical units. The corpus is also available for download from the Ortolang platform Access via permanent identifier https://hdl.handle.net/11403/corpus14 , a repository for language resources, which provides long-term archiving and allows for metadata harvesting by means of the OAI-PMH protocol. Semantic annotation of persons and places Recent developments of the project mainly concern the semantic annotation of Named Entities, especially persons and places. These are identified following a workflow including both an automated tool, REDEN Onlineand a manual annotation, and eventually marked up according to the guidelines of the TEI module for names, dates, people and places. Place references are linked to an external knowledge baseswhenever possible, whereas persons are referenced to an internal index. NEs in the correspDesc and in the body are annotated in the same way, but their position in the TEI tree determines their role, so that a simple XPath can return for instance only people and places that are mentioned in a letter. Such analysis shows that places and persons evoked in the correspondences between soldiers and their families are more likely to refer to their previous lives at home. This may be due to the wish of soldiers to maintain the connection with their network and their link to the household environment, which is one of the main functions of these letters. Ongoing work: organisation of correspondence networks Various ways of visualising information related to people, places and dates in correspondences, especially when it concerns metadata, have been developed by several projects. We cite among others The Migrant letter; Visual Correspondence http://www.correspondence.ie ; Mapping the Republic of Letters http://republicofletters.stanford.edu/ ; Early Modern Letters Online http://emlo.bodleian.ox.ac.uk/home ; Clavius on the Web http://claviusontheweb.it/ . Specifically, the investigation of spatial references can be facilitated by a cartographic visualisation. Our main aim in developing our own visualisation interface is to adhere to and exploit existing standards, such as TEI and Semantic Web technologies, so as to create a framework that could ideally be reused for similar corpora. The visualisation proposed for the Corpus 14 project displays each set of correspondenceson the map, with its sender and receiver addresses identified by two distinct colors, as well as markers for places mentioned in the letter The visualisationwas developed in collaboration with Pietro Barbieri, Chiara Capone and Luca Ciccone, MSc students in computer science, supervised by Marina Ribaudo, associate professor at DIBRIS, Università degli Studi di Genova. . This way of visualising spatial and personal information contained in letters brings an additional layer to the reading, as well as to the analysis of Corpus 14 correspondences, which have already been carried out in Steuckardt et al.and Roynette et al.inter alia. "
	},
	{
		"id": 117,
		"title": "A Workflow For On The Fly Normalisation Of 17th c. French",
		"authors": [
			"Gabay, Simon",
			"Riguet, Marine",
			"Barrault, Loïc"
		],
		"body": " Normalisation can be produced with various solutions, but recent research have demonstrated that neural machine translationis the most efficient. However, moving from test to production of a working tool is not an easy task, because of the amount of training data required for machine learning. This paper present a solution to create a parallel corpus and deliver an NMT-based normaliser for early modern French. A first test corpus A first test has been made with the 1668 edition of Andromaque of Jean Racineand the 1624 edition of the Lettres of Jean-Louis Guez de Balzac. Author Text Date Lines Tokens Characters Corpus Guez de Balzac Correspondance 1624 1723 49,589 298,486 Racine Andromaque 1664 1756 13,884 86,612 Total 3479 63,473 385,098 This proto-corpus is deliberately heterogeneous to test our workflow. Guezs Correspondance is a collection of short letters in prose using a graphic system from the first half of the 17 th c. Racines Andromaque is a play in verse with a graphic system from the second half of the 17 th c. Transcriptions have been produced directly from PDF fileswith a model specifically designed for 17 th c. prints. It has been trained on both low-qualityand high-qualityimages of books using various fonts and the extracted text preserves abbreviationsand special charactersbut not ligatures. Fig. 1 Racine, Andromaque, Paris, BNF, RES-YF-3206, p. 2 Pre-processing Following previous successful experiments, a rule-based system for pre-orthographic French has been developed. It is based on two lexical resources: Morphalou, an open lexical database of inflected forms of contemporary French, and LGeRM, an open morphological lexicon for middle Frenchnow covering also 17 th c. French. Based on these two databases, the normaliser applies transformations on each token, before a manual correction of the result. Normalisation consists of aligning 17 th c. graphic systemsto 21 st c. orthographySource Target Sur tout ie redout ois cette Mélancolie Surtout je redout ais cette Mélancolie Où ja y v eu ſi long -temps v oſtre Ame ense uelie. Où ja i v u si longtemps v otre Âme ensevelie. Ie craign ois que le Ciel, par vn cruel ſecours, Je craign ais que le Ciel, par un cruel secours, First results with an NMT-based normaliser We have decided to use NMTPYTORCH. The baseline model is composed of a 2-layer bi-directional GRUencoder and a 2-layer conditional GRUdecoder with MLP attention. The encoder and the decoder both have 256 hidden units and their initial hidden state is initialised to 0. The embedding dimensionality is also set to 256. Two versions of the system have been trained. The first one is a word level system and the second one uses the byte pair encodingwhich operates at the subword level. The corpus has been divided into two parts: 90% of the lines have been used for training and 10% for testing. Lines Tokens Characters Train 3,133 5,6825 348,098 Test 346 5,959 37,000 Total 3,479 62,784 385,098 Five trainings have been made with different initialisations on the two different models: words and subwords. Accuracy of the result is calculated with BLEU scores. Model Average BLEU Best BLEU Words 79.27 82.960 BPE 75.79 77.070 These BLEU scores still have to be used with extreme care considering the limited size of our corpus. They are however promising enough to engage in the production of a large-scale corpus for a NMT-based normaliser. Future developments To be as universal as possible, our training data must reflect all the lexical and graphic variety of 17 th c. French. We are therefore engaging in the construction of a representative corpus of early modern French, including excerpts of literaryand non-literary texts, in verse and in prose, spread diachronically across the century, and taken from original editions, reprints or illegal prints. Along this compilation phase, the OCR model and the rule-based normalising solution will be regularly improved to increase their efficiency before a final open source release. The final corpus, expanded with back translation, will be used for the training of an NMT-based solution. On top of words and subwords, character-level NMT will also be tested to provide the most efficient tool. A special model, trained to normalise the results of the rule-based system rather than the raw OCRised text will be tested, to evaluate the efficiency of a hybrid system using both technologies. "
	},
	{
		"id": 118,
		"title": "Stylometric Analyses of Character Speeches in French Plays",
		"authors": [
			"Galleron, Ioana"
		],
		"body": " It is usually understood that literary authors have style, and numerous papers have been published about the usefulness of digital approaches and techniques for identifying stylistic specificities of a writer, for confronting styles, for attributing texts, etc.. However, another traditional type of stylistic analysis in literary studies has been less operationalized in a digital paradigm, aimed at observing how the characters speak. This paper contributes to the testing of digital tools for such an approach; in other words, it tries to answer, with digital tools, whether literary characters have a style, or if the signal of the author that creates them is prevalent over all other kinds of linguistic specificities. In order to answer this question, the paper focuses on theatrical texts from the 17 th and 18 th French centuries. To the contrary of what happens in narrative texts, characters discourses have clear boundaries in plays, and can be easily extracted from an XML/TEI encoded text. Also, characters in plays have been somewhat less studied with digital tools than characters in novels. A sample of 8 comedies staged approximately from 1630 to 1740 has been put together; the sample tries to balance well known play writers, such as Molière, Corneille Destouches and Marivaux, with more obscure authors. However, all the plays are grandes comédies in 5 acts and in verses, with comparable lengths and quite similar numbers of characters. A total amount of 82 discourses has been extracted using an XQuery under BaseX. First, the plays as a wholehave been submitted to a PCA using the stylometric library under R written by. As expected, differences between author styles are well underlined by their distribution on the graph, with Molière in the middle and Regnard closer to him than to the authors from the 18 th century. This first analysis has been conducted only to confirm that the tool works on the type of texts it has been fed to, and yields sensible results. Second, characters discourses have been submitted to the same kind of analysis. After further adjustments, such as the exclusion from the corpus of too short roles that were skewing the general distribution on the graph, and the testing of various algorithms, the following representation has been obtained: As it can be observed, characters do not group by origin, nor do they display a clear historical split - to the difference to what was happening with the authors. This tends to confirm that characters do have a style, whose parameters are to be further identified and tested. Moreover, when merging in a same txt file discourses of feminine, respectively masculine, characters from the same play/ author, significant differences appear in certain cases, with Molières Agnès and other feminine characters being the most intriguing case: In a third stage, an analysis of characters speeches is conducted with TXM, so as to delineate the stylistic differences pointed to by the PCA and to attempt an explanation. Verbs do not seem to be useful discriminators, even when the texts are lemmatized. The calculus of specificities shows that personnel pronouns, names and possessives are the most discriminant features. To these, one may add the adjectives, which appear more frequently as a characteristic of male speeches according to the table of preferred words built with the oppose function under stylo library in R. While the importance of pronouns of dialogue for characterizing plays has already been underlined, the other features are a bit more surprising, since one would have expected, for instance, verbs to play a more prominent role, related to the actantial position of the characters. Also, it is not clear why male characters would use more adjectives than their feminine counterparts; sensibility and a trend towards pathos are more often evoked in relation to the second ones. After further inquiries with a new set of discourses, from other plays by the same authors, but also from other authors, so as to confirm the above mentioned phenomena, the paper will try to propose some explanations based on a close analysis of the contexts. "
	},
	{
		"id": 119,
		"title": "Laboratoire Numérique Pour l’Étude De Paratextes: l'Exemple de Tacitus On Line",
		"authors": [
			"Garcia-Fernandez, Anne",
			"Cogitore, Isabelle"
		],
		"body": " Introduction Nous proposons dexposer une modélisation et des outils de visualisation pour létude de paratextes. À partir du corpus des commentaires de Juste Lipse aux Annales de Tacite, nous défendons lintérêt de solutions propres aux objectifs scientifiques du projet tout en respectant des standards et en permettant la documentation et la réutilisation des outils : nous parlons de laboratoire numérique. Notre démarche est fondée sur les principes suivants : le questionnement préalable de la nature de lobjet détude et sa définition ; la volonté de servir avant tout les objectifs scientifiques du projet ; et la mise en place de solutions permettant la réutilisation tant des données que des outils et méthodes. Le projet et son corpus Tacitus On Line a pour but dinterroger le rapport entre le texte de Tacite et les savoirs qui se sont développés à partir des commentaires à ses œuvres, afin de produire une synthèse sur le développement de la pensée des humanistes dans ce quelle doit à lantiquité romaine, en particulier chez Juste Lipseà qui on doit la véritable découverte de Tacite. De manière volontairement paradoxale, on cherche à comprendre non pas ce que le commentaire apporte au texte antique mais comment le texte antique a pu donner naissance aux commentaires humanistes et ouvrir ainsi à de nouveaux modes de pensée : les commentaires ne sont pas des « parasites » du texte antique mais occupent une place centrale. La démarche consiste à partir dune édition de 1608 qui présente les commentaires de Juste Lipse aux Annales de Tacite et des commentaires dérudits antérieurs, de la numériser pour faciliter des études cibléeset arriver à une étude synthétique sur Lipse et Tacite. Le corpus détude est composé du Livre I des Annales de Taciteet des 273 commentaires de Lipse à ces sous-paragraphes. Modélisation et enrichissements Le corpus a été transcrit et encodé en XML-TEI. Plusieurs arguments soutiennent ce choix parmi lesquels sa qualité pour représenter les manuscrits, la pérennité du formatet sa large utilisation dans la communauté. Il existe de nombreux articles et ouvrages qui décrivent ce format. Pour nous, un avantage majeur du format est quil ne présage pas des utilisations du corpus et incite à encoder la source le plus objectivement possible tout en laissant la place aux enrichissements et annotations scientifiques. Les commentaires étant notre objet central détude, nous avons fait le choix de les encoder dans la balise <text>. Ils sont organisés dans des <div> indiquant grâce aux attributs @resp et @xml:id lauteur et lidentifiant du commentaire. Le repérage du sous-paragraphe des Annales auquel un commentaire se rapporte est encodé avec la balise <ref>. Les Annales sont quant à elles au sein du <front>, dans des <div> de type paragraphe ou sous-paragraphe ayant pour @xml:id les identifiants des Belles Lettres. Cette structure front/text reflète de fait notre point de vue sur les données. Catégorisation des commentaires Les commentaires peuvent concerner différents aspects du texte. Afin détudier la façon dont Lipse travaille et commente Tacite, une catégorisation des commentaires a été effectuée. La proposition est de travailler à partir dun ensemble détiquettes de base, de les confronter aux données et de faire émerger une typologie en même temps que létude des commentaires se fait. Pour chaque commentaire, une ou plusieurs catégories est attribuée dans lattribut @ana. Une catégorie de travail a été ajoutée et apparaît sous létiquette « À corriger » permettant de repérer les commentaires pour lesquels les catégories nont pas encore été définies. Des outils pour exploiter les données en cours de construction Nous avons fait le choix dune interface web répondant à des critères propres au projet : voir les commentaires et texte de Tacite en vis-à-vis, pouvoir faire des recherches plein texte ; observer les catégories des commentaires et rendre compte de la présence de citations. Figure 1: Interface de Tacitus Online présentant les commentaires et leurcatégorie, le texte des Annales et les filtres de visualisationLinterface proposée est le résultat dun développement utilisant des technologies web et XML classiques. Les sources sont accessibles surhttps://gitlab.com/litt-arts-num/tacitus-on-line et sous licence CC-BY-4.0 ce qui permet leur réutilisation et modification. Dans la figure 1, on observe un ensemble de filtres permettant dafficher les commentaires soit par exclusion, soit par inclusionselon différents critères : catégories du commentaire, des citations, auteur… Dautres filtres sont en cours de développement en particulier le filtrage par paragraphe. La vue obtenue peut être partagée par un lien contenant lensemble des filtres actifs facilitant le travail collaboratif et la citation des sources. La frise des commentaires Limportance accordée à la catégorisation des commentaires fait quil est intéressant davoir une vue dédiée à ces catégories. Nous avons ainsi créé une frisepermettant de voir les commentaires comme des événements qui occurrent au fil du texte de Tacite. Elle est cliquable et permet daller au contenu correspondant. Figure 2: Frise synoptique des commentaires Cette vue synoptique permet de refléter labondance ou la raretéde commentaires pour certains paragraphes ou encore la variété ou la spécificitépour dautres. Elle permet aussi de rendre compte de zones de texte plus ou moins commentées. Conclusion Le laboratoire numérique proposé permet de se libérer de la contrainte de publication des données : celles-ci sont accessibles en ligne, mais servent avant tout les objectifs propres au projet. La réutilisation des données est permise par plusieurs biais : laccessibilité du site, le partage des filtres de recherche et lutilisation du standard XML-TEI. Les développements sont accessibles, réutilisables et, à condition de respecter nos choix dencodage TEI, loutil peut héberger des données autres. De nombreuses évolutions sont envisagées comme des vues statistiques ou lintégration doutils issus de la linguistique de corpus. Remerciements Les autrices souhaitent remercier la MSH-Alpes,les Éditions des Belles Lettres, la Bibliothèque Municipale de Lyon, le réseau national des MSH ainsi que lUMR Litt&Arts et ELAN, et les étudiants ayant participé au projet : accès aux sources, encodage, développement de loutil, conceptualisation et modélisation du corpus, déplacement à Utrecht nauraient pu se faire sans leur soutien financier, administratif ou encore humain. "
	},
	{
		"id": 120,
		"title": "Developing and Integrating Training Materials on Research Infrastructures into Higher Education course design: The PARTHENOS Experience",
		"authors": [
			"Edmond, Jennifer",
			"Garnett, Vicky",
			"Goulis, Helen",
			"Schuster, Kristen",
			"Wuttke, Ulrike"
		],
		"body": " Background It has been almost 25 years since Gibbons et. al. introduced the idea of Mode 2 research, and Boyer asked us to reconsider our idea of scholarshipas encompassing not only discovery, but teaching, application and integration as well. These two contemporaneous works encouraged higher education to re-evaluate the way it envisioned the skills profile of a researcher, and imagine ways to meet these skills needs. Yet, in the intervening decades, neither Boyers ideas nor those of Gibbons and his co-authors have had much impact on the arts and humanities, where the most common mode of scholarship is still single authored, basic research, and the organisation of higher education is still highly disciplinary. One place where these new modes of research are taking hold in the arts and humanities, however, is in the emergence of the uniquely European class of large-scale, transnational collaborative research infrastructures constructed as ERICs, or European Research Infrastructure Consortia. Because the ERICs primary mission is to provide a higher baseline for all contributing researchers, both transdisciplinary application and integration find a natural home within their activities, regardless of the communities of practice they serve. These modes of research do not replace the traditions of discipline-based investigation, but complement them, bringing to the fore new skill sets that are useful for the economy See, for example, the Institute for the Futures Future Work Skills proposal, which highlights transdisciplinarity, collaboration and new media literacy among their growth areas.and for the organisation of research itself See, for example, the emphasis on skills in the EU report on the long term sustainability of RIs, Sustainable European Research Infrastructures, a Call for Action, https://publications.europa.eu/en/publication-detail/-/publication/16ab984e-b543-11e7-837e-01aa75ed71a1/language-en. In spite of these developments, the view of Higher Education Institutionsas the primary delivery mechanisms for training and education is understandable. Certification and validation of skills acquired requires its own infrastructure, one of policies and processes, and one which universities have developed over many centuries. Yet the complementarity between these two modes of knowledge creation presents a clear opportunity for convergence and creation of added value for both kinds of institutions, not to mention students. For this reason, the PARTHENOS project, a cluster project of Digital Research Infrastructuresand large projects within the Humanities and Social Sciences, based one of its areas of shared development on an investigation of, and pilot programme in, the shared transmission of knowledge between RIs and HEIs. Scoping the Relationship between Higher Education Institutions and Research Infrastructures In order to ensure the approach taken matched the requirements of the variety of courses already in existence across HEI programmes throughout Europe, the PARTHENOS team commenced their work with a survey of Digital Humanitiesprogrammes; first through desk research and a survey, and then through two course providers workshops, which discussed DH training provision, and how RIs were includedwithin the course syllabus. Results from the survey and desk research, complemented by the course providers workshops, gave an overview of the level of granularity many of the courses took to specific RI training. The majority of responses to the survey came from course providers, with around 20% of responses provided by students or recent graduates of DH courses. Initial indications showed that course providers are keen for more reusable teaching materials around RIs to enhance their existing training in the subject, and that while some did already provide some manner of experiential learning such as an internship or a more hands-on practical element to their course, the majority mostly included RIs either as something that is integrated throughout the courseor as a one-off lecture. The responses during the course providers workshop echoed this, revealing similar approaches: either by incorporating information about RIs regularly throughout the course training programme or by tackling it in one lecture. Where training around RIs was not included, it was either through lack of confidence in the subject or lack of knowledge of how to approach on the part of the course provider. Provision of reusable training materials, such as those created by PARTHENOS, therefore attempts to address the requirements of DH course providers, certainly across Europe. Designing an optimised course The PARTHENOS project applied the knowledge gathered during this exploratory phase to the design of a module for students undertaking a Masters programme at Kings College London. The module, developed by Dr. Kristen Schuster, incorporated materials specifically around the issues of research data management, an area in which both HEI-based research programmes and research infrastructure-based training have developed significant complementary expertise. The module was 10 weeks in duration and used a two-pronged approach, with some classroom-based training materials and lectures being used in the first 5 weeks of the course, and a practical element forming the majority of the training in the second 5 weeks of the course. The PARTHENOS training materials in particular used throughout the module included video lectures with accompanying downloadable presentation slidesand shorter videos around basic concepts in Research Infrastructures, and links to content in sections of the Manage, Improve and Open Up your Research Data PARTHENOS module, which was given as required reading in preparation for lectures in class. The students participating in the course were mostly non-native English speakers, and many of them came from outside Europe. Based on the feedback we received from course providers and students, we used two questions to develop lecture themes and seminar activities: what is data, and what do we mean by research infrastructures? Asking what is data? helped to contextualise the functional requirements for creating, describing and preserving structured information. Over the course of the module, students explored standards and policies that inform the development of best practice guidelines for handling structured information. Students gained introductory knowledge to metadata, database management and protocols for data exchange. Understanding theories and practices for defining and managing data segued to descriptions and analyses of RIs. Asking what do we mean by research infrastructures?, allowed the students to explore different requirements for developing policies, protocols and technical specifications for collaborative endeavours to create, use and re-use research data. Readings, discussions and activities provided students with opportunities to explore technologies, workflows and documentation that support research data management from a variety of perspectives. This combination of activities required students to practice the skills discussed during lectures and seminars and build a professional vocabulary for discussing research data management as a practice that supports the curation, preservation, use and re-use of digital assets. Overall, gaining a foundational knowledge of data management through this module prepared students to manage digital assets that support cultural heritage work and research. The Course Providers Experience Introducing broad themes through questions like what is data?, what are the social and technical roles of RIs and how can research data management improve the impact and function of RIs in the humanities engaged students in discussions and debates, which enabled students to work through their own areas of interest and come to conclusions supported by a body of research and expertise. Learning was supported through creative problem solving in a two-part group project. By working with a teaching fellow and external partner at the Royal United Service Instituteit was possible for students to develop and revise research data management plans and present wireframes for data portals and guidelines for institutional repository participation in RIs. In general, students progressed from confused to curious to engaged over the course of five weeks, and began their group projects with a great deal of confidence . Student Evaluation of the course materials To evaluate the usefulness of PARTHENOS training materials within the course, we also asked the students to complete two surveys: one at the end of the theoretical classroom-based section in weeks 1-5; and a second at the end of the practical section in weeks 6-10. We were interested in how the students found the materials from a classroom learning perspective, and how the materials might be received and referred to in a task-based setting. Preliminary results point towards a dual-format approach of accessible videos used in conjunction with written content as the ideal approach, especially given the non-native English speaking cohort within the student group. While the more mobile-friendly materials such as videos maybe useful for those who are comfortable with spoken English, the additional support of written content to provide context and allow for longer rumination is of benefit to those who are less comfortable with spoken English and all its dialectal varieties currently presented within the PARTHENOS training materials. Outline and proposal for a Long Paper This long paper will set a context for training around Research Infrastructures, and their inclusion in formal Higher Education courses. Using PARTHENOS materials as a case study, we shall discuss the rationale for and process of designing the course to incorporate training materials around RIs. It will then discuss in more detail the evaluation process we have undertaken, and look to lessons learned and recommendations for the incorporation of training materials from Digital Research Infrastructures. "
	},
	{
		"id": 121,
		"title": "Early Career Researchers and Research Infrastructures: Barriers and Pathways to Engagement",
		"authors": [
			"Papaki, Eliza",
			"Garnett, Vicky"
		],
		"body": " The European Commission notes that, in order to solve Europes economic and societal challenges, innovation in science and technology, pursued through Research Infrastructures, is vital. Efficient RIs enable the greatest discoveries in science and technology, attract researchers from around the world and build bridges between research communities. They allow the training of researchers and facilitate innovation and knowledge-sharing. One such research infrastructure in particular, DARIAH, was established as a European Research Infrastructure Consortiumin August 2014. Currently, DARIAH has 17 Member states and many cooperating partners across 11 non-member countries. As a pan-European network, DARIAH aims to enhance and support digitally-enabled research and teaching across the arts and humanities. And yet, despite this wide interdisciplinary and international reach, RIs such as DARIAH remain a distant concept to many of the researchers who could directly benefit from them. The Community Engagement Working Group within DARIAH has, since Nov 2017, been investigating the often complex reasons for a lack of engagement with RIs among researchers as part of our wider research into Research Communities. Over the course of our exploration around these themes, we have conducted a webinar, an online survey, interviews with researchers at different stages of careers, and a roundtable session at a discipline-specific regional conference. Many of our respondents to these various methods of data gathering have reported that they are aware of RIs, such as DARIAH, but for various reasons they did not choose to engage with them. We analysed their responses by both career level, and by some of the disciplinary groups to see if further patterns emerged. Researchers experience different pressures and issues at all stages in their career. However, we have chosen to look at Early Career Researchershere as the pressures on ECRs are well known, often taking on a sort of apprenticeship role where not only the direction of the research is somewhat predetermined, but also membership of organisations and indeed RIs is decided by the more senior members in a team. An ECR does have some autonomy in terms of how they network, how they communicate with others in their field, and of course which teams or researchers they choose to work with in the first place. To find out how RIs could communicate more directly with ECRs, we focus here on the results of our research from that group in particular. We found differences in how ECRs communicate with others in their field compared with mid-career and senior researchers, specifically that ECRs favoured more face-to-face approaches in networking and communicationover social media such as Facebook, Twitter or LinkedIn, as was widely used by the mid-career and senior researchers. When we looked at specific reasons for not engaging with RIs, the responses from the ECRs either indicated a lack of awareness about them, or focussed much more on time constraints due to competing priorities such as getting publications accepted and completed, or finishing their PhD. Insecurity in their current employment also created anxiety that meant they were unable to form a long-term view beyond trying to find the next contract, and thus unable to spend the perceived time and effort it would take to learn how to work with an RI. Responses from more senior researchers tended to err on the side of exceptionalism, with many taking the view that, while they were aware of RIs, their own area of research was specialised, and therefore not likely to benefit from an interdisciplinary RI. Steps that RIs have taken to try to reach potential members at all levels in their career have so far included creating national contact points to act as advocates, and by establishing some manner of training; either as occasional Summer or Winter Schools, or through online training resources, the PARTHENOS cluster-project http://training.parthenos-project.eu/, part of the PARTHENOS project., and CESSDA https://www.cessda.eu/Temp-archive/Trainingare three such examples). However information from and about these initiatives is often disseminated via social media, so despite these interventions there are still gaps between provision to enable engagement, and the modes of communicating these provisions. This poster will present these results in more detail, and offer recommendations for how RIs might integrate the needs of this specific research community into their wider communications practices. "
	},
	{
		"id": 122,
		"title": "An Unsupervised Lemmatization Model for Classical Languages",
		"authors": [
			"Gawley, James O'Brien"
		],
		"body": " The lemmatization task, wherein a computer model must group together inflected forms derived from the same stem, or lemma, is a fundamental problem in language modeling. Software that handles more complex humanistic tasks like authorship attribution or intertextuality detection relies on lemmatization as an early step. In classical languages, the current standard depends on training sophisticated model with supervised data sets. These data sets include correct lemmatization tagged by expert readers, a labor intensive task. Modern languages can avoid generating supervised training data by taking advantage of much larger data sets. Moon and Erk, for example, used an aligned corpus of English and German to infer lemmatization schemes without recourse to hand-tagged training data. Classical languages do not feature very large aligned corpora, and may not have access to a database of expert annotation for training new models. This paper presents a technique for inferring a lemmatization model without training data, and tests the performance of this technique in classical Latin. Performance is on par with both supervised models of Latin, and models of modern languages derived from large, aligned data sets. In ambiguous cases, where a token might derive from more than one lemma, the model identifies the correct choice in roughly 66% of trials, or roughly twice as often as random chance. The technique presented here delivers this performance without including any input but raw text, and can be applied to languages for which such training data is limited or non-existent. The data set used to train the model was provided by the open-source Tesserae project. The test data was supplied by the Classical Language Toolkit. The lemmatization model begins by determining the relative frequency of all lemmata found in a given corpus. The underlying assumption is that selecting the more common lemma is the most computationally efficient way of disambiguating between possibilities. To illustrate with an example: in the first line of Vergils Aeneid , the word Arma might stem from the verb armō, a verb meaning to arm or arma, a noun meaning weapons. One intuitive way to resolve ambiguity is to select the lemma that appears more frequently in the corpus. But how do we determine the frequency of each lemma, without training data, when the tokens in a given corpus are ambiguous? To resolve this problem, the present study assumes that all possible lemmata are present in each ambiguous case. Error! Reference source not found. below illustrates this process with three tokens from Vergils Aeneid . Figure 1 As illustrated, the word form arma might come from the verb armō or the verb arma, but the form armis, found later in the poem, might come from the noun arma or the noun armus. Different forms of the noun arma overlap with different lemma, but all of them share arma as a potential stem. In other words, each lemmatization is correct in the same way, but incorrect in a different way. Over several million of tokens in the classical Latin corpus, the wrong answers begin to cancel each other out and the frequency counts in the model gradually begin to reflect the true rate of appearance of each lemma. Once the unsupervised frequency model has been trained, the lemmatizer simply selects the most frequently seen stem in ambiguous cases. Given an inflected form which might come from either of several lemmata, the lemmatizer selects the lemma that is most frequent in the corpus. The example of arma is shown in Error! Reference source not found. : because the noun arma was seen more often in the corpus, it is selected as the correct lemmatization here. Figure 2 Tested repeatedly against hand-lemmatized Latin text from the CLTK training model, this unsupervised lemmatizer selected the correct stem for roughly 89% of all tokens. This performance is comparable to the more sophisticated models currently in use for Latin lemmatization. It also exceeds the performance of random selection, which identifies the correct stem in only 79% of all tokens. Roughly 73% of Latin tokens are unambiguous. Languages with greater ambiguity, such as classical Hebrew, may not derive performance at this level. However under-served languages such as Coptic Egyptian might use this model to build reliable lemmatizers without consuming resources in the annotation of training data. "
	},
	{
		"id": 123,
		"title": "Developing MORROIS (Mapping of Romance Realms and Other Imagined Spaces): Digitizing Geographic Data Drawn from Literary Sources",
		"authors": [
			"Geck, John A.",
			"Jaravaza, Shamiso S.",
			"Winslow, Sean M."
		],
		"body": " Place and space are critical elements of medieval popular romance, both in the journeys undertaken by the romance protagonist, and in the transmission of texts across space and time. These phenomena have driven critical interest in spatial markers in literary texts. The Morroisproject, a digital geographic concordance of literary spaces, collects line-by-line instances of explicitly geographic place-name usage in Middle English manuscripts. The end goal of Morrois is to explore the research possibilities afforded through distant reading and various data visualizations. Our poster will present data migration from Omeka Classic to RDF. In its current form, Morrois uses the Omeka Classic content management system, and maps the places associated with the Alliterative Morte Arthurethrough the Neatline framework. Since the beginning of this academic year until August 2020, we will be migrating this and other datasets to a new triple-store database platform with more nuanced and transferrable metadata schema. The rest of the texts from the Lincoln Thornton MS and Edinburgh, National Library of Scotland Advocates MS 19.2.1will also be added to the database, and we hope to offer a series of new visualizations via SPARQL Queries, as well as a more robust GIS that will allow for fuzzy spaces, a critical element of medieval geodata. Our poster highlights our methodology, including the selection and customization of flexible and transferable ontologies and the benefits of RDF metadata modelling. We address some of the challenges inherent in data migration from a traditional relational database format to one geared for Linked Open Data. In the case of Morrois, Omeka Classics implementation of Dublin Core was effective for information about the manuscript texts themselves, but unsuited for the line-by-line references found within the texts. Migrating data from this shoehorned format to more accurate schema required considerable overhaul. Finally, we will address the current state of Middle English manuscript studies, wherein texts can be found in editions preserved in simple HTML format, printed and non-digitized critical and diplomatic editions, or in hard copy or digital manuscript facsimile. This situation means that data collection must follow different methodologies and different forms of data storage, including CSV or TEI. "
	},
	{
		"id": 124,
		"title": "Liquid Galaxy Visualization of IMS's Photographic Collections",
		"authors": [
			"Giannella, Julia",
			"Velho, Luiz",
			"Buccalon, Bruno",
			"Burgi, Sergio",
			"Rezende, Rachel"
		],
		"body": " Introduction This poster presents the first results of an ongoing project using Liquid Galaxyplatform with a particular interest in its applications for panoramic geographic-based visualization within the scope of a research agreement between two Brazilian institutions. One of the main goals of this agreement is to research and develop immersive panoramic and geospatial navigation interfaces using LG platform to present Instituto Moreira Sallesphotographic collections. Marc Ferrez photographs at Instituto Moreira Salles IMS is an important institution in the Brazilian cultural scenario. It holds cultural heritage collections in four areas: photography, iconography, music and literature. The photographic collection of about 2 million images is distributed among 52 collections, often the complete works of its photographers, representing a relevant set of the nineteenth and twentieth-century photography in Brazil. Marc Ferrez is the most significant nineteenth-century photographer at IMS. His work comprises a collection of approximately 7 thousand photographs. Born in Rio de Janeiro in 1843 descended from a French family, Ferrez is best known for shooting natural and landscapes and urban scenes of his hometown. His work documents Rios transformation over time and represents pieces of evidence of urbanization and industrial development. Panorama of Rio de Janeiro, seeing Glória and Catete in a photograph taken from Santa Teresa, c. 1885. Marc Ferrez/ Gilberto Ferrez Collection / IMS. The presence of known natural and architectural landmarks in his photographsmakes it possible to estimate the location from which they were taken. In this sense, the Liquid Galaxy platform offers an opportunity to engage with Ferrezs work through geographic-based visualizations. The Liquid Galaxy platform LG is a multi-display data visualization platform that enables immersive panoramic experience through interactive tours using KML data, videos, photos, and 2D and 3D graphics. Its applications cover a large range of markets and industries, from GIS consulting to museums and research. We are interested in how LG can be used as a medium to explore photographic collections with a particular attention to its geospatial features over time. LG standard setup includes 7 integrated HDTVs screens resulting in a wide FoV display with a high definition resolution, a touchscreen, and a 3D navigation controller. LG was originally developed by Google as an open source project to showcase Googles geospatial technologies but has been extended by End Point Corporation to become a more general data visualization tool. In 2017, End Point donated an LG installation for research purposes and, in 2018, a research agreement was signed aiming IMSs photographic collection. First demo and LG functionalities For a first demo, we selected ten outdoor photos of Ferrez and manually estimated the positioning from where they were taken. Using Google Earthwe set PhotoOverlay parameters to be exported for each image as KML files. Then, using LG authoring CMS we created a presentation tour to visualize the nineteenth-century photographs and metadata over the actual urban landscape, generated by GE data in the background. The demo fulfills the task of comparing the same view in different periods of time. Layout for a scene on LG including different assets: on the background, GE shows a KML file with a PhotoOverlay image; on the foreground, 2D graphics shows a metadata infobox and a high-resolution version of the Ferrezs photograph. Once the tour is finished, the guest can play and interact with it using the touchscreen and the 3D controller. The touchscreen shows a grid of thumbnail scenes, each one corresponding to a photo taken in a different place in Rio. The joystick controller enables 6-axis navigation within a scene. In this demo, guests are invited to use the controller to navigate GE while comparing the current view of the city with the past view registered by the photographer. A video of the demo can be watched on https://youtu.be/yZpTpdq-j14. LGs touchscreen and 3D controller. Considerations and future work We believe LG platform can be approached as a geographic-based visualization interface for engaging with photographic collections. Positioning photographs in space and comparing urban views over timecomprise cognitive tasks that can potentially bring new meanings to the photographs being visualized in LG. To provide effective and productive applications, however, the thorough understanding of which is the targeted audience seems indispensable. As such, we plan to conduct controlled experiments, observing the reactions of different types of guests. External links Keyhole Markup Language developers guide Google Earths Documentation Instituto Moreira Salles Photographic Collections Liquid Galaxy by End Point "
	},
	{
		"id": 125,
		"title": "Conceptual Modelling of Subjectivity, Temporality and Vagueness with ConML",
		"authors": [
			"Gonzalez-Perez, Cesar",
			"Martín-Rodilla, Patricia"
		],
		"body": " Abstract Research and practice in the humanities often involve the management of large and complex bodies of information representing objects of study, hypotheses, work in progress or even conclusions. Although natural language is usually employed to convey these, semi-formal languages have shown to be useful in a number of situations due to reduced ambiguity and ease of implementation on computing platforms. Conceptual modelling, the technique to construct semi-formal models of the world that can be understood by humans and machines, is thus an excellent approach to fight the complexity that very often makes humanities a difficult endeavour. By using conceptual modelling, we can explore unknown areas of research, document entities in the world for latter reference, communicate complex situations and processes to others, prescribe how computer systems should be constructed, and facilitate the interoperation of heterogeneous bodies of data through the gradual refinement of models. Within conceptual modelling, however, some areas are still poorly understood and weakly supported by existing technologies. One of these areas is that of soft issues, that is, the modelling of issues that are crucial to human understanding, but which have been traditionally regarded as incompatible with or almost intractable by computers. Some examples include the passage of time, the subjective perception of the world, and vagueness. As these issues are usually difficult to implement as part of computer systems, they are often left out of formalised representations, which results in models that become too simplistic to be useful. For example, very few databases in the humanities are capable of recording information diachronically, subjectivelyand vaguely. However, soft issues are a crucial component of research and practice in digital humanities, so they should not be left out. In this tutorial, we use ConMLto demonstrate how these issues can be tackled, and how conceptual models can be constructed that express temporality, subjectivity and vagueness. ConML is a conceptual modelling language developed with a special orientation towards the humanities and social sciences, and with the aim that specialists with little or no experience in computer science would be able to learn it and use it in very little time. ConML has been successfully used in a number of projects, such as the development of the Cultural Heritage Abstract Reference Model. Specifically, ConML supports the expression of temporal, subjective and vague aspects of information through a number of mechanisms, which will be presented and exercised in the tutorial. The tutorial will employ theoretical explanations combined with hands-on group exercises, a technique that we have been since 2012 in our postgraduate courses on information modelling for the humanities at the University of Santiago de Compostela. Attendees will acquire practical skills to construct expressive models, as well as the theoretical underpinnings supporting them. They will have access to proven modelling techniques as well as new and experimental findings. By incorporating the modelling of soft issues into their toolbox, attendees will be able to construct conceptual models that document the object of study much better, communicate intentions in a richer manner, and allow the implementation of computer systems capable of catering for these complex but essential aspects of the humanistic inquiry. Outline The tutorial will be organised as follows. 1. The need to represent. Benefits of conceptual modelling. 15 minutes. 2. Introduction to ConML. Types and instances. Classes, enumerated types, attributes, associations, generalization relationships. Basic modelling exercise, in groups. 1 hour. 3. Expressing subjectivity. Theory of disagreement. Subjective aspect classes. Existence and predication qualifiers. Modelling exercises. 30 minutes. 4. Expressing temporality. Theory of change. Temporal aspect classes. Existence and predication qualifiers. Modelling exercises. 30 minutes. 5. Expressing ontological and epistemic vagueness. Dealing with imprecision. Dealing with uncertainty. Using inaccuracy. Modelling exercises. 30 minutes. 6. Wrap up and conclusions. 15 minutes. Target audience The tutorial is targeted at researchers, instructors, students or practitioners of digital humanities, including anthropology, archaeology, art studies, history, law, linguistics, literature and other disciplines, who need to manage data or information as part of their job and feel an inclination to semi-formal approaches to knowledge representation and management. The tutorial does not assume previous knowledge of conceptual modelling, but some experience with information management systems and languages will help. Ideally, attendees should be between 10 and 15. Instructors Dr. Cesar Gonzalez-Perez is Staff Scientist with Incipit CSIC in Spain. He leads a co-research line in software engineering and cultural heritage. His main interest is to understand and assist the knowledge-generation processes that occur in relation to cultural heritage. Dr. Patricia Martín-Rodilla is a Postdoctoral Researcher with CITIUS at the University of Santiago de Compostela. She works on knowledge extraction from textual sources and the representation of discursive knowledge in the humanities. Both Cesar and Patricia have extensive experience in teaching conceptual modelling in the humanities, having collaborated in the design, teaching and management of postgraduate courses in this field since 2012, as well as over 10 related workshops at conferences over the last few years. "
	},
	{
		"id": 126,
		"title": "Subjectifying Library Users to the Macroscope Using Automatic Classification Matching",
		"authors": [
			"Gooding, Paul Matthew",
			"Terras, Melissa",
			"Berube, Linda",
			"Bennett, Mike",
			"Hadden, Richard"
		],
		"body": " Introduction Libraries are sources of large-scale data: both in terms of their collections and the information they collate on their spaces, users, and systems. These data provide opportunities to explore technical, methodological, and ethical questions from the valuable interdisciplinary perspective of Data Science and the Digital Humanities. In light of this, we will explore our analysis of library datasets using Subjectify The code and documentation for Subjectify is available on Github at https://github.com/mbennett-uoe/librarytools. , an automatic classification matching tool developed to assist analysis of UK Non-Print Legal Depositcollections. NPLD regulations were introduced to the UK in 2013 to support legal deposit libraries to collect electronic publications. Access restrictions mean that readers may only use these materials on fixed terminals within the physical walls of the six legal deposit libraries. The resultant web logs are therefore unambiguous sources of NPLD collection usage within UK legal deposit libraries. Our study is part of an established tradition of user studies in the digital humanities. To date, these have focused on user behaviour with digital resources. Web log analysis has been used successfully in this context for over twenty years. These studies adopt methodological approaches and topics of study that contribute directly to our understanding of information sources in the digital humanities. However, there have been fewer studies that address critical humanistic perspectives to inform approaches to the data itself. This paper addresses that gap by describing our research into the users of NPLD materials in the United Kingdom, and the implications of automatic classification matching for library dataset analysis. It will address the following questions: what insights into users of digital library collections can be derived from automatic classification matching? What limitations are introduced by the use of existing classification schemes? And, in light of ongoing debates on responsible data curation in DH, how might DH and LIS scholars collaborate to inform ethical analysis of large-scale library datasets? Methodology Our analysis follows Bates observation that scholarly communication practices function differently across domains, and that these many differences do make a difference 3.3.CO;2-M,author:[{family:Bates,given:Marcia}],issued:{date-parts:[[1998,11]]},accessed:{date-parts:[[2011,3,23]]}},locator:1200,suppress-author:true}],schema:https://github.com/citation-style-language/schema/raw/master/csl-citation.json}?>to information access and use: as such, we should be able to identify differences in behaviour by studying the subjects requested by users. To this end we were provided with two datasets comprising title-level NPLD access logs from the reading rooms of the six UK legal deposit libraries The six libraries are the British Library, National Library of Scotland, National Library of Wales, Bodleian Libraries, Cambridge University Library, and Trinity College Dublin Library. . The anonymous logs contained only bibliographic records of NPLD materials accessed by users, excluding both identifiable information about users and interactions with discovery systems and other materials. The first dataset comprised metadata for all eBook requestsfrom 31 st July 2015 to 31 st March 2017. The second dataset comprised metadata for all eJournal article requestsover the same period. Each dataset contained the following metadata: date and time of access request; originating legal deposit library; title of book or article; journal title; publisher; and ISBN or ISSN. Each dataset was provided as a CSV file, then cleaned by the research team in OpenRefine to address metadata errors. Although our dataset contained no identifiable information about users, it may still be possible to infer information about users from the works they consult. We therefore decided to abstract our data and undertake a macroanalysis of user behaviour. To achieve this, we created a small Python-based tool called Subjectify. This tool uses the OCLC Classify2 API service http://classify.oclc.org/classify2/api_docs/index.html to automatically obtain Dewey Decimaland Library of Congressclassmarks from CSV files using key data fields such as title, author, and ISBN. It additionally provides for different options to locate relevant fields to allow input from different data sources. Subjectify found a matching classmark for 76.42% for eBooks, and 55.53% for eJournals. This was partly due to missing key data fields in records for eJournals, and partly because many records did not have a corresponding classmark: time-consuming manual classification samples via OCLC Classify2 achieved only slightly higher accuracy rates. We discarded unclassified records used the remaining records to represent patterns of usage by DDC subject. Due to the large number of repeat requests due to system timeouts, we split the remaining records into unique titles, and total results. Our results show that findings were not unduly influenced by repeat requests. Findings The following charts show the most commonly accessed subject, by DDC, for titles viewed by users of eBook and eJournal materials. We found that usage by DDC differs distinctly from the spread of classmarks across, for instance, the BLs entire collections The sub-subroutine blog has produced a fascinating visualisation of the BLs collections, which is worthy of comparison. : Social Sciences texts were notably among the most commonly accessed titles for both formats. The most common subject for eJournals was Technology, whereas for eBooks both Social Sciences and Literature subjects were more frequently accessed. Our findings reflect differing information behaviour across domains: books, for instance, remain a vital source for the Arts and Humanitieswhereas technology and science subjects rely on journals, which tend to provide faster publication of new research. Indeed, Stones flagship early study noted that retrospective coverage may be more important to the humanists than having access to current material. The Social Sciences, on the other hand, are shown by our findings to be more hybrid in their reading patterns. Analysing individual subject categories can derive a better sense of the difference between institutions. The following charts show usage of books in the DDC600-699 classmark, for the Bodleian Libraries, British Library and Cambridge University Library. The British Library receives proportionally fewer requests for NPLD medical and health materials. Our interviews with staff at the Bodleian and Cambridge University libraries showed that local medical school staff were a key user group, so we can trace a direct correlation between the user communities of these libraries, and the subjects of the NPLD material used. Finally, the following table demonstrates usage of 800-899sources to underline a key problem with DDC for automatic classification: Library classification is a subjective process undertaken by humans within the biased frameworks provided by existing classification schemes. Here, for instance, DDC provides distinct categories for English, American, and classical European schools of literature, while lumping the rest of the world under other literatures. This bias emerges from the nineteenth century North American perspective embedded within DDC. By relying on automatic matching, we inevitably embed problematic perspectives into our data: while our case study uses UK legal deposit collections, which comprise works represented strongly by DDC, the applicability of this method for library datasets in other parts of the world is questionable. Each time we zoom in with the macroscope, the bias of our chosen classification scheme becomes increasingly evident in the resultant data structures – yet in order to report on literature from without the established canon, we have to do precisely this. The use of established classification schemes is therefore both a methodological and epistemological problem, and future work will be needed to refine our approach. Conclusion Our results demonstrate that Subjectify was successful in allowing us to analyse user behaviour at scale. It contributes to macroscopic analysis of library data in two ways: first, it allows us to report on bibliographic library users while maintaining privacy through data abstraction; and second, this abstraction allows us to identify patterns of user behaviour with NPLD materials. We believe this approach would work for subject-based analysis of similar collections of bibliographic data, and that it does so in a way that closely reflects how collections are represented in libraries. However, we are also aware that automated classification introduces the biases of those classification schemes into our own data. This is an unfortunate side effect of the growing scale of library data. Weingartnotes that the role of the humanities is to tie the very distant to the very close, in order to become ethical stewards of our data. It is therefore essential, when viewing library datasets from a humanistic perspective, to consider the ethics of data representation in our own work. Our priority for future work is to consider how a fruitful conversation between DH and Information Science might develop more nuanced approaches to representingof library data. This should include further consideration of the consequences of how bias in library classification schemes affects microanalytic approaches to bibliographic datasets. "
	},
	{
		"id": 127,
		"title": "Four Years of correspSearch – Challenges, Potentials and Lessons of Open Data Aggregation",
		"authors": [
			"Dumont, Stefan",
			"Grabsch, Sascha",
			"Müller-Laackman, Jonas"
		],
		"body": " The main goal of the correspSearch web service https://www.correspsearch.net , developed and hosted by Berlin Brandenburg Academy of Sciences and Humanities, is the aggregation of open and freely accessible metadata from printed and digital editions of correspondences. For a general overview see Dumont. Over the last four years we have successfully aggregated metadata for about 52.000 letters. Most of the data is obtained from external contributions ranging from a wide variety of scholarly editions and institutions. With this recap we aim to infer successful recipes and practices for the decentralized aggregation of domain specific open metadata. Furthermore we will show the possibilities which arise from the aggregation of such metadata on a bigger scale and discuss ways to manage as well as explore the complex realities of our data. Gathering a large quantity of suitable metadata about published letters is the precondition and one of the basic functions of the web service correspSearch. The format used in a data aggregation project such as ours has to fulfill a wide array of requirements, ranging from interoperability with existing standards to ease of use and a straightforward creation process. The TEI Correspondence SIG https://www.tei-c.org/activities/sig/correspondence/ developed a format for the purpose of simplifying correspondence metadata and thus offering a way for a standardized exchange of data, the Correspondence Metadata Interchange Formathttps://correspsearch.net/index.xql?id=participate_cmi-format&l=de . The flat hierarchy and simple node structure of CMIF offers an easy way to process metadata not only in a machine readable way but also in a way that does not require much prior knowledge to get in touch with. Given the specific nature of correspondence metadata, the focus of CMIF lies on people, places and dates. The format also encourages the extensive usage of authority control data. By relying on metadata as the core of the exchange format, it is not only of use for editions that are already present in a digital form, but can also be employed to print editions. Since the aggregated data is stored decentralized with each edition, participating editions and data providers preserve full ownership and control over their data. Based on the TEI-XML standard, CMIF follows strong guidelines and utilizes the strengths of XML to prevent faulty or ambiguous data while taking into account the heterogeneity of metadata. Furthermore, CMIF requires the usage of authority files for names and places in order to account for their ambiguity and linguistic heterogeneity. All in all, CMIF successfully provides a model for describing correspondences in an easy and rather flat hierarchical way, while relying on a strict ruleset in favor of standardized and machine-processable data. As the metadata aggregated by our service is licensed under CC BY, it remains open and thus stands for an Open Access-based approach to correspondence metadata aggregation. Open Data provides an easier ground on which research can be conducted without having to take care of licensing beforehand and with a much larger data pool. The analysis of a specific network of letters, as for example in csLink, can only benefit from this approach. In addition and in accordance to the licensing for the aggregated data, all software developed by correspSearch is published as Free Software. The CMIF Creator https://correspsearch.net/creator/index.xql, https://github.com/correspSearch/CMIF-Creator that was released in its second version in 2018 pursues our approach to an easy handling of CMIF-based data. It enables the creation of correspondence metadata without any prerequisite knowledge about XML/TEI or the CMI format. With the CMIF Creator we offer a clean graphical user interface to enter available metadata and transform the entered data to valid XML. Lowering the barrier of generating and contributing data has been a key factor for successful and lively external data contributions in the last years. As the CMIF Creator is implemented as a browser-based application, all entered and processed data is saved locally and thus stays within the control of the user at any time. As CMIF heavily relies in authority control data, authority files data for names and places can be acquired directly from GND https://www.dnb.de/ and GeoNames https://www.geonames.org/ via the lobid https://lobid.org/gnd/api and GeoNames APIs. The CMIF Creator also offers a validation service as well as the option to locally save drafts in JSON format so that work can be continued at any time. The final CMIF files can then be provided for aggregation into the correspSearch database. The benefits of using the CMIF Creator are obvious when it comes to the actual experiences we made: Besides the low barriers for data entry – it does not require any prior knowledge and experience of TEI-XML – the average time for a student assistant to process and enter the metadata of a single letter out of a printed letter edition is approximately 30 to 60 seconds, depending on the necessity to further disambiguate authority control data. Thus, large quantities of letters can be processed in a reasonable amount of time. The output format is standardized and does not deviate from TEI specifications, which reduces the incidence of errors in the final XML. Another potential of the open data approach with a rather analytical purpose is followed in our development of the application csLink https://github.com/correspSearch/csLink . csLink is a widget for websites that can be implemented and included in existing digital editions of letters. It establishes a network of letters, displaying the correspondence network of a certain person and reaching beyond the scope of a single edition of letters. Customized by optional parameters given, csLink provides a list of other letters from the same network of letters, as well as a list of persons, who are part of the wider correspondence network. By relying on CC BY licensed metadata the widget is available for anyone interested in such a visual representation of the correspondence network belonging to a person. Being able to acquire a visual impression of different letter networks and the corresponding persons offers immediate epistemological gains in the study of complex network relationships. Utilizing the aggregated metadata enables csLink to situate letters in a single edition within a wider context of correspondence and communication. Examples for applications of csLink are the digital editions humboldt digital https://edition-humboldt.de , as well as Weber Gesamtausgabe https://weber-gesamtausgabe.de/de/Index . We are currently developing additional ways to visualize the aggregated data and to make it more accessible for analysis and valuable for academic purposes. This necessarily includes a critical discussions of the interpretational character of visualisations, as for example in which ways a network visualisation already is making statements based on the composition of nodes on a canvas. In opposition to closed data services, the open data approach of CMIF and its potential in enabling any editionto provide their very own correspondence metadata is extremely beneficial to mapping wider networks of communication. Since any letter network to be exploredonly shows the information that is available as data already entered into correspSearch from letters that are already edited and in some way published, a larger base of people actually committing reliable data substantially improves the database and reliability of epistemological gains from this data. The development and usage of the CMIF Creator has proven very valuable for increasing the amount of aggregated metadata. Further addition of data and increasing connections between the letters in our data form a kind of crowd based validated open metadata that does not rely on a single contributor or institution. It is thus not only beneficial for network analysis in a strict analytical sense but also contributes to and implements an agenda to further establish open data principles in the digital humanities. As a leading provider for the decentralized aggregation of metadata from letter editions with the purpose to facilitate research on letter editions and correspondence networks on the basis of a standardised and open XML-format, correspSeach, together with CMIF and correspDesc, were awarded with the Rahtz Price for TEI Ingenuity 2018. "
	},
	{
		"id": 128,
		"title": "Translating NetworksAssessing Correspondence Between Network Visualisation and Analytics",
		"authors": [
			"Grandjean, Martin",
			"Jacomy, Mathieu"
		],
		"body": " Network interpretation is a widespread practice in the digital humanities, and its exercise is surprisingly flexible. While there is now a wide variety of uses in different fields from social network analysisto the study of document circulation metadataor literature and linguistic data, many projects highlight the difficulty of bringing graph theory and their discipline into dialogue. Fortunately, the development of accessible software, followed by new interfaces, sometimes with an educational dimension, has been accompanied in recent years by a critical reflection on our practices, particularly with regard to visualisation. Yet, it often focuses on technical aspects. In this paper, we propose to shift this emphasis and address the question of the researchers interpretative journey from visualisation to metrics resulting from the network structure. Often addressed in relation to graphical representation, when it is not used only as an illustration, the subjectivity of translation is all the more important when it comes to interpreting structural metrics. But these two things are closely related. To separate metrics from visualisation would be to forget that two historical examples of network representation, Eulerand Moreno, are not limited to a graphic reading. In the first case, the demonstration was based on a degree centrality measurement whereas in the second case the author made the difference between stars and unchosen individuals while qualifying the edges as inbound and outbound relationships. This is why this paper propose to examine the practice of visual reading and metrics-based analysis in a correspondence table that clarifies the subjectivity of the translation while presenting possible and generic interpretation scenarios. Visual approach: making the global structure readable The way we read networks has changed over time. Historically the question of network readability was asked in terms of aesthetic criteria. In the word of Jacob Moreno the fewer the number of lines crossing, the better the sociogram. Even in the nineties, when giving birth to the modern layout algorithm, Früchterman and Reingoldaimed at minimizing edge crossings and reflecting inherent symmetry. However these criteria do not seem so crucial to practices observed nowadays in digital humanities. Fig. 1 Different contexts for network visualisation in DH2016, DH2017 and DH2018 abstracts. Looking at recent papers in digital humanities, networks appear to have a wide range of usages. Their visualisations are either self-sufficient [fig. 1.a.], an optional help to understanding [fig 1.b.]or strongly connected to the text. Some authors use them to highlight the position of a specific node [fig. 1.c.], to compare layouts [fig. 1.d.]or the layout of the same graph in time [fig. 1.e.]. They may aim at visualising communities [fig. 1.f.], mapping a general structure [fig. 1.g.], tracking density patterns [fig. 1.h.]or monitoring algorithms like modularity clustering [fig. 1.i.]. These usages reveal a different perspective in network visualisation where we expect the visual to translate underlying relational structures. It helps to give different names to these two different approaches. We call diagrammatic the perspective where the network is a diagram that we read by following paths. We do not want the edges to cross and we use aesthetic criteria to bring clarity. It was Morenos perspective, and is still relevant to small networks and local exploration. Then we call topological the perspective where the network is a structure that we read by detecting patterns. We expect the visualisation to help us retrieve structural features like clustering or centralities. It is a common practice in digital humanities, more holistic and relevant to larger networks. Aside or in complement, classic data visualisation is also employed to visualise non-relational structures. Fig. 2 Various layouts do not follow a force-driven algorithm to make non-relational dimensions of the data explicit. In the topological perspective, a standard procedure is to assign nodes a position using a force-driven algorithm. This family of algorithms is known for displaying clusters that match a widely used measure of community detection, modularity clustering. Its translation remains however difficult to interpret locally, as we can never give a simple explanation for a nodes position. Classic data visualisation also translates non-relational structures, by itself or combined with a relational perspective. Different structural features may require different visualisations: the examples of fig. 2 shows curated visualisations using categories [fig. 2.a boys and girls, in the famous example of], temporality [fig. 2.b]or hierarchy [fig. 2.c]. Though very different from force-driven placement, they display better certain structural features. Objectifying the structure with metrics Often opposed to visual interpretation, of which they would be a more objective and reliable representation, centrality measures have a history that goes back to more than half a century and shows that they are not immutable and require constant adaptation to usage Moreover, Freemaninsists on the fact that the notion of centrality is the result of several intuitive conceptions. To remind that these metrics are based on intuition means to recognize that they have no meaning in themselves and that their interpretation must be rediscussed - and therefore translated - according to the context. This paper thus proposes to list and evaluate to which extent these metrics are applicable to humanities and social science data and can, if necessary, be translated into this language to complement visual analyses. Fig. 3 Three levels of interpretation that can be articulated: visual analysis, use of global metricsand use of local metrics. Global properties Statistical analysis allows for comparing networks across multiple dimensions at once. For instance, comparing the number of nodes and edges of different graphs of the same typecan be a ranking tool that is directly translatable into natural language. In addition to that, studies suggest that densityis relevant to analyse character networks, especially when compared within a homogeneous collection. This is also the case when measuring average path length. Local properties With regard to local measures, the degreeis the simplest centrality, and the only one systematically used between the late 1950s and early 1970s, before the development of more diversified metrics. Its simplicity allows for a transparent translation: in a literary network, for example, it counts the number of times one character speaks to another. The notion of betweenness centrality disrupts the conception of what the centre of a network may consist of. Its ability to reveal structural elements bridging large, immediately visible clusters makes it popular in the social sciences since the emergence of Granovetters concept of weak ties. Betweenness is very closely linked to the notion of circulation: it counts the shortest paths to detect intermediate bridges or key passages capable of opening or locking certain parts of the network. Depending on applications, these are therefore both positions of power and vulnerable places. The closeness centrality allows to highlight the geographical middle of the graph. In networks of a certain density and when they are not divided into several distinct communities, the closeness is generally fairly evenly distributed and allows a good translation of the notions of center and periphery. For its part, the eigenvector centrality is quite complicated to translate since it works iteratively and is very much dependent on the structural context at short and medium range around a node. Prestige or influence centrality, named power centrality by its author, it qualifies a nodes environment while operating in cascade: a well connected node gives its neighbours a part of its authority capital, and so on. It is therefore particularly useful when trying to analyze the hierarchy of the nodes in a graph. The most well-known use of this measure is the backbone of the Google search engine: the PageRank algorithm. Towards mixed approaches This contribution proposes a table of correspondence between the concepts of graph theory and the practice of visual network analysis in the social science and humanities. This effort must not be understood as a demarcationist attempt at telling the right method from the wrong. The dictionary is not exhaustive and only aims at helping to bridge two worlds that have more in common that what meets the eye. By focusing on translating methods, we want to stress that crossing points are real even though they do not come without issues, and thus require our methodological attention. We also note that the analysis should not be limited to a catalogue of well-known methodsbut that approaches combining several of those should be encouraged to obtain an optimal and innovative translation. In this way, we could compare metricsor combine them to establish rankings. Furthermore, the enrichment of the networks by means of categories that are not dependent on the structure, like the gender of individuals in a social networkor the discipline of projects in a scientometric analysis, allows to test translation and interpretation hypotheses by avoiding the blind approach of testing all possible graph metrics. "
	},
	{
		"id": 129,
		"title": "A Digital Catalogue of Medieval and Early Modern Manuscripts",
		"authors": [
			"Granholm, Patrik"
		],
		"body": " Introduction This paper presents the guiding principles and ongoing development of Manuscripta, a digital catalogue of medieval and early modern manuscripts in Sweden, which started as a project specific database but has since evolved to become a national infrastructure. The manuscript descriptions are encoded in TEI, which is a highly suitable metadata format for detailed, scholarly catalogues since the hierarchical structure of TEI corresponds to the four parts traditionally used in cataloguing: description of contents, codicological description, provenance, and bibliography. The digitised manuscripts are presented using the IIIF API, and the images are available free of restriction under the CC0 public domain dedication. The infrastructure is built entirely using open source software, and the source code, together with the TEI-files, are available on GitHub. Cataloguing and encoding principles Medieval and early modern manuscripts are seldom monographs, but often contain multiple texts by different authors. Furthermore, they have usually gone through several stages of production and use, e.g. been expanded, taken apart, lost certain parts, or been rebound with new additions. Occasionally new texts may have been inserted on previously blank pages. This historical stratigraphy has not, until recently, been taken into account in manuscript cataloguing, but recent scholarship and methodological developments have shown that the notion of codicological units is an essential requirement for detailed, scholarly catalogues, not least to distinguish different dates and provenances of various units, and to clearly indicate this information for each particular unit, which has previously not been the case. The manuscript descriptions are therefore structured around the notion of codicological units, and the customised TEI encoding schema, which also provides cataloguing guidelines with examples and documentation of the elements and attributes used, has been tailored to this end. This is one of the first digital catalogues to implement this state-of-the-art research, which at times has been called a codicological revolution. There have so far only been a few printed catalogues offering this form of stratified cataloguing. The structure of the TEI-encoded descriptions follows as closely as possible the hierarchical structure of the manuscripts: the intellectual content, physical description, and history, where applicable, of each unit are described in separate <msPart> elements, whereas information common to all units, e.g. the binding, provenance, and bibliography, is described one level higher, directly under the <msDesc> element. The <msPart> element is always used, even when a manuscript consists of only one codicological unit, to reflect the notion of the monomerous codex, a term coined by Gumbert. Technology The infrastructure is built entirely using open source software which is an essential requirement for transparency and for ensuring long-term maintainability. It runs on an XML database called eXist-db, which offers advanced indexing and search functionality through XQuery, and built-in functions for converting TEI to HTML and PDF. The digitised manuscripts are displayed adjacent to the descriptions, with page references linked to the digitised manuscript, enabling immediate access to specific locations in the manuscript. Part of the description is given in running text, and TEI provides a variety of possibilities for tagging words and phrases, e.g. information such as names, places, dates, writing material, and watermarks. These tags then allow for advanced text search queries. In addition, names, places, and bibliographical references are linked to authority files which have information on alternative name forms, short biographical and geographical data, and links to other external resources. Since TEI is based on XML, it can easily be converted into other formats like HTML, PDF, MODS, JSON, RDFa. This is an essential requirement for the data to be transferable between different platforms and to enable Linked Open Data. Cataloguing is done using a web-based editing interface, which does not require any knowledge of TEI encoding and therefore simplifies and reduces the time required for the cataloguing process. The interface is built with React.js. Previously, it has been necessary to use an XML editor which had a steep learning curve, and was time-consuming as well as error prone, even when validating with an encoding schema and using detailed cataloguing guidelines. Future plans More descriptions will be added to Manuscripta continuously, not only born digital descriptions, but also descriptions from legacy databases like Microsoft Access and FileMaker, and printed manuscript catalogues, using OCR and data extraction. These will then be encoded into the same schema as the born digital descriptions. Further development will include adding a controlled vocabulary for technical terms used in the manuscript descriptions and authority files for works preserved in the manuscripts; enriching the HTML with Microdata, i.e., machine-readable data for the semantic web which would enable search engines to give users more accurate search results; creating stylesheets for converting the TEI of the authority files into linked open data formats like RDF and JSON-LD. "
	},
	{
		"id": 130,
		"title": "Re-inventing the Past Through Singapore Memory Project: Socio-political Complexities of Digital Crowd-sourcing Techniques",
		"authors": [
			"Grincheva, Natalia"
		],
		"body": " In my presentation I will explore how online audiences experience time in digital museum communities, and how these experiences change their cultural perceptions and identities. The project will focus on the online museum case study: Singapore Memory Project. It is an online national initiative for public memory preservation. It was facilitated by the National Libraries and Museums Board in 2011 to collect and provide access to Singapores culture through crowdsourcing. It aimed to tell the Singapore Story through the voices of ordinary people in the live stream of their shared memories. The construct of time appears to be the most important in the Singapore Memory Project, which even in its title speaks through the language of time. The recreation of the cultural past is an important political task for Singapore. Being a young country, the government strives to establish the countrys legitimacy through representations of evidence of its very existence in time, as this brings more leverage when negotiating identity and interests with various parties in the global context. Crowdsourcing techniques, involving ordinary people in recreating national memory, appears to be a quite sophisticated technology of manipulating the construct of time within the national consciousness. Under the pressure of national identity crises, the memory project worked as an important step to naturalize public perceptions and understanding of the nation within their political and cultural history. Due to extremely rapid economic growth in the last fifty years, which completely reshaped physical, social, and cultural landscapes of the country, many important cultural places, symbols, and objects of significance were destroyed or lost. In this situation, it was imperative for Singapore to rebuild its cultural dimension through revitalizing and nurturing cultural memory and reconstituting national cultural identity. Aiming to rebrand the cultural image of Singapore, the government aimed to build upon public efforts of collective memory construction. Employing interviews with governmental officials and museum managers, as well as content discursive analysis of the online memory portal, in my research I analysed how this digital space reconstructed time through museum narratives communicating political messages across borders. I also explored how these narratives were aligned with national and foreign policy objectives of the country revealing social and cultural complexities of the memory crowdsourcing exercise. The presentation will contribute to the topic of the conference by providing an analysis of the contemporary politics of chronology through the lenses of digital diplomacy implemented within online museum environments changing human perception of time. The project will draw on the conceptual framework of French critical philosopher Bernard Stiegler, who explored the technologies of human consciousness manipulation in his seminal series, Techniques and Time. He elaborated on the processes of industrialization and externalization of memory which in digital networks operate through much shorter circuits of informational flows and exchanges on a global level. The increased speed of communication in online networks makes it both easier and less expensive to deliver texts, music, symbols, and images to people around the world, thus accelerating global consumption of information and formation of cultural environments. According to Stiegler, the mechanisms that are in place in the reconstruction of human experiences through interaction within digital networks can be explained by the ability of digital communication to represent the pasts of others while being in the present for an individual. As a result, history, traditions, and communities can be instrumentalized and transmitted, thus creating possibilities for reconstructing historical past through digital narratives changing human perceptions of time. The presentation will identify and outline political implications of reconstructing time and memory through an empirical analysis of the Singapore Memory Project. This online museum portal provided an excellent illustration of the theoretical paradigms developed by Stiegler to describe the modern political technologies modulating public consciousness in the digital realm. More importantly, though, the Singapore Memory Project provided a platform where the power of digital technologies to change human perception of time could be tested. Through a focused analysis of the public engagement with the portal my research reveals that a crowdsourcing activity can actually turn into a political machine of inciting nationalism and constructing citizenry. "
	},
	{
		"id": 131,
		"title": "World-Historical Gazetteer",
		"authors": [
			"Grossner, Karl",
			"Mostern, Ruth M."
		],
		"body": " This paper reports on World-Historical Gazetteer, a three-year project funded by the US National Endowment for the Humanities, now two-thirds complete. Goals and Purpose WHGazetteer is a scholarly infrastructure project intended to support historical research across many disciplines. It is principally a web-based software system for aggregating open access data about places and linking it with data about historical entities associated with those places. We have seeded the system with a global spine dataset of some 30,000 places, developed expressly for this purpose. There are existing repositories of historical place data, and many more are in development. A few are explicitly termed gazetteers E.g. Pleiades; Vision of Britain , some are historical GIS web resources E.g. China Historical GIS ; HGIS de las Indias , and others comprise place data tables developed and published in the course of other research projects. Typically, each concerns a particular geographic area and a particular temporal scope E.g. Map of Early Modern London ; Syriaca.org . Bu contrast, WHGazetteer is soliciting and aggregating attestations of historical places for all regions and periods, along with annotations of records about historical entities with identifiers for those places. The project is conceived from a world-historical perspective. By this we mean several things. First, that it is intended to facilitate representations of connection and movement; second, that it scales up to global processes and the longue durée; and finally, the places for us includes ethnonyms for cultural regions, physical features, and ecoregions, helping ensure coverage for all parts of the globe and providing geographic context. In many respects WHGazetteer follows and extends the pilot implementation of Peripleo, the linked data gazetteer system developed by the Pelagios Commons project. We are working closely with the Pelagios group to leverage their accomplishments, maintaining and extending the trajectory they established. Places and Traces WHGazetteer will solicit contributions of two kinds of place-related data. One is attestations of place names found in historical texts and maps: Places. The other is annotations of records about any sort of historical entitywith place identifiers from published gazetteers: Traces. Places WHGazetteer catalogues place references associated with a time period, which may be a date of publication for an historical source or temporal information about a name as in a modern historical atlas. In either case, a historical gazetteer records attestations from sources that assert that a name existed at a certain time. This approach to temporal scoping differentiates us from modern gazetteer data sources like GeoNames and Wikidata. A single place mayhave many linked attestations for different periods, where names, spellings, place type, relations, and geometry vary. Traces Traces are assertions of spatial and temporal scope for historical items of almost any kind: the footprint of an individuals lifepath; the findspot of a coin hoard; the itinerary of a journey; the extent of a war; the places referred to in any sort of text, from a treaty to a novel; and so on. Standard Data Formats We are developing two standard contribution formats conforming to linked data requirements. The first, Linked Places format, is essentially complete and is being tested with early contributions. Linked Traces format, based on the W3C Web Annotation Data Model, will be completed in late spring, 2019. Contribution Pipeline Projects contributing to WHGazetteer come in all sizes and have varying levels of technical capability. Some have elaborate web interfaces and maintain permanent URIs for thousands of place records. These projects will have little difficulty exporting abbreviated records transformed to Linked Places format. Others have dozens or at most hundreds of place references drawn from sources specific to their domain of interest and are unable to stand up per-place pages with permanent URIs. The WHGazetteer contribution pipelineallows both groups to a) upload compatible Linked Places or CSV files, b) reconcile their records against Getty TGN, DBpedia, GeoNames and Wikidata as well as the WHGazetteer index itself, c) review and validate results of that matching process, thereby augmenting their records with closeMatch or exactMatch links, and d) contribute reviewed records to the index, published under permanent URIs provided by WHGazetteer. Backend Stores and Middleware The WHGazetteer system backend is comprised of a PostgreSQL relational database and multiple Elasticsearch index stores. These and the APIs for internal use and public access are managed with Django, a Python-based web development framework. Interfaces WHGazetteer will provide a public API supporting machine queries to all indexed data. A graphical web interface will support contribution activities for authenticated users, and provide search and mapping capabilities for both Places and Traces. Linking Place and Trace data in a single backend allows us to present linked data portals for all indexed places, which will grow richer over time as our indexes expand. Using either the public API or graphical interface one might discover, for a given place: its names, shapes, and relations with other places over time; people whose lifepaths have intersected it; journeys for which it has been a waypoint; and texts and artworks for which it is a subject. Our contribution pipeline interface will enable several kinds of pedagogical applications. For example, students and instructors will be able to create custom Traces associated with course material. Advanced students will be able to upload CSVs of gazetteers they have created, and use the WHGazetteer reconciliation tool to augment their records with information in the WHGazetteer index. Status Version 1 of WHGazetteer is scheduled for launch in January 2020. A beta version will be available by July 2019. We have established data partnerships with roughly a dozen contributors of large datasets covering a range of regions and periods, and also have a queue of many smaller projects. Data pipeline functionality to receive these is nearly complete. Sustainability WHGazetteer has been designed to require minimal hand-holding for contributions, and tools for efficient curation and maintenance. University of Pittsburghs World History Center is committed to maintaining the system for the foreseeable future. Figure 1 – Data pipeline:upload dataset;perform reconciliation against modern authorities;review/validate reconciliation hits "
	},
	{
		"id": 132,
		"title": "Diachronic Topics in New High German Poetry",
		"authors": [
			"Haider, Thomas Nikolaus"
		],
		"body": " Introduction Statistical topic models are increasingly and popularly used by Digital Humanities scholars to perform distant reading tasks on literary data,. It allows us to estimate what people talk about. Especially Latent Dirichlet Allocation, see, has shown its usefulness, as it is unsupervised, robust, easy to use, scalable, and it offers interpretable results. In a preliminary study, we apply LDA to a corpus of New High German poetryand interpret salient topics, their trend over time, and use the distribution of topics over documents for a classification of poems into time periods and for authorship attribution. Corpus The Digital Library in the TextGrid Repository represents an extensive collection of German texts in digital form. It was mined from http://zeno.organd covers a time period from the 16th century up to the first decades of the 20th century. It contains many important texts that can be considered as part of the literary canon, even though it is far from complete. We find that around 51k texts are annotated with the label verse, not distinguishing between lyric verse and epic verse. We will further call this verse portion TGRID-V. However, the average length of these texts is around 150 token, dismissing most epic verse tales. Furthermore, the poems are distributed over 229 authors, where the average author contributed 240 poems with a median of 131. A drawback of TGRID-V is the circumstance that it contains a noticeable amount of French and Dutch. To constrain our dataset to German, we filter foreign language material with a stopword list, as training a dedicated language identification classifier is far beyond the scope of this work. Figure 1: 25 year Time Slices of Textgrid PoetryExperiments We approach diachronic variation of poetry from two perspectives. First, as distant reading task to visualize the development of clearly interpretable topics over time. Second, as a downstream task, i.e. supervised machine learning task to determine the yearof publication for a given poem. We infer topic distributions over documents as features and pit them against a simple style baseline. We use the implementation of LDA as it is provided in genism. LDA assumes that a particular document contains a mixture of few salient topics, where words are semantically related. We transform our documentsto a bag of words representation, filter stopwords, and set the desired number of topics=100 and train for 50 epochs to attain a reasonable distinctness of topics. We choose 100 topicsas we want to later use these topics as features for downstream tasks. We find that wordformsare more useful for poetry topic models, as these capture style features, orthographic variations, and generally offer more interpretable results. Topic Trends We retrieve the most importantwords for all 100 topics and interpret theseword lists as aggregated topics, e.g. topic 27contains: Tugend, Kunst, Ruhm, Geist, Verstandand Lob. This topic as a whole describes the concept of artistic virtue. In certain clusterswe find poetic residuals, such that rhyme words often cluster together, e.g. topic 52 with: Mund, Grund, rund. To discover trends of topics over time, we bin our documents into time slots of 25 years width each. See figure 1 for a plot of the number of documents per bin. The chosen binning slots offer enough documents per slot for our experiments. To visualize trends of singular topics over time, we aggregate all documents d in slot s and add the probabilities of topic t given d and divide by the number of all d in s. This gives us the average probability of a topic per timeslot. We then plot the trajectories for each single topic. See figures 2–6 for a selection of interpretable topic trends. Please note that the scaling on the y-axis differ for each topic, as some topics are more pronounced in the whole dataset overall. Figure 2: left: Topic 27 Virtue, Arts, right: Topic 55 Flowers, Spring, GardenFigure 3: left: Topic 63 Song, right: Topic 33 German NationFigure 4: left: Topic 28 Beautiful Girls, right: Topic 77 Life & DeathFigure 5: left: Topic 60 Fire, right: Topic 42 FamilyFigure 6: Most informative topics for classification; left: Topic 11 World, Power, Time, right: Topic 19 Heaven, Depth, SilenceSome topic plots are already very revealing. The topic artistic virtueshows a sharp peak around 1700—1750, outlining the period of Enlightenment. Several topics indicate Romanticism, such as flowers, songor dust, ghosts, depths. The period of Vormärz or Young Germany is quite clear with the topic German Nation. It is however hardly distinguishable from romantic topics. We find that the topics Beautiful Girlsand Life & Deathare always quite present over time, while Girls is more prounounced in Romanticism, and Death in Barock. We find that the topic Fireis a fairly modern concept, that steadily rises into modernity, possibly because of the trope love is fire. Next to it, the topic Familyshows wild fluctuation over time. Finally, figure 6 shows topics that are most informative for the downstream classification task: Topic 11 World, Power, Timeis very clearly a Barock topic, ending at 1750, while topic 19 Heaven, Depth, Silence is a topic that rises from Romanticism into Modernity. Classification of Time Periods and Authorship To test whether topic models can be used for dating poetry or attributing authorship, we perform supervised classification experiments with Random Forest Ensemble classifiers. We find that we obtain better results by training and testing on stanzas instead of full poems, as we have more data available. Also, we use 50 year slotsto ease the task. For each document we determine a class label for a time slot. The slot 1575–1624 retrieves the label 0, the slot 1625–1674 the label 1, etc.. In total, we have 7 classes. As a baseline, we implement rather straightforward style features, such as line length, poem length, cadence, soundscape), and a proxy for metre, the number of syllables of the first word in the line. We split the data randomly 70:30 training:testing, where a 50:50 showsworse performance. We then train Random Forest Ensemble classifiers and perform a grid search over their parameters to determine the best classifier. Please note that our class sizes are quite imbalanced. The Style baseline achieves an Accuracy of 83%, LDA features 89% and a combination of the two gets 90%. However, training on full poems reduces this to 42—52%. The mose informative featuresare: Topic11, Topic 37, Syllables Per Line, Length of poem in syllables, Topic19, Topic98, Topic27and Soundscape. For authorship attribution, we also use a 70:30 random train:test split and use the author name as class label. We only choose the most frequent 180 authors. We find that training on stanzas gives us 71% Accuracy, but when trained on poems, we only get 13% Accuracy. It should be further investigated is this is only because of a surplus of data. Conclusion & Future Work We have shown the viability of Latent Dirichlet Allocation for a visualization of topic trends. While most topics are easily interpretable and show a clear trend, others are quite noisy. For an exploratory experiment, the classification into time slots and for authors attribution is very promising, however far from perfect. It should be investigated whether using stanzas instead of whole poems only improves results because of more available data. Also, it needs to be determined if better topic models can deliver a better baseline for diachronic change in poetry, and if better style features will outperform semantics. Finally, only selecting clear trending and peaking topicsmight further improve the results. "
	},
	{
		"id": 133,
		"title": "Misremembering Machines: A Creative Collaboration on Memory in AI-driven Storytelling",
		"authors": [
			"Hall, Elliott",
			"Smithies, James"
		],
		"body": " Context and collaboration The Applying AI to Storytelling project was a high-risk research and development project at the intersection of the digital humanities, computer science, and the creative industries, funded by Innovate UK. The project embedded a university-based Research Software Engineerfrom a Digital Humanitieslab in the core development team of a start-up company pioneering interactive storytelling techniques, with a particular focus on character-based chatbot technologies powered by artificial intelligence. The project therefore sits at a crossroads between academic DH, literary-critical analysis, the creative industries, computer science, and software engineering. In doing so it represents the emergence of a new strand in DH practice that seeks to take lessons learned over decades of incubation within universities and cultural heritage organisations into the wider world. The work is also representative of a wider socio-cultural convergence of advanced technologies with creativity and academic research. 1 Research Software Engineeringis growing rapidly in the United Kingdom, United States, and Australasia and is increasingly being used as an umbrella category that DH practitioners can contribute to 2. That is the case at our university, where DH teams work within a wider RSE community comprising specialists in bio-informatics, imaging science, and data analysis. 3 While RSE teams in the science and technology sector have long-established relationships with industry partners--creating a significant traffic in ideas, methods, and personnel across HE and industry boundaries-- DH RSE teams have until now focused mostly on developing relationships with the cultural heritage sector rather than industry. Embedding a DH RSE within a creative industry SME as a connection point between industry and academia aims to facilitate a similar traffic in ideas and methods between the Creative Industries, RSEs and academics while at the same time demonstrating the value of DH to higher education and government sectors that are increasingly focused on the creation of tangible economic value. DH methods are well suited to such an undertaking, providing an important element in both the creative and technical processes, and assisting with the translation of research in digital literary studies, 4 media history, 5 post-humanism, 6 and narrative design 7 to a commercial product that has captured the interest of companies in the United States and Europe. The partner company involved in this project is developing a system that accelerates the creation of AI-driven storytelling, recommending possible dialogue to the author using natural language processing and visually managing the complex branching structures these narratives require. This system is a product that is currently in the beta stage and already licensed to external companies for the creation of interactive stories, as well as film and television productions. Our collaboration extends established techniques in immersive storytelling and extended realityinto the commercial world of product design and development. 8 This kind of work is being encouraged by the UK government through a variety of funding mechanisms, as it aims to maximise the countrys natural advantages in cultural and creative production. 9 The partnership used the companys core product to produce a ground-breaking translation of an immersive theatre piece into an AI-driven game written in the Unity engine. The show relies on audience participation, assigning them tasks and roles and changing the direction of the narrative based on their choices. The adaptation places audience members with a cast of AI characters, using machine learning to inform their personalities, intent, and emotional interactions with the player. The problem of memory The key focus of the collaboration with the DH team is the problem of memory that results from characters who remember what the player has said and done. This question is what brought the tech company CEO to our DH team in the first instance: they had reached a point where their technology had moved beyond their writers understanding of narrative. Their goal in partnering with a DH RSE was to better understand the literary-critical issues associated with digital characters who can remember information given to them by readers, and improve the writing product they aim to sell into the creative industries. Perhaps surprisingly, given the long history of network fiction 10 that stretches back to the electronic literature movement of the early 1990s and beyond to Multi-user Dungeons of the 1970s, they were struggling to find writerscomfortable with the radical level of emotional interaction support for memory creates. The persistence of memory in narrative has been noted as an affordance of even simple hypertext fiction, 11 but holds additional micro-level implications for interactions with players, and profound macro-level implications for the overall direction of a narrative, when super-charged with AI. AI characters in the game remember, misremember and outright lie, challenging the players decisions, and even their own memory of what has transpired in the game. Maximising the potential for this to support high quality narrative is non-trivial, as is developing an elegant writing tool capable of supporting such new modes of imagination and reader interaction. In building this prototype the project faced a number of overlapping, multidisciplinary challenges. The narrative authoring system had to be comprehensible to non-technical users -- especially writers -- to be used effectively, while at the same time retaining technical scalability and sustainability. The optimisation of the machine learning algorithms to create convincing character interactions required a critical examination of their output in both literary and social science terms, identifying how conscious and unconscious bias informs the authors conception of the character and the players responses to it. This convergence of technical, creative, academic and commercial factors make the project a natural fit for Research Software Engineers working within Digital Humanities. Difficulties in structuring the dialogue progression within the UI, or in how the algorithm follows particular narrative paths and not others, are often technical, conceptual and cultural questions all at once. These interrelated problems require not only the range of skills provided by a partnership between academia and the creative industries, but the high-risk, experimental nature of the work demands the strong connective tissue that Digital Humanities provides in mediating across scholarly, technical and business languages. Versatile and eclectic RSE The unique nature of this collaboration, and evidence of the significant value DH skills and mindsets offer the commercial product development process, is demonstrated by the fact that the lead RSE is also a writer who has worked in immersive theatre. This relatively rare set of skills enabled him to work with the shows author on the adaptation; developing characters, designing narrative, and writing and editing dialogue. At the same time he provided granular, detailed feedback to the development team on the usability of the UI, and the challenges that both he and the writer encounter in structuring a shared immersive experience as a single player interactive game. Rather than being only a conduit for the academic and creative sides of the collaboration, therefore, the lead RSE was involved in both the creative and technical processes that created the final product. As the project evolved, changing priorities and deadlines led the RSE to take on a more creative role, becoming the primary writer of a showcase demo for the immersive theatre project and developing the narrative structure for the adaptation of a novel into the system. As a result of this change in responsibilities, the RSE identified issues as a writer that he then helped solve as a developer. As an example, in order to better test the narratives he was creating, he developed a tool to automatically run a set of player responses, effectively creating a unit test for the story. He then used this tool as a writer to edit and refine the narrative paths inside the AI-driven system. This model of collaboration created an intermediate space where DH methods allow seamless movement between the roles of developer and writer, often on the same question. This ability to not just translate between the creative spaces of game development and writing, but to inhabit both simultaneously, is essential to the success of creative collaboration, and more accurately reflects the current state of digital creativity in the independent sector. A participatory model of creative co-production An important secondary aspect to the research is the development of a model capable of defining optimal modes of working between DH RSE teams and the creative industries. The model was created through an ethnographic approach pioneered in the Science and Technology Studiescommunity by researchers such as Latour and Woolgar, Knorr-Cetina, 13 and Fujimura 14 in conjunction with industrial theories such as lean production15 and disruptive innovation. 16 The projects Principal Investigatordeveloped the model by embedding himself in the creative process, observing interactions between writers, technologists, business people, and the DH RSE over the course of 12 months. The insights gained will be used to generate a best practice model to facilitate a mutually beneficial partnership between academia and industry, one that can be used by other DH RSE teams, increasing capability across the global community and demonstrating the special value DH has for companies and governments increasingly struggling to recruit and retain staff with convergent skill sets spanning creative and technical fields. Effective collaboration between universities and industry is essential for the higher education sector to maintain its leading status in research and innovation, and to ensure industry can resolve the tension between convergent creative and technological pressures. Alongside this industry innovation, of course, is the equally important parallel development of a critical infrastructure within the humanities to effectively engage, modify and critique these emerging forms of narrative. This project provides an example of how Digital Humanities RSE teams can act as key participants in this process, using their blended culture of development and research to avoid an excluded middle, where academic and creative teams lack the vocabulary, practical experience, and cultural experience to collaborate effectively together. "
	},
	{
		"id": 134,
		"title": "Deep Digitization: Considerations and Tools for Imaging Cultural Heritage Beyond the Basics",
		"authors": [
			"Hanneken, Todd R."
		],
		"body": " A digital image of a painting is not the painting. A digital image of a folio is not the folio. It is an artifact in its own right that conveys, or fails to convey, the information one would want to study. Just as human perception is full of complexities, the construction of a digital facsimile is full of complexities. A very simple image may be recognizable as a representation of the original, while a very advanced image may fail to satisfy the questions of research. The project of digitization requires reflection on the nature of scholarly perception of the artifact and the digital tools suited to capture and represent the pertinent information. It is no longer enough to consider spatial resolution. This presentation considers first the modes of perception in scholarly investigation of artifacts such as manuscripts including the importance of texture and interactivity for humanities questions. Second, we will consider the ability of spectral imagingto meet and surpass the capabilities of the human eye on first-hand inspection. Third, we will consider the tools for capturing and representing texture and interactivity with raking light photography and Reflectance Transformation Imaging. Finally, we will present the results of a recent project, funded by the U.S. National Endowment for the Humanities, to integrate Spectral Imaging and RTIin capture, processing, and visualization. The software and documentation created by the project are freely available online and accessible for use off the shelf by imaging teams. Scholarly investigation utilizes a variety of modes of perception depending on the questions brought to the artifact. Often the interest is in reading text written on the artifact, which can be difficult if the manuscript was deliberately erased for reuse or otherwise damaged. The difficulty only increases if the writing was secondary in the first place, as with marginal or interlinear notes or quire numbers. Some scribal markings, such as scoring lines, are not even inked in the first place. Similarly, dry-point notation could be an important object of study, yet completely invisible in diffuse-light photography. Limited considerations for digitization may be appropriate if a manuscript is valued only as a text container. Different considerations for digitization are appropriate if a manuscript is valued as an artifact of scribal cultures. To all these examples from manuscripts many more could be added for other media, such as paintings. The study of such objects in person, without digital or other mediation, involves first and foremost movement. The scholar moves the relationship between the eye, the object, and the light, and changes the lighting. The movement could mean moving closer or farther, to a different angle, or even holding the object up to a light to see if light passes through. Just as no one still position satisfies every inquiry, no one still photograph creates an adequate facsimile for scholarly investigation. Indeed, the experience of interaction surpasses the sum of individual moments of perception. The value of an interactive experience is strong for research, and even stronger for teaching. For all these reasons it is still common to hear there is no substitute for first-hand experience. While experience with a digital surrogate may never be identical, the number of questions that can be answered from the surrogate depends on the methods of digitization. The limits of digitization are not absolute. When justified, the use of spectral and texture imaging may allow a digital copy to approach and in some ways surpass first-hand experience. The following images show the same page digitized six ways. The object is Biblioteca Ambrosiana C73 inf page 110, a palimpsest with the Testament of Moses overwritten with Eugippius anthology of Augustine.1-bit black and white by way of a print editiongrayscale by way of microfilmDSLR camera 2011Multispectral accurate color 2017Accurate color with raking lightMultispectral enhanced color with raking light. Spectral imaging expands the range and resolution of color perception of the human eye. The human range spans from violet to red, excluding ultraviolet on one end and infrared on the other. Human color resolution is limited to three receptors, so all the colors we see are combinations of intensity of three elemental colors, red, green, and blue. Digital spectral imaging measures reflectance of pixels in an image at specific wavelengths of light. These wavelengths can extend to ultraviolet and infrared, and resolve colors more finely than the three receptors of the human eye. Just as someone with only two kinds of receptors can be called color blind, all the more so a person with three receptors is limited compared to the sixteen or more possible with spectral imaging. In addition to reflectance, spectral imaging today often captures fluorescence and transmission, comparable to use of a blacklight and holding a light behind the object, respectively. Because the capture is digital, the numerical values of light intensity at each picture element under each of dozens of conditions can be processed in relationship to each other. As a result, color can be captured and rendered more accurately than a simple color camera, and algorithms can enhance contrasts, such as erased ink on parchment, that could never be perceived with the unaided eye. Texture imaging is most simply accomplished with raking light. That is, the light is positioned from one direction at a low angle to the object. Any readable image of a coin, inscription, or cuneiform tablet uses this approach. The main limitation of raking light imaging is the judgment of the photographer in anticipating all the light positions that would be necessary to interpret the artifact. One solution is capture a complete set of raking light images from forty to sixty different positions. Again, since the information is digital and can be easily processed by algorithms, such a data set could be processed into a dynamically relightable image. This technology, called Reflectance Transformation Imaging, can extrapolate to light positions not directly captured, can enhance the texture, and is fully interactive. Imaging the texture of a surface is distinct from, but complementary to, 3D imaging. 3D imaging, using techniques such as laser scanning or photogrammetry, digitizes the boundary structure of an object. Texture imaging digitizes the reflectance properties of the surface as a function of light position, usually at much higer resolution. 3D imaging is well suited to capturing the shape of an object. For any one surface on that shape, RTI is well suited to capturing the fine texture, roughness, and specularity. The two technologies are entirely compatible, as a 3D engine can render both the shape of many surfacesand the shader properties of each surface. The first image following is derived from a 3D model created by laser scanning. The second image is photographed with raking light. Each has advantages and can be interactive in different ways. The object is Cyprus Museum 1885, a Cypro-Minoan tablet with an undeciphered writing system. Digital spectral imaging and RTI have been in development since the beginning of the century when high-quality digital imaging became widely available. The recent development is the publication of the technique, open-source software, and documentation for integrating spectral and reflectance transformation imaging. A pair of grants from the U.S. National Endowment for the Humanities supported the experiments and development of the technique, followed by full-scale implementation, documentation, and open-source software that performs the processing entirely within a graphical user interface. The fundamental premise of the integration is that chrominance and luminance can be combined in color spaces such as YCbCr. Spectral imaging is concerned with chrominance and strives to avoid highlights and shadows by using diffuse illumination. RTI is concerned with luminance variation as a function of light positionand pays no more attention to color than what is captured from a conventional camera with a Bayer filter. All the data can be captured with one camera and combined in post-capture processing. All the required software is freely available, most notably the SpectralRTI_Toolkit plugin for ImageJ. "
	},
	{
		"id": 135,
		"title": "Copyright and Humanities Research: A Global Perspective",
		"authors": [
			"Hannesschläger, Vanessa",
			"Kamocki, Pawel",
			"Scholger, Walter"
		],
		"body": " Despite the fact that the first copyright acts were written for the Encouragement of Learningor to promote the progress of Science, the research community regards todays copyright law as a foe, rather than as a friend. In the digital world, copyright has evolved into a framework regulating access and re-use of all sorts of contents. This evolution did not spare the research community whose activities were, even until quite recently, regarded as de minimis from the point of view of copyright. As copyright is within the scope of national legislation, international cooperations to carry out research in a digital environment have to deal with a highly complex legal situations. Largely based at universities, cultural heritage institutions or other public research institutions, humanities research is usually non-commercial and based on a public mandate for education. Openaccess to sources and results of this research gains further importance because most national funding agencies demand open access to research publications and data as a requirement for funding. On the other hand, researchers themselves have a keen interest in defending their own intellectual property rights, in part due to economic concerns but also in terms of academic credit. This conflict of interest is summarized in the Universal Declaration on Human Rights, which sets the premise that the right freely […] to share in scientific advancement and its benefits, but goes on to say that everyone has the right to protection of the moral and material interests resulting from any scientific, literary or artistic production of which he [sic] is the author. For digital humanities scholars, this axiomatic situation combined with recently developing trends towards open licensing has created an urgent necessity to educate themselves on legal issues and discuss how copyright legislations impact and shape what wedo; [t]he rapid worldwide expansion of digital humanities work demands that we begin to deal with the complex tangle of rights around digital humanities knowledge production before others do it for us.One of the most important reasons for the necessity to deal with copyright issues from the scholars perspective is that copyright laws are the framework within which we have to negotiate our research ethics, especially with regards to questions of the possibility of access to our work for all. For this reason, Zafrin et al. began the discussion of copyright issues for DH in a global context with their panel session at the 2017 DH conference in Montréal. At this years DH conference, we would like to build on Zafrin et al.s work and continue the discussion in the framework of a full-day workshop. Zafrin et al. presented perspectives on copyright from Mexico, India, and the USA. We would like to broaden the scope and discuss the topic on a global scale, as the concept of copyright has developed in manifold ways in different areas of the world for historical reasons. The continuation of this discussion is especially important for the digital humanities community because the transcending of national borders is inherent to the nature of digital research. In most countries across the globe - today also in the USA, which was not always the case -, authors dont need to apply for copyright protection because it follows the authors pen across the page.This might sound reasonable in principle, especially for cultural works. However, it has been argued that copyright is an unsuitable legal structure for scientific works [because] scientific norms guide scientists to reproduce and build on others research, and default copyright law by its very purpose runs counter to these goals, the de facto situation of copyright legislations requires us to consider them when developing our research, both in a national context and from a global perspective. However, copyright legislations by nature stop at national borders. While international agreements can smoothen this contrariety to some extent, it is still vital to know and understand the different concepts of copyright in the diverse legal traditions across the world. The European Union has taken a first step in the direction of copyright law harmonization by accepting the Copyright in a Digital Single Market Directiveproposal, by means of which copyright is supposed to be modernized and adapted to the realities of the digital world throughout the EU. From the point of view of the research community, the proposal has its upsides and downsides; while it does aim to implement a general permission of text and data mining in a scientific context, the articles on Protection of press publications concerning digital usesand Use of protected content by information society service providers storing and giving access to large amounts of works and other subject-matter uploaded by their usersare being discussed controversially. As the directive proposal was approved by the EU Parliament in September 2018, but has not yet been formalized, the effects on the legal conditions of research in Europe are not yet clear. Another example of the harmonization lack discussed by Darlingis the possibility to grant rights to unknown uses, a topic of interest in the context of open licensing: [c]onsistent with notions of freedom of contract, United States copyright law allows authors to grant publishers the rights to all known or unknown uses of a work. Despite the ostensible clarity of this norm, courts have struggled considerably with cases where the scope of rights transferred is uncertain. New media developments have generally prompted litigation and the issue of which exclusive rights can and should be implicitly licensed has never been resolved with consistency. [...] Looking across borders, it is apparent that other countries have been dealing with similar issues within their copyright systems. Many countries, however, have chosen a different approach to the problem. This diagnosis is true for many aspects of copyright legislations. The problem of different approaches to intellectual property is not only true on a global scale, but can even be an issue within a relatively harmonized area such as the EU: As IP law in the European Union is merely harmonized and not unified, the exact scope of copyright and similar rights may differ between Member States.It is thus vital to foster exchange between scholars coming from different national situations in order to better understand the situation we are dealing with in the borderless land that is the internet. Therefore, a call for contributions to the workshop will be issued, inviting scholars from all over the globe with expertise in handling copyright issues in the context of digital research to discuss the following questions: What are the specific benefits and pitfalls of the copyright legislation in your country for digital scholarship? Are the existing international copyright-related agreementssufficient? What aspects of digital scholarship do current copyright legislations and agreementsnot cover or even actively hinder? Does the copyright legislation in your country facilitate or hinder the attribution of free licenses or certain types of free licenses in any way? What exceptions for research purposesdoes the copyright legislation in your country foresee? Are they sufficient? Contributors can address one or more questions from this non-comprehensive list of issues. Discussion of further aspects is welcome, especially relating to data privacy issues, licensing questions, and new challenges arising from emerging trends towards open science. Workshop leaders Vanessa Hannesschläger ACDH-OeAW - Austrian Centre for Digital HumanitiesE-mail: Vanessa.Hannesschlaeger@oeaw.ac.at Vanessa Hannesschläger is a researcher at the Austrian Centre for Digital Humanities of the Austrian Academy of Sciences. She is the leader of the ACDH-OeAWs task force on legal issues, co-chair of the DARIAH working group on ethical and legal issues, member of the CLARIN legal issues committee, and co-chair of the Open Science Network Austrias working group Legal Aspects of Open Science. Her interest in legal aspects of humanities scholarship and cultural heritage was initially sparked during a project dealing with archival material carried out at the Austrian National Library. In the course of her Wikimedia Germany Open Science Fellowship, she has gained further expertise in the area of legal and licensing issues relating to open knowledge. Her research interests include legal frameworks of digital research, data modelling, and contemporary developments of gender issues in society. Pawel Kamocki ELDA - European Language Resources Distribution Agency E-mail: pawel.kamocki@gmail.com Pawel Kamocki received university training in both lawand linguisticsin France, Germany and Poland. Currently a Project Manager at ELDA, he worked with many European and international projects, including CLARIN, EUDAT and Research Data Alliance. His main field of interest are legal issues affecting data-intensive science, including copyright and data protection. He is a corresponding member of the editorial team of the Multimedia und Recht law review. Walter Scholger ZIM-ACDH - Centre for Information Modelling, Austrian Centre for Digital HumanitiesE-Mail: walter.scholger@uni-graz.at Walter Scholger studied history and applied cultural studies in Graz and Maynooth and has been the deputy director of the Centre for Information Modelling - Austrian Centre for Digital Humanities at the University of Graz since 2008. Having been active in several Working Groups in the context of DARIAH-EU and the DH umbrella organisation of the German-speaking countriesfor years, he is currently co-chair of the DARIAH-EU working group on Ethics and Legality in Digital Arts and Humanities, member of the CLARIN legal issues committee, and convenor of the DHd working group on Digital Publishing. He has been dealing with legal aspects of digitisation - with a focus on cultural heritage, research and teaching - and Open Science in projects, international workshops and university courses. In addition, he works on on aspects of DH teaching and training like OER, reference curricula and the international exchangeability of academic credit systems. Andreas Witt Leibniz-Institut für Deutsche Sprache Mannheim, Mannheim University, Heidelberg University & University of Cologne E-mail: witt@ids-mannheim.de Andreas Wittis professor for Computational Humanities and Text Technologies at the University of Mannheim and heads of the department of Digital Linguistics at the Leibniz-Institute for the German Language in Mannheim. His research is situated in the field of annotation science. Andreas Witt is an active member of standards bodies, viz. chair of an ISO working group on linguistic annotation and, together with Piotr Bański, co-chair of the TEI Special Interest Group, TEI for Linguists. Since many years he is involved in research on the legal situation linguists have to deal with. "
	},
	{
		"id": 136,
		"title": "The Narrow Scopes of Fake News: Detecting Fake News Using Topic Diversity Measures",
		"authors": [
			"Shepard, Dave",
			"Hashimoto, Takako",
			"Shin, Kilho",
			"Uno, Takeaki",
			"Kuboyama, Tetsuji"
		],
		"body": " 1 Introduction After the East Japan Great Earthquake on 11 March, 2011, rumors about an explosion at a petrochemical complex owned by Cosmo Oil spread rapidly on twitter. Stories of oil tanks exploding and releasing harmful substances into the air caused widespread panic until official government news releases corrected the misinformation the following day. This story demonstrates the importance of fake news detection: while an enormous real disasterwas unfolding, rumors of imaginary disasters spread misinformation and diffused attention on social media to imaginary dangers. The story of the fake Cosmo Oil fire provides an example of a situation in which fake news detection is important, and also provides a test case for studying the characteristics of fake news on twitter. We propose a method for fake news detection based on topic diversity. Our method computes topic diversity by a micro-clustering approach that makes clusters smaller than those produced by conventional clustering methods. We begin by extracting micro-clusters, small sets of keywords that represent topics, using a data polishing algorithmfrom tweets about the event. Next, we analyze clusters over time using visualization methods to understand how these topics change. We observe that a diversity measure for clusters, a measure of both the number of clusters and the number of words, in one cluster, shows topic transition. This observation has implications in the measure of the truthfulness of a story. Users tweeting about real news stories often show sudden changes in opinions, which causes a drastic increase in the diversity of the opinions expressed. Even if the number of tweets on the topic does not increase, topic diversity rises. On the other hand, users tweeting fake news stories are often not thinking critically about a topic, so there is no change on the number of clusters even as the number of tweets increases. Figure 1: Outline of Our Method 2 Topic Extraction using Micro-Clustering Figure 1 illustrates our method. First, we build a graph of tweets: each tweet is a node, and an edge between two nodes represents two tweets whose Jaccard similarity is greater than 0.3. This is our Input Data. Then, cliques are extracted by micro-clustering using Data Polishing. Micro-clusters represent small topics within a larger set of tweets. Micro-clusters are groups of records that have high similarity—in our case, tweets that include similar sets of words. To create micro-clusters of similar tweets, we perform maximal clique enumeration. A maximal clique is a clique included in no other clique within the graph. A maximal clique is not necessarily the largest clique in a graph, so the size of maximal cliques in the same graph can vary significantly. Because there are usually a huge number of maximal cliques in a graph, MCE is a computationally intractable problem. Data polishing reduces the complexity of MCE. It makes an edge between pairs of nodes if they seem to belong to the same cluster, and removes edges between nodes that do not seem to belong to the same cluster. It clarifies the graphs cluster structures, and thus makes MCE far simpler. We eliminate edges using the following procedure. If nodes u and v are in the same clique of size k, u and v have at least k − 2 common neighbors. Thus, we have |N u ∩ N v | ≥ k, so u and v are in a clique of size at least k. If u and v are in a sufficiently large pseudo-clique, they belong to a pseudo-clique with a high probability of being semantically meaningful; otherwise, they do not. To compute these nodes similarity, we compute the Jaccard coefficient, J, of their neighbor sets. We set the threshold s and consider each pair of nodes in the graph. If J> s, we add an edge between u and v. Conversely, if J< s, we remove any edges between them. Micro-clustering produces a set of topics, each made up of one or more clusters. Next, topic transitions are analyzed by calculating the diversity of clusters that constitute the corresponding topic. Micro-clusters are groups of similar or related records. In a graph, micro-clusters correspond to dense subgraphs, and non-edges in the dense subgraphs are ambiguities. We also consider edges not included in any cluster to be ambiguities. Our data polishing approach for micro-clustering consists of adding edges for these non-edges, and removing these edges from the graph. 3 Target Data Our target data set is the over 200 million tweets sent around the time of the Great East Japan Earthquake on March 11, 2011. We obtained this dataset from the social media monitoring company Hotto link Inc.. Hotto link tracked users who used one of 43 hashtagsor one of 21 keywords related to the disaster. Later, they captured all tweets sent by these users between March 9thand March 29th. This dataset offers a significant document of users responses to a crisis, but its size presents a challenge. We show our experimental result for tweets from 00:00 on March 11 to 24:00 on March 15, a total of 120 hours. We began by creating the sequence of tweet-word count matrices for our dataset for every 30 minutes, that means we had 240 slots. For example, the matrix for 30 minutes on March 11 before 14:30, contains 60,000-80,000 tweets. On the other hand, the matrix for 30 minutes on March 11 after 15:00contains 300,000-500,000 tweets. The number of tweets increased dramatically after the earthquake. The size of each matrix after 15:00, March 11 is around 15MB and they were all quite sparse. 4 Target Topic We selected the fake news about the petrochemical complex explosion that happened just after the earthquake. The story can be divided into four stages: Fact: around 15:00 on March 11, just after the earthquake, the Cosmo Petrochemical Complex in Chiba caught fire. Fake: Around 19:00 on March 11, the following fake stories appeared as tweets and were retweeted frequently: Radiation and harmful chemicals are leaking into the air from the petrochemical complex. Be careful! Dont go out! Because the rain includes radioactivity and harmful materials by the petrochemical complex explosion. Correction: Around 15:00 on March 12, the companys website and the local governments twitter account explained that there had been no explosion. Convergence: At night on March 12, the topic was converged. The fake news about the oil tank emitting harmful substances scared users as it spread. Finally, the government released a report correcting the misinformation and the fake news disappeared from Twitter. To evaluate the progress of the target topic, we investigated micro-clusters with the phrase Cosmo Oil over time. We examined the target topic transition and the diversity of clusters in each time period to show our methods effectiveness. Figure 2: # of Tweets vs # of Micro-Clusters that include the word cosmo oilFigure 3: Topic diversity for all tweets 5 Fake News Detection The progress of the fake news is shown in Figure 2. The graph shows the relationship between the number of tweets and the number of micro-clusters. Each circle on the graph shows one half-hour time period. Fake news stories show low topic diversity: Figures 2 and 3 illustrate the difference between a real story and a fake story. Both graphs plot the correlation between the number of micro-clusters that contain a phrase and the number of tweets that contain a phrase over each half-hour period. Figure 2 shows the topic diversity of a fake news story, tweets that contain Cosmo Oil; in contrast, Figure 3 plots all tweets from our data set during the same time. We use Figure 3 as an index of a known real story, the Great East Japan Earthquake, in contrast to the false rumor of the Cosmo Oil explosion. For a real news story, the relationship between tweet count and micro-cluster count is linear. Figure 3 shows the nearly linear relationship observed on the log-log plot for a real news story, which implies a power law relationship between the number of tweets and the number of micro-clusters. We can make the hypothesis that the line is the upper bound of topic diversity: that is, when a topic emerges independently, the total number of topics is equal to this upper bound. However, for a fake news story, the micro-cluster count is much lower relative to the tweet count in many time periods, and so the relationship is non-linear. In some time periods of Figure 2, highlighted in red, the number of micro-clusters is lower than the expected number. Figure 2 does not show a similar correlation between the number of micro-clusters and the number of tweets that contain the phrase Cosmo Oil. We suggest that a fake news story is more likely to have a lower topic diversity because there are fewer facts to report in such a story, and, as such, the story is likely to change little over time. Figure 4: Dynamics of the fake topic transition about cosmo oilWe evaluate the progress of the fake news over time. In Figure 4, first, the fact topic occurred. Then the fake topic appeared as rumors of the explosion spread, shown in Figure 4. Compared to the fact and the correction periods, the fake period shows low diversity: the measurements. Figure 4shows the correction period, when the government corrected the fake story and the correction overtook the spread of the rumors. At that time, the number of tweets and the number of clusters grew together, and so the diversity increased. Finally, In Figure 4, the convergence happened: the Cosmo Oil story started to disappear, while gthe diversity stayed high, but gradually the number of tweets decreased. The progress shows the dynamics of the topic transition and a kind of topic life cycle. Through the experiment, we confirmed that our method can extract quality micro-clusters by data polishing. In addition, we realized that micro-clusters can show dynamics of the topic transition using real tweets. 6 Conclusion This paper proposed a fake news detection method based on micro-clustering using data polishing. It showed that fake news stories follow a certain lifecycle. Furthermore, it suggests that topic diversity measures can help to detect fake news before an official correction is issued, as in the case of the Cosmo Oil story. Acknowledgements This work was partially supported by JST CREST JPMJCR1401, JSPS KAKENHI 19H01133 and 19K12125, 18K1143, and 17H00762. "
	},
	{
		"id": 137,
		"title": "Coping With The Complexity Of The TXM Platform Annotation Services With A Unified TEI Encoding Framework",
		"authors": [
			"Heiden, Serge"
		],
		"body": " Introduction TXM is a software platform offering textual corpora analysis tools and services. It is delivered as a standard desktop application for Windows, Mac and Linux and as a web portal server application, < >. TXM provides a consistent set of analysis tools combining qualitativetools such as word frequency lists, concordancing or text edition hypertextual navigation with synthetic quantitativetools such as factorial analysis, clustering, keywords or co-occurrence statistical analysis. To work on texts, the platform first imports the corpus sources to create a rich XML-TEI based internal pivot representation via the following general workflow: first the base text of each text is established: this operation implements digital philology principles and consists of decoding information in the various formats of the source documents to decide primarily where are the text limits, possible internal textual structures boundaries and the words of the text. To do this, TXM can analyze and represent three main types of corpora: corpora of written texts, possibly including paginated editions including display side-by-side of the transcription and the images of facsimiles; record transcriptions corpora, possibly time synchronized at the word or at the speech turn level with the audio or video source to provide playback; and parallel multilingual corpora aligned at the level of textual structures such as sentences or paragraphs. The result of this operation is represented in a pivot XML format especially designed for TXM called XML-TEI TXM extending the standard encoding recommendations of the Text Encoding Initiative consortium; then, natural language processingtools are optionally applied to the base text to automatically add linguistic information like sentence boundaries, grammatical categoriesand lemma of words by eg TreeTagger. As NLP tools generally dont take XML format as input, the pivot representation is first converted to plain text for NLP processing and results are injected back into the XML-TEI TXM representation; finally a specialized representation of the texts is built into TXM for efficient execution of its different tools. From a methodological point of view: the XML tags of the initial XML-TEI TXM representation in a) can be seen as manual annotations added to the base text, typically philologically edited with the help of specialized XML editorsoutside of TXM when the source is XML native, or as automatic annotations added by TXM when converting the sources from other digital formatsinto XML-TEI TXM. NLP tools processing results in step b) can be seen as automatic annotations added to the initial XML-TEI TXM representation of texts built in work step a); All TXM tools can then be applied indiscriminately to all types of annotations through a unified textual corpus data model regardless of their origin. Thus, so far TXM has implemented a traditional digital philology workflow combining an initial text source encoding and annotation step to a following application of analysis tools on annotated texts step. The text analysis tools use text annotationsto offer their services and produce their results. The workflow is unidirectional and the whole of it must be passed through again completely if any annotation needs to be corrected. To add or correct annotations, the user has to edit the sources or the annotations outside of TXM. For example word properties can be exported from the XML-TEI TXM representation in a file in tabulated format, edited in a spreadsheet and injected back into the texts before re-import into TXM see for example this tutorial based on TXM macros: . . This paper introduces new services developed in TXM to annotate directly texts from within the results view of specific tools for a better integration of philological and analytical work. Indeed, results views are great places to be aware of annotation errors or annotation needs, and to access what needs to be corrected or annotated. Interactive annotation services in TXM The three new annotation services concern both adding and correcting information, and all the annotations edited are meant for further exploitation by usual TXM tools. Concordance based SyMoGIH entities annotation The first service, developed in partnership with the LARHRA research laboratory in history http://larhra.ish-lyon.cnrs.fr , is based on the annotation of concordance pivots: any sequence of words composing the pivots can be annotated with any semantic category pivotscan also optionally be annotated with simple keywords or with key-value pairs, managed by TXM in a local repository. of the SyMoGIH historical knowledge base. In this architecture, the SyMoGIH platform hosts the ontology of historic facts and knowledge, and TXM concordances provide the user interface to link identifiers of those data to text spans for further analysis. As an illustration, see figure 1 the annotation of the Faculté de droit dAix entityin unverified OCRed texts of the Bulletin administratif de lInstruction publique corpus see the Bibliothèque historique de léducationproject: . Figure 1. TXM screenshot of a Concordance of a Faculté de droit dAix word sequence pattern to annotateand of browsing SyMoGIH semantic categories to find the CoAc13562 identifier to use for the annotation. TXM internal management of those annotations is equivalent to a re-import of the current pivot representation with the new annotations. After re-importthe new annotations are available for all TXM tools to work on like any original annotation of the texts. Concordance based word properties annotation The second service is based on the annotation of words of concordance pivots: a word present in the pivots in TXM, pivots of concordances can be composed of a sequence of words. of a concordance can be annotated with any property. The primary goal of that service is to annotate and correct pos and lemma properties of words, but it can help to annotate any property at the single word level. As an illustration, see figure 2 the correction of the pos property of some vers. words used in biblical references in Hobbes works lemmatized by Morphadorner. Figure 2. TXM screenshot of a Concordance to set the pos property to the n-ab value of two occurrences of the vers. word, selected by their concordance line. TXM internal management of those annotations is equivalent to a re-import of the current pivot representation with new annotations encoded in XML-TEI TXM at the word level. Full text URS annotation in text edition The third annotation service is based on manual annotation of sequence of words inside text editions with elements of a Unit-Relation-Schemaannotation model. URS type annotations are designed to encode complex discourse entities like co-reference chains in texts. As an illustration, see figure 3 the annotation of the ses loix sequence of words with a URS unit of type MENTION, having its grammatical category to the value GN.POS and its referent to the value les lois de la divinité, in the first chapter of the 1755 edition of De lesprit des lois by Montesquieu. Figure 3. TXM screenshot displaying the first page of an edition of De lesprit des lois highlighting in light yellow all URS units of type MENTION and in bold the unit currently selected, and displaying the current unit properties value input form: CATEGORY property at value GN.POS... TXM import/export management services represent those annotations as XML-TEI stand-off annotations anchored to the word elements of the XML-TEI TXM representation of texts. Discussion By using a common XML-TEI TXM pivot representation for internal management of corpora for all the annotation services, TXM unifies transcription, encoding and annotation activities in a single framework. In this framework, annotations can represent manual, semi-automaticor automaticinterpretation results used further for analysis and interpretation work. The reflexive nature of the resulting text analysis workflow is schematized in figure 4. Texts are first digitized by OCR, transcribed or converted from digital formats. They are then possibly philologically corrected and established through XML-TEI manual encoding. Then automatically processed by NLP tools while being imported into TXM to produce the TXM internal corpus model. Corpus analysis is then assisted by TXM tools applied to the corpus model. The pivot representation that gathers all annotations produced by annotation tools is figured as the node labeled « Pivot rep. » and the interpretation workflow itself is figured as a digital hermeneutic circle. Figure 4. Digital hermeneutic circle integration into TXM. Legend blue box: manual annotation activity black box: tool red box: automatic annotation activity green box: TXM corpus data model purple disk: data representation black arrow: activity green arrow: annotation equivalence Conclusion All the new annotation services integrated into TXM are building a comprehensive annotation-based digital text corpora analysis platform. From an epistemological point of view, the integration in TEI of the different annotation models and tools into the platform helps its users to better define and trace what comes from the source corpus they analyze and what comes from their own or from others interpretation work. This work was funded by the ANR and the DFG under grant numbers ANR-15-CE38-0008and ANR-14-FRAL-0006. "
	},
	{
		"id": 138,
		"title": "\"The Role Of Data Archives In The Humanities At The University Of Cologne\"",
		"authors": [
			"Mathiak, Brigitte",
			"Metzmacher, Katja",
			"Helling, Patrick",
			"Blumtritt, Jonathan"
		],
		"body": " There are three groups of stakeholders, when it comes to research data: Those who make data, those who use data and those who build infrastructure to match those two. In the literature, we find a lot of research on how to build infrastructure and how to share data, yet there is relatively little researchon what the third group, the users, or rather re-users, actually want and what they do. Most of these studies do also not focus on the Humanities. While for other area of studies, research data sharing and reuse through data archives or journals is far more institutionalised, this is not so in the subjects covered by Humanities. Methodology To study the practices and attitudes towards data sharing and reusing of researchers in the field of Humanities, we did an online survey on research data management practices and needs. It consisted of three sections on describing the data worked with or produced, reusing and sharing experiences, practices and attitudes, and knowledge and needs in the area of research data management. It was a follow up study of our research data management survey 2016adding a part on reusing and sharing experiences, practices and attitudes that was partly adapted from a survey conducted by the Specialised Information Service Social and Cultural Anthropology. The online survey was available between 06 June and 08 July 2018. It was conducted by the Data Center for Humanities, University of Cologne, in collaboration with the Cologne Competence Center for Research Data Managementand the Deans of the Faculties for Arts and Humanities and Human Studies. Data Center for the Humanities, http://dch.phil-fak.uni-koeln.de , Accessed 27.11.2018; Cologne Competence Center for Research Data Management, https://fdm.uni-koeln.de , Accessed 27.11.2018. It was consisted of 36 closed questions, categorial items always offered the possibility to add additional ones. The sample consists of 268 data sets, some of them did not answer all questions. The sample covers all subject groups and departments of two humanities faculties. For this paper, we are focussing on the questions on reusing and sharing experiences, practices, and attitudes. Results Reuse of Data Over 80% of our participants indicate the scientific benefit of searchable and reusable research data with rather high/high/very high for their field of study. Figure 1: Rating of the scientific benefit of searchable and reusable data for the scholars field of study. Purpose of reuse There are three aspects being rated highest in relation to the individual field of study: Over 85% of the participants have specified that reuse of data is important for reconstructing results, generating questions and comparison with similar data. In contrast only 59% indicate they would reuse data for reconstruction purposes personally. And even generating new questions and comparison with similar data is rated lower within the personal perspective. Figure 2: Purposes, scholars would like to reuse data for. Access to reusable data If researchers reuse data, only 22% have found it in an archive, while the more common way to find data is personal contact, either within the own research group, personal contactor even complete strangers. Over all less than 4% of our sample scholars categorically rejected the use of secondary research data. Figure 3: Experience with secondary data use by source. Handling research data Only 34% have stored data in an archive, at least 72% consider doing so and only 0,5% cannot imagine storing data in an archive. Nevertheless only one quarter of the 34% that have stored their data in a data archive do so in a openly accessible way. Figure 4: Accessibility of stored data in data archives. Reasons for not storing The main reasons for not storing data in a data archive is a lack of knowledge that this is possibleand not finding an appropriate one. Figure 5: Reasons for not having used a data archive so far. Conditions for an adequate archive When asked what conditions an archive would have to fulfill in order for researchers to save data there, the most important factors rated highest in very important would be data security, followed by factors of archives confidentiality and professionality. Looking of the combined important ratesmanageable effort for data curation, explicit agreements on licensing and usage, and the quotability of data are also ranked higher than 90%. Explicitagreements on licensing and usage of data came next. The next important factors were data must be clearly quotable and there should be specific security mechanisms for single information as well as coverage of the additional costs for data curation and storing by research funding organisations. The factors that were rated least importanteven though still more than 70% indicated them as being very important/ rather important/ important, were sophisticated access restriction, information on who uses the data for what and indexing the data set in different systems. Figure 6: Rating of factors if considering archiving own data in a data archive. If a data archive would fulfill all requirements, nearly 50% of all the scholars answered that all data should be stored. Surprisingly, all research data was ranked highest. That indicates a basic willingness to store data. Figure 7: What kind of data should be stored in a data archive - if it fulfills all requirements. Information sources for the evaluation of data archives Finally, even though data archives see themselves as information broker between producers and data reusers: if researchers decide on concrete archives to use, recommendations of colleagues and scientific organisations are most influential, followed by the popularity and reputation of the organisation that funds the archive.. Networks seem to play the major role for choosing a data archive. Figure 8: Important factors for choosing a data archive. Conclusions Sharing data is common and important in the Humanities, but that doesnt mean that the data ends up in a data archive. Instead, most sharing happens in research groups, with personally known colleagues and even strangers, while data archives only broker 20% of the research data transactions. This is similar to what other disciplines have found. For those who could share, there is a conflict of interest between adding to the knowledge commons and self-interest. In recent years, several policies have been set in place to encourage data sharing outside of the social network, e.g. the requirement of third-party funding agencies to submit a data management planand through journal data submission policies. Bibliometric studies have shown that sharing research data increases citation rate. Yet, in the humanities, these are not the decisive motivations for publishing data. Third-party funding is not as prominent as in the hard sciences and neither is pressure to publish in high-ranking journals or getting cited a lot. As a consequence, data archives cannot rely on scholars to seek them out for data deposits. But there are other options. Scholarly communities are key when it comes to finding and sharing data, but too much of the data gets lost, due to insufficient storage policies. Data archives can help with storing and managing data, but they have to be integrated in the community as indicated by Fig. 8. Awareness of suitable archives is still not as high as one would like, which also can be improved with community-based measures. For our data center, we have decided to concentrate on the PhD students in our graduate school. What we have found is that they have quite different questions and problems than more senior scholars. While experienced scholars usually have a setup of tools and research data from previous projects as well as an established network of collaborators, PhD students have to cold-start their research in most cases. This gives us the opportunity to introduce them to the possibilities of re-using and sharing research data, while also educating them on digital tools and data management in general. In our talk, we will introduce some more of the results from the survey and discuss more thoroughly the mechanisms of data sharing and non-sharing. We will also report on our experiences with addressing PhD students and discuss some of the other policies to raise awareness. "
	},
	{
		"id": 139,
		"title": "Art History and Big Data: Complex Collaborations between Institutions and Researchers",
		"authors": [
			"Helmreich, Anne",
			"Brosens, Koenraad",
			"van den Heuvel, Charles",
			"Scheltjens, Saskia",
			"van Ginhoven, Sandra",
			"Pugh, Emily"
		],
		"body": " INTRODUCTION This panel contributes to Complexities by examining the topic of art history and big data in complex collaborations between institutions and researchers. With art museums and cultural heritage institutions now digitizing their collections, moving towards open access policies, and other relevant trends, scholars of art history and material culture can develop datasets at an unprecedented scale. At first glance, when compared to data in the natural and social sciences, big data in the cultural heritage community appears very different. Nevertheless, this panel argues for the relevancy of the framework of big data for cultural heritage and art historical institutions and their data. Most critically, these data are sufficiently large and complex that they cannot be run on local laptops, or managed by a solo researcher, as has been the research practice in cultural and art histories up to now. The case of art history is of particular interest for the digital humanities in the age of big data given the tension between the methodology of pattern recognition associated with big data and the strong art historical disciplinary tradition of close readings and contextualization of singular objects. This big data holds the promise of allowing scholars to study the history of works of art and the lives of artists collectively and to test critically the conclusions of previous generations of individual researchers who attempted to identify significant patterns in the study of the arts using relatively small data sets. Adopting the framework of big data also aids in identifying significant issues for advancing the field of cultural heritage studies. Researchers in this sector, with this relatively new access to unprecedented amounts of data, are facing important questions regarding data standardization and data modeling, while recognizing the challenges of ambiguity in historical humanities data, in order to curate and to preserve this data sustainably within large institutional infrastructures and servers. This data will become even bigger when it is disseminated as Linked Open Data within the Semantic Web, which is a common aim shared across the panel. These complex problems must be addressed to make optimal use of these costly digitization and research infrastructure programs. Furthermore, the emergence of large data sets in cultural heritage institutions require a necessary critical reflection on epistemological, methodological, and analytical/hermeneutical issues concerning their use in research and education. We aim for a discussion around these topics that is not only relevant for art/cultural historians but also for digital humanists who are seeking to assemble analogous research datasets or to utilize such datasets in the Linked Open Data environment. We have identified several complexities as emerging from institutions investing in building big data infrastructures that are intended to serve researchers and students as well as broader disciplines and even potentially general audiences. We focus on complexities in data curation, user interfaces, and the skills needed and training of humanities researchers. Data Curation : Academic projects or individual researchers are in need not only of data, but also of data curation that supports specific research questions. Yet, cultural heritage institutions often have a wider public mission to produce data sets for the general public, and sometimes regard data curation for specific research purposes as a contradictory to their aim to provide objective data available for all. User Interfaces : Academic projects and individual researchers need to analyse, annotate, and contextualize big data provided by these cultural heritage institutions for their own research and to store the results locally. Collection infrastructures often provide access to their big-data via apis and sparql endpoints. But not every project or individual researcher will have the equipment to run and align the data for their own purposes. Moreover, most humanities researchers lack the skills to query these big data sets using sparql. The focus of these big data interfaces on analyzing patterns does not allow for hermeneutic approaches that require data handling from multiple perspectives in continuous, iterative processes. The annotation and contextualization of incomplete and ambiguous data that result from bringing heterogeneous datasets together require the co-development of user interfaces by computer scientists, information specialists and humanities scholars. These interfaces need to include the provenance of this data and to express differences in data-quality. Skills and Training : What would it entail for cultural heritage institutions to be open to modeling and curating theirdata together with researchers and to offer them user interfaces to enrich their data? What knowledge and skills will researchers need to acquire? For example, they would presumably need to work with standards, vocabularies, and ontologies to ensure their knowledge and expertise is computer-readable and interoperable. This will require changes in the working practices and education of humanities researchers who still have a strong focus on the peer review publication model and whose training may not have prepared them for such perspectives on working with data. It also requires rethinking how we acknowledge various contributions to these complex collaborations. Big Data Art Histories Projects in Europe and US: Sharing Expertise This panel brings together projects in Europe and the USthat engage with art histories and big data, in particular Linked Open Data, in academic and cultural heritage institutions. These projects share an engagement with questions of art history and cultural historiesas well as human-computer interaction and the digital humanities in general. While initiated separately, as the projects have developed, the participants have come to recognize a common need and interest in developing research infrastructures that rely upon the preparation of data for research in standardizedways in accordance and compliant with the standards developed/used by the large organizations that support digital humanities research, such as Dariah, CLARIN, CLARIAH etc. For the art historical and cultural history community, the formulation of shared data standards is not only a question for academically based research projects but also for museums and libraries. How might, then, standards developed for museums, such as CIDOC/CIDOC-CRM, translate to the academic research community? What other ways of working with cultural heritage materials can be extended across the academic, archive, and museum communities and their admittedly inherently different original infrastructures? How do we negotiate and leverage our institutional legacies with respect to data, and its standardization, management and presentation while also looking forward to a shared future of Linked Open Data and large datasets? How might such efforts contribute to the larger effort to developnational research infrastructures for digital humanities research? PROJECTS :  Golden Agents: Creative Industries and the Making of the Dutch Golden Age , a partnership of the Huygens ING, Meertens Instituut, University of Amsterdam, University of Utrecht, Vrije University Amsterdam, Rijksmuseum, KB National Library of the Netherlands, City Archives of Amsterdam, RKD Netherlands Institute for Art History, and Lab1100, aims to establish a sustainable infrastructure for the study of the interactions between producers and consumers and between the various branches of the creative industries across the long Golden Age of the Dutch Republic. Hereto, it uses a combination of semantic web and multi-agent technologies to link datasets of the production of the creative industries to essential indexed archival resources such 2 million scans of digitized notarial acts, including probate inventories, and other related archival documents, in order to investigate the consumption of cultural goods in all layers of society. The discussion on the complexities of this project will be focused on the identification of name- and geo-entities and creation of user-interfaces for data-alignment and for the representation of the provenance, completeness, and level of certainty of heterogeneous data. Project Cornelia , an interdisciplinary research project funded by the University of Leuven and the Flemish Science Foundation, examines the creative communities and industries located in seventeenth-century Antwerp and Brussels, a period of intense productivity particularly in the fields of painting and tapestry. Project Cornelia propagates slow digital art history. It develops hybrid, novel, and transferable strategies to analyze, explore, model, present, and visualize complex datawith a threefold aim: to revisit traditional/ analogue art historical questions; to raise new questions that could have not been asked let alone addressed before the digital turn; to bring together both art historians and computer scientists as they develop ways to navigate the complexities of the data universe presented by cultural heritage data. The Getty Provenance Index , a major research endeavor of the Getty Research Institute, assembles together data relevant to the ownership, transfer, and exchange of works of art as documented in archival inventories, auction sales catalogues, and dealer stock books. It currently holds over 1.7 million records that are being transformed into Linked Open Datain order to make this information more accessible and usable, and to support research on the dynamic relationships that governed the mobility of cultural artefacts from the early modern period to the mid-twentieth century. The discussion will focus on the challenges of moving from a transcription-based to an event-based data model and developing two interface systems, as well as challenges stemming from linking and modeling disparate data. Ed Ruschas Streets of Los Angeles Archive  is a research project of the Getty Research Institute built upon the archive of over half a million images of Los Angeles streets created from 1974–2010 by Ed Ruscha. Focusing on streets such as Hollywood Boulevard, Melrose Avenue, and most famously, the Sunset Strip, Ruscha generated a staggering collection of images that document the changing urban landscape of Los Angeles. Since the majority of images exist as unprocessed negatives and film contact sheets—that is, inherently unstable media subject to rapid deterioration and degradation—digitization enabled both preservation and access, but also created new challenges. The scale of the digital archive presents complexities: how to allow researchers to search 130,000 digital images in a way that is intuitive, comprehensible, and retains the nature of the archive? What tools could be leveraged to conduct analysis on hundreds of thousands of images? This initiative investigates new approaches to tackling the complexities presented by large-scale image-based archives, including how to generate metadata in tandem with image digitization, the enrichment of image metadata with geospatial coordinates, and how to make such information accessible to researchers.The Department of Research Services at the Rijksmuseum has embarked on the transformative project to align all collection information and data of the Rijksmuseum. This work entails data cleaning, metadata alignment and data modeling in order to move data out of its former silos into a linked format that will enable new connections to be observed across the museums collections and beyond the institutions walls.Format: After an introduction by Anne Helmreich, each project representative will present a brief overview of their respective projects. Then the panelists turn to overarching questions and themes which emerged as initial outcomes of their recently organized Lorentz workshop : Art Histories and Big data. The ensuing discussions, which we will open up to the digital humanities community at large, will inform the process of developing a white paper as an output of this workshop. The following topics are intended to discuss the emergence of large data sets in cultural heritage institutions in tandem with reflections on the epistemological, methodological, and analytical/hermeneutical issues concerning their use. DISCUSSION: 1) How can academic and cultural heritage institutions with different missions and publics collaborate in developing new infrastructures that support the use of big data in art historical/cultural heritage studies? University-based research projects, for example, tend to produce data sets designed to support specific research questions whereas institutions need to realize projects in more generic ways that support multiple communities. Technical issues of data storage and retrieval, data modeling, data alignment, and complex problems of the representation of data quality and provenance of heterogeneous datasets with incomplete and ambiguous data will be addressed. 2) How can scholars analyse, annotate, and contextualize big data provided by these cultural heritage institutions for their own research and store the results locally? How might these enhanced datasets be associated with their data origins? These and related questions provide insight into the interfaces scholars need to interact with big data. How can we produce generic user interfaces that harness the capacities of Linked Open Data while sustaining current research hypotheses and stimulating new research questions? How can institutions that develop infrastructures for research optimally leverage the expertise of individual scholars? 3) How can the scholars of the future be best prepared? What are the implications of big data for education in art and cultural histories? How can we formulate common research questions, and develop curricula to train and prepare students for the study of art histories/cultural heritage in the digital era? "
	},
	{
		"id": 140,
		"title": "A data-driven approach to the changing vocabulary of the ‘nation’ in English, Dutch, Swedish and Finnish newspapers, 1750-1950",
		"authors": [
			"Hengchen, Simon",
			"Ros, Ruben",
			"Marjanen, Jani"
		],
		"body": " This project aims to mine two centuries worth of digitised newspapers in four languages, and to propose a methodologically sound, reusable approach to carry out quality historical research on the changing vocabulary relating to nationhood. The newspapers stem from different sources and countries, and are available in different formats. Massive digitized newspaper collections are increasingly used to address historical questions through mining textual data. For recent examples and further discussion see for instance Bos & Gifford; Brandzæg, Goring & Watson; Buntinx, Bornet and Kaplan. For a discussion of the role of digitization of newspapers for historical research see Cordell; Milligan. They are more seldom used for comparative projects cross linguistic and national boundaries. In this paper, we address the methodological challenges the use of newspapers from different political contexts, languages and datasets poses, and lay out our approach to tackle a comparative study for the Netherlands, Finland, Sweden, and the UK. Working with historical newspapers from different countries to look for the evolution of a concept poses several methodological challenges. A first problem is actually getting the data, and shaping it in a way that makes its use possible. For the UK, we use the Burney and Nichols collections and the British Library 19th Century Newspapers, both provided by Gale, and accessible through an API, OCTAVO. Tolonen et al. The full texts of the Dutch newspapers, as well as their corresponding metadata, are retrieved through the Delpher API. , accessed August 2018 . We would like to thank Dr. Steven Claeyssens at the Royal Library of the Netherlands. The original script used to query the API has been written by Juliette Lonij. Finally, Finnishand Swedish newspapers are first queried through the KORP interfaces made available by the language banks of those two countries, respectively the Kielipankki https://korp.csc.fi , accessed August 2018. and the Språkbanken, , accessed August 2018. and then fetched via their API. The full datasets were used for the UK, Sweden, and Finland. The colonial newspapers were filtered out of the Dutch dataset, so as not to bias the comparative analyses. The justification is twofold: first, only the Dutch dataset has an extensive coverage of colonial newspapers. Second, Dutch colonial newspapers showed a great uniformity because their news supply was unique and controlled by the official news agency, ANETA.In addition, Finnish newspapers published outside the historical borders of Finland were also disregarded from our analyses. After getting access to the different data and shaping it in a way that a single pipeline can be reused for all languages and historical realities comes the trade-off between the computational, distant reading of the text, and the actual research question. We focus on the process of nation-building in Europe, and to achieve that goal we utilise several methods. Whilst historical processes or concepts do not appear as such in texts, and thus cannot be the object of a mere tallying across time, it is obvious that words do. We thus use words as a proxy to study the process of of nation-building, and carry that out in several ways. In doing this we also limit the study of nation-building to the development in which the nation became a self-evident frame for social and political affairs. As a first step in exploring this idea, we look at how bigrams We borrow this methodology from Hill et al, who studied the public sphere in 18 th century Britain. starting with the adjective national Or nationale , nationaal , nationell , nationella , and kansallinen . For the sake of clarity, the remainder of this abstract will use English terms as examples. In the case of bigrams containing meaningless words such as conjunctions, we expand the query until we arrive at the noun modified by the adjective. For English and Finnish, languages for which a surface form of the substantive shares the same spelling as the adjective, those occurrences of nouns are filtered out. Frequent compounds were decompounded and where needed harmonised, eg: nationalbiblioteket -> national biblioteket ; kansalliskirjasto -> kansallinen kirjasto , etc. behave in our datasets, in terms of absolute and relative frequencies. This paints a picture of how common the idea of something national is mentioned in newspapers in different countries at different periods. We complement this picture with an analysis of the creativity and productivity The definitions of productivity and creativity are fluid within subfields of linguistics, as already discussed in Lyons. In this paper, we use productivity in its corpus linguistics sense, i.e. the proclivity of a linguistic unit to beused. Creativity, on the other hand, will characterise this units new forms: in the case of a bigram, any new bigram following the construction  national + _  . of the national + noun bigram: by looking at how creative writers are with the linguistic unit, and by looking at how its use evolves across time, we have a glimpse at the vocabulary of the nation, and can identify key junctures in the transformation of this vocabulary. We notice that the French revolution, the political ruptures of 1848 and the Franco-German War of 1870 were particularly important for the diversification in the vocabulary of national, in all of our cases, but can also show how local political and publishing conditions produced local reactions. The differences also point out how events abroad affected domestic vocabulary, making the development a transnational one. By focussing on bigrams, we can trace the domains in which the word national was used. In doing so we do not trace the theoretical development of the concept of nation or even the intentional processes of shaping Dutchness, Britishness, Swedishness, or Finnishness, but rather focus on a much more implicit process in which the nation became a natural frame for conceptualizing the societal issues or -- to speak with Benedict Anderson-- an imagined community that became inescapable for citizens of any state. Approaching bigrams is limited to mere counts, and whilst it hints at change, it does not qualify it. To remedy this weakness, we cluster bigrams by themes, in two different ways: on the one hand, domain experts assign a theme to the top-300 bigrams. Those themes, viewed in a diachronic way, add more colour to the simple tallying of bigrams, and of the creativity and productivity of the construction. Analyses based on our manual annotation point toward a hypothesis that, on a general level, the vocabulary of national tended to be focused on economic discourse in the late eighteenth century, but soon gained a stronger presence in relation to political issues and ultimately also entered the domains of culture and social affairs during the course of the nineteenth century. The other approach, which we believe will be a useful one for researchers wanting to reproduce our methodology, is data-driven, and should help reduce a researchers bias. Clustering words semantically in a data-driven way is a challenge. Indeed, most current approaches rely on topic models to assign some sense to documents, but not directly on words. On the other hand, relying on external knowledge, such as Wordnet -- a database that groups lexical items into synsets, i.e. synonym sets -- proves itself to be difficult, due to the varying quality of the OCR. Additionally, Wordnet does not allow for a more fine-grained relationship between words than a dichotomous answer to the question are they part of the same synset or not?. To circumvent those problems, we train word embeddings on the full texts of our corpora, and then calculate the semantic distances between each of the top 1000 nouns that appear next to national. As such, we believe this approach is similar to the one proposed by Wevers et al. We then use k-means clustering on the 0.5 million different distances Prior to the distance calculation, the vectors have been unit-normalised, which allows us to use k-means clustering. Embeddings were trained on tokens with a frequency threshold of 300, a CBOW architecture, 100 dimensions, and with a window of 5. Furthermore, mimicking Kim et al, the embeddings were trained on different time slices, where embeddings for slice t+1 are initialised with the embeddings for slice t , hence bypassing the need for a temporal alignment of the vector space. to generate semantic clusters of words. Each word is subsequently assigned a label -- its centroid sense, which allows us to look into the thematic distribution of national across time. An advantage of using word embeddings is that words with OCR errors also get distributional similarity. The weakness of such an approach is that only the primary sense of a word is captured, making the technique sensitive to frequency. To make sure that the data-driven clustering is not mere chance, we attempt to replicate the results in two ways: first, we use SCAN, a dynamic topic model that infers, for a specific target word, word senses across time. Second, we calculate word movers distancebetween 11-gramsin which national appears -- the same input as for SCAN --, and cluster them using the same method as for the top words, in a move to go slightly beyond the primary sense limitation of the word embeddings. Data-driven clustering further confirms our hypothesis of the broadening domains of national, but, more importantly, also paints a clearer picture of how the politicization and culturalization of national took place. Further, it shows how the word national moved from being an abstraction of terminology such as French, German, Dutch to becoming indicative of a political community, and thus more often used similar to adjectives such as public, common, or international in referring to state institutions. A further way of better qualifying our findings is using available metadata to zoom in on periods of instability and ruptures in creativity and productivity curves and tie these empirical findings to theories of semantic change. Another way of making our analyses more precise includes a more linguistically- and culturally-aware preprocessing of the texts so as to go beyond the national + noun bigram: different cultures refer to a comparable reality differentlyvs English national archives; the same word is used in Swedish from Finland, but not in Finnish, despite Finland not being a monarchy). To successfully implement more advanced preprocessing, future work will rely on the comparative findings of the present study. "
	},
	{
		"id": 141,
		"title": "Challenging Stylometry: The Authorship of the Baroque Play La Segunda Celestina",
		"authors": [
			"Hernández Lorenzo, Laura",
			"Byszuk, Joanna"
		],
		"body": " Introduction In November 1675, Spanish writer Agustín de Salazar died leaving unfinished the play La Segunda Celestina, which he was writing on commission for the birthday celebrations of the Spanish Queen. The play was probably finished by an anonymous writer and performed the following year. In 1989, Guillermo Schmidhuber published a newly discovered suelta of SC with the anonymous ending he claimed had been written by Sor Juana Inés de la Cruz, a prominent Hispanoamerican writer of the time, whom he also thought to have made significant changes to the original. Literary hypothesis In 1700, the editor of the works by SJ, Castorena y Úrsua, mentioned that she finished and improved a literary text by Salazar. When her entire works were published in 1957, one of the editors, Salceda, connected Castorenas mention with SJ referencing a comedy about Celestina in her own play Los empeños de una casa, and concluded that the said play could be SC. Later, Schmidhuber tried to prove it making historical, linguistic and even primitive stylometric arguments. After a comprehensive research, Georgina Sabat de Rivers concluded that although there was not enough evidence to make it a fact, it was highly probable that SJ indeed wrote the ending, but not edited the whole text. Dataset In the course of our study, we found that availability of digitized Spanish texts, especially historic ones, poses a great problem, due to few resources or repositories, and poor state of digitization – for Spanish works it mostly means scanning images of early editions, and their typographic variance makes OCR results not very useful. As a result, our corpus was composed based on various sources. The text of SC was extracted from a digital edition, and converted into plain text. Other dramatic works by SJ were extracted from the Cervantes Virtual Library. Salazars texts were much more difficult to find, as there are no digital editions. We had to use the image digitization of his texts offered by the Biblioteca Digital Hispánica. The OCR provided by the library and our softwareproduced so many irregular errors that we decided to transcribe El amor más desgraciado and Más triunfa el amor rendido. To place the problem in a broader perspective, we used the Canon-60 corpus, a collection of digitized Spanish Golden Age plays that includes canonical baroque works. However, this corpus is imbalanced – some authors are overrepresented, whereas for others – less famous or relevant for the literary history – there is only one play. The final corpus combining the Canon-60, SJs and Salazars texts lacked balance in terms of genre, gender, and, finally, nationality – all the authors are Spanish-born, except for SJ, born in Nueva España. We therefore limited our corpus to only one genre: la comedia de capa y espada. Analysis Works from New Spain in the Spanish Baroque perspective We approached the issue of verifying SJs authorship in a multi-step study, starting with a distant look on the literary surrounding of SC. With the primary network analysis conducted on the large corpuswith Bootstrap Consensus algorithm for 100-1000 most frequent words, as implemented in the stylo package, we determined optimal settings granting stable results – deciding against using culling which completely distorted any authorial signal and relying on Cosine Delta distance measure. Importantly, network analysis confirmed some existing inspiration links between authors, e.g. SJs work bearing similarities to Calderón and Salazar, whom Paz cites as ones she mimicked in her youth; and unveiled new connections, e.g. to Antonio de Solís and Juan de Vera Tassis. What also supports claims of SJ extraordinary talent surpassing her times is the fact that while works of other authors cluster mostly together, her works span across the whole corpus. Fig. 1 Network of all works in the corpus. SJ is marked with black edges. Strength of authorial signal and determining authorship Preliminary authorship attribution and verification tests showed very unstable classification results. For cross-validation with SVM, Delta and NSC, and verification with the so-called Imposters methodvarying on settings a number of candidates were recognized as the author of anonymous part – from Calderón and Moreto to SJ, Solís and de Vera Tassis. We decided to examine the strength of authorial signals in our corpus, which led us to excluding those who could not author the anonymous part for objective reasons such as the time of its creatingor being hub authors – strongly connected to every text in the corpus. Inspired by Eders evaluation of authorial signal in short samplesand thanks to his courtesy in making the script from the study available to us, we conducted a series of evaluation tests on our corpus until we were left with two authors beside Salazar: SJ and Solís. Of the three considered authors SJ had the most stable signal. Fig. 2-4 Accuracy of recognition of particular authors by classification algorithm. In the final part of our examination we once again performed cross-validated classification and verification on the small corpus consisting of one-genre works by the mentioned 3 authors against anonymous part of the text. As the anonymous part is only 4863 words, we used sequential sampling of 1000 words. In this case, SJ was attributed as the author in almost all settings, with the most reliable results produced by SVM and 100-500 MFWs scope. Interestingly, parts of works by other authors were consistently misclassified as SJ, which might indicate either/both her domineering style or her taking inspiration from either of the authors, of whose works she must have been aware. Editorial influence in the non-anonymous part and the ending This problem of authorship requires detecting multiple authorial voices as we know for sure Salazar wrote significant part of the play before other person finished it, which is why we apply Rolling Classifyto detect authorial takeovers. This allows to discover both who authored the ending, and if this author made significant changes to the rest of the play. We marked two important points in the whole play: mark b represents the place at which, according to Vera Tassis, Salazar left the play unfinished. As it can be observed in the figures 5, 6 and 7, the ending is attributed to SJ in SVM and NSC, and to Solís in Delta. The most surprising thing is that Salazars signal is not detected at all, which may be related to the weakness of his signal detected in previous analyses, and Sor Juana seems to dominate the rest of the play. Could it be that, if it was SJ who finished the play, she altered the rest of the text to the extent that we are not able to see Salazar anymore? This would confirm the claims of some of the scholars who defend her authorship. We also marked the beginning which portrays the first encounter between protagonists: doña Beatriz and don Juan, with the mark a. It is retold and alluded to several times in the play, something unusual in Golden Age theatre. The actual first encounter is a very feminist confrontation, later in texts belittled in don Juans retelling. It seems to betray a female writer which is why we examined it. As it can be observed in the figures, in all tests this fragment is attributed to SJ. Figures 5-7. Rolling SVM, NSC and Delta on SC. 500 MFW and 5000 words per slice. Conclusions Our experience emphasizes the need for and usefulness of taking corpus evaluation steps in all analyses, and especially in the case of historic works, for which it is impossible to create a truly balanced corpus. Various authors seem important for the text and the situation is quite blurry. Second best candidate, well above chance level, Solís, is a new discovery, and his possible relation to the SC and SJ, especially in terms of influence or themes, may also be of interest to future studies. However, quantitative analysis and literary evidence history show that the influence of SJ is definitely the strongest, supporting the theory of her being the author of anonymous part and editor of the whole text. Further Materials See our corpus, evaluation of the OCR difficulties which led us to transcribe the texts, and list of plays in our GitHub: "
	},
	{
		"id": 142,
		"title": "Pre-Conference Workshop “DLS Tool Criticism. An Anatomy of Use Cases”",
		"authors": [
			"Herrmann, J. Berenike",
			"Frontini, Francesca",
			"Rebora, Simone",
			"Rybicki, Jan",
			"Bories, Anne-Sophie"
		],
		"body": " Pre-Conference Workshop DLS Tool Criticism. An Anatomy of Use Cases The current panorama in DLS presents a plethora of tools, protocols and practices for processing, analysing and visualising data. This diversity of practices and tools originating from different areashas resulted in a rich, but atomised situation. Using a broad definition of tool understood as method, the ADHO-Special Interest Group Digital Literary Stylisticsorganizes a workshop that taps into the DLS Tool Inventory, which is a first attempt to gather information on the practices of the various traditions present in DLS. The DLS-TI features methods and suites for data analysis, including desktop GUIs, online Virtual Research environments and libraries for R or Python, as well as general purpose tools such as Excel spreadsheets. As tools have the power to reify theoretical a prioris, the community needs a handle for gauging their validity, applying a sense of craftfrom the perspective of tool criticism. Building on the DLS-TI, in concert with other initiatives, we aim at taking stock, but also reflect on, our methods. Three use cases representing different types of digital tools will undergo an anatomy: Textométrie, Stylometry, and Semantic Text Mining. Three scholars will each present a use case, elucidatingreasons for choosing the method;the methods impact on the analysis and literary modeling;advantages and limitations. The discussion will also address traditions of Digital Literary Stylistics between digitaland analog, addressing the fit of data and method to literary modeling. Anatomy of tools: A closer look at textual DH methodologies Based on what has emerged so far from the DLS-IT and further observations of research practices, we have identified a three groups of tools that can be covered in this half-day workshop and will be represented by an exemplary use case: 1.Textométrie Textometry is a traditionally French approach to statistical text analysis, often based on methods such as Correspondence Analysis, which has produced a number of tools, alongside a productive body of research in the domain of stylistics and corpus linguistics. Textometry and stylistics: which tools and practices for literary interpretation?Textometric tools are widely used by researchers to explore literary corpora. This session intends to propose a critical feedback of experience on a stylistic analysis guided by a textometric exploration of Apollinaires poetic corpus with TXM. Several issues will be discussed: first, how to analyze in concrete terms the evolution of the poetic writing of a single author, Apollinaire, between 1898 and 1918, from a diachronic perspective? In a contrastive study, which are the advantages of TXM. We propose to review the importance of specificities calculations and the various visualizations proposed, as well as the corpus scores allowed by an annotation of the structural units of the poetic corpus. Finally the methodological and epistemological contributions of a tool such as TXM for stylistic analysis will be discussed from a critical point of view. What observable results does it provide? What does it make visible? How to interpret the salience of certain results? What silence can he throw on other stylistic points of the text? 2. Stylometry Stylometry uses a series of tools and methods for the statistical analysis of style, based on advanced calculations on word frequencies, including multi-dimensional measurements and machine learning techniques. Their main applications have been both authorship attribution and distant reading. Initially developed through the use of spreadsheets, they have been fully implemented into programming languages such as R and Python, and integrated by a wide variety of visualizations, derived from research fields such as philogenetics and network theory. Less than countless. Options to move beyond word counting in stylometryIn a fairly dramatic critique of computational literary studies, Nan Z. Da recently made a controversial case against the application of quantitative methods to literary texts. She argues that much work in this field essentially boils down to counting words. This view is somewhat reductive but not without merit: it certainly applies to much of the present-day approaches that are dominant in stylometry and, consequently, to many of the tools that are available. While this methodological focusis to some extent justified by previous empirical work, I will reflect on under-explored options for stylometry to move beyond naive word counting. Stylometrists, for instance, often take pride in the fact that their tools typically work on raw texts that require little preprocessing. In this, stylometry ignores much of the achievements of literary theory in the twentieth century, such as the importance of focalizationor thereader. Richerprocessing pipelines, that also tap into syntax and discourse, might allow stylometry to revitalize its connection with literary theory, but comes with significant barriers for non-Anglo-Saxon literatures. In this talk, I intend to review some of the less conventional work in stylometry that leads the way in this respect. 3. Semantic Text Mining Semantic Text Mining applies tools for text analysis and visualization based on semantically enriched and co-occurrence methodologies, such as sentiment analysis, topic modelling, and word embeddings. They offer the potential of addressing key questions in literary theory and narratology, from the identification of genre to the visualization of plot. These emerging approaches are now beginning to broaden the scope of computational literary studies and to open up new, still-unexplored potentialities. LDA Topic Modeling for Semantic Corpus AnalysisTopic models based on Latent Dirichlet Allocationand Gibbs Sampling are a tool for exploring and analyzing the content and semantic structure of digital text corpora that has become popular in digital humanities research in recent years. They allow researchers to model a corpus content in terms of so called topics, groups of apparently semantically related words, and show the distribution of these topics within the corpus. Thanks to an increasing number of available tools and libraries, the method is, by this day, accessible to a wide range of users. In contrast to this technical accessibility, the methodology of topic modeling is rather intricate, and users cannot generally use them without making a number of decisions that require some deeper understanding. Additionally, there are aspects of topic modeling that are still in need of systematic methodological research. The session will give a hands-on introduction on goals and method of LDA topic modeling, demonstrate how to experiment with topic models using a simple desktop tool, and address open methodological issues. 4. Discussion During the discussion we will pose questions on methodological and epistemological levels – including humanistic enquiry vs. data science, explorative vs. confirmative, and qualitative vs. quantitative approaches; as well as the range of research questions, from text similarity to aesthetic effects. The aim of each use case-anatomy is to give an overview of the usability and strengths of the toolin research, as well as pointing out problems and formulating specific avenues for further development. Results will be documented on a special SIG-DLS webpage. Through this, we will produce a guide for DLS-scholars orientation, as well as the beginning of a roadmap for further tool development. The target audience is scholars interested in methods/tools and their linkage to literary modeling. We welcome newbies who look for initial orientation, old hands who wish to progress methodological development, and any scholars interested in modeling, i.e. epistemological aspects of style-tool criticism. Calzolari, N., Del Gratta, R., Francopoulo, G., Mariani, J., Rubino, F., Russo, I. and Soria, C.. The LRE Map. Harmonising Community Descriptions of Resources. Proceedings of LREC 2012, Eighth International Conference on Language Resources and Evaluation. Istanbul, Turkey, pp. 1084–1089 http://lrec.elra.info/proceedings/lrec2012/pdf/769_Paper.pdf Da, N. Z.. The Computational Case against Computational Literary Studies. Critical Inquiry, 45, 601–639. https://doi.org/10.1086/702594 Es, K. van, Wieringa, M. and Schäfer, M. T.. Tool Criticism: From Digital Methods to Digital Methodology. Proceedings of the 2Nd International Conference on Web Studies.. New York, NY, USA: ACM, pp. 24–27 doi: 10.1145/3240431.3240436. http://doi.acm.org/10.1145/3240431.3240436 Franzini, G.. Catalogue of Digital Editions Zenodo doi: 10.5281/zenodo.1161425. https://zenodo.org/record/1161425#.XCoDJhNKh8c. Flanders, J., & Jannidis, F.. Flanders, J., & Jannidis, F.. Knowledge Organization and Data Modeling in the Humanities. A Whitepaper. https://www.wwp.northeastern.edu/outreach/conference/kodm2012/flanders_jannidis_datamodeling.pdf Koolen, J., Gorp, M. van and Ossenbruggen, J. van. Lessons Learned from a Digital Tool Criticism Workshop. Proceedings from DH Benelux 2018. Amsterdam, The Netherlands. McCarty, W.. Humanities Computing. London and New York: Palgrave. Piper, A.. Think Small: On Literary Modeling. PMLA, 132, 651–658. https://doi.org/10.1632/pmla.2017.132.3.651 Piper, A.. Enumerations: data and literary study. Chicago: The University of Chicago Press. Underwood, T.. Distant horizons: digital evidence and literary change. Chicago: The University of Chicago Press. "
	},
	{
		"id": 143,
		"title": "Quantifying narrative perspective in Ancient Greek: Narrator language and character language in Thucydides",
		"authors": [
			"Hess, Leopold",
			"Bary, Corien"
		],
		"body": " Narrative perspective is the fascinating, but poorly understood in linguistic terms, phenomenon whereby literary texts often present events through the eyes, or minds, of a character in the story., from Jane Austens Emma, for example, is presented from within Emmas consciousness:The hair was curled, and the maid sent away, and Emma sat down to think and to be miserable. – It was a wretched business, indeed! – Such an overthrow of everything she had been wishing for. – Such a development of everything most unwelcome! – Such a blow for Harriet! – That was the worst of all. Narrative perspective is even more difficult to investigate in texts written in ancient languages, such as Ancient Greek, for at least two important reasons: one, the absence of the stylistic device of Free Indirect Discourse which is one the main ways of manipulating narrative perspective in modern literature; two, the impossibility of querying native speaker intuitions about fine nuances of meaning and use. In response to these difficulties, in this paper we present a quantitative approach to studying narrative perspective in the Attic historian Thucydides, an author who today is still highly esteemed for his dramatic uses of perspective shifts. His manipulation of perspective is perceived as very subtle and nuanced and classicists up to today have tried to get a grip on it.We provide a new quantitative approach to this question based on statistical analyses of the distribution of vocabulary in direct speech reportsand outside of such reports. The grounding of narrative perspective in the use of language at the level of lexical choice has been observed in narratology andlinguistics, but quantitative studies are rare. The most important contribution for Ancient Greek in this respect is constituted by de Jongsseminal narratological studies of Homer. As one element of her analysis, de Jong contrasts character language with narrator language in the Homeric epics. Character language comprises expressions that appear predominantly in speeches and rarely in narrator text. They are typically charged, evaluative or emotive, words. Their scarcity outside speeches contributes to the impression of Homers narrator as impersonal and objective. When words from character language do - infrequently - appear in narrator text, they still are to be interpreted as conveying a characters perspective, and they are essential to the creation of certain narrative effects. This may happen in indirect discourse, as ἀλείτης the sinner in, but also outside speech representation, as ἐνηής gentle in:φάτο γὰρ τίσεσθαι ἀλείτην hethought to himself that he would take revenge upon the sinner Homer, Iliad 3.28κλαίοντες δ᾿ ἑτάροιο ἐνηέος ὀστέα λευκὰ ἄλλεγον weeping, theycollected the white bones of their gentle companion Patroclus Homer, Iliad 23.252-3 Both ἀλείτης the sinner and ἐνηής gentle are to be evaluated from the characters rather than narrators perspective, as de Jongargues on the basis of their distribution in the text. We apply a similar analysis to Thucydides. However, we proceed in a highly automated way: rather than deciding first which words are potentially interesting evaluative and emotive words and then counting their occurrences in narrator and character text, we identify character language by looking at the relative frequencies of all words within and outside character speeches and identifying those whose distribution is the most skewed. They need not correspond only to highly charged vocabulary that a narratologist conducting a manual analysis would identify beforehand. This procedure makes it possible to uncover even subtler perspectival effects achieved by the narrator with the use of expressions that could intuitively seem entirely descriptive or non-perspective-sensitive. For our analysis we preprocessed the text in the following way. We created a lemmatized version of the text, so that we could retrieve frequencies of lemmas rather than inflected wordforms. The lemmatization was done with the help of GLEM, operating by combination of lexicon look-up and memory-based learning, which has been found to out-perform, for Ancient Greek, lemmatizers using only one of these components. We divided the text of Thucydides Histories into separate files corresponding to chapters in modern editions, and segregated those files into three subsets: CT - character text, containing passages of direct speech, QT containingquotations made by Thucydides from existing documents and literary sources, and NT - all the rest, i.e. narrator text. We disregarded QT as passages in which lexical choice was not in Thucydides full control.Splitting Histories into separate chapter-files and two subsets meant treating the text of Thucydides as a corpus divided into two sub-corpora, which allowed the application of corpus-like methods. Our investigation proceeded in two steps. First, we calculated relative frequencies of all lemmas in the sub-corpora and compared those that occurred in both, looking to identify those that are importantly more frequent in CT than NT, but do sometimes appear in NT, as these would be, by hypothesis, the character language words that may contribute to narrative perspective. We assessed the differences in distribution of lemmas between CT and NT using log-likelihood ratioand ranked them according to how strongly skewed the distribution was in favor of CT. Lemmas that exhibit a most skewed distribution can be called character language words; they are those words that appear predominantly in character speech, and as such they are the precise object of our interest here. We then proceeded to categorize and analyze the NT occurrences of the top ten of highest-ranked lemmasfocusing on their role with respect to narrative perspective in the textual context in which they appear. Table 1: Top 10 character language lemmasLemma # NT # CT Nrel Crel Ratio Crel:Nrel LLratio Crel:Nrel 1 δίκαιος dikaios just 14 50 1.2 15.9 13 93.2 2 ἀδικέω adikeō do wrong 44 74 3.8 23.5 6.1 93 3 ἀγαθός agathos good 68 81 5.9 25.8 4.3 76.5 4 χρή khrē should be 49 66 4.3 21 4.9 69.7 5 κίνδυνος kindunos danger 41 59 3.6 18.8 5.3 65.9 6 ἀμύνω amunō defend 46 62 4 19.7 4.9 65.5 7 ἀρετή aretē virtue 8 31 0.7 9.9 14.1 59.6 8 ἴσος isos equal 37 51 3.2 16.2 5 54.9 9 αἰσχρός aiskhros shameful 6 27 0.5 8.6 16.4 54.6 10 πάσχω paskhō suffer 35 49 3 15.6 5.1 53.5 Second, we hypothesized that character language lemmas may cluster together in the text in passages that are especially perspectivally rich or dramatic. To test that we treated each chapter of NT as an individual document and ranked them based on how many occurrences of most salient character language lemmas they contain, relative to size. We then studied the content of the highest-ranking chapters with respect to their narrative mode and perspectival effects. We found that character language lemmas in NTtend to occur predominantly in indirect reports, both at the level of individual occurrences and chapters - this is to be expected, as indirect reports attribute thoughts and words to the perspective of a character rather than that of the narrator. More importantly, we also found that where character language appears in NT outside of reportative contexts it is also most often used to express a perspective or mode different than that of the default objective narrator - either the perspective of a characteror that of Thucydides commenting on the events or on general truths. Furthermore, we found that our automated and quantitative approach allowed us to identify as character language words that would not prima facie stand out as evaluative or subjective in any way, but are in fact used by Thucydides in a perspectivally charged way. One prominent example is κίνδυνος kindynos, meaning danger, as in the following passage, where it refers to Nikias reasoning and therefore his perception of potential dangers....ὁ δὲ τὰ κατὰ τὸ στρατόπεδον διὰ φυλακῆς μᾶλλον ἤδη ἔχων ἢ δι᾿ ἑκουσίων κινδύνων ἐπεμέλετο. [Nikias] attended to the affairs of his army, keeping it from this time on the defensive to avoid any unnecessary dangers. Thuc. 7.8.3 In conclusion, we saw that quantitatively identifiable character language contributes to narrative perspective in Thucydides text, and our method allowed us to identify otherwise elusive aspects of the historians narrative style. Moreover, our findings are important for linguistic theories of perspective-sensitivity, suggesting that it may be a matter of pragmatics and patterns of use at least as much as of semantics. As much as being a study of Thucydides, this paper provides a proof of concept for a method of addressing narratological questions with the use of quite simple, but powerful quantitative corpus-based techniques. "
	},
	{
		"id": 144,
		"title": "Digital Animal Studies: Modeling Anthropomorphism in Animal Writing, 1870-1930",
		"authors": [
			"Googasian, Victoria",
			"Heuser, Ryan James"
		],
		"body": " Introduction Is an animal a person? The question is far from idle; it is in fact fraught with urgent ethical and legal consequences for both animal and environmental rights and shapes scientific norms for the study of animal behavior. It remains a constant theme in Western philosophy from Rene Descartes through present-day Critical Animal Studies. However, lawyers, philosophers, and ethologists are not the only deciders in this question: cultural representations of animals also mediate their relation to personhood. Fiction, for instance, excels in the representation of human individuality, interiority, and action; complex, round characters of course populate the long history of prose fiction. How, then, does fiction engage with the personhood of animals? In fiction, is an animal a character? What do animals do in the pages of fiction? Do they make decisions, have feelings, express interiority? Do animals function more similarly to human characters, or to things, objects, and machines? In what follows, we approach these questions with computational methods in one of the first attempts to apply digital methods to animal studies. For an experiment on biodiversity archives and species extinction, see Ursula Heise, Imagining Extinction: The Cultural Meanings of Endangered Species, 55-86. In a variety of corpora—from popular natural history to scientific writing about animal behavior to animal-driven fictions historically accused of anthropomorphism—we compare the semantic and syntactic footprints left behind by animals and humans. We discover that, from a computational standpoint, animals in fiction are indeed recognizable as characters, albeit characters who register intentionality through physical movement over speech and display a mental paradigm delimited by instinct and associative learning. Natural history writing, on the other hand, narrates animals in ways that seem surprisingly human-like when compared to animal representations in fiction more broadly. Corpus Around the turn of the twentieth century, reading audiences in the United States developed a penchant for fiction about nonhuman animals. These so-called wild animal stories appeared in popular magazines and short story collections by writers such as E.T. Seton, Charles G.D. Roberts, William J. Long and Jack London. Despite their popularity, the stories proved to be a flash-point for scientific controversy over the appropriate way to narrate animal behavior. Over a period of several years, prominent public intellectuals—including the naturalist John Burroughs and then-president Teddy Roosevelt—would attack the writers of wild animal stories for attributing inaccurately human-like qualities to their nonhuman characters. The entire debate would come to be known as The Nature Fakers episode. For a detailed literary-historical account, see Ralph Lutts, The Nature Fakers: Wildlife, Science, and Sentiment , Fulcrum Publishing, 1990. Though concerned a minor and moderately pulpy literary sub-genre, the controversy marks a moment when science and fiction butt heads over the meaning and appropriateness of anthropomorphism, as natural historians attempt to police the limits of fictional character. This corpus thus offers a high-stakes proving ground for the very concept of anthropomorphism in narrative fiction and science writing. We have assembled 54 texts from the writers involved in this controversy, comprising short story collections and novels by eight of the most prominent animal story authors, published between the 1870s and the 1930s and concentrated in the first two decades of the twentieth century when the Nature Fakers debate was at its most intense. As a point of comparison for these fictions, we have also assembled 17 of John Burroughs natural history monographs, which detail the doings of wild animals in the idiom of popular science. Finally, we selected approximately 400 American novels published between 1870 and 1930 as a control corpus with no particular interest in nonhuman animals. These 444 novels derive from two sources. For the late nineteenth century, we turned to a collection of about 325 American novels published between 1875 and 1905. Compiled by Marissa Gemma, these texts were selected based on their inclusion in the Annals of American Literatureand their availability in Project Gutenberg. Twentieth-century novels are those compiled by Mark McGurl and Mark Algee-Hewitt in Between Canon and Corpus: Six Perspectives on 20th-Century Novels, Stanford Literary Lab 8, https://litlab.stanford.edu/LiteraryLabPamphlet8.pdf. Methods Our digital methods collect the words that characterize animals, humans, objects in fiction. We base our methods on BookNLP, a Java program which clusters fictional character names together, and then collects the words associated with each character in specific ways: verbs for which each character is either a subjector an object; as well as the nouns each character possesses, the adjectives attributed to each character, and all words spoken by the character in moments of dialogue. David Bamman, Ted Underwood, and Noah Smith, A Bayesian Mixed Effects Model of Literary Character, ACL 2014 , http://www.cs.cmu.edu/~ark/literaryCharacter/. For a recent application of BookNLP to historical fictional practices of gendering human characters, see the recent article by Ted Underwood, David Bamman, and Sabrina Lee, The Transformation of Gender in English-Language Fiction, Cultural Analytics, DOI: 10.31235/osf.io/fr9bk. We applied BookNLP to our corpus of wild animal stories, and then annotated its returned characters for whether they were human or animal in 15 of the cleanest texts. BookNLP was in general accurate at identifying named animal characters—for example, in Londons White Fang, BookNLP failed to recognize only one repeatedly occurring named character, White Fangs father One Eye. The results of our annotations are recorded below in Table 1. Author #Stories #Words in stories #Chars#Chars#Words#Words#Words%Effect on Totals Ernest Seton 5 162,595 70 59 6,830 4,072 85 42.2% William Long 3 117,069 21 5 768 684 56 5.6% Charles Roberts 2 129,408 0 6 0 1,394 232 5.4% Harriet Miller 2 113,588 2 6 76 502 72 2.2% Jack London 2 103,676 20 26 4,049 5,107 199 35.4% Clarence Hawkes 1 33,784 3 5 1,372 1,009 298 9.2% Table 1 . Statistics regarding the small corpus of 15 texts whose BookNLP-identified characters have been annotated for their species. Sorted by the number of stories they contribute, these authors vary widely in the number of word-to-character associations they generate. For instance, although William Long contributes more stories, words, and characters than Jack London, his effect on the aggregate data is smaller: this is because he attributes on average only 56 words per character, whereas London attributes on average 199 words to each character. This is likely owing to Longs heavier usage of non-proper nouns, often referring to characters as the mother, the cub, etc. BookNLP has the obvious limitation of overlooking unnamed characters—both human and animal alike. To address this limitation, we designed a Python program to extend BookNLPs logic to all nouns. Our program collected approximately the same information: the verbs of which each noun is a subject or object; the nouns it possesses; and the adjectives it is modified by. Our Python program ran the corpus through spaCy, a syntactic dependency parser for Python, and collected all moments when a noun had a syntactic relation to another word in a sentence of any of the following kinds: subject; passive subject; direct and indirect objects; possessives; and modifiers. We used SpaCys default English-language model, trained on web discourse. That this model was trained on the hyper-modern form of language of the internet is of course regrettable, given the historical material of our project, but unfortunately this is common and largely unavoidable problem in literary text mining. We then produced a lists of nouns for animals and humans, which were drawn from the Harvard General Inquirer lists of those names. Roger Hurtwitz, General Inquirer Home Page, http://www.wjh.harvard.edu/~inquirer/Home.html. Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith and Daniel M. Ogilvie, The General Inquirer: A Computer Approach to Content Analysis. Uncertain or ambiguous entries were pruned, reducing an original list of 930 words to a refined list of 371 words. For example, the refined list of 287 human words included words like detective, ambassador, pope, freshmen, executive, commoner, manager, scientist, and human; the refined list of 96 animal words included words like turtle, owl, shark, oxen, grouse, roachback, moth, crow, hare, and jackrabbit. Whenever a noun from these lists appeared in the parsed sentences of the corpus, we record its appearance along with its associated syntactic relation. Finally, to determine whether a word issignificantly more likely to associate with a human or animal character, we conduct a Fishers exact test on a 2x2 contingency table:xThis weights our expectation for the number of times words appear by the number of times characters appear. The Fishers exact test returns an odds ratioalong with a p-value. Words with a p-value of less than 0.1 are shown in the results below. Results Due to limited space, we present only a brief summary of some of our findings in experiments that draw on BookNLP and named charactersand on our own programs collection of words associated with animal and human nouns. Finally, we apply machine learning techniques to assess the overall distinctiveness of human and animal nouns. Experiment 1: Comparing how animal and human characters are narrated in the Wild Animal Stories using Book NLP As seen in Figure 1, human characters are more likely to be the subject of speaking verbs such as asked, called, cried, announced, and, most especially, answered. Even in these wild animal stories, animals do notspeak. Instead, animal characters in these texts are far more likely than humans to register intentional response through embodied activity--such verbs as fought, bristled, and licked. Emotionally, these results suggest that animals are characterized more by aggression, humans by sociability. Think of Londons dog heroes, who only learn to love toward the end of their stories, when paired with the appropriate human master, but who never experience love in the company of other dogs. More subtly, animals learn and know things, while a human character is more likely to have thought, a result that could suggest something about the prevailing paradigm of animal mental life, characterized by instinct and associative learning, rather than rational reflection. Figure 1: The verbs for which animal and human characters most often act as subject. Shown are words that are statistically significantly distinctive of animal or human characters. The farther to the left, the more strongly a word is associated with animal characters; the farther to the right, the more to human characters. Experiment 2: Comparing how animal nouns are narrated differently in Wild Animal Stories and Natural History writing Our comparison of Burroughs natural history writings with the Wild Animal Stories does not, on the face, suggest that animals are any less person-like in Burroughs anti-anthropomorphic paradigm than they are in fiction. Indeed, as seen in Figure 2 below, Burroughs animals are statistically more likely to know, make, and become than their fictional counterparts. Many of the most distinctive active verbs between these two corpora might be explained by their relative interest in different species--for example, Burroughs writes about birds more frequently, so his animals are more likely to fly and sing. On the other hand, the strong influence of Jack London in the fictional corpus results in a high incidence of sled dogs, who are statistically more likely to draw a sled. Figure 2: The verbs for which animal nouns most often act as subject. Shown are words that are statistically significantly distinctive of wild animal stories or of natural history writing. The farther to the left, the more strongly a word is associated with the former genre; the farther to the right, the more to the latter. Experiment 3: Classifying human and animal nouns in wild animal stories, natural history, and novels The computer had only a slightly harder time distinguishing between humans and animals in the fictional corpus than in Burroughs natural history texts, with median accuracies at 70% and 71% respectively. Comparatively, when fiction with no particular interest in non-human animals is added to the mix, the computer was able to distinguish humans from animals with a median classification accuracy of 82%. This discrepancy suggests that, contrary to the terms of the Nature Faker debate, both wild animal stories and natural history construct a semantic similarity between humans and animals. Figure 3. Classification accuracy rates to determine whether a noun is a human or an animal. An identical number of instances of human and animal words were sampled from each genre. All human and animal words with more than 10 instances were used, leaving anywhere from 97 to 145 words depending on the corpus. In one hundred runs per genre, 30 animal words and 30 human words were randomly sampled. These were classified for their speciesby a logistic regression, trained and tested according to a leave-one-out classification model. "
	},
	{
		"id": 145,
		"title": "Embracing Complex Interfaces Linking Deep Maps and Virtual Interiors to Big Data of the Dutch Golden Age.",
		"authors": [
			"Li, Weixuan",
			"Piccoli, Chiara",
			"Heuvel, Charles van den"
		],
		"body": " Although semantic web technologies are gradually introduced in the digital humanities and cultural heritage institutions the representation of linked data is still very abstract and hardly allows for interactions by researchers or other users. SPARQL, for instance, is not a user-friendly language to query the Semantic Web and representations of Big Data, of historical networks or of Geographical Information Systemsare still just too flat to exploit them in depth for historical research in the humanities or for the disclosure of cultural heritage.. Humanities scholars have challenged the use of digital analytical methods of patterns in Big Data. Valuable as these studies are for close readings of small data, they do not provide solutions yet for handling Big Data and visualizations hereof. How can we embrace complexity, ambiguity, and uncertainty in the analysis and visualization of big data? This question is central in the project Virtual Interiors as Interfaces for Big Historical Data ResearchIt builds upon the current NWO-large infrastructure project Golden Agents: Creative Industries and the Making of the Dutch Golden Agethat aims at analyzing interactions between various branches of the creative industries and between producers and consumers using a combination of semantic-web and multi-agent technologies and circa 2 million scans of notary acts, such as probate inventories of the City Archives of Amsterdam. Here we present the first experiments with the creation of complex 2D/3D/4D interfaces on top of the Semantic Web, that express uncertainties in/allow users to interact critically in multiple ways with data. The 2D interface aims to preserve and present the complexities rooted in historical sources through deep mapping. Deep mapping creative industries in Amsterdam embraces the uncertainties to see, experience, and understand space in all its complexity, and enable the visualization and analysis of migration pattern of the creative individuals within the city. Methodologically, this project proposes a mechanism of translating the descriptions of location-related information in historical sources, which are often incomplete or imprecise, into concrete-georeferenced locations. The physical, geo-coded locations, as vectorized in the first cadastral map of Amsterdam by the HisGIS project, serves as a basis in this geo-translation process and as an anchor for the alignment of pre-cadastral maps, archival materials, and modern databases like biographical database such as ECARTICO and visual collections like RKDimages, creating a multi-layered deep map of the early modern Amsterdam. This research develops a framework to analyze the fabrics of painting and other creative industries in an urban space and to understand their choices of location within the framework of location theories from economic geography. Data concerning the features of urban space, such as the accessibility to public service, market, and customers and the housing price, are collected and analyzed to contextualize the complex living environment of artists. The first experiment focuses on Rembrandts neighborhood. The deep maps of Rembrandts neighborhood represent the spatial and social connections among Rembrandts neighbors and the rental value of housing in the areas Rembrandt lived before and after his bankruptcy. Reconstructing the historical road system of Amsterdam during Rembrandts lifetime, the accessibility of Rembrandts neighborhood is evaluated to revitalize the physical surroundings of the artist. The 3D/4D interfaces, which will be anchored at the GIS map layer, will act as a hub to connect the heterogeneous data that are available on 17th century creative industries in a spatially coherent context. Specifically, this part of the project focuses on the creation of virtual reconstructions of domestic interiors on the basis of the information provided by probate inventories and other notary acts, surviving material culture, and structural information from the houses building history. Documents with the richest descriptions are compared to archival sources, building floor plans and elevations, surviving objects, archeological finds and contemporary images to find suitable matches. Since this match is certainly not always possible, we need the big data of the Golden Agents project and other collections to select feasible case studies for the creation of some demonstrators. These demonstrators offer us a lens to zoom in into how individuals created, used, displayed and experienced cultural goods in their homes over time, and serve as spatially enhanced interfaces to existing and ad-hoc developed databases on the creative industries of the Dutch Golden Age. The real-world measurements in which the virtual environments are created allow us moreover to engage with the physicality of the reconstructed domestic space and to use them as exploratory tools to test and show alternative hypotheses about the use of space and the positioning of paintings and other objects within each room. Although the possibility of cross-referencing information from the abovementioned interdisciplinary data sources enhances our ability to create a reliable reconstruction, a varying degree of ambiguity will still remain. Uncertainty regarding e.g. the house layout and the appearance or position of furniture and objects belonging to the household calls for a structural solution. One of the aims of this project, which makes it relevant to any application in the field of digital humanities beyond this specific temporal and geographical context, is indeed to develop a consistent way to express uncertainty, to describe the source selection criteria and to explain the reasoning behind the 3D reconstruction process in a transparent way. Concerns about the reliability and the powerful agency of virtual reconstructions have been raised since their inception in the historical and archaeological domains. Despite the fact that this discussion has resulted in recent years in issuing guidelines for best practices, a reliable and widely applicable practical workflow for 3d/4d interfaces in which these issues of complexity and uncertainty are systematically taken into account is still missing. The case studies that will be presented here will show the work in progress towards an implementation that aims to fill this gap. "
	},
	{
		"id": 146,
		"title": "Disciplinary Topologies: Using dissertations to map deviant interdisciplines",
		"authors": [
			"Higgins, Devin",
			"Calvert, Scout",
			"Nicholson, Shawn"
		],
		"body": " With this proposal we explore the question: How can we characterize disciplines by looking at the discursive flows between scholars in university departments, and thus describe interdisciplinarity amid shifting topologies of knowledge? Taking as a provocation the premise that every field of knowledge is the centre of all knowledge, we explore paths of connectedness between disciplines, as constructed from a dataset of approximately 5,000 theses and dissertations, in order to elucidate the boundaries, shapes, and concentrations of disciplinary knowledge in the making. Analyzing the content and metadata of our institutions collection of ETDs has allowed us to draw suggestive connections about disciplinary groupings locally and in broader contexts, highlighting both points of sturdy disciplinary borders and points of porousness, where boundaries are fading or non-existent. Boundaries, but also passages, between disciplines form and re-form depending on the mode or scale of analysis, which can shift between an entire corpus of disparate texts, a single ETD as rhizomatic agglomeration of its authors disciplinaryexperience, and the shared metadata features linking them together. Thus far we have cleaned and normalized metadata, and built network graphs based on shared features among dissertations, and among academic programs. Still to come is a text-analytic component to model similarity among the full text. The resultant maps of interdisciplinary connectedness have been rendered in terms of specific features, including the ETDs author-defined keywords, author-selected topical descriptions, librarian-supplied Library of Congress subject headings, departmental affiliations, shared advisors and committee members, and topic models generated by analyzing the full text of all ETDs. The messy or chaotic connections that emerge are in contrast to the neatly hierarchical model of colleges, departments, and programs that is used to define the disciplinary structure of the university at an administrative level. Yet network graphs themselves constitute knowledge models that belie the complexity of a terrain , reducing slopes, rifts, and dunes to a set of nodes and edges, circles and lines, often, with a limited set of visual features. Attempts have often been made to classify disciplines based on theoretical knowledge categories, wherein hard-pure disciplines, such as physics, are described as cumulative, atomistic, concerned with universals, quantities, simplification, resulting in discovery/explanation, while the soft-pure disciplines, such as history, are described as reiterative, holistic, concerned with particulars, qualities, complication, resulting in understanding/interpretation. As pleasing or poetic as descriptions such as these may be, there has been a strong trend toward seeing disciplines as more or less social groupings with shared discursive traits: If the essence of discipline formation and evolution is self-referential communication, as Weingart suggests, then for interdisciplinary projects to be successful, there must be ways of breaking into hermetic epistemic groupings. Groupings are not just based on shared knowledge structures or methodology, but something like a culture, each with its own tradition of thought and practice, meaning that succeeding in interdisciplinary work is much about coming to an understanding of cultures that are different from ones own, and navigating the structures of power that maintain it. Undertaking such a mission of cultural outreach is especially difficult if Latours description of group formation applies, wherein the spokesperson looks rather frantically for ways to de-fine them…rendering the group definition a finite and sure thing, so finite and sure that, in the end, it looks like the object of an unproblematic definition. Breaking down these cultural boundaries is just the precondition to setting up new ones, in an ongoing cycle of shifts and ruptures. Fuller coins the term deviant interdisciplinarity to describe projects that aim to recover a lost sense of intellectual unity, typically by advancing a heterodox sense of intellectual history that questions the soundness of our normal understanding of how the disciplines have come to be as they are. If we admit that any discipline couldbe connected to any other, that the terrains of knowledge we construct are contiguous, then what are the divisions which prevent these maps from actually existing? Can the features around which disciplines coalesce at this moment be changed? Our analysis of ETD metadata will mediate between high-level disciplinary constructs and the on-the-ground reality, exposing the blind-spots of each. Visualizing these connections in network graphs has allowed us to highlight the difference, for instance, between the potential and actual disciplinary collaborations underway, and to measure the distance between Mode 1 disciplinary grouping, wherein increasingly specialized disciplines are seen as the natural outgrowth of the knowledge production process and Mode 2, which is the result of exposing the collective blindspots of the Mode 1 process. An initial interactive graph highlights several possible models of interdisciplinarity instantiated through an ETD collection, partially revealing that underneath the tidy departmental websites and faculty listings are a multitude of micro-cultures in the form of collaborations, research labs, and interdisciplinary courses: the makings of deviant irruptions of non-disciplines and future-disciplines. "
	},
	{
		"id": 147,
		"title": "Patterns of Early Modern Authorship: Using Metadata as Historical Record",
		"authors": [
			"Hill, Mark J.",
			"Säily, Tanja"
		],
		"body": " This paper makes use of complex bibliographic metadata – the English Short Title Catalogue– to create a dataset which is analysed with quantitative tools in a way allowing for novel insights into historical perceptions of authorship and the structural backdrop for them. In doing this it demonstrates the relevance of both these tools and datasets for humanistic research. Background Historical conceptions of authorship, despite perhaps initially seeming obvious in meaning, are far from clear. Theories of authorship have evolved from the Romantic isolated, originary author, through the Foucauldian author as a discursive formation, to the New Bibliographys definitional idea of authorship attribution, and finally to a point where scholarship has come to regard authorship and the status of the author not as ahistorical givens but as contingent constructs and institutions whose changing shapes represent responses to particular social, cultural, and economic pressures. Recent research into the early modern periodbuilds upon this perspective by emphasizing the role of the book trade in the development of the concept of authorship. Dobranski, for example, highlights the paradox of the authors growing symbolic presence versus early modern writers limited practical authority; while authors were increasingly named in publications, which sometimes even included a likeness and biographical information, they did not have control over the publication itself. It was the publishers – and, to some extent, printers, booksellers, and other actors in the book trade – who controlled publication, decided which details about authors to print, owned the copyright, and reaped most of the financial benefits. In this way, one could not be an author without being part of a more complicated network of actors within the book trade. The relationships between authors and these other book trade actors were not abstract. By a large margin, the majority of early modern publications in English came from London, and the book trade was made up of unique interconnected networks. Even internationally, London played a key role in the book trade – as publishing established itself within North America, imports from Britain increased more quickly than domestic production. Thus, the importance of the changing historical relationships which made up the book trade as a whole cannot be ignored when examining authorship as a constituent aspect of it. And while scholarship agrees that the situation changed over time, and that the institutions and relationships which made up the book trade were key to these changes, the details of this transformation remain debated and murky. A Quantitative Approach Partly due to lack of suitable data, quantitatively oriented studies of early modern authorship have been sparse. However, advances in the digital analysis of traditionally humanistic resources – both bibliographic catalogues and full-text databases – are facilitating new quantitative approaches. This paper, therefore, turns to the historical records of the book trade – specifically bibliographic metadata in the form of a harmonized and enriched version of the ESTC – to digitally reconstruct the historical details of authorship in a way which allows for quantitative measurement. While the process has been complex, with tens of thousands of lines of code required to parse the catalogue, the outputs are promising: we have extracted over 800,000 actors involved with roughly 480,000 printed documents, from which a total of 52,917 unique authors have been identified. Additionally, the data has been enriched with information including publishing location, years of activity, and pseudonyms - and by making use of open data resourcesand building a custom name-gender dictionary out of UK parish records, gender has also been attributed. In doing this we have been able to construct a dataset which can be used to test previous historical claims – both quantitative and qualitative – and, by making new historical claims, demonstrate the value of these digital methods and approaches when combined with traditional, yet novel, historical data. To this end, the paper has two approaches: testing the relationship between the data and historical reality, and making new quantitative claims. Testing Previous Claims As noted, there is existing research with regard to the history of authorship and the book trade. However, this research has been built upon smaller datasetswhich has meant simpler quantitative measurements were sufficient for analyses. This does not mean the claims are incorrect, however. Thus, we begin by making use of both our dataset and new quantitative methods to test previous research claims. Specifically, we follow up on the early quantitative findings by Crawfordand Stantonon female authorship in the seventeenth and eighteenth centuries. These include the influence of the civil war, population growth, and literacy rates on the number of female writers. Additionally, Rosesuggests that changes in copyright legislation, such as the elimination of perpetual copyright in 1774encouraged publishers to support new authors to acquire copyrights. There are also more general historical events which impact our data, such as the civil war and great fire of London. To demonstrate the relationship between this quantitative data and historical reality we also turn to these specific historical moments as case studies. FIGURE 1: Actors involved in the book trade 1500–1800. New Quantitative Approaches Built upon previous research claims which note the key role the book trade itself played in concepts of authorship, we make use of our data to develop new quantitative descriptions of historical authorship. Specifically, we demonstrate that complex analyses are necessary to develop competing representations of authorship both over time and during a given moment. With regard to the former, we are able to create a historical typology of authors which measures the transformation of the role through, for example, the number and type of professional relationships authors had, and how this changed over time. FIGURE 2: Authorial connections by actor-type 1500–1800. With regard to the latter – for example, the eighteenth century debate between authors as hacks and gentlemen– we can move beyond making qualitative claims and instead recognize distinct categories of authorship within our data. Additionally, we are able to measure how these changes are reflected in distinct categories of authorship. For example, with network analysis it is possible to identify distinct categories of authors through their relationships with outsiders. FIGURE 3. Subsection of intellectual communitiesas detected in the ESTC. Quakers: blue; poets: green; main book trade community: red. All of this allows us to recognize a number of interesting details within the data which could not be assessed previously. For example, it becomes possible to statistically identify individuals whom we consider historically important in ways which more basic analyses of publication records are unable to. Figure 4, for example, is a graph constructed out of four network centrality measurements and overall publication counts. It can be seen as a set of competing historical timelines constructed out of the statistical significance attached to particular authors during the early modern period. What is of particular methodological interest here is that the Most Published category is generally the worst metric for constructing this type of timeline. Thus, we are able to use the data in a way which satisfies historical intuition and addresses methodological concerns. FIGURE 4: Timelines based on categories of authors identified using network centrality measurements. Conclusion Although throughout the paper we aim to show how these types of analyses are able to draw accurate conclusions with regard to both qualitative historical tradition and quantitative historical data, it is not the aim of this work to simply make historically accurate quantitative claims which have been extracted from a non-traditional dataset. Instead, we hope to demonstrate how the structural frameworks of the book trade were directly linked to the complex act of authorship, and that these relations are a part of the way one could be conceptualized as an author at a given time. That is to say, the transformation from the hack to the professional, or the profound shift of women from an underrepresented group to exemplars of the 19th century novel, are changes tied as much to historical structures of authorship as they are to conceptual changes. It is these structures – as they are identifiable within the data – which allow us to recognize this complexity, and it is in this way that the paper makes both methodological and historical contributions: By demonstrating how historical metadata can be transformed into historical records with their own particular, and important, insights into the concept of authorship, we demonstrate ways in which existing, yet complex, data and historical knowledge can be used to make new historical claims. "
	},
	{
		"id": 148,
		"title": "Semantics of Shame in Social Media Discussions of Reality TV Fans",
		"authors": [
			"Hladík, Radim",
			"Štechová, Markéta"
		],
		"body": " Introduction We examine the discussions on the Facebook Page of the Czech Reality TV show Výměna manželek. A commercial TV Nova acquired the originally British program for the Czech market in 2005. In 2018, the show is in its 10th season and consistently ranks among the most popular prime-time programs. We intend to find out if the shows viewers active on social media partake in the shaming of lower class participants on the show. Specifically, we map the semantic space of shame in the comments associated with negative sentiment, interrogate the space for class-based content, and compare it with the alternatives. Related work The global uptake of the Reality TV format in the last decades has been associated with a demotic turn that allowed more people to appear in content previously exclusive to the elites, although empirical evidence suggests that the increased working class presence is coupled with overrepresentation of the upper classes. By showing ordinary people in what appears, despite heavy scripting and editing, to be everyday situations, Reality TV shows – such as, in which two wives exchange their homes for one week and both receive a reward for completion of the required swap – alter the way in which mass media represent social hierarchies. But more is not always better and a higher number of working class or poor characters does not guarantee their favorable representation. Skeptical authors relate the success of the Reality TV genre to its affinity with dominant neoliberal values, such as entrepreneurship and individual responsibility. In the framework of neoliberalism, low social status becomes a signifier not of class identity, but of individual failure. Various on-screen events in a reality show may trigger politically charged response among audience members, but class issues on the Reality TV are typically not discussed explicitly despite their centrality to the orchestrated narrative of the show. Instead, class distinctions are marked through lifestyle differences and tastesor supposedly individual moral shortcomings. Purposeful positioning of individuals outside of their value systemunderscores class divisions by subjecting members of the lower class to middle-class gaze. In Swedish reality shows, working class participants systematically become an object of ridicule. The competitive environment in the market-driven media helps to cultivate content producers who are more likely to generate negative portrayals of welfare and poverty. The treatment of lower class individuals in Reality TV effectively resembles the practice of shaming - enforcement of norms through the generation of negative collective affect and public identification of a trespasser. Originally a punitive measure, shaming has proliferated recently thanks to the social media platforms and ranges from benign vigilantism to criminal bullying. People with middle class status are more likely to engage in this practice. Data and methods Using Facebook Graph API, we collected 5-years worth of postings of all available types: posts by page, commentsand repliesfrom January 2012 to March 2017. For further analysis, we obtained lemmas and part-of-speech tags using NLP tools for Czech language and retained only content-bearing words. After data collection, we used sentiment analysis to structure our corpus into groups based on the prevailing emotional polarity of postings. Next, we trained word embeddings for each of the sentiment groups. Finally, we generated the semantic neighborhood of shame from the trained vectors. To choose between the two public language resources available for sentiment analysis in the Czech language, we manually tagged 150 randomly sampled postings with sentiment on the negative-neutral-positive scale. Neural Monkey classifierperformed worse than the most frequent class scenario. To test SubLex, postings were assigned to one of the three classes by the following rules: if the numbers of positive and negative words are equalthe tag is neutral, more positive words yield positive tag, and more negative words result in negative tag. This approach reached 52% accuracy on three classes problem. In the absence of alternatives, we chose the sub-optimal lexicon-based method. We note that the satisfactory specificity of 88% reached by SubLex ensures that we can at least be reasonably confident that the negative postings are correctly identified as such. After performing sentiment analysis, we trained word2vec modelsof dense word vectors with the sliding window parameter set to 10, one model per sentiment segment. Reducing data and comparing word vectors across slices is effective in tracing semantic shifts. In the end, we added the vectors for shame and its several synonyms together and extracted 50 terms closest to this aggregate vector. These terms delimit the semantic spaces of shame in our paper. In future work, we will build a bespoke sentiment classifier that would provide more accurate segmentation of our corpus. We shall also test the robustness of the obtained word vectors. Currently presented results are therefore preliminary. Results Figure 1 Figure 1B shows the results of the sentiment classification, although the proportions of the sentiment classes need to be read against the poor performance of the classifier. Still, the increase in the proportion of negative postings that corresponds to the overall influx of participants and the growing use of the reply feature on Facebook could indicate that a part of negative sentiment is due to the interactions of the discussants themselves. The intersections of the semantic spaces of shameshow essentially no overlap. In contrast, for a sanity check, we extracted 50 most frequent words in each segment, which resulted in 39 overlaps. We have therefore good reasons to believe that shame is constructed differently based on the dominant sentiment in the discussion. When shame occurs in connection with negative sentiment, a noticeable cluster of meanings related to hygiene emerges. Out of 50 words, more than 10 relate to dirtor cleanlinessand include interjections of disgust such as yuck!. Expression of disgust over mess thus appears as the main distinctive feature with which viewers can scold the participants in the Reality TV shows as inferior. This result provides support the notion of the middle class gaze, as order and tidiness are its integral components. Furthermore, in the semantic space of negative sentiment the word for laziness also appears and expands the discussion to neoliberal values of diligence and constant self-improvement. Figure 2 We focused on a very specific aspect of social media audience response to the Reality TV genre and found that the viewers engage in the shaming of reality show characters by affirming personal hygiene as the demarcation line between acceptable and unacceptable poverty. "
	},
	{
		"id": 149,
		"title": "Difficult Play: Developing Games with/in Complex Political Narratives, Threatened Environments and Challenging Histories",
		"authors": [
			"Holloway-Attaway, Lissa",
			"Rouse, Rebecca",
			"Dionisio, Mara",
			"Koenitz, Hartmut"
		],
		"body": " In our panel, we will discuss the ways in which gamesoffer users and designers critical modes for research and engagement with complex socio-political narratives, threatened environments and difficult histories via digital intervention. As media forms that promote convergent, performative and hybrid modes for interaction—moving between digital and material worlds and discourses—our games offer rich opportunities to explore contemporary Digital Humanitiespractices focused on multiply layered human and more-than-human agencies and subjectivities within narrative structures. Exploring principles such as performativity and citizen engagement, and analyzing transmedial storytelling and mixed reality games designed to support complex subjectivities, we argue that games are an exemplary medium for deploying DH practices where human/technical boundaries are deeply entangled. We recognize the importance of theoretical and practical models that view computational narrative systems as fluid, interpretative, and multi-dimensional, opened by/for humanist intervention, not only in their hard-coded design. Ideally then, our games, like contemporary DH media, are performative, not mechanistic, and they are unfolded as they are iteratively encountered.. The multimodal storytelling experiences in the game-play we design are reinforced, then, not only through technical interface affordances, but through carefully considered contexts for play where users, environments, discourses, and cultures are entangled. Such complexity is addressed in current reflections on DH increasingly focused on identifying texts/users /environments as recursive and mutually influential, not on digital media and computation as tools for recovering meaning from text-based content. N Katherine Hayles, for example, in How We Think: Digital Media and Contemporary Technogenesis offers a framework to support more interdisciplinary DH methodscritically informed by investigations into the changing role and function of the user of technologies and media and the human/social contexts for use. Hayles, explicitly claims that in DH humans think, through, with, and alongside media, and in essence, our thinking and being, our digitization and our human-ness are mutually productive and intertwined. Furthermore, we argue that our games are multisensory and as such provide access to digital/physical worlds that may reorient our traditional agencies and affects, offering novel ways for understanding and engagement with socio-political issues. Such re-distribution creates new ways to encounter challenging histories and hidden environmentsand they create rich contexts for DH scholars working to deepen their understanding of performative and active interventions beyond texts and tools. As such, we believe that DH designers and developers have much to learn from a rich body of games, interactive narrative and heritage research, particularly that which is focused on critical and rhetorical design for play, Mixed Realityapproaches and users bodies as integral to narrative design. References Anderson, E. F., McLoughlin, L., Liarokapis, F., Peters, C., Petridis, P., de Freitas, S.Developing Serious Games for Cultural Heritage: A State-of-the-Art Review. Virtual Reality 14. Bogost, I.Persuasive Games: The Expressive Power of Videogames. Cambridge, MA: MIT Press. Burdick, A., Drucker, J., Lunenfeld, P., Presner, T., Schnapp, J.Digital_Humanities. Cambridge, MA: MIT Press. Flanagan, M.Critical Play: Radical Game Design. Cambridge, MA: MIT Press. Gold, M. K.Debates in the Digital Humanities. Minneapolis, MN: University of Minnesota Press. Hayles, K.N.How We Think: Digital Media and Contemporary Technogenesis. Chicago, Il: University of Chicago Press. Mortara, M., Catalano, C.E., Bellotti, F., Fiucci, G., Houry-Panchetti, M., Panagiotis, P.Learning Cultural Heritage by Serious Games. Journal of Cultural Heritage, vol. 15, no. 3, pp. 318-325. Rouse, R., Engberg, M., JafariNaimi, N., Bolter, J. D., Eds.Understanding Mixed Reality. Digital Creativity, vol. 26, issue 3-4, pp. 175-227. Sicart, M.The Ethics of Computer Games. Cambridge, MA: MIT Press. Paper 1: AR Design for Hidden Histories: Community Engagement, Co-Design, and Interdisciplinary Collaboration Rebecca Rouse This paper presents a set of pedagogic and research strategies for developing co-designed ARdigital heritage games focused on contested and marginalized histories, based on a digital humanities community engagement course developed over the past six years with community partners including an historical society, state office of historic preservation, municipality, university archive, and museum of science and innovation. The approach in teaching and research to this topic is based on work in foundational texts such as Helgueras Education for Socially Engaged Art, Sanders and Stappers research on CoDesign, Bennetts discussion of the political history of display The Exhibitionary Complex, Dunne and Rabys approach to critical design in Speculative Everything, and Rouse et als interdisciplinary humanistic approach to understanding mixed reality. Documentation of projects produced will be presented, including Below Stairs, an AR role playing game that tells the story of an Irish immigrant working as a domestic laborer in a wealthy household in the 1850s, The Foerster Files, focused on the more recent history of urban renewal in the 1970s and social conflicts associated with economic development in urban centers, Discover Cohoes, an AR game about the major contributions of the Native American nations in the Cohoes area, and Harriet Tubman: Guided by the Night, an AR game and interactive planetarium experience that tells the fuller history of Tubman, through the lens of her STEM knowledge in astronomy. Best practices will be shared, along with a discussion of key challenges for developing work in this field using collaborative methods. References Bennett, T.The Exhibitionary Complex. new formations, No. 4, Spring, pp. 73-102. Dunne, A., Raby, F.Speculative Everything: Design Fiction and Social Dreaming. Cambridge, MA: MIT Press. Helguera, P.Education for Socially Engaged Art: A Materials and Techniques Handbook. New York, NY: Jorge Pinto Books. Rouse, R., Engberg, M., JafariNaimi, N., Bolter, J. D., Eds.Understanding Mixed Reality. Digital Creativity, vol. 26, issue 3-4, pp. 175-227. Sanders, E. B.-N., Stappers, P. J.Probes, Toolkits, and Prototypes: Three Approaches to Making in Codesigning. CoDesign, Vol. 10, No 1., pp. 5-14. Paper 2: The Story of A Fish: Promoting Environmental Citizenship and Playing with Baltic Sea Ecologies and Ecosystems Lissa Holloway-Attaway In my paper, I will outline the context for development of an Environmental Humanities project focused on engaging citizens around the Baltic Sea in activities focused on the ecological and environmental threats to the sea, impacted by climate change and other human factors. Within the project, multiple disciplinary strategies, drawn from Bioscience, Ecology, Art, Literature, Performance, Games, Computer Science, Robotics, and Digital Publishing are deployed to create a diverse, intra-active set of transmedial materials to tell the stories of the Baltic ecosystem. Through play and active digital/material intervention we hope to expose citizens to the biodiversity and political narratives of their region. I will primarily focus on one specific mechanism for such storytelling, a 2D mobile platform game developed to teach citizens about some of the threats to the Baltic Sea ecosystem through a played encounter with a pregnant Cod-Fish looking for a place to lay her eggs —while evading threats to her natural habitat. The game is designed as only one element in a series of other mediated encounters intentionally developed to support non-human interventions for the Age of the Anthropocene—the current geological epoch said to be defined by human impact on the Earths atmosphere, geology and other natural systems for evolution. As such, like many DH projects that combine complex computational intervention with scientific inquiry, critical art practice, and humanities approaches, it is intended to be a diverse system for intervention. Our Anthropocene-oriented design strategies are reflective of many intra-disciplinary initiatives in Environmental Humanitiesthat seeks alternative methods for writing and reflecting beyond human-only perspectives. For example, Boes and Marshallin Writing the Anthropocene: An Introduction, suggest we must find alternative narratives, other modes of mediated composition where our contemporary species-being expresses itself not in denotative speech acts but rather in performative interventions In this way, humankind functions as both subject and object of the discourse, becoming a sender and recipient, and all ways in between. In line with Ian Fosterin How Computation Changes Research, we work across disciplines, and with a focus on computational processes as only one element in a set of manyto radically alter our understanding of the worlds that engage us. References Boes, T., Marshall, K.. Writing the Anthropocene: An Introduction. The Minnesota Review 83: 60-73. Foster, I.How Computation Changes Research. In Bartscherer, T., Coover, R.. Switching Codes: Thinking Through Digital Technology in the Humanities and the Arts. Chicago, IL: University of Chicago Press. Paper 3: Engaging People in Biodiversity and Natural Heritage Through Interactive Storytelling Approaches Mara Dionisio The decrease of biodiversity is a shared concern among several scientists. According to Miller, conservationists have failed to convey the importance, wonder and relevance of biodiversity to the general public, preaching to the choir rather than reaching the unconverted. He suggests that more effort should be put in making the natural world fundamental to peoples lives as there is evidence showing that people who have personal connections with natural areas are more highly motivated to protect such environments. This is aligned with what Novaceksuggests, to engage people in biodiversity and other environmental issues, one must provide the opportunity for enhanced understanding that empowers individuals to make choices and take action based on sound science and reliable recommendations. Novacek suggests a strategic use of the Internet to reach new and expanded audiences. In the past two decades, continuous improvements and uptake of mobile smart devices capabilities, provided a ground for the flourishing of experiences that connect and educate audiences/players regarding local cultures and history. Such approaches showcase the potential of new technologies; in particular, location-aware technologies allied with the power of storytelling, as they have been successful approaches in allowing participants in immersing themselves in the locations while supporting meaningful engagement and involvement. In our paper, we propose to discuss a transmedia entertainment education experience we have developed and share insights and reflections on how interactive storytelling can be used to drive interventions that require people to ponder the benefits of local natural heritage and its underlying ecosystem services. The experience is shaped by the Madeiran context in particular and by Madeiras natural heritage - the Laurisilva Forest, which holds great importance for its biodiversity conservation. Many tourists overlook the value of this environment, a very unique ecosystem in Europe. The experience we designed is mediated through a mobile application that uses location-aware sensing to guide the audience in the discovery of different parts of the story. At the end of each plot point, a teaser video interview with local scientists and inhabitants is offered to put the audience in touch with real events and facts about Madeira. The complete version of these interviews can be viewed on the web-platform. References Farman J.The Mobile Story: Narrative Practices with Locative Technologies. New York and London: Routledge. Laurisilva of Madeira World Heritage CentreWorld Heritage Centre Periodic Report - Section II. Miller, J. R.. Biodiversity conservation and the extinction of experience. Trends in ecology & evolution, 20, 430-434. Novacek, M. J.. Engaging the public in biodiversity issues. Proceedings of the National Academy of Sciences, 105, 11571-11578. Williams, K.J.H. and Cary, J.Landscape preferences, ecological quality, and biodiversity protection. Environmental Behavior 34, 257–274 Paper 4: Interactive Narrative Design for Complexity – the case of the Multiple Lives of Walter B. Hartmut Koenitz Interactive Digital Narrativesprovide a particular opportunity to represent complex historical and contemporary topics by means of multiple competing narrative strands, choices and resulting consequences and replay. In this paper I will present a specific model for IDNinspired by complex representations in the sciences and report on a historical project applying this model.IDN builds on the affordances of the digital medium: procedural, participatory, spatialand encyclopedic. In interactive digital narratives, Murray reminds us, the audience has agency, the ability to make meaningful choices and in the process, experiences a transformation not only of the virtual world, but also one on a personal level through the awareness of alternative paths and perspectives. An artwork realized as a physical installation, The Multiple Lives of Walter B. invites participants to explore how a number of interrelated decisions change a characters biography. The participants engage with the piece by physically interacting with objects and locations, thus creating a sensory experience. Inspired by motives from the life of media theorist and philosopher Walter Benjamin, the work is simultaneously an exploration of history, through the lens of an individual character. Benjamins multifaceted life provides ample motives for an interactive treatment. Simultaneously, the many junctures in his biography open up a space for speculation – what would have become of him, if he had taken a different turn? At different points in time, he could have stayed in Sweden, in Ibiza or in Moscow. And what would have happened as a consequence? If he would have chosen Moscow, would he have returned to Germany as a Communist party functionary and ended his life as Minister for Culture? If he would have stayed in Ibiza, would he have been known as the first Hippie and a symbol of counterculture later? It is this kind of questions the project asks its audience to ponder. The project has been realized as a physical installation and shown in Seoul 2015 as well as Copenhagen 2015 and Porto 2017. It features a number of small objects, a suitcase, a map drawn on the floor, and a projector. The objects represent significant aspects of Benjamins life, for example a Communist party membership card or a love letter. The significance of specific objects as well as the locations are purposefully opaque to invite speculation and playful exploration. The tactile and spatial experience of handling objects and moving the suitcase across the map creates an intimate and immersive relationship with the intangible character Walter B. and give the interactor the feeling of agency in the creation of the biography. Simultaneously, the interactor is aware of their recreation of a historical life, one that has already passed and thus becomes an explorer of history. References Koenitz, H.. Towards a specific theory of Interactive Digital Narrative. In H. Koenitz, M. Haahr, D. Sezen, & T. Sezen, Interactive Digital Narrative: History, Theory and Practice . New York: Routledge. Koenitz, H.The Multiple Lives of Walter B. A Biographical Exploration. Digital Installation based on motifs from Walter Benjamins life. ELO: Electronic Literature Organization 2017 Art Exhibition, Porto, Portugal. Murray, J. H.Hamlet on the Holodeck: the future of narrative in cyberspace, 2 nd Ed. New York: The Free Press. "
	},
	{
		"id": 150,
		"title": "Paleo Codage - A machine-readable way to describe cuneiform characters paleographically",
		"authors": [
			"Homburg, Timo"
		],
		"body": " Introduction Cuneiform characters have been described using various systems in the past and the varieties of systems used in the literature as well as in daily work varies from language to discipline. Commonly, sign listsare created and published in the form of dictionaries in a non-machine-readable form. Similarly, for computers, the only way to distinguish cuneiform characters is currently to assign them different numbers in a list) and consider a distinction on this level. Therefore we are left with many systems and numbers to describe the same cuneiform sign.. Contrary to listing cuneiform signs,took another approach in creating a searchable cuneiform character encoding based on wedge types which would be implemented in applications such as CuneiPainter. Character image recognition has also been performed in the past, but never yielded a machine-readable representation of a cuneiform characters paleographic information which could have been useful as a means of validation for machine learning recognitions. This publication therefore introduces Paleo Codage, a paleographic distinct machine-readable description inspired by the Manuel de Codage encodingfor Egyptian Hieroglyphs. Motivation A machine-readable paleographic description despite yet representing another encoding scheme could link all systems of cuneiform character descriptions, as it directly describes the characters shape and positioning parameters. Scholars could register newly found characters easily in a machine-readable way and provide the basis for computational analysis on the paleographic shapes of cuneiform characters. Such paleographic information would ideally be integrated into currently emerging Semantic Dictionaries for cuneiformto enrich linguistic linked open data and thereby profit the respective scholars. In addition a machine-readable paleographic description provides the basis to capture sign variants of characters currently described in unicode. It is very common for on unicode codepoint to have many sign variants describing the same meaning over the centuries in which cuneiform has been written. Those sign variants have never been assessed digitallyand could provide valuable insights for philologists. Approach Paleo Codage builds on the description of, by using simple character descriptions for certain wedge types and by extending it with a Manuel de Codageinspired set of relational descriptions. Cuneiform wedges are distinguished as follows: Vertical wedge 𒀸Horizontal wedge 𒁹Diagonal wedge 1-4 𒀹,𒀺Winkelhaken 𒌋The system encodes relations between wedges as shown by the following most frequent examples: Wedges that pass through other wedges situated right to themWedges that do not pass through other wedges situated right to themWedges under another wedge possibly passing through other wedgesWedges under the current wedge not passing through other wedgesDiagonally under another wedgeWedge inversionIn addition size variations of cuneiform wedges are common and can be encoded as follows: Capital letters signify a bigger version, wedges prefixed with a small s a smaller versionLastly, angles of diagonal cuneiform characters may vary between characters which required angle modifiers to be added to the encoding. The angle between the diagonal wedges inis bigger than the angle between the diagonal wedges in. The angle can be halved by using the | operator. While the order in which cuneiform wedges were drawn is not always agreed upon by the respective scholars, PaleoCodages order independent of this dispute is from left to right and then from up to down in order to avoid ambiguities concerning cuneiform sign definitions. In order to facilicate the representation of displaced wedge groups PaleoCodage also includes the following positioning modifiers. Further operators could be added if needed by glyphs which can currently not be modeled. Proof Of Concept A proof of concept is provided on a representative subset of 200 cuneiform unicode characters https://en.wikipedia.org/wiki/Cuneiform_which were analysed to infer the relations described section Approach. Table 1 includes further encoding examples. Image Unicode Main Transliteration Borger Gottstein Paleo Codage 𒁹 U+12079 DIŠ 748 a1 a 𒀸 U+12038 AŠ 001 b1 b 𒀹 U+12039 AŠ ZIDA tenû 575 C1 c 𒀺 U+1203A AŠ KABA tenû 647? c1 e 𒌋 U+1230B U 661 d1 w 𒈦 U+12226 MAŠ 120 a1b1 :b-a 𒁇 U+12047 BAR 121 a1b1 ;b-a 𒇲 U+121F2 LAL 750 a1b1 a-b 𒈨 U+12228 ME 753 a1b1 a-:b 𒃵 U+120F5 GAM 576 c2 c.c 𒋻 U+122FB TAR 009 a1c2 c.ca 𒌀 U+12300 TIL 114 b1c1 bc 𒉽 U+1227D PAP 092 b1c1 C:d 𒂢 U+120A2 EZEN x A 288 a7b6 :sa-:sb::sb-ab;b-:sa-:sa:sa-a-:sb::sb-:sa 𒅈 U+12148 IGI RI 726 a4b2d2 :w-a-:b_-:b-a-a-:::w-a Table 1: Cuneiform Encoding Examples A generated similarity graph for verification purposesusing the new encoding method shows the applicability of the encoding to identify subglyphs that are included in other glyphs which in turn is useful information to be included indictionaries. Further similarity measures on the encodingcould reveal additional connections between cuneiform character representations. Figure 2: Cuneiform Character relations as graph: Only by verification of the encoding the computer can e.g. now recognize that the glyph IMIN3is contained by the glyph ilimmu3. Using the Gottstein System such a conclusion could not be made as they would be classified as b7 and b9 respectively. Application Given the paleographic information encoded in a standardized way users have the ability to draw a rudimentary shape of the character in order to detect the character they are seeing in front of them. This functionality is currently being implemented in CuneiPainter , improves its accuracy when matching cuneiform characters and will be ready as a showcase for DH2019. A showcase in JavaScripthighlighting all currently encoded characters is already available for testing , allowing users to verify and create their own encodings easily. In addition, the testing tool allows to export created cuneiform characters as SVG and as OpenType fonts in-browser, creating the basis for an easier automated font creation for cuneiform characters. Figure 3: Paleo Codage InputFigure 4: Cuneiform Numbering Systems: Semantic Dictionary for Ancient Languages "
	},
	{
		"id": 151,
		"title": "Towards Creating A Best Practice Digital Processing Pipeline For Cuneiform Languages",
		"authors": [
			"Homburg, Timo"
		],
		"body": " Introduction Ancient languages have recently become a research field gaining more and more attention from researchers in the DH community. This has led to various standards of digitization for different application cases which could be applied by other researchers to achieve easily accessible and often interlinked datasets. Among those research areas gaining momentum in digitization, archaeology and cuneiform languages have been assessed in several past and ongoing projects. For this DH2019 conference, the PaleoCodage encoding system for cuneiform languages has been accepted as a short paper presentation. This poster publication followinglikes to introduce developments in this year in relation to PaleoCodage to create a cuneiform digital processing pipeline involving 3D-Scanning, paleography, transliteration, dictionary and signlist creation, automated font generation, linguistic annotations, semantic annotations and show how to publish said data in a sustainable form using a versioning system such as git. Related Workcreated a proposal to annotate Sumerian cuneiform linguistically by extending the common CoNLL formatto support RDF. While this concept works very well for linguistic annotations and natural language processing purposes we would like to present a solution which once setup can be used by nontechnical endusers and is at the same time usable by a variety of research communities. In addition to linguistic annotations our concept also contains paleographic sign variant descriptions of the respective cuneiform sign and conceptually the inclusion of 3D images. Digital Processing Pipeline The proposed pipeline is shown in Figure 2 and its stages are described in detail in the following sections. The pipeline is still a work in progress and feedback as well as suggestions are very welcome. The pipeline could be incorporated into a project workflow as shown in Figure 1 which is included into an currently pending project proposal. Figure 1: Workflow of a research project utilizing the proposed cuneiform processing pipeline. The color green indicates the usage of a versioning system, the color yellow indicates exports which can be used by various communities. Figure 2: Digital Processing pipeline – schematic. A first a cuneiform tablet is 3D scanned, then transliterated and paleographically processed until it is further enriched and a semantic dictionary is created. Finally, several applications can produce domain-specific outputs which are useful for a variety of application fields. Stage 1: Transliteration and Paleography At first, cuneiform tablets are 3D scanned and character positions annotated by a professional. Next, a manual transliteration is created by the professional. In this step, the professional uses PaleoCodageto create a machine-readable description of each cuneiform sign, thereby producing a sign list. The sign list refers to the unicode representations of the cuneiform signs if applicable and is modelled using Semantic Web standards. It needs to be noted that one unicode codepoint may refer to multiple cuneiform representations in PaleoCodage as sign variants are quite common. An example of this is given in Figure 3. Figure 3: Cuneiform Sign Disambiguations of the same cuneiform sign E: Scholars consider these cuneiform signs to represent the same Cuneiform Unicode representation depending on the context of the cuneiform tablet By describing sign variants using PaleoCodage, it is possible to automatically create a cuneiform OpenType font from the sign list which is able to display the original cuneiform representations of the transliterations including sign variants digitally, which is a novelty in this community. The current State Of The Art merely includes drawing of cuneiform tablets as shown in Figure 4. While such drawings can also capture the shape and broken parts of the cuneiform tablet they are currently not machine-readable. Using the PaleoCodage encoding, texts can be enabled to be searched by sign variant form which may be interesting for applications in philology. By using the OpenType font, the cuneiform text representation can be recreated in any application supporting OpenType fonts such as LibreOffice or MS Word ensuring the interoperability of the text. Figure 4 : Cuneiform Sketch by the UniversityOf Pennsylvania Ligatures The OpenType font is enriched with sign variant descriptions as ligatures. For example, if we were to describe the cuneiform sign E in the signlist and discover that variants of E exist in the cuneiform texts, the variants can be described as E\\_v1...E\\_vn and can be shown in the sign list. Using this given description, a ligature E,\\_,v,1 is created which is subsequently substituted using OpenTypes GSUB table to display the correct version of the character encoded in the automatically created OpenType font. The idea is the same as the concept of in which latin characters are replaced using appropriate emojis. An example text shows the font creation and the application of the font using an example text. Stage 2: Annotation and Enrichment At the start of this stage cuneiform texts have been manually transcribed into transliterations and have been saved in the Git repository in the ATF format for cuneiform. We use an automized process to translate ATF documents to TEI XMLbuilding up on specifications provided by the ETCSL project . Having converted ATF into TEI XML, it can be annotated in CWRCWriterwhich we intend to extend to support linguistic annotations. The result of this enrichment process is a TEI XML which is a first publishable result on the project homepage. Using TEI Boilerplate, the texts can be visualized in a way that is suitable for scholars. An appropriate template is created during the work with CWRCWriter and can be seen in. In addition the annotation process results in RDF representations of important elements in the text. We hereby annotate: Linguistic Elements as well as Semantic Elementswith the goal to interlink as many facts about the artifacts and texts as possible in established Semantic Web vocabularies, DBPedia, GeoSPARQL, Pleiades). Figure 5: CWRCWriter using a cuneiform TEI template for enriching contents Stage 3: Dictionary Creation Linguistic Elements become the basis of a semantic dictionary of the corpus which is worked on the outlines of which have been discussed inand shown online . This dictionary is created on-the-fly as the annotation progresses and is based on the Lexicon Model for Ontologiescombining Semantics with linguistic annotations. Figure 6: Lexicon Model for Ontologies: Bridging the gap between semantic web concepts and textStage 4: Analysis, Evaluation and Deployment In this stage, project specific analysis on texts can be conducted. One example could be the extraction and location of interesting at best spatiotemporal localizable information such as names of emperors, cities or typical goods that were traded during the time of cuneiform tablet creation. The created resources are also interesting for other research communities: The annotated 3D scans in combination with PaleoCodage can be used to improve cuneiform character recognition using machine learning, annotated lingustic resources can be used as gold standard data to experiment with automated POSTagging or further text analysis. Also deployment options are implemented which include: SPARQL Endpoint for the Semantic Web Community providing access to every resourcein the project and provides interlinks TEI XML and ATF Repository with Webservice access Annotated 3D Image Data Storage using Git LFS https://git-lfs.github.com "
	},
	{
		"id": 152,
		"title": "The Invisible Translator Revisited",
		"authors": [
			"Hoover, David L."
		],
		"body": " A translator normally replaces almost all the original authors vocabulary except proper nouns. Most authorship attribution methods are based on the frequencies of the most frequent words or n-grams, the latter themselves derived from the sequence of words. Given these facts, one might expect attributions of translations to identify them by translator rather than author. Yet that is not the case. Rather, despite the replacement of the original authors language by that of the translator, translations are normally attributable to their original authors, rendering the translators virtually invisible. Jan Rybicki, himself an accomplished translator, has presented some important discussions of this peculiar state of affairs, but a further investigation of this curious phenomenon seems worthwhile. As a first step, consider a test of twenty texts by Chekhov translated by five translators. Figure 1 shows a Stylo bootstrap consensus tree, based on cluster analyses of the 200-2,000 most frequent wordsin increments of 100 words and with culling from 0% to 100% in increments of 20%, consensus .5. Fig. 1 Chekhov Translations by Multiple Translators Here multiple translations of the same text rather than multiple translations by the same translator cluster consistently, suggesting that text identity is a stronger signal than translator. Note, however, that three of the four Garnett translations of texts not translated by any of the other translators group together. Next, consider the bootstrap consensus tree of multiple translations of five Russian authors by Constance Garnett seen in Fig. 1, which does an excellent job of grouping authors even without the effect of multiple translations of the same text. Fig. 2 Garnett Translations of Multiple Authors The strength of the original authors signal in translations can be tested more thoroughly using Stylos Classify function. For the first test, 30 texts form the training set: 5 Chekhov texts by 4 translators, 9 Dostoevsky texts by 7 translators, 5 Gogol texts by 4 translators, 7 Tolstoy texts by 3 translators, and 4 Turgenev texts by 2 translators. The test set contains 47 texts by the same authors: 10 Chekhov texts by 4 translators, 13 Dostoevsky texts by 6 translators, 8 Gogol texts by 3 translators, 9 Tolstoy texts by 4 translators, and 7 Turgenev texts by 4 translators. No translations of the same text appear in both groups, eliminating the signals of individual texts. Thus the task is to attribute a set of test textsto the original authors of a different set of training texts. Based on the 100-2,000mfw, with 40% culling and pronouns deleted, NSCclassification is 94.5% accurateand SVMclassification 96%. These results would be strong even on texts that had not been translated. A second much stricter test involves 34 training texts: 7 Chekhov texts by 2 translators, 7 Dostoevsky texts by 2 translators, 10 Gogol texts by 2 translators, 6 Tolstoy texts by 2 translators, and 4 Turgenev texts by 1 translator. The test set contains 44 texts by the same authors: 5 Chekhov texts by 4 translators, 14 Dostoevsky texts by 6 translators, 10 Gogol texts by 4 translators, 8 Tolstoy texts by 2 translators, and 7 Turgenev texts by 4 translators. These texts were chosen so that no translations by the same translator for the same author appear in both training and test sets. Thus the task is to attribute a set of test texts to the original author when the translators of the training texts by that author are different from the translators of the test texts by that author. The results on this testare naturally less accurate, but NSC classification is still 85.8% accurateand SVM 87.6%. This seems almost incredible: the original author of a set of English translations by one group of translators is usually correctly identified as the author of a different set of that authors texts translated by a different set of translators. In spite of the strength of the authors signal, however, further analysis shows that the translator can be made visible again by filtering out the authors signal. Consider a different kind of test. The training set contains 6 translations of Tolstoy by Garnett and 5 translations of Dostoevsky by Pevear and Volokhonsky. The test set contains 33 texts: 10 translations of Chekhov, 1 of Goncharov, and 9 of Turgenev by Garnett, and 13 translations of Gogol by Pevear and Volokhonsky. With authorship neutralized, the translator becomes startlingly visible again. On these tests, NSC is 81.2% accurateand SVM 93.9%. Clearly Garnetts translations of Tolstoy are similar enough to her translations of Chekhov, Goncharov, and Turgenev that she can readily be identified as their author. The same is true of the translations of Dostoevsky and Gogol by Pevear and Volokhonsky. A final test can begin to show how this is possible. Zeta analysis identifies the characteristic vocabulary of these two translators–words consistently used by each and avoided by the other. It contrasts two groups of texts by measuring the consistency of inclusion and exclusion of a large set of words in large groups of sections of text of the same size by the two translators. For this test, Garnetts translations of Chekhov and Turgenev are treated as her authorial set and the Pevear and Volokhonsky translations of Dostoevsky and Tolstoy as their authorial set. An initial analysis showed that many proper names appeared in the characteristic vocabulary, and that British vs. American spellings and Garnetts use of hyphenated forms of words like to-day, to-morrow, to-night, etc. had a significant effect, so I manually culled out more than 4,000 such words and retested, with the result shown in Fig. 3. Given the proven strength of the authors signal, Fig. 3 makes an important point. None of the Garnett Ind. Sections or P and V Ind. Sections influenced the distinction between the two translators, and all these texts are by Gogol. Many of themare translations of the same work. Nevertheless, they are easily placed near the texts by their translator and separate from each other. Fig. 3 Zeta Analysis of Garnett vs. Pevear and Volokhonsky The 40 most distinctively characteristic words for the two translators shows some interesting patterns: Consistently used by Garnett and avoided by Pevear and Volokhonsky: till, fancy, passed, drawing-room, upon, air, flung, answered, muttered, walked, scarcely, cap, sound, slowly, hair, expression, hardly, every, fellow, near, silence, instant, distance, white, low, soft, bent, walking, deal, sky, grew, poor, shoulders, lips, fond, rather, dark, ought, haste, country, black, faint, beside, suppose, window, observed, continually, clever, creature, sank Consistently used by Pevear and Volokhonsky and avoided by Garnett: therefore, everyone, precisely, also, finally, Ill, despite, maybe, became, anyone, especially, decided, terribly, youre, having, start, impossible, Im, unable, obviously, main, Id, someone, contrary, hes, moment, until, started, order, situation, Ive, didnt, because, terrible, firmly, front, silently, purpose, earlier, otherwise, immediately, certain, understood, lets, barely, theyre, lit, youll, former, youve The words till for Garnett and until for Pevear and Volokhonsky are a classic authorship pair. Pevear and Volokhonsky clearly use a less formal style, as indicated by the large number of contractions among their markers. They also use more -ly adverbs, with nine in the list above compared to only four for Garnett, and only they have indefinite pronouns in their list. By contrast, Garnetts list contains many concrete nouns, while Pevear and Volokhonskys list contains none. It also contains many more full verbs and adjectives than Pevear and Volokhonskys. There is no space here to investigate these differences fully, but this analysis suggests new ways to study the elusive signal of the translator. The seeming paradox of the invisible translator can be resolved: although the strength of the authors signal normally renders the translators individual style invisible, the translators own signal is quite strong enough to allow the attribution of translations to their translators once the authors signal is eliminated. "
	},
	{
		"id": 153,
		"title": "The Prepare and Visualize Mallet Data Spreadsheet",
		"authors": [
			"Hoover, David L."
		],
		"body": " Topic modeling is a popular tool for analyzing texts, but Mallet, probably the most commonly used program, produces output that is not very easy to interpret or understand. The Excel spreadsheet presented here provides an automated way to import, reformat, visualize, graph, and perform some kinds of analysis on Mallet topic models in either Windows or Mac OS. It is especially useful for those who want to use Mallet to explore small amounts of text that they know well.The most basic function of the spreadsheet, Import_Mallet_Data, copies the information from Mallets output files into four sub-sheets: Mallet Data for Visualization, Mallet Data for Wordle, Mallet Topic Proportions, and Graph Topics in Texts. For example, it copies Mallets data on how each word in the texts is distributed in the topics into the Mallet Data for Visualization sheet. Here is a snippet: 0 ring 10:26 13:1 3:1 1 hanging 10:6 17:1 2 quivers 19:2 7:2 10:1 2:1 8:1 3 hangs 10:8 7:1 4:1 11:1 4 loop 15:3 14:1 7:1 While importing this information, it also extracts the weights of the words in each topic and sorts the words in descending importance, rewriting the information in a form that can be imported into Wordleto create a word cloud and also into the Graph Topics in Texts sheet for use in highlighting topics in texts. A third function is a simple visualization macro that highlights the words in each topic with different colors and text-sizes that reflect the importance of the words in that topic. A snippet can be seen in Fig. 1. Though fairly crude, this visualization gives more information than a word cloud, consistently marks the same frequencies with the same color and font size, and makes coherent topics more clearly visible and interpretable. A fourth function reformats Mallets data about the proportions of each topic in each text and about the overall weight of each topic in the model. It lists the topics for each text in descending order of weight and lists the texts with the greatest weights for each topic in descending order of weight. A snippet is shown in Fig. 2. This reorganization makes it clear at a glance that, in the model shown here of the six voices of Virginia Woolfs The Waves, topic 10 is the most important in all six of the voices, and that each voice also has its own individual important and characteristic topic, always the second most important. Not all of the individual topics are shown in Fig. 2, but topics three and four are clearly characteristic of Susan and Louis, respectively. A fifth function graphs the weights of each topic in each text, producing a scatter graph like that in Fig. 3. This interactive graph allows the topics to be toggled on and off to highlight any topics and sections that seem interesting, and clicking on a topic weight in the graph opens the section of text in which that weight appears and highlights the topics most important words in the text. Up to three more topics can be highlighted in that same text, each with its own color. The sixth major function compares two topic models, either the same model run twice, or models with different numbers of topics or different parameters. After importing the first model and copying its results into the Compare Topic Models sheet, the user creates a second model, copies it beside the first, and runs the comparison macro, which shows which topic in the second model is most similar to each topic in the first and which topic in the first model is most similar to each topic in the second.The results of a comparison of a thirty-topic and a twenty-five topic model of Jane Eyre are shown in Fig. 5The macros that perform the functions above and several parametersare customizable. The initial font for the visualization macro, the minimum frequency of a word in a topic to display, and the maximum number of topic words to compare in the comparison function can be set by the user. The user can also choose how many texts to display for each topicby setting a divisor for the maximum topic weight. In Fig. 2, only texts that have at least one fifth the weight of the topic with the greatest weight are shown. The Instructions sheet contains detailed instructions for its use. Near the bottom of the sheet are Mallet commands for creating a topic model that can be copied out and dropped into Mallet. The_Prepare_and_Visualize_Mallet_Data Spreadsheet provides a gentle, automated introduction to topic modeling that is especially appropriate for users who want to explore the possible value of topic modeling in texts that interest them without facing a steep learning curve. "
	},
	{
		"id": 154,
		"title": "An Evaluation of Rhyme Detection Using Historical Dictionaries",
		"authors": [
			"Houston, Natalie M"
		],
		"body": " Introduction As part of a larger project in distant reading nineteenth-century British poetry, a method for detecting line-end rhymes was devised that utilizes rhyme dictionaries published in the eighteenth and nineteenth centuries. This method was proposed in order to account for historical debates about the definition of poetic rhymes in English as well as historical changes in pronunciation. This paper describes an evaluation of this approach that compares it to a method commonly used in computational analysis, which is based on the CMU Pronouncing Dictionary, in order to understand what significant differences occur. Historical context Rhyme in English poetry is generally defined as the connection between two syllables that have identical stressed vowels and subsequent phonemes but differ in initial consonantif any are present. Line-end rhyme is the most common use of rhyme, and it contributes to the structure and effect of particular poetic forms, like the sonnet and triolet, and to stanza patterns like the Spenserian stanza. One syllable, or masculine rhymes, predominate in English poetry, as do perfect or exact rhymes, in which the vowel sounds are identical: cat/hat. Yet many poets have also used imperfect or near rhymes, in which the vowels are somewhat different: young/song. Literary critics in the nineteenth century frequently debated the rules for rhyme, either pointing to such examples as justification for a relaxed definition, or deriding them as bad poetry. Alongside these debates, many different rhyme dictionaries were published in the nineteenth century, which offered critical definitions and examples of poetic rhyme, as well as lists of rhyme syllables and rhyming words in English. These dictionaries were aimed at would-be poets, students of poetry, and those wishing to improve their pronunciation of English. Rhyme dictionaries can thus serve as a data source for understanding both historical theories about rhyme and historical British pronunciation. Rhyme detection with historical rhyme dictionaries In previous work, a method for rhyme detection using John Walkers Index of Perfect and Allowable Rhymes was demonstrated. Each entry consists of a rhyme syllable, a list of words that end with that syllable, other perfect rhymes, and a list of allowable rhymes: AM Am, dam, ham, pam, ram, sam, cram, dram, fam., sham, swam, epigram, anagram, &c. Perfect rhymes, damn, lamb. Allowable rhymes dame, lame, &c.A key-value table was created from these entries. The rhyme detection script uses the key-value pairs to identify perfect rhyme words and syllables first, followed by allowable rhyme words and syllables within the poem. Rhyme patterns are also visualized as a sequence of capital letters, as is standard in literary studies. This method makes possible the detection of rhyme words, rhyme syllables, and rhyme patterns in large document sets. This method for computational historical poetics can compare different historical theories of rhyme as well as use them to evaluate rhyme usage in large document collections. This method also contributes to the study of rhymes effects on poetic vocabulary more generally in the nineteenth century. Rhyme detection with the CMU Pronouncing Dictionary The Carnegie Mellon University Pronouncing Dictionary is an open-source machine-readable pronunciation dictionary for North American English that contains over 134,000 words and their pronunciations. It is widely used for a variety of language analysis tasks and is available through NLTK. Several researchers have based their work on rhyme detection on the CMU Pronouncing Dictionary. The rhyme-plus package for node.jsand the pronouncing package for Pythoninclude functions for rhyme analysis based on the CMU Pronouncing Dictionary. This wide availability has made it standard for dictionary-based digital humanities work involving pronunciation..) Evaluation In this evaluation project, rhyme detection using historical rhyme dictionaries is compared to rhyme detection using the CMU Pronouncing Dictionary. First, the rhyme syllable and word pairs from Walkers rhyme dictionary are compared against the CMU dictionary to discover which rhyme word pairs are found in both dictionaries; which rhyme word pairs are found only in the CMU dictionary; and which rhyme word pairs are found only in Walkers dictionary. Preliminary evaluation with a random sampling from Walkers dictionary suggests that a significant proportion of historical rhymes labeled as perfect, as well as most of those labeled as allowable by Walker, are not discovered by using the CMU dictionary. Several reasons are suggested for this: pronunciation differences between American and British English; vocabulary differences between literary and general English; and vocabulary and pronunciation differences between nineteenth-century and contemporary English. A second phase of evaluation tests the historical dictionary method and the CMU Pronouncing Dictionary over a corpus of 1500 British poems published between 1800-1900 to evaluate how significant the differences between the dictionaries are for the analysis of a literary corpus. "
	},
	{
		"id": 155,
		"title": "Deep Watching: Towards New Methods of Analyzing Visual Media in Cultural Studies",
		"authors": [
			"Bermeitinger, Bernhard",
			"Gassner, Sebastian",
			"Handschuh, Siegfried",
			"Howanitz, Gernot",
			"Radisch, Erik",
			"Rehbein, Malte"
		],
		"body": " A large number of digital humanities projects focuses on text. This medial limitation may be attributed to the abundance of well-established quantitative methods applicable to text. Cultural Studies, however, analyse cultural expressions in a broad sense, including different non-textual media, physical artefacts, and performative actions. It is, to a certain extent, possible to transcribe these multi-medial phenomena in textual form; however, this transcription is difficult to automate and some information may be lost. Thus, quantitative approaches which directly access media-specific information are a desideratum for Cultural Studies. Visual media constitute a significant part of cultural production. In our paper, we propose Deep Watching as a way to analyze visual mediausing cutting-edge machine learning and computer vision algorithms. Unlike previous approaches, which were based on generic information such as frame differences, color distributionor used manual annotation altogether, Deep Watching allows to automatically identify visual informationin large image and video corpora. To a certain extent, Tilton and Arnolds Distant-Viewing Toolkit uses a comparable approach. However, by means of our customized training of state-of-the-art convolutional neural networks for object detection and face recognition we can, in comparison to this toolkit, automatically extract more information about individual frames and their contexts. Research object The focus of our project is Ukrainian nationalist Stepan Bandera, who during World War II collaborated with Germany and tried to forcefully establish a Ukrainian national state against Polish and Russian opposition, and his instrumentalisation in the recent Ukraine conflict by both the Ukrainian and the Russian side. In the Russian narrative, Bandera is used as an example for Ukrainian fascism, whereas for Ukrainian nationalists he symbolises the uncompromising fight for national independence. New media such as video clips uploaded to YouTube are used extensively to disseminate these contradicting interpretations of Bandera; a first study showed that this instrumentalization is present in all major digital media and was already immanent before 2014. Our paper builds on this preliminary work and traces Banderas image and position within cultural memory in Poland, Ukraine, and Russia from the Euromaidan in 2013 up until now. Methodology We use the first 200 Youtube search results for the terms Stepan Bandera and Степан Бандера as a corpus. Because of overlapping search results, our corpus comprises 274 videos, uploaded to Youtube between 2007 and 2017 with a total length of 3 days, 11 hours, 49 minutes, and 16 seconds. It should be noted that YouTubes search engine does not provide direct access to its database, but rather adapts the result list according to country, browser, and other details of the user. The 274 videos are split into their individual frames. In order to analyse the corpus, we trained Detectron, an open source framework developed by Facebook AI Research, to recognize 12 emotionally charged symbols, which help identify in which context Bandera is presented and thus, hint at different instrumentalization. Table 1 presents these symbols within their four different main classes and the corresponding numbers of manual training annotations used. Table 1: List of all 12 symbols within their 4 distinct classes To create the training set we manually annotated 793 images with 1731 annotations, i.e. an average of 144 annotated objects per symbol. An annotation consists of point coordinates indicating the outline of the object and the corresponding name of the symbol. Between 1 and 13 annotations are assigned to an image, on average 2.2; the median is 1. For proper testing and evaluation, the corpus is randomly divided into training, testing, and evaluation data using a ratio of 70/15/15. We use Intersection over Unionas the evaluation metric for symbol recognition. IoU covers the interval 0 to 1, 1 being a perfect match between proposed and predefined region. In our experiment, we reach an average IoU of 0.68. On closer inspection, our results show that objects which have not been recognized in one frame are likely to be recognized in a subsequent one. Hence, the recognition of symbols is even better then the test result suggests. A sample visualization of recognized symbols in a single frame can be seen in Figure 2. Figure 2: Recognized symbols in a single video frame, taken from https://www.youtube.com/watch?v=axFz-SU8cIM. Unfortunately, our instance Detectron is not optimized for individual face recognition; trying to recognize distinct personsin individual frames led to a high error rate within this class. Hence, we decided to combine Detectron with OpenFace, an implementation of the FaceNet algorithm. We are currently evaluating recognition accuracy in a test corpus and will present the combined results of Detectron and Facenet at the conference. Results As Figure 3 shows, symbols related to Polandand Russiaare seldom encountered in our corpus, whereas the flags of Ukraine and UPA are rather common, as is the Ukrainian coat of arms. The Ukrainian flag, for example, shows up in 2% of all video frames in the corpus. Also common, albeit less frequently occurring than their Ukrainian counterparts, are the symbols of the Third Reich. This distribution suggests that Bandera is presented in a Ukrainian nationalist context and his connections to the Nazis is underlined, whereas his position in the Polish and the Soviet context does not play a big role. This interpretation becomes even more clear when symbol co-occurrencesare plotted. Both the Ukrainian and the Nazi symbols not only co-occur within their group but also with the respective other groups. This finding hints at the dominance of a Russian nationalistic discourse on Youtube, which frames Bandera as an example of Ukrainian fascism. Figure 3: Mean percentage of occurrence for each symbol Figure 4: Symbol co-occurrences in 274 videos, adjusted for symbol frequency Figure 5: Total symbol occurrences over time The next step is to combine the detection results from Detectron with the appearances of Hitler and Bandera in our corpus. What is more, we plan to compare the results discussed above with a second video corpus about Bandera, which was collected in 2013 as part of previous work. This comparison may uncover how the Ukraine crisis changed the way Bandera is represented in Youtube videos and will be presented at the conference. A first glance at diachronic symbol occurrences is presented in figure 5; this visualisation suggests that specific symbolic discourses rise and fall in the course of time, and most symbols peak in 2014 when the conflict is in the most heated stage. Discussion and outlook Recognizing specific symbols allows for new ways to study large visual corpora. Nonetheless, this approach is tied to a specific research question because a RCNN has to be trained to recognize predefined symbols. This limiting factor led us to experiment with a more general approach which focuses on visual depictions of human bodies as embodied signs, a key question of Cultural Studies. We are currently evaluating additional algorithms to automatically recognize specific people and assess both their posture and mimic on a sample corpus of 1000 trading cards from the 1930s depicting German-American actress Marlene Dietrich. Body postures can be analyzed on the basis of keypointssuch as hands, feet, head, etc., which result in different postures.These postures can be used to study symbolic meanings communicated through the body. The connection between postures and gender stereotypes is much discussed; in the case of Marlene Dietrichs androgynous self-staging which relies on elements connoted as male, a quantitative analysis of postures and their changes over time allows new insights. Figure 6: Posture detection with Detectron, Face analysis by OpenFaceGeneric information on Faces can be extracted by the algorithm OpenFace 2.0which extracts three-dimensional orientation and keypoints such as eyes or nose from a given digital image.Moreover, these keypoints are compared with a standard face model defined by the Facial Action Coding System, which describes facial expressions. In the case of Marlene Dietrich, the expression outer brow raiser is often encountered, which can be explained by the makeup trends of the 1930s. Thus, our approach can to a certain extent assess fashion and style-related questions in a quantitative manner. By means of combining various algorithms to automatically identify symbols, objects, faces, posture, and mimics, we propose a potent framework to study large corpora of visual media. We are convinced that Deep Watching will advance the quantitative methodology of CulturalStudies significantly. "
	},
	{
		"id": 156,
		"title": "“The Ties That Bind': The Creation, Use, And Sustainability Of Community Generated Histories",
		"authors": [
			"Hughes, Lorna",
			"Benardou, Agiatis"
		],
		"body": " The ties that bind: The creation, use, and sustainability of community generated histories The use of digital content, tools and methods allows new insights into historical research, through enriched engagement with primary sources via digitisation and datafication, and the use of data analysis, visualisation, and immersive approaches. In response to these digital opportunities, the commemoration of the Centenary of the First World War has seen a digital big bang: more than any other historical period, the research community has access to a large number of publicly accessible digital resources: outputs of projects created by universities, libraries, museums, archives, and community groups. In addition, numerous organisations and initiatives have created opportunities for individuals and communities to create and/or share digital Community Generated Content, making accessible public historical content from personal collections, or providing expertise and knowledge to collection or catalogue descriptions. While many of these initiatives simply present personal collections and content alongside official archives, collections, and narratives, they can also present an opportunity to explore the potential of community histories and content to challenge notions of professionalism and the authority of the expert voice. This paper will address ways that digital CGC has been used in digital First World War initiatives across Europe, focusing on three core aspects of this approach. Everybody is your neighbour, everybody is your friend The first is the significance of community generated history to the commemoration of the First World War. Due to political, educational and public interest in the decade of commemoration of significant centenaries in twentieth century history, there has been an enormous interest in community digitisation projects that allow members of the public to digitise their family collections that relate to the First World War. The first project to do this was the Oxford Great War Online project. This methodology was adopted by Europeana 14-18, which has generated ca. 200k items related to the First World War at workshops around Europe. Smaller, national/local projects have also carried out workshops to generate content within communities. In this presentation, we will examine motivations for developing community generated content, and how analysis of these activities shows the value of participating in digitally-based activities to develop CGC can increase engagement with primary sources and provide a strong example of the power of digital heritage to facilitate experiential value, opportunities and benefits. However, the interaction of creators and producers to produce meaningful experiences in the digital heritage sector is poorly theorised, particularly where the community is both the creator and the producer. We argue that those involved in creating CGC in the digital heritage sector can experience value in four different areas: firstly, the value of producing distinctive content; secondly, the value of a feeling of useful participation; thirdly, the value of the transformation such involvement can engender in the participants, and finally, the value of CGC in helping coalesce acommunity through the work itself. It is within this matrix of value that existing discussions of empowerment, identity production, and challenges to the expert voicesit, and these issues often span multiple parts of our matrix. A fuller understanding of such values and a fleshed-out model of each aspect is needed for us to understand the gaps a participant may have between their expectations and their experience when becoming involved in CGC, and to establish a concrete analysis of the value of CGC for historical research. The time has come to let the past be history The second strand is the use and reuse of this material for research. The most recent Call for Europeana Research Grants invited early-career researchers to explore the Europeana 1914-1918 Collection to address digital humanities research questions in projects that were transnational in scope. But the development of the Europeana 14-18 Collection, and most other GCG initiatives, did not intend research as the end goal. Rather these resources were developed with the objective to mobilise communities; to stir and engage. But they leave in their wake a vast corpus of heterogeneous data that can be used beyond the initial design of these initiatives, and when non-professionals create data that can be used by communities and professional researchers alike, the transformative nature of this material is inevitably magnified. This issue raises several questions. Principal amongst these is: do the methodologies of creation, and the value-based motivation of those developing projects and people who submit material to these CGC projects, support re-use? What, therefore, is the research potential for this content? Are they just localised Wikipedias or blogs, designed for superficial non-professional reference? Once this data isused for research, then how is the past reconstructed and reinterpreted in the digital domain by historians– in short, does CGC subvert or just supplement existing paradigms of digital information use in history? How can we take the existing model of landmark commemorations, with their significant peaks of public engagement, and systematise our findings? In the paper we will point to some contextual answers to these questions. Our initial conclusion is that the meshing of CGC and research has not delivered on its transformational promise, primarily due to structural factors in the engagement of researchers with this material, and so this lack of research value must be understood as having a negative impact on participants, as outlined in the matrix of value described in section 1, above. Some say forget the past, and some say dont look back But for every breath you take, youll leave a track The final strand is an examination of the parallels between community generated digital content and the establishment of community archives and History from below, the basis of a significant body of archival research. The real parallel is the fragility of the digital content, as much CGC ends up as effectively orphan content – the poor likelihood of digital sustainability parallels exactly the fragility of similar analogue content, and raises similar challenges of post-custodial care. This directly mirrors the fragility of community archives generally. The Digital Preservation Coalition have rated community archives and CGC as critically endangered . While the sustainability of such content is itself an issue, as with our research concerns above it is essential to recognise that the fragile sustainability of CGC negatively affects its value to participants. In fact, the interplay of participant value and future sustainability should be recognised, alongside infrastructural issues of preservation : the value of the content may be incidental to the experience of creating it. To quote a respondent in the Europeana 16-18 study, their largest takeaway from the experience of CGC was that my neighbours and I have the same history. However, while acknowledging this, we must also challenge the digital heritage community to communicate their expectations and responsibility for sustainability of CGC to participants in their projects. The authors will explore the three stands and the value of bringing them together in synthesis, drawing on their research into digital approaches to the First World War Centenary generated through the Europeana Research initiative, and two UK Arts and Humanities Research Council projects, Living Legacies and Reflections on the Centenary of the First World War. By digging into the centenary of the First World War and the digital big bang of content it has engendered, it is possible to create a detailed case study to address the value and digital legacy of community generated content that is, methodologically, of significance to broader issues around using and sustaining digital histories. "
	},
	{
		"id": 157,
		"title": "Buy Healthy, Tasty, Pure! A Digital Text Analysis of Neoliberal Trends in Dutch Food Culture",
		"authors": [
			"Huijnen, Pim",
			"Wevers, Melvin"
		],
		"body": " Background Notion of what constitutes healthy eating, as well as healthy food are highly susceptible to medical, economic, political, and cultural forces. A particular striking shift in western food cultures has been located in the second half of the twentieth century. Not only has it seen the finalization of the turn from a quantitative-based food culture to one based on qualitative aspects, which took off in the Interwar period. It is also the period in which the adage of eat more is replaced by the dictum of eat less. This particular trend has been studied in the light of the rise of neoliberalist thinking, understood here as the spread of market thinking to all spheres of life. Guthman and DuPuis have summarized the impact of neoliberalist thinking on food culture with the paradox that [t]he worthy neoliberal citizen must want less while spending more. Neoliberal thinking, in other words, underlies the western disciplining of bodies by creating societies that reward thinness above being overweight. It can, as such, be seen as an example of Foucauldian biopolitics that set in even before state administrations started adopting neoliberal policies in the 1980s. Research question and data Since the neoliberal way of life impacted most western countries to more or lesser extent, the question is opportune whether similar changes in food culture can be localized in Dutch postwar notions of good food. This paper tries to locate these changing notions in the commercial food magazine Allerhande See, for the online historical archive of this magazine: https://albertheijnerfgoed.courant.nu/periodicals/A. . After all, food companies,restaurants as well as retailers have played a vital role when it comes to the impact of neoliberal ideas on food culture. The industry is known to have adopted a number of strategies to impact the spending habits of customers, from undermining dietary advice to introducing food innovations. The Allerhande waspublished and distributed by retailer Albert Heijn, currently the largest supermarket chain in the Netherlands. The aim of this paper is to see to what extent Albert Heijn as a commercial food enterprise appropriated the above-mentioned tendencies in its framing of the products it tried to sell. The proof of concept presented in this paper are based on the first 19 Allerhande volumes, from its initial issue in 1954 up until 1973. For the final paper, these will be supplemented with the volumes up until 2010. The volumes used here range from 100,000 to 300,000 words per year. Related work Although food history is a lively field of study in the Netherlands, the link with neoliberal discourse in this domain remains understudied. This paper, particularly, draws from frequentist text mining techniques that others have adopted to study the rise of a neoliberal discourse. Also, it builds on experiences in creating text mining pipelines in the study of US American influences on Dutch public discourseThis paper is, consequently, not so much concerned with the introduction of novel text mining techniques for the study of food history, as it is with arguing how proven digital methods can help humanities scholars in bringing discursive patterns to the fore in large textual corpora. First results and conclusion When looking at adjectives associated with food, a striking discursive trend in the Allerhande volumes 1954 to 1973 is the steady increase of the qualification lekkerfrom 240 to 410 per million words. At the same time, the word gains a strong collocative association with both gezondand slankafter 1964. Part of the reason for this is the sharp rise in relative frequency of the word slank from 1965 onward. While before mentioned about 32 times per million words annually, references to thinness are more than doubled afterwards: 76 per million words. Before slimness is quantified by stressing calories and nutrientsand koolhydratenstart to sharply increase at the end of this period) it is, therefore, already gaining traction in a more qualitative discursive manner. What does the dataset yield about what, then, is considered tasteful, healthy, and, by consequence, good for the human figure? Vitamins remain an important factor for this discourse throughout this period. Interesting, however, is how margarine is pulled into the slimness discourse. Although its relative frequency of about 190 per million words does not change considerably between 1954 and 1973, its collocative strength with words like gezond and slank does increase strongly after 1965. Initially branded as a low-priced alternative for real butter, in this period it gains traction as a healthy substitute—particularly thanks to its low level of saturated fats and variety of added vitamins. Striking are the associations within the same discourse of slimness-health-tastefulness with purity. When it comes to margarine, purity is, after all, not an obvious qualification. In sum, the Allerhande played its part in changing the discourse around food from eat more to eat less, as becomes apparent in its rising emphasis on slimness. In line with the literature is the way Allerhandes increasingly stresses the association of slimness with health, but also with tastefulness and purity, while at the same time offering a platform for products like margarine that—not always in coherence with scientific viewpoints—brand themselves as such. The data for the remaining period will have to show whether this development will continue and, as expected, radicalize after 1973. "
	},
	{
		"id": 158,
		"title": "Labor Witnesses at U.S. Congressional Hearings: Historical Patterns",
		"authors": [
			"Hulden, Vilja"
		],
		"body": " Congressional investigations and testimony before Congressional committees is a commonly used source in labor historyCongressional investigations into the causes of economic depressions, the problems of industrial relations, anti-labor practices, and alleged malfeasance within labor unionshave provided an important body of evidence for students of labor-capital relations, working-class culture, business ideology, and the likeAs a subject in its own right, however, testimony before Congress has received only limited attention from labor historians, and little enough from other scholars, though there is relevant work on specific committees and general lobbying. The focus in this paper is on what these hearings can tell us about the relative power of workers in the society over time. The U.S. has no labor party, and American unions have traditionally been ambivalent about electoral politics and legislation. Yet they have sought to be heard in Congress. This paper combines metadata about testimony at Congressional hearings and data about union membership and strike frequency in the U.S. to argue that this effort has been most successful when union penetration of the civil society as well as union involvement in electoral politics have been strongest, emphasizing the efficacy of a multipronged and organizing-based approach . Basics about the data and processing The data set used here is extracted from the ProQuest Congressional database, which contains metadata on hearings and witnesses for all Congressional hearings. These are proprietary, but all publishable data as well as scripts will be available at https://github.com/vhulden/congressionalhearings/The full data set contains between about 62,500 and 85,000 unique hearingsand a total of 941,302 instances of testimonyThe average number of witnessesper hearing is 12.2The subset under closer examination here consists of those hearings that concerned organized labor, wages, jobs, working people, labor management, and the like. The basic data was further processed to attempt to assign witnesses into broader categories by their organizational affiliation, which the data contains for 83 percent of the instances of testimony. Since the focus in the current paper is on what hearings data can tell us about workers and their relative strength vis-à-vis business representatives, the main categories considered here are labor, companies, and trade associations; in addition, I have included the two largest categories of witnesses, the federal bureaucracy and the political partiesLabor topics and witnesses Figure 1 suggests that Congressional attention to matters related to work and labor has been fairly constant. Figure 1: Hearings related to work and labor as percentage of all hearings, 1877-1990 Unsurprisingly, labor has been better represented at hearings on labor topics than at the average Congressional hearing, as shown in Table 1, which displays what percentage of the testimonies come from different witness categories. Witness category In full dataAt labor-related hearingsLabor 2.5 5.9 Companies 7.2 5.9 Trade associations 2.0 1.9 Federal government 10.6 7.5 Political parties 12.0 7.5 Table 1: Instances of testimony at Congressional hearings, 1877–1990 Companies and trade association representatives dominate over labor ones in both data sets, but much less so at the labor-related hearings. Moreover, plotting the representation of different groups over time reveals that these percentages have not held constant. Figure 2: Representation of witness groups over time, by percentage of instances of testimony In two ways, this chart emphasizes the significance of electoral politics. One, the sheer number of testimonies from inside the federal bureaucracy and the political parties emphasizes that even at Congressional hearings, succeeding in electing friendly politicians mattered. Two, it is clear that labor representation was strongest in the periods when organized labor concerned itself with electoral politics. The New Deal era, when labor unions formed an important part of the Democratic coalition, forms the only period when labor testimonies were consistently on a rough par with business and trade association testimonies. Similarly, significant spikes of labor representation around the turn of the twentieth century and in the Progressive Era coincide with the American Federation of Labors campaign to support labors friends in either party, while some spikes in the 1920s perhaps relate to the brief but significant challenge to the major parties from the Farmer-Labor Party. Strikes, union density, and labor representation at hearings How well labor has been represented at hearings also seems to correlate with labors strength outside the electoral context in the form of strikes and union densities. Figure 3: Number of strikes, percentage of strikers of labor force, and union density correlated with labor representation at Congressional hearings Note in particular how labors representation stabilizes as strike activity becomes more consistent in the 1930sHowever, note also how the downward trend of labor representation at Congress tracks declining union density and the decline in strike density, despite a spike in the number of strikes in the 1970s. Discussion and future work The data at hand is, of course, limited and imperfect. Nevertheless, at the very least these charts underline that labors power is consistent mainly when labor is strong by a number of measures, from strike activity to union density to electoral participation; individual strike waves around the turn of the twentieth century did not correlate with consistent labor representation at Congressional hearings, despite occasional spikes in the prevalence of labor testimony. The lack of impact from the spike in the number of strikes in the 1970s also seems to suggest that incidents of labor strife are insufficient by themselves, if the penetration of laboris low or declining. More careful statistical analysis is neededto tease out a more exact relationship, but overall, these correlations seem to suggest a multipronged, organizing-intensive strategy to increase representation. Future work might consider electoral campaigns and legislative outcomes to further elucidate labors fortunes in Congress. "
	},
	{
		"id": 159,
		"title": "Exploring Intertextuality in the Mahoyoga Section of the Rin chen gter mdzod",
		"authors": [
			"Mei, Ching-Hsuan",
			"Hung, Jen-Jou"
		],
		"body": " In Tibetan culture, treasure texts refer to those written scrolls hidden secretly centuries ago by the great master Padmasambhava in the Yarlung Dynastyand discovered by later generations. Those who have the ability to detect these hidden objectsare called treasure revealers. This practice is especially significant in the Nyingma School, one of the four major Schools in the Tibetan Buddhist tradition. It was a creative invention in response to the other three schools, who claimed that their doctrinal texts were translated from Sanskrit, and thus of pure Indian origin. The Rin chen gter mdzod is a large corpus compiled by Jam mgon kong sprul blo gros mtha yas, a famous prolific writer and one of the propagators of the non-sectarian movement in the East Tibetan region in the 19 th century. Kongtrul made efforts to collect scattered treasure texts to prevent their loss. It contains the works of 108 treasure revealers under the classifications of Mahayoga, Anuyoga and Atiyoga. This project focuses on texts classified in the Mahayoga section, in total, 54 out of 72 volumes. This project will shed some light on the meaning of treasure rediscovery and textual invention and reuse in Tibetan religious culture. Considering the amount of data, we try to implement digital technology to compare each phrasing in order to detect reused sentences, thus we can further interpret the so-called intertextuality of Tibetan treasure literature. Although Tibetan scholars have already noticed the phenomenon of text reuse in the treasure literature, it remains difficult to conduct a large scale comparative reading and further identify repeated sentences and locate their origin. Deducing from previous studies, we estimate that there might be greater intertextuality embedded in the writings of treasure texts than has already been noted. There has been no systematic analysis of large Tibetan textual collections in academic circles so far, thus we propose to apply digital textual analysis technology to deconstruct the great corpus of Tibetan treasure— the Mahayoga section of the Rin chen gter mdzod. After a trial period spent on this research project, we find it is an approachable goal. Detection of Text Reuse The data to be analyzed in this study is the content of the Rin chen gter mdzod. We use the XML file published by the BDRC organization on its website The files of the Rin chen gter mdzod published on the BDRCis digitalized based on the Shechen edition, which consists of 54 volumes. We numbered each document by volume and by document in each volume, and obtained a total of 2,643 files from 1-1 to 54-40. as the primary source. The contents of these files are in Tibetan. In order to simplify the difficulties in the subsequent processing, we have used the Wylie transliteration systemto transliterate the Tibetan texts of the documents into the Roman alphabet. Here is an example of Wylie Transliteration. In the Tibetan writing system, the double perpendicular strokeis used to separate complete sentences. Therefore, we can use the punctuation to separate the text into sentences. In order to be able to effectively find the part of textual reuse between sentences, we adopt the Local Alignment algorithm, which is commonly used in the field of bioinformatics to perform matching of long DNA sequences. The biggest advantage of Local Alignment is that it can efficiently find the maximum matching area of two strings, and consider possible insertions and deletions of characters in the strings. An example of the comparison result is as follows: The ○ in the comparison result denotes inserted characters of the string, and  ◎ denotes a deletion of original string. After comparing all sentences, we found 14,478 paired sentences with more than 20 words repeated. Database and Web Interface Construction In order to assist other researchers who are also interested in exploring this topic, we have created a database with an easy-to-use web interface http://syda.dila.edu.tw/RCGM/ . This website is still under development, yet currently provides the following two functions: When a user enters a complete section of a text, the system will highlight those sentences which repeatedly occur in other texts. For example, the figure below shows the content of the 401 st sentence of text No.5-5 has two other repetitions: the 715 th sentence of text No. 30-19 and the 350 th sentence of text No. 26-20. � unable to handle picture here, no embed or link The user may further query the details of a compared result of two highly overlapped sentences. For example, the figure below shows the content of the 401 st sentence of text No. 5-5, and the 715 th sentence of text No. 30-19 in the same window. In addition, the contexts of the two sentences are also displayed on the screen, which makes it convenient for the user to assess. Preliminary analysis According to our preliminary analysis, we obtained the following results. Firstly, reused sentences do recur among the works of different revealers in different times. Taking two important revealers from the 12 th-13 th centuries as example, there are 242 matches between the works of Nyang ral nyi ma od zerand Gu ru chos kyi dbang phyug. Other detected and highly duplicated sentences are derived from works of revealers in the 14 th, 15 th and 17 th centuries. In particular, we noticed that the Eight Collected Teachings of Sugatawas composed by Guru chos dbang, Rig dzin rgod ldemand Gyur med rdo rjerespectively. This study enables us to take a closer look at their actual contents and editing methods. Secondly, regarding textual comparison, we randomly chose a sentence, as follows:  on kyang bla med thun mong ma yin pai skabs dir dam ye gnyis med dbugs rngub pai sbyor ba dang bstun rang la ◎ pas mi shigs pai thig ○ ◎ ro gcig tu ○ ○ bsams kyang legs so. We notice that this sentence has appeared both in text No.14-25, The Pacifying Homa-teaching on splendid great bliss from the Collected Teachings of SugataVolume 14/ Pages 745-762 / Folios 1a1 to 9b4. and text No.31-23, The Blaze of the Sharp Blade of Vajrakilaya – The Homa of quick bestowal of pacification and enrichmentVolume 31/ Pages 489-516 / Folios 1a1 to 14b1. . Both can be traced back respectively to Nyi ma od zers and Guru chos dbangs writings. It is not clear for us yet why Kongtrul classified them in different categories, nevertheless, through close reading we find that there is an entire duplicated ritual section, which is far beyond the scope of the detected sentence. Further analysis will rely on close reading. Yet it is the power of artificial intelligence that has brought us to this domain. "
	},
	{
		"id": 160,
		"title": "Using Network Analysis to Do Traditional Chinese Phonology Study",
		"authors": [
			"Hu, Jiajia"
		],
		"body": " Traditional Chinese Phonology, lacking of alphabetic system of phonetic notation such as IPA, had to deal with large written materials in Chinese characters, and used Chinese characters as a tool to analyze sounds of words. This brings up a significant feature of its study, that is, the relationships of words sounds are more important than their phonetic values. Xìli ánis one of the most important methods in traditional Chinese phonology. Its fundamental is to build networks of Chinese characters having same syllabic elements. This paper takes Xìli án of Fǎnqiè in Gu á ngyùn as an example to show how to use network analysis and visualization software to improve traditional Chinese phonology study. In general, Chinese characters are monosyllabic. A Chinese syllable can be divided into three parts: the Initial, the Final, the Tone. The final can be further subdivided into the Medial, the Main Voweland the Coda, while the Medial and Coda are optional. Fǎnqiè is a Chinese method to indicate the pronunciation of a monosyllabic character by using two other characters. The first one, known as Fǎnqiè-Sh àngzì, has the same initial as the desired character, known as Bèiqièzì. And the second one, known as Fǎnqiè-Xi àzì has the same final and tone as Bèiqièzì. Here is an example. 端, 多官切. The Bèiqièzì is 端. The Fǎnqiè-Sh àngzì is 多 indicating that the initial of 端 is d. The Fǎnqiè-Xi àzì is 宗 indicating that final of 冬 is u ɑn and the tone is -. Naturally, any Fǎnqiè was meaningful when it was created, but may not keep its perfection as time goes by, due to phonetic changes. For example, 東, 德紅切. 德 still has the same initial as 東, while 紅 has the same final but different tones. Thus, the systematic use of such Fǎnqiè in ancient rime dictionaries is an invaluable resource for the work of historical linguists. Guǎngyùn is a Chinese rime dictionary compiled in 1008AD. It is a revision and expansion of Qièyùn, the influential rime dictionary published in 601AD. It is generally accepted that Qièyùn recorded the voice of Chinese at that time, maybe not as a spoken language, but rather how characters should be pronounced when reading the classics. So Guǎngyùn, as the most accurate available account of Qièyùn in the past, was used by traditional scholars as a major source on the reconstruction of Qièyùn system, the code name of Middle Chinese. Guǎngyùn is split into four tones in five volumes. Each tone is split into rimes, with total of 206 rimes. Each rime is divided into groups of homophonous characters, with a character as the representation, and the pronunciation of each group given in Fǎnqiè formula. It is Chénlǐ, in his masterpiece Qièyùn-Kǎo published in 1842, who first introduced Xìli án method in the study of Fǎnqiè in Guǎngyùn. The Principle of Fǎnqiè-Xìli án comes from the idea that the relation between Bèiqièzì and Fǎnqiè-sh àngzì or Fǎnqiè-xi àzì is an equivalence relation. In mathematics, an equivalence relation is a binary relation that is reflexive, symmetric and transitive. Any equivalence relation provides a partition of the underlying set into disjoint equivalence classes. Two elements of the given set are equivalent to each other if and only if they belong to the same equivalence class. That means, if we look for the Fǎnqiè-sh àngzì of each Fǎnqiè-sh àngzì and link them together with one another, we can obtain equivalence class of Fǎnqiè-sh àngzì representing a same initial. By systematically applying this method, it becomes possible to make classes of Fǎnqiè-sh àngzì for the initials, and Fǎnqiè-xi àzì for the rimesof Guǎngyùn. When two classes were unable to link each other by any method, it may conclude that they represent distinct initials and distinct rimes. Figure 1: A page in Guǎngyùn In old days, Fǎnqiè-Xìli án of Guǎngyùn would cost a lot of time and the process is hard to display. So faced the same materials, researchers had to put lot of energy in repeating works to verify others results. It was often difficult to find out what went wrong when there were disagreements. Today, with the help of the network analysis and visualization software package like Gephi, it becomes much more easily to display ones own work or review others work in both researching and teaching. Fǎnqiè-sh àngzìare the nodes. The equivalence relation between them are undirected links. Then the components in the undirected network are equivalence classes of Fǎnqiè-sh àngzìwhich represent different initials. The process is simple and repeatable. The first step is to convert the text of Guǎngyùn to a structural form, see Table 1. The second step is to find the no repeated Fǎnqiè-sh àngzìappeared in Guǎngyùn. The third step is to find the Fǎnqiè-sh àngzì of each Fǎnqiè-sh àngzìin step 2, see Table 2. The fourth step is to transform the equivalence relations between Bèiqièzì and Fǎnqiè-sh àngzì or Fǎnqiè-xi àzì into links between nodes representing these characters. The fifth step is to convert those links in step 4 to a network with a network analysis and visualization software package, like Gephi, see Figure 2. The sixth step is find all the components in the network, Figure 3 shows an example. However, it is not the end of our study of Middle Chinese Phonology, but only the beginning. Over more than 100 years, there is still no consensus on exactly how many initials and finals in Middle Chinese. The reasons are complicated, partly due to different versions of Guǎngyùn, partly due to various origins of Fǎnqiè in Guǎngyùn. Owing to digital method, it becomes much more convenient to demonstrate the works of key scholars and to locate the exact Fǎnqiè that caused their disagreements. Table 1: The Structural Table of Guǎngyùn Rhyme Homophonous Group F ǎ nqiè-sh à ngzì F ǎ nqiè-xi à zì 上平1東 東菄鶇䍶 𠍀 倲 𩜍𢘐 涷蝀凍鯟 𢔅 崠埬 𧓕 䰤 德 紅 上平1東 同仝童僮銅桐峒硐 𦨴𧱁 筒瞳㼧 𤭁 罿犝筩潼曈洞侗橦烔䴀挏酮鮦㼿 𦏆𦍻 眮蕫穜衕 𩍅𢈉 䆚哃 𢏕 絧 𨝯𨚯𪔝𩦶𪒿 徒 紅 上平1東 中衷忠 𦬕 陟 弓 上平1東 蟲沖种盅爞 𦬕 翀 直 弓 上平1東 終衆潨 𣧩𧑄𩅧 䝦䶱䈺螽鼨蔠柊鴤泈 職 戎 …… …… …… …… Table 2: The Fǎnqiè-sh àngzìof each Fǎnqiè-sh àngzì in Guǎngyùn ID F ǎ nqiè-sh à ngzì in Guǎngyùn F ǎ nqiè-sh à ngzì of F ǎ nqiè-sh à ngzì 1 德 多 2 徒 同 3 陟 竹 4 直 除 5 職 之 6 敕 恥 7 鋤 士 8 息 相 9 如1 人 …… …… …… Figure 2: The Network of Fǎnqiè-sh àngzi Figure 3: A Component in the Network of Fǎnqiè-shàngzi "
	},
	{
		"id": 161,
		"title": "Relational Search in Cultural Heritage Linked Data: A Knowledge-based Approach",
		"authors": [
			"Hyvönen, Eero",
			"Rantala, Heikki"
		],
		"body": " Abstract. This paper presents a new knowledge-based approach for finding serendipitous semantic relations between resources in a knowledge graph. The idea is to characterize the notion of interesting connection in terms of generic ontological explanation rule patterns that are applied to an underlying linked data repository to instantiate connections. In this way, 1) semantically uninteresting connections can be ruled out effectively, and 2) natural language explanations about the connections can be created for the end-user. The idea has been implemented and tested based on a knowledge graph of biographical data extracted from life stories of 13100 prominent historical persons in Finland, enriched by data linking to collection databases of museums, libraries, and archives. The demonstrator is in use as part of the semantic portal BiographySampo of interlinked biographies. Approaches to relational search Serendipitous knowledge discoveryis one of the grand promises and challenges of the Semantic Web. This paper concerns on the problem of discovering serendipitous relationsin semantically rich, linked Cultural Heritagedata, i.e., Knowledge Graphs. In particular, we focus on the problem of finding interestingconnections between the resources in a KG, such as persons, places, and other named entities. Here the query consists of two or more resources, and the task is to find semantic relations, i.e., the query results, between them that are of interest to the user. This problem has been addressed before in different domains. The approaches reported in the literaturediffer in terms of the query formulation, underlying KG, methods for finding connections, and representation of the results. Some sources of inspiration for our paper are shortly reviewed below. Inthe idea is applied to association finding in national security domain. Within the CH domain, CultureSampocontains an application perspective where connections between two persons were searched using a breath-first algorithm, and the result was a list of arc chains, connecting the persons based on the Getty ULAN knowledge graph of historical persons. In RelFinder, based on the earlier DBpedia Relationship Finder, the user selects two or more resources, and the result is a minimal visualized graph showing how the query resources are related with each other, e.g., how is Albert Einstein related to Kurt Gödel in DBpedia/Wikipedia. Both gentlemen, e.g., worked at the Princeton University. In WiSP, several paths with a relevance measure between two resources in the WikiData KG can be found, based on different weighted shortest path algorithms. The query results are represented as graph paths. Some applications, such as RelFinder and Explass, allow filtering relations between two entities with facets. From a methodological perspective, the main challenge in these systems is how to select and rank the interesting paths, since there are exponentially many possible paths between the query resources in a KG. This problem can be approached by focusing only on simple paths that do not repeat nodes, on only restricted node and arc types in the graph, and by assuming that shorter, possibly weighted paths are more interesting than longer ones. For weighting paths, measures such as page rank of nodes and commonness of arcs can be used. The graph-based works above make use of generic traversal algorithms that are application domain agnostic. In contrast, this paper suggests an alternative, knowledge-based approach to finding interesting connections in a KG. The idea is to formalize the notion of interestingnessin the application domain using general explanation patterns that can be instantiated in a KG by using graph traversal queries, e.g., SPARQL. The benefits of this approach are: 1) non-sense relations between the query resources can be ruled out effectively, and 2) the explanation patterns can be used for creating natural language explanations for the connections, not only graph paths to be interpreted by the end user. The price to be paid is the need for crafting the patterns and queries manually, based on application domain knowledge, as customary in knowledge-based system. In the following, a case study of applying this approach is presented in the Cultural Heritage domain by using a KG of biographical data. In conclusion, lessons learned are discussed, and further research suggested. Finding semantic relations in a biographical knowledge graph In historical research, one is often interested in finding out relations between certain types of things or persons, such as Finnish novelists, and larger areas, such as South America. Our tool, Faceted Relator, can be used for solving such problems. Faceted Relator combines ideas of faceted searchand relational search. The idea is to transform a KG into a set of instances of interesting relations for faceted analysis. A relation instance has the following core properties: 1) a literal natural language expression that explains the connection in a human readable form. 2) a set of properties that explicate the resources that are connected. For example, the following illustrative example of a tertiary relation <X, Y, Z> connects Leonardo da Vince to Vince and to year 1452 based on the explanation pattern Person X was born in place Y in Z for birth events: :c123 a :BirthConnection; :explanation Leonardo da Vinci was born in Vince in 1452; :place :vince; :time 1452; :person :Leonardo_da_Vince . :BirthConnection rdfs:label Person X was born in place Y in time Z . Relation instances like this can be searched for in a natural way using faceted search, where the facets are based on the properties of the instances, that can often be organized hierarchically. In this case, there would be a facet for explanation types, and facets for places, persons, and times. By making selections on the facet hierarchies, the result set is filtered accordingly and hit counts in facets recalculated. This method was tested in the context of BiographySampo, a linked data service and semantic portal aggregating and serving biographical data. The knowledge graph of this system includes several interlinked datasets: Biographical data extracted in RDF form from 13144 Finnish biographies. The data includes, e.g., 51937 family relations, 4953 places, 3101 occupational titles, and 2938 companies. HISTO ontology of Finnish history including more than one thousand historical events. Data for the events includes people and places related to the event. The data was available in RDF format. The Fennica National Bibliography is an open database of Finnish publications since 1488. The metadata includes, among other things, the author of the book and the subject matter of the book, which can include places. BookSampo data covering virtually all Finnish fiction literature in RDF format, maintained by the Finnish Public Libraries consortium Kirjastot.fi. The Finnish National Gallery has published the metadata about the works of art in their collections. The metadata is described using Dublin Core standard and was available in JSON and XML format. The collected works of the J. V. Snellman portal includes the texts written by J. V. Snellman, the national philosopher of Finland. The data includes, e.g., 1500 letters. We transformed the data into RDF. The focus in our demonstrator is on finding relation instances describing connections between people and places in Finnish cultural history. The relations listed in Figure 1 were created using SPARQL CONSTRUCT queries with natural language explanations. For example, the following template is used for explaining artistic creation relations related to painting collections: < person name> has created a work of art called < painting name> in the year < year> that depicts < place name>. Relational instances extracted from the data Demonstrator at work Faceted Relator was published as part of the BiographySampo portal, and is in use as a separate application perspective in it. Figure 2 depicts the user interface of the application. The data and interface are in Finnish, but there is a Google Translate button in the right upper corner of the interface for foreign users available. Faceted Relator can be used for filtering relations with selections in four facets seen on the left: 1) person names, 2) occupations, 3) places, and 4) relation types. The system shows a hit list of the relation instances that fit the selected filtering criteria in the facets. The user is not required to first input a person and a place, but can limit the search at any time with any facet. This allows searching for relations between groups of people and larger areas instead of single places. Each relation instance is represented in a row that shows first a natural language explanation of the relation, then the related person, place, and data source as links to further information, and finally the relation type. Different types of relations are highlighted in different colors and have their own symbols in order to give the user a visual overview of relations found. At any point, the distribution of the hit counts in categories along each facet can be visualized using a pie chart—one of them can be seen in the left upper corner of Figure 2. View of the user interface For example, the question How are Finnish painters related to Italy? is solved by selecting Italy from the hierarchical place facet and painter from the occupation facet. Any selection automatically includes its subcategories in the facet. For example, places such as Florence and Rome are in Italy, and Vatican further in Rome. The result set in this case contains 140 connections of different types whose distribution and hit counts can be seen on the connection type facet. In the same way, the person facet shows the hit count distribution along the person facet. Any facet could be used to filter the results further, if needed. In this case the 140 hits include, e.g., connection Elin Danielson-Gambogi received in 1899 the Florence City Art Award and Robert Ekman created in 1844 the painting Landscape in Subiaco depicting a place in Italy. In a similar way, the question Who has got most awards in Germany can be solved by selecting the connection type Received an award in a place and Germany from the place facet. The hit distribution along the person facet shows that general Carl Gustaf Mannerheim is the winner with eight German awards. The demonstrator is based on an architecture with the server side consisting of a Apache Jena Fuseki graph store and the client side consisting of an application written with AngularJS. The faceted search was implemented with the SPARQL Facetertool. Discussion An informal initial evaluation and testing of the demonstrator showed that it works as expected in test cases, and that a layman can potentially learn new information by using the system. However, more testing is needed to find out how interesting and surprising the results are for an expert of CH and how a system like this can be used for DH research. We also found out needs to improve the usability of the system. For example, the demonstrator now sorts results based on firstly the name of the person and secondly on the name of the place. The user should probably be offered the possibility to sort the relations freely along any facet. Acknowledgements Our research was supported by the Severi project, funded mainly by Business Finland. The authors wish to acknowledge CSC – IT Center for Science, Finland, for computational resources. "
	},
	{
		"id": 162,
		"title": "Digging Into Pattern Usage Within Jazz Improvisation (Pattern History Explorer, Pattern Search and Similarity Search)",
		"authors": [
			"Höger, Frank",
			"Frieler, Klaus",
			"Pfleiderer, Martin"
		],
		"body": " Introduction Music Information Retrieval offers new options for musicological research, particularly for methodologies which are hardto carry out manually, e.g., large corpus studies and measurings of acoustical properties. One such field of application is the mining of patterns. Patterns – and repetitions in general – play an important role in nearly all music styles and are thus of interest for many sub-fields of musicology. In particular, melodic patterns – or licks as patterns are often called in jazz parlance - form a crucial component of jazz improvisation. Given the significance of patterns and licks in jazz, several research questions arise, concerning historical issues, e.g., the oral tradition of melodic patterns and the development of a typical jazz language, or more systematic issues, e.g., the psychology of the creative process, where patterns can be regarded as necessary to accomplish the highly virtuoso feat of modern jazz improvisation: To what extent are patterns used to shape an improvisation? When and by whom are patterns created and how are they transmitted between players over time? Does pattern usage change with time and styles? Is there an influence of jazz education on pattern usage? How are patterns used to build phrases, e.g., to construct a typical bebop line? Which role do external musical influences such as quotes, and signifying references play in jazz improvisation? This paper presents three novel user interfaces for investigating the pattern usage in monophonic jazz solos and exemplifies how these interfaces could be used for research on jazz improvisation. Related work Several web-based melody search engines have been developed in the past, e.g. Themefinderwhich allows searching for patterns in both classical and folk music and Musipedia, a user-generated database of melodies, providing more sophisticated and user-friendly search interfaces like a virtual piano keyboard for entering melodic patterns and a query by tapping interface for rhythm search. A more thorough overview of search engines can be found in. Unfortunately, many of these projects are discontinued or use out-dated web technology. An example for an up-to-date search interface including metadata filters is the RISM catalogue search. User Interfaces The Pattern History Explorer is a Shiny application that allows exploring a set of 653 patterns that are most commonly used by eminent players. The Pattern Search is a web interface for a general two-staged pattern search Martin2019-04-30T10:24:00 Nicht nur dort!featuring regular expressions. The Similarity Search faces the challenge of finding and grouping similar patterns, i.e., patterns that differ in one or several tones. The applications aim on the one hand at an expert audience of jazz researchers to facilitate generating and testing hypotheses about patterns in jazz improvisation, and on the other hand at a wider audience of jazz teachers, students, and fans. The main goal of the Pattern History Explorer is to enable the exploration of interval patterns in jazz solos by providing information from many different angles. It maintains an overview of interval patterns frequently used by a selected subset of performers and traces their usage in the Weimar Jazz Database, allowing for the discovery of cross-artist and cross-temporal relationships. Currently, 653 interval patterns with 11,630 instances are included. The pattern corpus was created by mining for interval patterns in solos of eminent performers using the partition mode of the melpat module from the MeloSpySuite. Subsequently, all instances of these patterns were searched for in the Weimar Jazz Database, and the results were included in the application. In general, the user of the Pattern History Explorer selects a certain interval pattern from the overall set of 653 patterns. Several options are available in order to filter the pattern set or to change the ordering of the patterns according to several criteria, tonality type, or tonal content). For the selected pattern, various kinds of information can be accessed in several tabs. While pre-computing a set of patterns is helpful for exploratory investigations, searching for instances of arbitrary patterns of any length and frequency of occurrence within a database requires a different approach. The web-based Pattern Search interface provides most of the functionality of the melpat search module of the MeloSpySuite while also extending it with audio and score snippetsfor visual and aural inspection. To execute a basic search, the user can enter a pattern on a virtual keyboard or as a space or comma separated list of elements and choose a corresponding transformation, that is, a mathematical mapping of the basic melodic representation. Currently, ten pitch-related transformations for primary search are offered. An additional 18 transformations, such as duration, inter-onset interval classes and various structural markers, are supplied for the optional secondary search. Additionally, the search space can be constrained by several metadata categories, like performer, style, or recording year. Search patterns can be regular expressionswhich allows searches for variants in a single run. The secondary search can be used to refine the result space, e.g., by filtering out certain rhythmic or metrical configurations or by confining instances to a single phrase. The underlying search algorithm is built upon the basic Python regular expression module, which is fed with virtual Unicode strings constructed from the different melodic representationswith different alphabets. Scores are generated with the help of Lilypond, while audio snippets are directly extracted from the solo audio files using ffmpeg. The latest addition to the web-based toolset is the Similarity Search web application. Basically, the interface follows the design and concept of the Pattern Search , however, with some significant differences. Not only identical pattern instances, but also similar patterns, that differ in one or more tones from the query, can be searched for. The calculation of search results differs from that of the Pattern Search in that the actual similarity calculation is done using the underlying PostgreSQL database system implementation of the Levenshtein distance. This distance measure has been shown in various studiesto be a good approximation to similarity judgements of melodies by human experts. The similarity search operates on a database of the complete set of pitch and interval n-grams of up to 20 elements that were previously extracted from the Weimar Jazz Database using the melpat module. This amounts to about 4 million n-gramsfor MIDI pitch and 3.5 million for interval n-grams. By entering a patternas a space or comma separated list of elements and choosing a transformation, similar n-grams can be retrieved from the database. To further control the result set, the search interface provides options for parameters such as minimum similarity, maximum length difference, or the preservation of melodic contour and pitch range. All searches can also be refined using metadata filters for performers, instruments, etc. Visualizations Search results are presented in tabular form together with two graphical representations allowing for visual inspection – an n-gram network graphand a timeline chart, both generated using the D3.js data visualization library. In the case of the network graph utilizing a radial layout, n-grams can be grouped, e.g., by metadata attributes or same similarity value and n-gram class, resp. Network graph for interval pattern -1,-2,-1,3,3,3,-1,-2 grouped by performer; the biggest bunches stand for those patterns played by Charlie Parker, Dizzy Gillespie, Sonny Rollins and Dexter Gordon. Node colours denote identical patterns which are connected by edges. Node size represents the degree of similarity, where bigger means more similar. The timeline chart depicts when and by which performer pattern instances were played. Audio snippets for all n-grams found are generated and can be listened to by clicking on the nodes in both the network graph and the timeline chart as well as via the play buttons in the result table. Timeline chart for interval pattern -1,-2,-1,3,3,3,-1,-2. Node colours denote identical patterns. Node size represents the degree of similarity, where bigger means more similar. Discussion The three applications are already usable interfaces for the Weimar Jazz Database and serve as prototypes for applications to explore large databases, which are going to be automatically extracted from large collections of jazz recordings, as aimed at within the transatlantic research project Dig That Lick: Analysing Large-Scale Data for Melodic Patterns in Jazz Performances. All three tools can be primarily viewed as bespoke interfaces for the specific needs of jazz researchers, but could easily be transferred to other melodic corpora, too. They could also be of interest to teachers, students and music fans as well as to training courses in computational music analysis. "
	},
	{
		"id": 163,
		"title": "Remember How: The Place of Visualization in Preserving the Memory of Repressions of the USSR Against the Volga Germans",
		"authors": [
			"Iashchenko, Iuliia"
		],
		"body": " The problem of political repression in the Soviet Union, despite the interest of a number of researchers, remains insufficiently studied. Some aspects of the implementation of repression or the life of the repressed were left outside of the research focus. For example, the problems of repression against the Volga Germans during the Second World War remain on the sidelines of research due to the following reasons. Firstly, these are not popular topics for modern Russia, and secondly, access to sources is rather difficult today. There are not so many documentary and oral sources, numerous of those people who were repressed left for Germany or stayed in endangered villages in the Urals and Siberia, so it is really difficult to gather materials for the research. At the same time, it is customary for the historical discourse of modern Russia to include the 1940s in the discourse of the Second World War winners. The great construction of unity among the winners leaves no place for dialogue on traumatic experiences, which includes repression on political and ethnic grounds. As a result, the historiography of repressions against the Volga Germans is quite fragmentary. And today, scientist researcher should give an answer not only to the questions of the academic community but rather to the questions of the repressed and their descendants, who are asked in an interview the only question «For what?». This project is particularly relevant in terms of preserving the memory of the repressed Germans. Most of the Germans were repressed between 1941 and 1946 when the USSR entered the war with Nazi Germany. There was virtually no real investigation or trial, in accordance with personal directive by I. Stalin. The only reason for deportation was the ethnicity of Germans. The purpose of this study is to identify the characteristics of the repressive practices of the USSR government regarding the ethnic group Volga Germans, as well as the analysis of everyday life of repressed people in special settlements and concentration camps in the Molotov region. Also, the study analyses memories from the perspective of the trauma phenomenon and the role/ability of victims and witnesses. The chronological framework of the study covers mainly the period of the Second World War and the 1950s, when the liberalization of the regime in the USSR occurs. The study is based on the approaches of historical anthropology, as well as methods of digital humanities and oral history. In addition, important aspects of the study are the consideration of these events, as well as the memories of the repressed adults and children in terms of the theory of historical memory and the phenomenon of historical trauma and secondary historical trauma. The study uses the methods of descriptive statistics, information modelling, and content analysis. In addition, an electronic map is being developed, where German deportation routes, special settlements and concentration camps on the territory of Nyroblag will be visualized. All objects of the electronic map were equipped with a historical information, which is compiled on the basis of archival documents and interviews. In the process of the research, there was one ethnographic expedition to the north, to the old villages, where we managed to collect unique materials, interview the repressed, as well as the children and grandchildren of the repressed Germans. The method of realization of the study comes, first of all, from the idea of synthesizing sources of various origins. Initially, a dictionary with lemmatized word forms is created in several subject groups, all other words are entered into the stop list. Then analyze the statistics on the words count, this option can help to understand the overall context of the problem. Also, it could be helpful for the understanding the difference between victims and witnesss vocabulary that we use for describing of the repressions experience. The information obtained is divided into four main blocks: events, place names, daily life, and political situation. For conducting content analysis, the software MAXQDA-12 is used. The electronic map contains more than 150 objects, which are accompanied by historical information. In the future, the map will be further replenished. However, the main special settlements of the repressed and blocks of concentration camps have already been marked. Special settlements that have disappeared from the surface of the Earth are also partially introduced, they were found thanks to the help of local residents and children of the repressed. Also, the main routes of the repressed movement from the Volga region to the Molotov region are created. For maps creation we used the web-service Carto.com. The final point of the project will be the publication of maps, photo and video content in an open portal created on the basis of WordPress, which will partially solve the problem of commemorative practices in modern Russia about traumatic scenes in the history of the 20th century. Thus, an important step towards public rehabilitation of the repressed the Germans will be done. As mentioned above, self-awareness is still preserved as repressed, not only among those who went through it, but also their children and grandchildren. "
	},
	{
		"id": 164,
		"title": "Analytical Edition Detection In Bibliographic Metadata",
		"authors": [
			"Ijaz, Ali Zeeshan",
			"Roivainen, Hege",
			"Lahti, Leo"
		],
		"body": " The aim of analytical bibliography is to understand books and other printed objects as artefacts, and, most importantly, how they were produced. Systematic methods can unlock patterns otherwise hidden and provide an overall view of the publishing history. Hence, bibliographic metadata can represent important historical trendsas well as resolve long standing issues, such as the ordering of editions and impressions of books. In this paper we present the state of the art analytical approach for determining editions and their ordering. This enhances the applicability of metadata towards bibliographical analysis, and provides a systematic quantitative perspective on early modern publishing. Furthermore, it will be a great aid for projects aiming to do large data set oriented text mining, by providing harmonized data and information on historical developments in book production. Such analysis will be essential for recognizing how books and other textual artifacts have organically evolved over time, and for delivering a broader context for full text mining and interpretation. State-of-the-art Contemporary text mining approaches typically ignore edition level information, or provide very generic solutions that omit many details. Related earlier work includes the Commonplace Cultures project, where large-scale text mining of the ECCO dataset was carried out, though with only the earliest edition information present. Other projects include BookSampo, a semantic portal which draws from FRBRoo ontology. It covers metadata on Finnish fiction literature, however at the work level and does not have complete edition information. Additionally, commonly used analysis algorithms such as Latent dirichlet allocation are inherently time agnostic, and although newer approaches such as Topics over Time can include time spans, they are not applicable to the problem of edition detection, due to their focus on topics. Hence these methods may not be able to contextualize historical developments in book printing and publishing in a chronological fashion. Effectively, these projects are limited in their scope by providing a simplistic and static view into the nature of book production. Data We have demonstrated these ideas based on The English Short Title Catalogue, which provides a wealth of knowledge with regard to the books, their publication and editions. However, it follows the Machine Readable Catalogingstandard, which is unsuitable for research in its raw form. This is a common characteristic of the metadata in general. To overcome this limitation, we have developed dedicated and semi-automated harmonization techniques that convert free-form textual information into more coherent and consistent entries that are readily amenable for statistical analysis. This required robust handling of differences in title texts, spellings, and more, hence going beyond simple textual comparisons. The harmonization approach Developed in the popular statistical environment R, our approach begins with cleaning up edition information present in the edition statement field, MARC Field 250, provided by ESTC. Unfortunately, this information is unavailable for the majority of entities and hence the publication date, MARC Field 260, is used to provide a starting point for editions. Next, a new work field is constructed to collect titles into a small collection or work, by using title uniform, which is MARC Field 240, or a cleaned up title of the book. Considering the variety of titles in the ESTC, the spelling variations, and different styles of writing, we developed an algorithm to handle these issues. It iteratively builds up a title for the collection using the initial work field and collects similar books into the collection. The algorithm performs sub string matching using a variety of methods such as grep, fuzzy or exact matching, on a word by word basis. Alongside manual corrections, the algorithm allows for gaps in the matching and uses a coverage metric to determine if two titles are similar. The benefit of this approach is then realized as similar titles are grouped together despite variations in word spellings, title length and more. At the moment, the algorithm is being worked on to improve its applicability across different genres. In between automatic and manual The project has been supported by manual checking and corrections. While we are looking at the whole of ESTC, our priorities are focused on published books. Considering the scale of the ESTC dataset, with the number of documents going beyond 460,000, a smaller sample of the works of 7 authors were selected, keeping in mind the diversity present in ESTC as well as the popularity of these authors. The list of authors consisted of William Shakespeare, David Hume, Jonathan Swift, John Locke, Isaac Watts, Alexander Pope and Daniel Defoe. The dataset sample was then used for development of the cleanup techniques and algorithms. Finally, the harmonized entries were manually checked to determine the corrections needed for the cleanup. Different techniques may be required for different genres. Such as the case with works concerning poetry or religious sermons, which contrast significantly with the works consisting of popular books. The issue is further complicated by the fact that different spellings have been used for the same words, titles have been written in different manner across different genres, and therefore may have no universal clear pattern. Validation Validation of the algorithms is essential in order to assess the performance and the ability to generate correct data. In the context of this work, it is imperative that such algorithmic techniques are robust, so as to develop a correct view of the underlying data and obtain reliable conclusions. Therefore, in the interest of reproducibility, a gold standard is developed for the purpose of validating the harmonization process. We sampled a total of 250 authors, each with 5 to 50 publications with unique titles in the ESTC. Then we manually evaluated these samples to determine those titles, which should be grouped together as the same work based on the non-material content of the entries. Each of the records are considered as a distinct impression or edition. This way we did not have to bother ourselves with publication years or page counts signifying reprints: the chronological ordering of the entries computationally is straightforward once the correct grouping is known. Construction of the gold standard for validation We created several edition layers for the gold standard. The first one was a simple straight-forward layer, with all the works with exactly the same content but with spelling mistakes or obvious occasional word replacements in the title defined as a single work. The second layer combined the first print with new revised editions and those with added content as a singular work. Also we connected the multi-volume works with each volume annotated separately together with the works containing all the volumes in the same entry. On the third layer we regarded the same content with differing time periods as one work. This type of layer was required for including calendars designed and marketed for a specific area as the same work, as well as music performance handouts for different dates. We also added yet another level signifying that the work is a collection of other works. Seeking out collections allows to research which part of an authors curriculum actually was revered at a given time. Additionally, we made a rudimentary classification of the genre, so that automating exclusion from subset based on the record type would be possible, even if the genre field had not been annotated for the record. For example, formally structured documents, such as proclamations, court case reports, meeting minutes or dictionaries, in subsets designed for word embeddings would skew the outcome. Conclusion Our overarching aim is to provide a transparent and reproducible ecosystem of bibliographic data and algorithms that can be used to harmonize and analyze various library catalogues. This project complements the overall analysis by investigating the harmonization of the edition field, and by providing the first harmonized version of the data. A quantitative perspective on early modern publishing would be greatly improved by combining the edition level information with publisher data. Accurate description of the publishing network and the various changes it had undergone in the eighteenth century would then become available. Combining the edition level information from ESTC with text mining of large datasets such as ECCO would provide a finer description of what was the first edition of a book as well as the subsequent changes between it and the later editions. This would be supplemented by text reuse approaches, enabling a more detailed account of the evolution of the written text during the early modern period and hence can serve as the foundation for more descriptive analysis. "
	},
	{
		"id": 165,
		"title": "And The First One Now Will Later Be Last, For The Times They Are A-changin': Modeling Land Communication In Roman Crete",
		"authors": [
			"Ilvanidou, Maria"
		],
		"body": " The present contribution has a twofold aim: on the one hand it will seek to demonstrate how the use of digital tools and methods enabled the reconstruction of the road network in Crete, Greece during the Roman period, while on the other hand it will showcase how the rapid developments in digital tools often deems research in the field of the Humanities outdated or obsolete. Back in 2005, when I first started working on digitally modeling land communication in Roman Crete, the puzzle I was trying to put together was looking for the bits and pieces of relevant and useful information within a variety of diverse and scattered sources: ancient written sources, archaeological evidence, historical maps, travel literatureand topography. Material heterogeneity and diversity of information and data collected called for sophisticated modeling; that is, an information schema that would capture and document any determinant detail, and, when implemented, be able to facilitate correlation and answer my research questions: which settlement typeswere connected through the road network, what were the distances between them, how long it took to travel and by what means of transportation, which route was followed by which traveler and for what purpose, do the routes mentioned by different travelers change across time, how accurate or credible is the information provided by ancient sources with regard to distances between settlements, what parameters affected the course of the route and the planning of the road, to what extend Cretan topography determined the route direction, what were the local topographic characteristics that affected transportation on the island, and can we reconstruct the original trace of the Roman road network with the use of digital tools and methods even though the archaeological evidence is scarce and fragmentary? The other challenge was to combine the use of Geographical Information Systems in order to restore visually the spatial data and exploit the GIS functionality to check and assess parameters that affected the planning of the road network in question. The digital tools that made this endeavor feasible were two. The first one was ArcGIS, a commercial GIS software still largely in use, for the spatial analysis of geographic data. The selection of the second one, the tool that would enable me to manage, store and correlate all historical and archaeological data, has proved more challenging and changed over the years. From MS Access, for the implementation of a relational database, back in 2005, to BetaCMS, an open source web-based content management platform, which used XML schema definitions to represent content, in 2010. The BetaCMS, later called Astroboa, was developed by a Greek IT company, Beta Concept, and allowed fast and easy modeling, storing and querying of all data, thus providing what it seemed to be, at the time, a suitable alternative to switch into. The alternate and combined use of both systemsallowed for a long and intriguing iterative process from which a network of optimal paths emerged as a result. This enabled me to propose a reconstruction of the original trace of the public road network connecting the major cities and settlements of Roman Crete, and in specific cases test it against field trip data, with very promising results. Such an initiative as the integration, connection and modeling of complex data on Roman road networks in the digital domain was indeed quite innovative in 2005. Had this venture been undertaken manually, i.e. without employing any digital methods or tools, it would undoubtedly have taken longer and it would most probably not been as accurate. Correlating the data so that they become meaningful and usable for my analysis and making more realistic calculations over geographic space, could not have been performed as efficiently relying on analogue methods alone. However, an analogue approach would still have been up to date and re-usable, unlike my 2005 and 2010 end-results. Sustainability of my Roman roads modeling project has proven to be a great challenge, as BetaCMS is not supported any longer, while ArcGIS is not an open source tool. Therefore, one could argue that, what the digital so generously offered my work, it has taken it back rather fiercely. In this short presentation I will be going through my methodology and results of the Roman Crete land communication modeling and will be raising questions as to their usefulness, curation, re-usability and sustainability as well as the implications to Humanities research, also looking into the prospect of the employment of current innovative digital methods and tools that are available openly. While there is a considerable number of studies regarding digital preservation strategies and planning on an institutional level, my intention is to address those very issues from the individual scholars perspective. "
	},
	{
		"id": 166,
		"title": "Early Modern Computer Vision",
		"authors": [
			"Impett, Leonardo Laurence"
		],
		"body": " Computer vision necessarily embodies a scientific theory of vision. Since the early 1980s, this has largely been a neuroscientific theory of human vision, chiefly with reference to David Marrs posthumous Vision. This metaphorical link has been even more explicit since the dominance of Convolutional Neural Networks as a general technique for image-understanding in the 2010s, with commonplace metaphorsbetween artificial and human neurons and the subsequent hierarchical visual processing structure they entail. This paper reports the prototyping of a computer vision which is instead founded on early modern theories of optics, vision and visual perception. In doing so, we create not only an experimental apparatus to investigate those theories; but also a computational lens through which to see the image production of the time. In that sense, it is an attempt to construct a computational version of Michael Baxandalls Period Eye- a lens through which we can escape our habitual ways of seeing images. In Shadows and Enlightenment, three decades after he had introduced the notion of Period Eye in Painting and Experience, Baxandall himself had drawn parallels between early modern theories of vision and computer vision; the possibility of a computer vision based on those early modern theories was implied, though no implementation was attempted. Baxandalls rationale starts from Molyneuxs problem, a 17th century thought-experiment which asks: could a person born blind, and taught only by touch to differentiate between spheres, cubes etc, recognise these objects visually if suddenly given the ability to see? Baxandall se es computer vision as a contemporary equivalent to Molyneuxs thought-experiments: If one is going to draw on modern thinking about vision, it must first be clear that the newly sighted man of Molyneux has been displaced from his central part in the thought experiment. The exemplary figure addressing the cubes and spheres now is often an array of electronic sensors, feeding light measurements into one or another conformation of circuitry controlled by such-and-such a program. The period in which Baxandall was writing Shadows and Enlightenment - the mid 1990s - coincided with a particular crisis in artificial intelligence. The failure of symbolic or rules-based intelligence, also known as GOFAI, led to an interest in alternative approaches: embodied computation, human-machine interaction, machine learning. Baxandall was not only aware of the two competing paradigms of artificial intelligence research in the mid 1990s - he saw them as embodying differing historical theories of vision. In discussing different algorithms that predict 3D shape from 2D shading, he differentiates between a parallel distributed processorwith a learning algorithm, not a serial symbolic system with a processing algorithm... There is no symbolic language, no pre-set procedure except the network structure itself. Baxandall frames the tension between symbolic AI and machine learning as a continuation of Enlightenment debates around the Molyneux problem; going so far as to suggest that, as a solution, neural networks would have pleased Condillac more than Locke. Baxandall then puts his oppositional thought-experiment into practice, in the reading of an early 18th century drawing by Tiepolo. Having discussed the relative mechanics of bottom-up and top-down computer vision, he sees Tiepolos drawings as containing two models of perception in incomplete relation. The central part is scale-free, and might as well be a crumpled paper bag; the head, single hand and single foot hint at a schematic mental mannekin; and neither of these two readings being quite resolved, there results a persisting element of flicker between readings. This project aims to go beyond what Baxandall had hinted at in Shadows; not only to use different computer vision techniques as metaphorical thought-experiments, but to use their technical implementations as experimental apparati. A first attempt at this can be seen in Figure 1 - details from the Tiepolo example are processed by object detection and caption generation, and the peripheral elementsare indeed more immediately schematisable than the scale-free robes. Figure 1. Detected objects and generated captions on Baxandalls Tiepolo example, using convolutional neural networks. The detections are Person: 63%and Kite: 62%. Caption generator from, code available at https://github.com/DeepRNN/image_captioning ; object detector from Huang, code available at https://github.com/tensorflow/models/tree/master/research/object_detection .Taking our historical perspective beyond Locke and into the sixteenth century, we use Giovanni Paolo Lomazzos Temple of Paintingas the scaffold on which to build our Early Modern Computer Vision. An influential text in Italian mannerism, it is both well-ordered, scientifically explicit, and specifically directed towards the visual arts: it intersperses optical theory with practical recommendations for painting. In contrast to Molyneuxs imaginary subject, Lomazzo had practiced as a painter before becoming blind. Figure 2. Giovanni Paolo Lomazzos colour systemLomazzos Temple of Paintingcontains a scientifically and artistically substantial theory of colour; which, according to Barasch, make Lomazzo a turning point in the history of color concepts in the theory of art. The backbone of this theory of colour is a colour scale, ranging between white and black, in a single sequence - as dictated by the Neoplatonic thought of the time. We have no shortage of colour-systems today, but all digital-display systems use three channels. The neoplatonic imposition of a one-dimensional scale through three-dimensional colour-space can therefore be visualised as a single vector path from white to black - Figure 3 shows an example in the HSLdouble-cone. Figure 3. Lomazzos colour system as a path through the Hue-Saturation-Luminance double-cone. Lomazzo specifically urges the painter to avoid stark contrasts in adjacent colours in his sequence: a colour is friendly to one that stands next to it… while it is hostile to a color separated from it by other shades; contradicting Leon Battista Albertis earlier advice in De Picturato set adjacent robes in contrasting colours, to give a greater impression of clarity. Using the path in Figure 3 not only as a visualisation but as an interpolation, Lomazzos harmonic sequence can become a digital colour space in the technical sense: any colour image, including Lomazzos own work, can be translated to points in the Lomazzo colour-space. Colour-harmonic relationships inherent to Lomazzos scale are shown, would be incompatible with current notions of a colour-wheel: red and ultramarine, for instance, are almost adjacent. Figure 4. Giovanni Paolo Lomazzo, Madonna e santi, chiesa di San Marco a Milano. Seen in original colourand interpolated using the 1-D Lomazzo colour system, then rendered back to RGB.. Barasch, M.. Light and color in the Italian Renaissance theory of art. New York: New York University Press. Baxandall, M.. Painting and experience in fifteenth century Italy: a primer in the social history of pictorial style. Oxford: Oxford University Press. Baxandall, M.. Shadows and enlightenment. New Haven, Connecticut: Yale University Press. Huang, J, et al.. Speed/accuracy trade-offs for modern convolutional object detectors. Proceedings of the IEEE conference on computer vision and pattern recognition. New Jersey: IEEE. Kietzmann, T. C., McClure, P., & Kriegeskorte, N.. Deep neural networks in computational neuroscience. bioRxiv, 133504. Marr, D.. Vision: A computational investigation into the human representation and processing of visual information. Cambridge, Massachusetts: MIT Press. Lomazzo, G. P.. Idea del tempio della pittura. Milan: Paolo Gottardo Pontio. Xu, K, et al.. Show, attend and tell: Neural image caption generation with visual attention. Proceedings of the 32nd International Conference on Machine Learning. ICML. "
	},
	{
		"id": 167,
		"title": "Lemmatizing Low-resource, Less-researched Languages: The Linked Open Text Reader and Annotator",
		"authors": [
			"Ionov, Max",
			"Chiarcos, Christian"
		],
		"body": " Background Lemmatization represents the basis for many experiments and analyses in computational philology and corpus linguistics, and although considered to be solved for modern major languages, producing lemmatized text remains challenging for languages for which little or no language resources are currently in existence. In particular, morphologically rich languages benefit greatly from the sparsity reduction achieved if automated pre-processing, annotation or distributional analysesare conducted on lemmata rather than the original word forms. We describe experiments and a tool on lemmatizing languages with insufficient resources for state-of-the-art linguistic, or philological work. We introduce LiOTrea, the Linked Open Text Reader and Annotator: Given a textand one or more dictionaries or lemma lists, LiOTrea uses the dictionaries to suggest lemmataand morphological features for words in the corpus. Several linking/lemmatization strategies are implemented. Their suggestions are ranked and can either be used as a pre-processing step in manual annotation, or in place of manual annotation. Furthermore, novel dictionary entries can be created during annotation. The technological innovation is three-fold:A number of lemmatization/linking strategies being implemented,native support for the Ontolex-lemon vocabularyand lexical resources from the Linguistic Linked Open Data cloud, anda language-independent technology. Ontolex-lemon is a standard of growing importance to the DH community, and tools for creating and publishing such datasets are available, but no tool, to the best of our knowledge, that currently uses this technology for analysing philological text. The technology is applicable to any language, for illustration we use real-world studies on languages from the Caucasus, Eurasia and the Near East, conducted in the context of philological researchand the documentation of endangered languages and cultures. The case studies thus cover two important strands of Digital Humanities. Methods Lemmatization is a sub-task of morphological analysis, with three prime strategies: Rule-based annotation: Building a formal grammar for the language in question using tools such as XFST, Foma, etc. Successfully applied for a myriad of languages with rich morphology. Downside: Requires a rich model of the grammar as basis for formalization. Lookup-basedannotation: Given a set of paradigmas, annotate only their word forms. This techniqueis the basic technique of popular tools in language documentation, ToolBox/FLEx. Downside: Limited coverage. Pattern induction: Learn morphological regularities from data by means of symbolicor machine learning techniques. Downside: Requires large amounts of data. We focus on lookup-based methods: For languages without annotated corpora, pattern induction is not applicable. In language documentation, rule-based annotation is normally not possible because the grammar is superficially understood, only. Instead, annotation is used as a scientific device to learn more about the grammar of the language. We generalize over plain lookup techniques by providing a fuzzy matching between dictionary entries and text words. Words get transliterated to IPA characters and then to a list of feature vectors using the PHOIBLE database. Transliteration is based on models created by language experts. We provide a phonological search with different similarity metrics over these lists identifying the most similar dictionary forms for the word. Ranked lists are presented to the user to be confirmed or overwritten during annotation. We report results for the following lemmatization techniques: WORD: Baseline. DICT: Use a lookup dictionary of word forms for a set of lexemes. If a word form is not found in it, annotate as unseen. DICT+WORD: Use DICT, for unseen words, use WORD. PHON: Given a dictionary, annotate every word with the phonologically most similar lexical entries in the dictionary. DICT+PHON: Use DICT, for unseen words, use PHON. DICT+PHON+WORD: Use DICT+PHON as long as anthreshold is exceeded. Use WORD for unannotated words. We experimented with different thresholds and report the best results. DICT is a simplistic baseline, but it yields acceptable results with a limited amount of annotation time. For PHON, we estimate phonological similarity between the corresponding vectors. For reasons of brevity, we only report results from the best metric. Experiments For reasons of brevity, we report results only for the Eastern Armenian National Corpus http://eanc.net , using two evaluation metrics: generalization, and accuracyas predicted from different amounts of previously annotated data. In terms of generalization, the WORD baseline drastically fails, whereas the other methods converge towards an agreement — but only with a dictionary covering more than 1,000,000 tokens. Combining PHON with WORD provides an analysis which is remarkably robust against overgeneralization. Fig. 2 shows that both PHON-enhanced dictionary lookup routines also outperform the baseline DICT in terms of accuracy. In our talk, we argue that these methods can significantly simplify annotation of corpora and creating lexical resources for minority languages with very little time effort, seamlessly providing access to them. Our main contribution is, however, to provide a tool that integrates these lemmatization strategies into an annotation and text analysis workflow, that can be extended with more advanced techniques. LiOTrea: Linked Open Text Reader and Annotator In order to facilitate linguistic annotation using these methods, we have developed an environment, LiOTrea, that allows a user to display texts and theirannotation, checking and correcting annotations at the same time. It supports using dictionaries represented in RDF in the Ontolex-lemon model as a basis for lemmatization, phonological search lookup functionality against any dictionary, and the enrichment/creation of Ontolex-lemon dictionaries during the annotation process. In our talk, we present this tool, demonstrate its capabilities and discuss scenarios for its use and future extensions. One application includes, for example, the comparison with dictionaries from another language as shown in Fig. 3, where phonological search is applied to detect Georgian loan words in Batsbi text. In addition to current applications, which include Eastern Armenian, Batsbi and Sumerian, future applications will include related varieties, including other North-East Caucasian languages and different historical stages of Armenian. Acknowledgements The research presented in this paper was primarily conducted in the context of the independent research group Linked Open Dictionaries, funded 2015-2020 from the eHumanities programme of the German Federal Ministry of Education and Science. The pilot on Sumerian was conducted in the context of the project Machine Translation and Automated Analysis of Cuneiform Languages, funded 2017-2019 in through the Trans-Atlantic PlatformDigging into Data Challenge by DFG, NEH and SSHRC. "
	},
	{
		"id": 168,
		"title": "Linking the TEI: Approaches, Limitations, Use Cases",
		"authors": [
			"Chiarcos, Christian",
			"Ionov, Max"
		],
		"body": " Background We provide a survey over the main strategies to harmonize and to integrate TEI/XML documents and Linked Open Data resources. As a highly popular community standard, the Text Encoding Initiativeprovides a the most frequently adopted model for the semantic markup of text data in the Digital Humanities. Likewise, applications of Linked Open Data technologies and resources in Digital Humanities are manifold, and where commonly used LOD and RDF technology is employed, the scientific challenges involved are comparable to those in other areas of application. A scientific problem specific to Digital Humanities is, however, how these technologies can be related to the TEI as the current de facto standard for computational philology. While benefits of LOD technologies have long been recognized in the DH community, and lead to the formation of a LOD SIG in 2014, there is no agreement on possible technological bridges between TEI/XML and LOD technology. With this paper, we provide an overview over existing solutions and their characteristics, and contribute to the discussion of the further standardization — and possibly, revision — of these possibilities. We focus on in-line XML, because Web Annotationalready provides a convenient and established W3C standard for establishing LOD as a standoff layer over XML documents. Complementary Intentions: The Text Encoding Initiative and Linguistic Linked Open Data Founded in 1987, the TEI is the authoritative body that develops and maintains an XML-based interchange format for textual data, in particular for the electronic edition of printedpublications. Beyond its historical focus on literary science and linguistics, the current edition of the TEI guidelines, P5, represents a de facto standard for the entire field of Digital Humanities. The TEI defines an XML format that aims to provide a compromise between a formal description of layout elementsand their abstract function: Its markup elements are given interpretable names, but the provided definitions are informative only, not normative, as the TEI standardizes only their form, context and structure using standards such as modular DTDs, Schematron or RelaxNG. For practical applications, the TEI thus takes a document-centric, and text-driven approach: the form, content and structure of the underlying text is preserved, and are enriched by markup elements. Markup elements can be validated with respect to syntactic constraints, but not directly with respect to their semantics. In application to objects of linguistic and philological interest, Linked Open Data and RDF technology have been applied with increasing intensity in the last years – as manifested in the Linguistic Linked Open Datacloud See http://linguistic-lod.org/ for the LLOD diagram; for specifications for lexical data, see https://www.w3.org/2016/05/ontolex/; for annotations, see https://www.w3.org/TR/annotation-vocab. , — but with a clear emphasis on semantics and data structures rather than markup: The original sequential structure of a document can only be preserved in RDF if explicit data structuresare being created. In opposition to that, sequential structure is inherent to and implicit in XML. The focus of XML technologies on structurealso affect its modes of validation: XML technology is capable of validating structure, and semantic information can only be validated in terms of constraints for their occurrence. In opposition to that, LOD technology, on the other hand, is based on knowledge representation by means of ontology languagesand graph technology, and allows to perform inferences, resp. pattern validation over semantic data structures — regardless of their sequential and hierarchical organization. In this regard, LOD and XML can be considered complementary approaches for the digital philologies. Not in the sense of contradiction, but in the sense that they specialize in different aspects and that synergies can be expected from their harmonization. Such a harmonization, however, requires the development of interfaces, and the re-interpretation or the modification of TEI vocabulary elements whose introduction preceded the emergence of Linked Open Data. Three Paths to Go, and How to Choose As it has previously been extended by necessary modules as they were needed, the TEI today provides a very rich vocabulary of markup elements, in the current TEI P5 guidelinescovering 569 XML elements and 231 attributes. The analogy-driven extension of semantics and syntax of markup elementsis a common strategy to counter the unrestricted growth of the TEI vocabulary. However, this is not a recommended practice, as it leads to compatibility issuesand semantic indeterminism. In addition to TEI-native strategies to represent RDF and Linked Data, we thus describe an alternative approach whose first implementation was reported in 2018: The generic extension of TEI with W3C-recommended vocabulary elements to represent RDF in XML attributes. We will give an overview over these three common strategies to formalize LOD references and data within the TEI, we present their original use cases, inherent limitations for each of them, and the implications of these limitations for future use. For reasons of brevity, this is summarized in in bullet points, only: Representing LOD references with TEI attributes, e.g., @ref in the Text Database and Dictionary of Classic MayanEmulating RDF triples with TEI elements, e.g., <relation> in Sharing Ancient WisdomsTEI+RDFa: extending att.global.linking with @about, @property, and @resource, e.g., in the Diachronic Spanish Sonnet CorpusWe analyze these approaches along two main dimensions: TEI P5 compliance and LOD expressivity and discuss implications and advantages of the respective limitations for prospective use cases, with main results as given in the following table: Strategy 1 allows to refer to LOD resources, it does not support typed links. Strategy 2 allows to refer to LOD resources with typed links, but is not compliant with LOD standards or technologies. Strategy 3 allows to publish LOD data directly as TEI, and to provide an LOD view on TEI data within a single data source. In the following table we present different strategies to encode triples in the TEI: In our presentation, we discuss the impact of the different modelling choices in terms of their support by existing infrastructures, off-the-shelf databases and APIs. Acknowledgements We would like to thank the anonymous reviewers for valuable insights and feedback. This abstract is a result of discussions that have been on-going in the TEI community for about a decade already, and to which the first author contributed since 2013. We would like to thank the members of the TEI discussion listfor their input and feedback to our earlier inquiries about the topic. The research presented in this paper was primarily conducted in the context of the independent research group Linked Open Dictionaries, funded 2015-2020 from the eHumanities programme of the German Federal Ministry of Education and Science. "
	},
	{
		"id": 169,
		"title": "Next Generation Research with Europeana: the Humanities and Cultural Heritage in a Digital Perspective.",
		"authors": [
			"Irollo, Alba",
			"Manguinhas, Hugo",
			"Hughes, Lorna",
			"Ross, Seamus",
			"Isaac, Antoine",
			"Freire, Nuno",
			"Hagedoorn, Berber",
			"Romanello, Matteo",
			"Colavizza, Giovanni"
		],
		"body": " Cultural Heritage represents a source for research in all the fields of the Humanities. Launched 10 years ago and funded by the European Union, Europeana is an unmatched example of a digital platform aimed at the cultural sector, not only in terms of partnerships reach but also of principles of sharing underpinning its creation. The platform currently gives access to 58+ millions of textual and audiovisual sources, provided by 3,700+ institutions: primarily, libraries, museums and archives. It also keeps growing thanks to projects selected on an annual basis under the European Commissions Connecting Europe Facility programme, which funds initiatives that can contribute to the creation of a European ecosystem of interoperable and interconnected digital services. In the past few years, the Europeana Foundation, which manages the platform, has strengthened its position in research. Supported by a research advisory board composed of academics with expertise in the digital humanities, it has developed the Europeana Research initiative. Digital humanists represent Europeana Researchs natural interlocutors, but at the same time, it aims to reach the humanists who are still quite traditional in their attitudes and might become the digital humanists of tomorrow. As part of its mission, Europeana Research facilitates them in their path towards new methodological approaches and research perspectives in regards to the use of digital cultural heritage, also by making available funding opportunities. The workshop will show the potential of the Europeana Foundation as a partner in research projects that bring together researchers and heritage professionals with a specific interest in research, and/or in initiatives aimed at improving the advanced digital literacy among the researchers in the humanities. To them, the foundation does not offer only a huge amount of digital sources, but also digital tools in the form of APIs. The workshop speakers will address the DH 2019 Conferences topic - Complexities - reflecting on the positive effects, but also the difficulties, generated by a dimension of research where the role of researchers and the one of IT-developers are both essential, but the main issue remains translating the language of the former into the language of the latter, and vice-versa, when using cultural heritage for their work. The workshop will be articulated in two sessions: a panel discussion and technical session. Panel Discussion Speakers Dr Giovanni Colavizza, University of Amsterdam Dr Berber Hagedoorn, University of Groningen Professor Lorna Hughes, University of Glasgow Professor Seamus Ross, University of Toronto Moderator Dr Alba Irollo, Europeana Foundation The panel speakers will discuss with the audience best practices and case studies related to the Europeana thematic collections that better lend themselves for extensive use in research, namely, the 1914-1918 Collection, the Manuscripts Collection, and the most recently set up Newspapers Collection. These collections have been already the focus of research funded by the Europeana Foundation. Berber Hagedoorn and Giovanni Colavizza will present the results of two projects supported in the Europeana Research Grants Programmes framework. As a researcher in Media and Cultural Studies, Hagedoorn will give an account of her experience in leading research based on data science techniques, specifically, topic modelling and sentiment analysis. Her project aims to develop models suitable for exploring digital collections creative reuse and takes the 1914-1918 Collection as a case study. Colavizza will talk more about the second project under the lead of Matteo Romanello that aimed at expanding the features of the Venice Scholar , a citation index of literature related to the history of Venice. As a research data scientist, Colavizza will explain how data processing, machine learning and data analysis have enabled the link between the Venice Scholar and the Europeana platform in order to enrich the citation index with sources that Europeana continuously makes available. Finally, Lorna Hughes and Seamus Ross will offer an overview of digital collections reuse cases and best practices, not only from their perspective of scholars but also from the broader perspective of the Europeana Research Advisory Board, of which they are members. In particular, they will discuss the new research perspectives opened up by the significant amount of sources in the Newspapers Collection, as well as the project and funding opportunities offered by Europeana Research. Supporting Links Europeana Portal Europeana 1914-1918 Collection Europeana Manuscripts Collection Europeana Newspapers Collection Technical Session Speakers Dr Nuno Freire, Europeana Foundation Hugo Manguinhas, Europeana Foundation Europeana offers a set of APIs - the Europeana REST API - with the goal to provide a service calibrated to the needs of professionals interested in the reuse of digitised or digital-born cultural heritage in education, research and the creative industries. They are all free of charge and require a simple registration. The set includes now the following services: Search API, Record API, Entity API, IIIF API, OAI-PMH and SPARQL services. Next in development is the Annotations API, seen as a high-potential service for research audiences. The workshops second session will be organised as a technical session, where Europeana staff members will introduce the Europeana APIs and will invite the audience to discuss their needs for carrying on their research projects, particularly making use of the Newspapers Collection. This session will be structured as follows: Part I: General introduction of the Europeana APIs On this part, Hugo Manguinhas and Nuno Freire - who are respectively the Europeana Product Manager API and the Europeana Senior Data Specialist - will introduce the range of APIs that make up the Europeana offer and will explain the model behind them, the Europeana Data Model. In addition, Manguinhas will make a brief tutorial on the Search and Record API taking Newspapers items as the main exploration use case. Part II: APIs related to historical Newspapers Behind the Europeana Newspapers Collection is a set of APIs that apply IIIF as their core technology. This part will walk the audience through the APIs and IIIF, explaining what data is available and how it is structured with a primary focus on the full-text associated with historical Newspapers. Manguinhas will also explain how large amounts of data can be accessed using the OAI-PMH service or downloaded directly as dumps. Part III: Open discussion and feedback We will end by asking the audience for feedback, including on how the Europeana APIs could be of use to the Research community. Supporting Links and Documents Europeana Newspapers Collection Reference documentation on the Europeana APIs Reference documentation on Europeana IIIF / Newspaper APIs General EDM documentation page EDM profile for full-textParticipants are invited to bring their laptops. "
	},
	{
		"id": 170,
		"title": "Tension Analysis in Survivor Interviews: A Computational Approach",
		"authors": [
			"Islam, Jumayel",
			"Xiao, Lu",
			"Mercer, Robert E.",
			"High, Steven"
		],
		"body": " Oral history plays a significant role for historians to understand the experience shared by the people from their past. One of the crucial benefits of oral history interviews, such as survivor interviews, is that it can shed light on important issues that might not have been present in previous historical narratives. Such interviews involve complex social interactions and different factors highly influence the interview situation such as complexity of human lives, age, intelligence, personal quality, etc.. Both the interviewer and the interviewee contribute to these components during this dialogical process. In survivor interviews about holocaust, an interview goal is to better understand the interviewees related experiences. While such experiences are often associated with negative emotions such as fear, anger, or sadness and the interviewee is not willing to recall that memory and talk about it, the interviewer often needs to get to that and have the interviewee engage in this reflection. Tension can thus occur, as a result of conflict of interest or uneasiness. Researchers study these moments to gain an understanding of the conversational dynamics in these interview processes. For example, tension can be shown as reticence in the interview. Laymanstudied how reticence can cause conversational shifts by interviewees therefore putting a constraint on the responses the interviewees will offer to the interviewer. Laymandiscussed the importance of being aware of these situations so that the interviewer can better judge whether to press the interviewee. In this work, we aim to identify the tensions in transcribed and translated interview transcripts of Rwandan genocide survivors. To approach this, we explored a list of potential indicators of tension including the use of hedging and boosting in the language, the sign of reticence, and emotion of the interviewees from their interview responses. Hedging refers to the technique of adding fuzziness to the propositional content by a speaker. People are known to use hedging to deal with controversial issues in conversations. For example, the use of I think…, Well, … in interviews give interviewees the authority to shape their narratives. Phrases such as In other words, In my understanding can also be used to shift a topic either completely or partially. It can be used as a filler or delaying tactic. Boosting, using terms such as absolutely, clearly and obviously, is a communicative strategy for expressing firm commitment to statements. It restricts the negotiating space available to the hearer. It plays a vital role in creating conversational solidarityand in constructing an authoritative persona in interviews. Interestingly, if booster words are preceded by negated words such as, it can act as hedging. We identified lists of boosting and hedging words and phrases in this study to help us identify the tensions. We also developed a hedging detection tool to automatically identify the occurrences of hedging in the interview transcripts and achieved accuracy of 85.4% and F1-score of 81.9% for hedging sentences. Tension can be defined as an emotion of physical and psychological strain joined by discomfort, unease, and pressure to look for alleviation via talking or acting. To analyze whether and how the interviewees emotional aspect indicates the tension during the interview, we developed an emotion recognition tool to recognize the interviewees emotions from the interview transcript. Emotion recognition in computational linguistics is the process of identifying discrete emotion expressed by humans in text. Emotions lead directly to the past and bring the past somatically and vividly into the present. Leveraging the high performance of deep learning compared to other machine learning approaches, we used a multi-channel convolutional neural networkmodel to recognize the emotions from the transcript. We achieved relatively high F1-scores in the emotion categories, compared to previous studies. For example, we achieved F1-scores of 72.6%, 73.6%, 76.8% and 46.0% for happiness, sadness, anger and fear, respectively. We also considered prosodic cues such as laugh and silence as signs of reticence, but also acknowledge that further exploration is needed to interpret these cues. For example, while laughter may indicate invitations for the next question sometime, it may also represent hesitation or nervous deflection, i.e., the tension. We explored the traces of tension in the situations when an interviewee gives unusually long or short answers for a specific question type, shorter or longer than three standard deviation of the average length of responses of that question type. Our algorithm of identifying the tensions in the interview transcripts combines all these aforementioned components. To evaluate the performance of this algorithm, oral history researchers first annotated tension points in the interviews, which provides us the ground truth. Then, we applied our algorithm to identify tensions from these interviews. With the two human labelled interviews available to us, the algorithm is able to identify six out of seven tension places in one interview, and three out of four in the other. The recall of the algorithm is thus satisfying. However, the algorithm suffers from low precision and we have 76 false positives in one interview and 56 in the other. On the other hand, given that the two interviews are very longbut there are only a few tension places identified by the researchers, this algorithm may work as a filtering tool in the tension annotation process. First, the algorithm identifies possible tension places, and then the researchers review them to identify the actual tension occurrences. In conclusion, this study is interested in developing computational techniques to analyze where the interviewees tensions can be detected in the interview data in the context of survivor interviews about Rwanda Holocaust. This contributes to a better understanding of where tensions occur and how in the interviews. Such understanding helps researchers in the exploration of dialogical space created by the interviewer and interviewee in these conversational interviews. "
	},
	{
		"id": 171,
		"title": "Comparing Assonance and Consonance for Authorship Attribution",
		"authors": [
			"Ivanov, Lubomir"
		],
		"body": " Introduction Author attribution is the task of identifying the writer of a text of unknown/disputed authorship. Automated attribution is based on selecting a set of stylistic features, which capture the intuitive notion of an authors style. The frequencies of use of these features in known works are used to train machine learning classifiers, which can be applied to recognizing the style of a document of unknown or disputed authorship. For an overview of the field, the reader is referred to. Recently, there has been interest in stylistic features based on prosody. Lexical stress for attribution was studied in. In, the role of alliteration for attribution was investigated. In this paper, we compare the usefulness for attribution of two other prosodic features – assonance and consonance. We present results from several machine learning experiments, based on extracted assonance and consonance patterns from a historical corpus of 18th Century works, the popular Reuters corpus, and two small poetry corpora. Assonance and consonance Definitions Assonance is defined as the use of a repeated vowel or diphthong sound in nearby words. Consonance is the use of a repeatedconsonant soundin nearby words. Examples of assonance and consonance abound in literature: Blakes Tyger, Tyger burning bright in the forest of the night illustrates the use of the assonant ai sound. The sentence I have a craving for scrambled eggs and marble rye toast illustrates the use of multiple consonant blocks. There has been little work on assonance and consonance except for a paperon rhyme identification in hip hop. A few earlier worksalso consider rhyme identification. To the best of our knowledge, neither assonance nor consonance have been used for authorship attribution prior to this study. Extracting assonance and consonance We developed algorithms for extracting assonance and consonance sequences from text. Both algorithms use a modified version of the Carnegie Mellon Universitypronunciation dictionary, augmented with word-pronunciation pairs from our historical corpus. The assonance algorithm takes as input a text and several user-specified parameters: The maximum inter-vowel distanceparameter defines how far apart assonant vowels/diphthongs can be. The second parameter specifies the scope for the assonance search: within sentences or within paragraphs. The third parameter indicates whether only primary stressed vowels or any vowels should be considered. The fourth parameter indicates whether only the longest or the two longest assonance patterns per block should be used. For each text block, the algorithm determines thelongest assonance sequence, and labels them with the vowel/diphthong they represent plus a short, medium, long, or very-long tag, e.g. AE_vl. The labelare entered into a map, which tracks the number of times different sequences appear in the text. The consonance algorithm accepts as input a text and two parameters: the maximum inter-consonance-distanceand the maximum number of consonant-sound/frequency patterns to be output. The algorithm considers all possible combinations of nearby consonant sounds and complexes. For example, in the phrase extremely strong, the algorithm keeps track of the str complex, the st and tr sub-complexes, and the individual s, t, and r consonant sounds. Both deliberate and accidental use of consonance is taken into account. Using the modified CMU dictionary, the algorithm converts a text into a string of syllables, and processes them using a table, which keeps track of the consonant sounds, their count, and their ICDs. The consonant sound patterns and their frequencies are output at the end. When all corpus files have been processed, separate programs create training/testing vectors, and write them to ARFF files for the WEKA data mining software. Experiments with the 18 th Century Historical Corpus Our historical corpus consists of 224 documents authored by 38 American and British 18 th Century political figures. The number of documents per author varies between 2 and 21, and the size of the documents varies between 959 and 19101 words. Baseline experiments The baseline experiments were conducted using JGAAP. We used all 38 and subsets of 15, 10, and 7 authors. Our set of stylistic features is described in Table 1. The classification was performed with WEKA support vector machines with sequential minimal optimizationand multilayer perceptrons. Table 1: Baseline accuraciesAssonance and consonance experiments All assonance experiments and most consonance experiments used leave-one-outvalidation. Only the 38-author consonance experiments used 10-fold validation. For assonance, we experimented with all combinations of the following parameters: a max-IVD of 5, 10, and 15, sentence and paragraph search boundaries, the longest- and the two-longest assonance sequences per block, and the all-voweloption. For consonance, we used max-ICD of 5, 10, 15, and 25, outputting the top 100 consonant-sound/frequency pairs. In all experiments, we used MLP and SMO classifiers. The maximum accuracies achieved are presented in Table 2: Table 2: Consonance vs assonance accuraciesIn the assonance experiments, for larger author sets, the use of the two longest assonance-sequences-per-block produced the strongest results. The results in the 7-author experiments were obtained using the longest-sequence-per-block only. With two exceptions, the maximum accuracy was achieved using a max-IVD of 15. The sentence boundary consistently produced higher accuracy compared to the paragraph boundary. In terms of classification methodology, SMOs outperformed MLPs in two-thirds of the experiments. For consonance , MLPs routinely outperformed SMOs, yielding stronger results in all but one experiments. The optimal max-ICD was 10 in all consonance experiments. Larger max-ICD values led to rapidly deteriorating results. Experiments with the Reuters corpus The Reuters Corpus v.1is an expansion of the popular Reuters-21578 corpus for text categorization. We selected a random 20-author subset of RCV1 for our second set of experiments. Each author had 50 texts with an average length of about 700 words. Baseline experiments The baseline results are listed in Table 3: Table 3: Baseline accuraciesAssonance and consonance experiments We repeated all experiments using the RCV1 corpus. The assonance and consonance parameters were varied as described in section 2.3.2. The maximum accuracies obtained are presented in Table 4. Both assonance and consonance performed at least as well as the baseline averages, outperforming most traditional features when the number of authors was relatively small. In all except one case, the maximum consonance accuracy was obtained with a max-ICD of 10. Assonance results were somewhat surprising: As in the case of the historical corpus, the highest accuracy was obtained using a sentence boundary, However, the max-IVD parameter did not affect the accuracy - identical results were obtained using an IVD of 5, 10, and 15. We suspect that this is due to the equal per-author distribution of documents and their sizes in the Reuters corpus. Interestingly, Random Forestclassifiers outperformed both SMOs and MLPs in all experiments. Table 4: Consonance vs assonance accuraciesExperiments with poetry We performed a third set of experiments with two small poetry corpora - 18 th Century American poetry, and a mixed 19 th-20 th Century poetry. All parameters of the assonance and consonance algorithms were as in the previous experiments. Table 5 present the assonance/consonance maximum accuracies. The baseline accuracies were 39.48% for both corpora. Table 5: Consonance vs assonance accuraciesOnce again, consonance and assonance performed at or above the baseline average. If fact, assonance may have a slight edge in performance in poetry experiments. However, we refrain from drawing conclusions since the corpora are too small to have any statistical validity. We are currently constructing a larger poetry corpus using the resources of. Conclusion and future work We presented an experimental study of the effectiveness of the assonance and consonance as stylistic features for authorship attribution. Both features exhibit a good performance for smaller author sets. Consonance appears to work better for attributing historical texts, while assonance may perform better on poetry. The performance of both features is affected by the predisposition of authors to using prosody: Certain author stylesare readily recognizable through the use of consonance and assonance, while other authors yield weak results. The most promising use of assonance and consonance is in ensemble classifiers, which use traditional features to carry out the initial classification, and prosodic features to fine-tune the attribution hypothesis. "
	},
	{
		"id": 172,
		"title": "ADHO SIG for Digital Humanities Pedagogy and Training Mini-conference and Meeting: \"The Complexities of Teaching DH in Global Contexts\"",
		"authors": [
			"Jakacki, Diane Katherine",
			"Siemens, Ray",
			"Croxall, Brian",
			"Faull, Katherine",
			"Scholger, Walter"
		],
		"body": " The ADHO Special Interest Group for Digital Humanities Pedagogy and Training offers a special mini-conference that builds upon the DH2019 conference theme, Complexities, we are eager to foster a discussion about the many ways in which DH is taught and learned in academic contexts across languages, borders, cultures, and academic structures. This half-day conference will consist of presentation and breakout sessions. The presentation format will focus on lightning talks of 5-8 minutes in length. Proposals are welcome on any topic informing or treating Digital Humanities Pedagogy and Training, includingthose focusing on DH training in an international context, i.e. how we articulate/coordinate/collaborate across international boundaries, and what we can learn from our differences and what do we share; how does teaching DH in a global context reveal the complexities of intercultural communication and pedagogy; developing a multilingual lexicon for teaching DH. During the mini-conference we will also break into groups to identify opportunities and possibilities for the SIG to engage in during the coming year. A Call for Proposals will be issued separately. Conveners of the Digital Humanities Pedagogy and Training Special Interest Group: Ray Siemens, Diane Jakacki, Katie Faull, Brian Croxall, Walter Scholger"
	},
	{
		"id": 173,
		"title": "Encoding the ‘Floating Gap’: Linking Cultural Memory, Identity, and Complex Place",
		"authors": [
			"Jakacki, Diane Katherine",
			"Faull, Katherine Mary"
		],
		"body": " In his essay on collective and cultural memory, theorist Jan Assmann points out the metonymous nature and mnemonic function of landscape through time. Drawing on Jan Vansimas coining of the term floating gap to describe the slippage between intergenerational collective memory and reified cultural memory Assmann foregrounds the necessarily metonymic nature of memory. One of the major mnemonic devices of both cultural and collective memory is that of landscape and the names that cultures and the agents of their institutions give features within it. In this poster the authors build upon work developed over two years – proposed first at DH2018 and expanded upon at the Linked Pasts IV conference – and present a model for encoding what ethnographers term the floating gap when constructing an historical gazetteer of place names. This step is especially crucial as scholars make intersections and linkages between place-based, data-driven research projects. Initially proposing a modification of the model put forward by Grossner, Janowicz and Kessler for building out digital historical gazetteers, the authors expand their argument that the concepts for Event and Place used to encode semantic relationships overlook the fact that it is the Actor or Agent who names the events, and thus by extension names the places at which those events occurred. As multiple agents participate in, witness, and/or record events, and the multiple perspectives of those agents directly impact upon descriptions of events, then the place names connected with those events must correspond to those multiple agents. Agents perspectives and identifications of place draw irrevocably on their identities and unique authorities. In traditional humanistic terms these multiplicities of relationships of persons to places is the stuff of critical interpretation. In the brave new world of linked data, the vagaries of named places constitute a vexed problem, and attempts to resolve the messiness and fuzziness of place, time, and perspective run the risk of eliding the floating gap of cultural memory. Drawing on the case study of the Moravian Lives project, which contains many place names in colonized spaces in North America, the authors model Shamokin, Pennsylvania, a place described by cultural representatives as diverse as Johann Wolfgang von Goethe, 19th-century American writer, Juliet H. Lewis Campbelland Oneida Sachem Shikellamy. We consider the importance of the agent in an event that occurs within a particular temporo-spatial setting. By including agentwithin this model we thus can deepen the geosemantic approach to place that recognizes that a place may be the setting for many events of significance, that significance being dependent on the view of the naming agent. In a larger sense, we ask how, as digital humanities projects move into a phase where historical place data is linked, can we resist the givenness of authority names? If we apply a metonymic chain of place names rather than subordinated variants can we call to mind the cultural and collective memories that form identities around place? Can we assign such a metonymic naming to place and therefore respect the multiplicity of peoples cultural memories about those places? "
	},
	{
		"id": 174,
		"title": "Who Teaches When We Teach DH?",
		"authors": [
			"Jakacki, Diane Katherine",
			"Croxall, Brian"
		],
		"body": " As the digital humanities have rapidly gained prominence and attention over the last decade, learning has shifted from individual experiences at training environments such as THATCamps and Institutes to more formal institutional instruction. This means that the number of people teaching digital humanitieshas had to increase. Who are these teachers? Where do they teach? Who are they teaching? What support do they have from their institutions? These questions are some that we hope to answer through a survey of those teaching DH—in any capacity. In this poster, we will present the work that we have done to develop a survey of those teaching digital humanities throughout the world. First, we will discuss the development of the survey, including the process of securinginstitutional review board approval and eliciting feedback from the broader DH community. Second, we will outline the methodology we have employed in developing the survey in order to best ascertain how and who these teachers are. Among these will be reports we receive from external reviewers of our survey from colleagues at institutions throughout the world. Third, we will begin in real time the data collection at the conference. At our poster, we plan to provide individuals with devices with which they can take the survey, thereby making this poster presentation truly interactive. We will also have print materials with links to the survey that we will distribute at the poster session and throughout the conference. Insofar as our research team is comprised of individuals from one continent, we are especially excited about the opportunity to reach a global audience in Utrecht so that our data can be as representative as possible. Some of what we hope to glean from the survey includes: Basic demographicsof those teaching DH Information about the environments in which this teaching occurs; for example, whether individuals are teaching in conventional courses, one-off workshops, week-long training institutes Information about teachers institutional homesand their employment status within those institutions Information about teachers own training in DH and how that informs their approaches to teachingInformation about the level of course being taughtand its sophisticationInformation about how instructors think about student participation in research-based learningEven as we write a phrase such as basic demographic information, we must acknowledge that individuals demographics are never basic. But as the world has moved from seeing categories such as gender as non-binary, so too has the concept of teaching within the digital humanities expanded to include many different types of instructors. In particular, we see this as one of the first tangible opportunities to engage in serious dialogue about the complexities of DH pedagogy in interdisciplinary, interinstitutional, and international contexts. Our goal is to reveal the complexities of these teaching experiences and challenge our colleagues to develop best pedagogical practices. Data collection, which will begin in Utrecht, will continue for 6 weeks after the conference, with notifications being sent out via Humanist, DHSI, Digital Library Federation, HASTAC, and other appropriate listservs, as well as social media, including Facebook and Twitter, and direct invitations to individuals the authors know teach digital humanities. The data will then be collected and analyzed prior to publication in a forthcoming volume on digital humanities pedagogy. This survey will certainly not be the first conducted in the field of digital humanities. As such, we will be building on the work of Nowviskie and Porter, who surveyed those caring for end of life DH projects, similarly beginning at the 2010 DH Conference in London; Sula, Hackney, and Cunningham, who examined the range of DH programs, including minors and majors; and Rasmussen, Croxall, and Otis, who conducted oral interviews with those teaching DH in libraries and concluded that a broad survey of DH pedagogues was needed. "
	},
	{
		"id": 175,
		"title": "Introduction to Natural Language Processing for DH Research with SpaCy - A Fast and Accessible Library that Integrates Modern Machine Learning Technology",
		"authors": [
			"Janco, Andrew",
			"Bernstein, Seth",
			"Lassner, David"
		],
		"body": " This half-day tutorial will introduce DH scholars to spaCy, a free and open- source library for text analysis. Developed by Matthew Hannibal and Ines Montari in Berlin, spaCy offers a suite of tools for applied natural language processingthat are fast, practical and allow for quick experimentation and evaluation of language models. These tools make it possible for individual scholars to quickly train models that can infer customized categories in named entity recognition tasks, match phrases, and visualize model performance. While comparable to the Natural Language Toolkit, spaCy offers neural network models, integrated word vectors, dependency parsing and a variety of new features that are not available elsewhere. Participants will learn how to use spaCy for common research tasks in the Digital Humanities and gain an understanding of how spaCy compares with other tools for NLP. We will also work with Prodigy, which is an annotation and active learning tool from the makers of spaCy. Prodigy allows a single researcher to quickly fine-tune a model for greater accuracy on a specific task or to train new categories and entities for recognition. This simple web- application can also be used to crowd source annotations. Target audience and expected number of participants This tutorial would appeal to a similar audience as attended DH2018s distant viewing and word vectors workshops. Given attendance at those workshops, we expect around 25-30 participants would register for our tutorial. Our participants will probably have some existing familiarity with natural language processing, text analysis, and TEI. Participants will leave the tutorial with the skills needed to install dependencies, train a model on categories that are relevant to their research and run inference on a text. They may have heard of spaCy and are interested in learning how it differs from NLTK. Our instructors come from a range of backgrounds in machine learning, the humanities, and information science. This diversity of perspectives and use cases of spaCy will appeal to a large DH audience and help participants to connect what theyve learned with their own research. Each participant should bring a laptop computer to the tutorial. No other technology is required. Links and instructions to install spaCy and Prodigy will be provided in advance of the tutorial, but installation can also be completed in the first few minutes. Brief outline Our tutorial is composed of three 45-minute sessions with ten-minute breaks in between, followed by a 15-minute concluding discussion. In the first session, we will discuss the installation and basic use of spaCy. We will introduce existing models and available tasks for language analysis. These include basic phrase matching, part of speech identificationand named entity extraction. We will demonstrate displacy, which is a highly useful tool to visualize a models results. By the end of the session, participants will be able to load a model and use it to identify basic linguistic features, such as the base form of words, part of speech, syntactic dependency, word shape and stop words. In a small introductory project, participants will use spaCy code in a prepared notebook to generate individualized vocabulary lists for foreign language learning. The script will take a text as input and output a list of lemmata. These lemmata can then be compared against a second text to produce a customized vocabulary list for a reader to consult when reading the second text. The second session will cover more advanced capabilities of spaCy. TEI XML is the de facto standard for digital scholarly editions. In order to make spaCy usable in the context of DH scholarship we present the design principles for conversion and show code examples to load TEI XML into spaCy for two different corpora: The German Text Archiveand the Berlin Intellectuals. In the case of DTA, a basic parser for plain text will be developed during the tutorial with metadata annotation on document level. The resulting spaCy document objects will be used for classification such as authorship attribution. For this classification task, the Convolutional Neural Network of the spaCy TextCategorizer In the case of BI which encompasses genetic encoding, character-level annotation techniques for document variants are presented. The resulting document variants are presented using displacy to offer a highly interactive exploration tool for such genetic editions. Finally, the integration of word vectors in spaCy is presented by neglecting the built-in word vectors and loading pre-trained fasttext vectors. SpaCy itself currentlyonly encompasses seven languages and only word vectors for four of them. To emphasize the pursuit of language diversity within the DH community we show how to load and apply word vectors for 157 different languages into spaCy. The third session will focus on Prodigy, which is an annotation tool used to train and evaluate spaCy language models. Building on knowledge from the second session, participants will learn how to use Prodigy to quickly train new categories and entities on existing language models. Prodigy utilizes a method called active learning in which human input and automated learning are both used to update the model. Prodigy sorts the models uncertain results and strategically asks for user input. These annotations are then used to update the model on new categories or to improve accuracy with a specific task. In this session, participants will learn how to train custom language categories and entities using Prodigy. Participants will leave the session with a clear end-to-end workflow from an initial text, to training, to the application of trained models. Building on Unit II, we will use the new model to automatically update a TEI document with the new categories and data. After the sessions, the instructors will open a group conversation about the strengths and weaknesses of spaCy and Prodigy for DH scholarship based on the experiences in the tutorial. The discussion should allow participants to identify specific use cases for their scholarly work. Session I, Seth Bernstein -email: sfbernstein@gmail.com Seth Bernstein is Assistant Professor of History at the Higher School of Economics. He holds a PhD in history from the University of Toronto. He is the author of Raised under Stalin: Young Communists and the Defense of Socialism. His current project is Return to the Motherland: The Repatriation of Soviet Citizens after World War II. Seths work also uses digital techniques like GIS and massive textual databases to extract and visualize data. Session II, David Lassner -email: lassner@tu-berlin.de David Lassner graduatedin computer science at TU Berlin in 2017, focussing on machine learning with a minor in German literary studies. Mr. David Lassner is now a PhD candidate researching machine learning in the digital humanities at the group of machine learning at TU Berlin, where his main focus is theanalysis of literature. Session III, Andrew Janco -email: ajanco@haverford.edu Andrew Janco is the Digital Scholarship Librarian at Haverford College. He completed his Ph.D. in History at the University of Chicago and MS in Information Science at the University of Illinois. Andy has a passion for inquiry-driven and community-engaged digital projects. He is the lead developer of a digital archive and research application for the Groupo de Apoyo Mutuo, Guatemalas oldest human rights organization. Andy has experience organizing two previous tutorials on spaCy and Prodigy. He works on applied machine learning for research applications in humanities and social science scholarship. "
	},
	{
		"id": 176,
		"title": "Towards Tool Criticism: Complementing Manual with Computational Literary Analyses",
		"authors": [
			"Jander, Melina Leonie"
		],
		"body": " Introduction In recent years, the term tool criticism found its way into the Digital Humanities. Blog posts, journal articles E.g., Underwood: New methods need a new kind of conversation. 28 February 2018. URL: https://tedunderwood.com/2018/02/28/raising-the-standards-for-computation-in-the-humanities/; Van Es: Tool criticism: From digital methods to digital methodology. Datafied Soiety Working Paper Series. 28 May 2018. URL: https://datafiedsociety.wp.hum.uu.nl/tool-criticism/. and workshops E.g., Tool Criticism in the Digital Humanities co-organized by the Centrum Wiskunde & Informatica, the eHumanities group of KNAW and the Amsterdam Data Science Center; DH Benelux workshop on Digital Tool Criticism. discuss the necessity of the deliberated exposure to computational methods. The aim is to understand the potential and limitations as well as the scope of application of the tools, leading to a sharpened awareness of the methodology. Sentiment analysis is one of the most popular methods among humanists expanding their research into the digital field. The technique can be easily implemented with tools developed for scholars without programming skills. While such tools are alluring in their applicability, their performance must be interpreted with caution which, in return, can contribute to developing new standards within the field and beyond. This research provides a case study which illustrates the complementation of manual and automated analyses when the possibilities and limitations of both approaches are considered. Research objectives To establish tool criticism within DH, the project serves as a basis for discussing certain computational tools frequently used for literary research. In more detail, experiments with different sentiment analysis tools are conducted on a self-designed corpus of dystopian novels; the outputs, then, complement the manual investigations of the texts and lead to further experiments. Data The corpus is composed of 102 dystopian novels dating from 1836 to 1979 in the languages American English, British English and German These languages were chosen because the dystopian genre emerged simultaneously in America and England and inspired German authors decades later.. A comprehensive body of secondary literature constitutes the background for deciding which novels were incorporated into the corpus. The categorisation of the works can be unclear, though, because there is no consensus about the texts containing enough distinctive features. Reading into the novels, therefore, was another necessary step in designing the corpus. The most prominent concepts of dystopian novels are a totalitarian regime, an oppressed society, the protagonistrebelling against the authorities and surveillance. The genre was chosen because it carries both distinctive features, e.g. a totalitarian society, as well as debatable characteristics, e.g., the exploitation of extraterrestrial life. This arouses certain assumptions which point the analysis in a specific direction, while also leaving room for findings that have not been prioritised by literary researchers yet. Language Novels Tokens Types Token-type ratio American English 39 3,167,702 136,954 23.1 British English 35 2,660,983 112,012 23.8 German 28 1,872,969 98,479 19.1 TOTAL 102 7,701,654 331,391 4 23.2 Table 1: Overview of the research data. Methodology Manual analysis Studying dystopian fiction qualitatively includes the thorough investigation of both secondary literature as well as other extensive sources like the frequently updated The Encyclopaedia of Science Fiction. The concepts defined within all secondary sources are the background for interpreting the output obtained through the quantitative analyses. Sentiment analysis Dystopian works are characterised as pessimistic narratives, thus, we hypothesise that sentiment analyses will provide empirical evidence for dystopian novels being a negative genre. The Stanford Sentiment Annotatorand the Berlin Affective Word List Reloadedwere used to investigate the research data. Results Stanford Sentiment Annotator Analyses undertaken using the Stanford Sentiment Annotator show the ratio between five classifications of V ery positive, Positive, Neutral, Negative and Very negative sentences in the English part of the corpus due to the tool only correctly identifying English texts. The method works sentence-based: A deep learning model computes the sentiment based on how words compose the meaning of longer phrases which delivers an analysis with an accuracy of 80.7%. Figure 1 and 2 represent the sentiment analyses for the novels written in Americanand British English. The outputs are similar: Slightly more than half of all sentences are defined as Negative, which proves true the classification of dystopian novels as primarily pessimistic. Besides, less than 20% of all sentences are classified as Positive and close to 30% as Neutral, while the percentages for Very positive and Very negative sentences are comparatively low. It is noticeable, though, that the British texts have a tendency of being slightly more positive than the American ones. The rare occurrence of extreme emotions can be explained due to not every utterance carrying a strong sentiment as well as the authors aims of primarily telling a story and not inculcating the readers with strong statements. Berlin Affective Word List – Reloaded The Berlin Affective World List Reloadedis a dictionary of more than 2,900 German words. These words were chosen by Võ and her colleagues based on their representation potential for negative, neutral and positive affective valences. The dataset was then annotated by 200 psychology students. Since the BAWL-R is a word list and not a tool per se, we wrote an algorithm to analyse the German part of the corpus. It scans the texts and searches for the terms the BAWL-R consists of. Then, it analyses the terms in the different categories based on the annotation in the list. For the current research, the values for emotionality, arousal and imageability are of interest. Emotionality is rated between -3and 3, arousal ranges between 1and 5and the imageability is measured on a scale from 1to 7. With an emotionality mean of 0.50, the novels are categorised as rather positive texts, but the dictionarys emotionality mean is still higher. The arousal value of the novels is higher than in the dictionary, which hints at dystopian texts issuing themes the reader feels personally connected with and touched by. It could be assumed that a relatively high arousal value is attended by a rather high imageability value, too. In the case of our German dystopian novels, this assumption holds true partially: The mean value is slightly below the dictionarys imageability mean, but still relatively high. This can be interpreted in the direction of German dystopian novels being written in a relatively vivid manner, so that the reader can imagine the contents well. Outlook Based on the findings presented above, an experiment with test persons will be conducted. This experiment is further grounded on the hypothesis that sentiments depend on a persons cultural and social background. Moreover, the emotionality a text can potentially arouse is never isolated, but it is tightly connected to a recipients personality and emotional state. With the example of Aldous Huxleys dystopian novel Brave New Worldwe can illustrate these hypotheses: A person who values personal freedom is more likely to interpret the novel negatively than a person who enjoys living in a well-structured society that cares for its citizens while simultaneously cutting off their individuality. To prove the aforementioned assumptions, the experiment will be designed as follows: Parts of the novel Brave New World will be annotated by test persons. Through a crowd sourcing platform, people will be reached globally. Like that, it is possible to work with a diverse group of annotators, representing different parts of different societies. The task will be twofold: The probands are asked to assign their sentiments, ranging between Very negative, Negative, Neutral, Positive and Very positive, to each sentence. The sentence-based method enables the reader to take textual context into account. Besides, they are asked to give some demographic information, which covers the aspect of the non-textual context. This information will help us to interpret the correlation between a probands background and his or her annotation. "
	},
	{
		"id": 177,
		"title": "Open Data, Open Edition: How Can the Inferences Between Scientific Papers and Evidence Be Managed?",
		"authors": [
			"Joffres, Adeline",
			"Rodier, Xavier",
			"Baude, Olivier",
			"Pouyllau, Stéphane",
			"Marlet, Olivier",
			"Buard, Pierre-Yves",
			"Parisse, Christophe",
			"Etienne, Carole",
			"Poudat, Céline",
			"Idmhand, Fatiha",
			"Lebarbé, Thomas",
			"Bertrand, Paul",
			"Perreaux, Nicolas",
			"Magnani, Eliana",
			"Laroche, Florent",
			"Granier, Xavier",
			"Chayani, Mehdi",
			"Mounier, Pierre",
			"Fargier, Nathalie"
		],
		"body": " Recent years have been marked by the emergence of the Open Science movement. It encourages, among other practices, open data, open publication, and open edition. Boosted by the digital turn, it has radically transformed research and academic communities and on academic communities. From now on, the EU requires funded scientific papers to be published open access and their full availability ensured in order to be reused by anyone. The EUs deadline is 2020, but many people think that it is unrealistic. Other initiatives, by NASA in the USA for example, decided that the time for conversing about Open Access is now past and that current discussion must focus on how we are going to achieve it in practice. What is the situation concerning SSH? This panel proposes to question the existing scientific publishing paradigm linked to the current changing relationship between the necessary publication of primary data and review papers and the SSH perspective. The challenge is to break with the descriptive model which links raw data and original research articles, and to offer the opportunity to combine data and review papers in order to ensure the link with evidence and to highlight interpretation and reasoning. It is widely recognized that the number of papers currently published in SSH is such that we only consult some of them, following our own selection strategies. However, we still write as if our work were to be read, without any attempt to redraft it from an alternative perspective. Digital publishing does not solve the problem: it makes it worse while, at the same time, the increasing accessibility of online data is not sufficiently exploited in conjunction with publications. Corpora made available for the sharing, interoperability and reuse of data sets are rarely considered in connection with scientific publications. Yet these are the bodies of evidence on which the published analyses and demonstrations are based. A special challenge when consulting publications is to quickly assess their relevance to our work, and to do so, to have easy access to the scientific reasoning and the provenance of the diverse itemson which it is based. This new paradigm raises a new challenge: how to provide papers linked to open access datasets. A spectrum of possibilities is available, ranging from short, scholarly publications that describe datasets accessible online, to full papers that integrate datasets from repositoriesor datasets linked to data papers. By bringing together representatives of various SSH communities and infrastructures, including through the SSHOC H2020 project this panel aims at reflecting on these questions. The introduction will briefly describe the different points presented in such a way that we can see the common points between the disciplines and the way in which the tools proposed can bring communities together around the practice of data-publications. Each talk will try to go in this direction without using too much disciplinary jargon in order to be understandable by all. It will review the specificity of each research community and how they are facing these challenges by focusing on the particular case of the French Infrastructure Huma-Num and its consortia. Experts from two research infrastructures for digital publishing in SSH will also contribute to this reflection. Is it possible to build a standardized model for SSH? How are French infrastructures dealing with scientificpublishing? Are the research communities and their infrastructures capable of working together in order to link and structure their syntheses with their data to highlight the chain of inference to evidence? Talk 1 = Simplifying the Writing of Logicist Publications in Archaeology: An Attempt by the French MASA Consortium through LogicistWriter Within the MASA Consortium, the Digital Document Centre of the Caen MRSH and CITERES-LAT are collaborating on the electronic publication of the Rigny excavationby Elisabeth Zadora-Rio in a logicist format developed by Jean-Claude Gardin. The objective is to highlight the chain of inferences in order to ensure the administration of evidence from the archaeological synthesis. As the archaeological experiment is not reproducible, field records are the main data on which researchers will rely. The publication of archaeological data is therefore essential since this data constitutes the evidence on which the reasoning that led to the synthesis is based. The logicist publishing interface set up by the Digital Document Centre of the MRSH of Caen provides different levels of access to content, allowing both quick reading and in-depth consultation. It is thus possible to visualize all the inference strings in the reasoning structure through synthetic diagrams but also to consult the ArSol database containing the field records that provide evidence for the initial propositions. The XML file containing the whole argumentation structure is based on the entities of CIDOC CRM to ensure the interoperability of this publication within the semantic web. The inference chains are mapped to CRMinfand ArSol records are mapped to CIDOC and its extensions CRMSciand CRMArchaeo. For the implementation of the electronic publication of Rignys archaeological excavations in logicist format, the researcher must inform all his/her logicist proposals in an XML-TEI file. From this XML-TEI source file, the Digital Document Centre of the Caen MRSH has set up a tool to automatically generate the reasoning graph in an SVGformat. However, writing logicism proposals in an XML-TEI file is difficult for the researcher. In order to simplify the writing of archaeological publications that respect the precepts of logicism, the MASA Consortium provides an online tool to assist in logicist formalization, based on a more intuitive process: LogicistWriter. In this application, the researcher begins by writing propositions in a schematic form and graphically linking them together in a graphical interface. The researcher can thus set up the structure of logicist propositions by building the tree structure of his/her reasonings in a visual way, linking the propositions to each other from the initial propositionsto the final propositionsin the form of a graph. A TEI-XML file containing the entire logicist structure of the reasoning is created on the fly from this graph. The XML-TEI file generated can then be expanded in an XML editor to enhance proposals with text, illustrations, bibliography or cross-references. Although the corpora of archaeological data does not allow the experiment to be replayed as in the experimental sciences, logicism highlights the sequence of reasoning and will ultimately make it possible to produce reusable databases of inferences. Talk 2= Validation of scientific results and FAIR principles in linguistic research within the French CORLI consortium Corpus research in linguistics has a long existence but has induced huge changes in the community in the last 30 years. The advent of high-speed Internet connections and large storage possibilities, with the generalization of the use of mathematical models and statistical software, have made it easier to build linguistic research on data attested by language corpora. There is a classical opposition in linguistics between theories based on examplesand theories based on data and corpora. The second approach corresponds to a large body of authors and linguistic models. The use of corpora makes it theoretically possible to control or reproduce the published research, which is not the case in studies based on examples. This control might be possible in linguistic fields that adopt an experimental framework, similar to other experimental sciences such as cognitive psychology for instance. In the field of First Language Acquisition, the use of corpora has been mandatory since the 1970s and researchers rapidly realized the interest of sharing corpora and working hand in hand, due to the high development cost of such corpora. For instance, Pine et al.were able to respond to the work of Schütze and Wexlerbecause the three corpora used by Schütze and Wexler were available on CHILDES. Pine et al.s response was based on a corpus that is also available on CHILDES. The existence and the reusability of the different corpora is a crucial element here to make it possible to replay or enrich a previous work. More importantly, these corpora follow the same well-known guidelinesto ensure interoperability and have been freely available on the CHILDES website for 30 years for the oldest data, and for 15 years for the more recent ones to ensure their accessibility, two key concepts in the CHILDES initiative. These new approaches in linguistics are an important step for the linguistic field as they make it possible to significantly control, replicate, and enrich research. Control of the proof is also something that might be possible today, if not only the corpus but also the methods and the tools are made publicly available. This is an important point we are trying to develop and reflect on within the French consortium in linguistics in CORLI. What are the conditions of such a procedure? How can colleagues be encouraged to deposit their corpora? What anonymization procedures need to be developed in the case of sensitive data? What are the legal frameworks for data? In this paper, we will address these questions, particularly in the framework of Huma-Num and within CORLI in France, or CLARIN in Europe. Indeed, they follow the guidelines of the FAIR principlesto promote and help develop the use of clear and rich metadata, common open formats, accessible tools, open archives and a correct citation of data or tools. Such methods and practices may benefit linguistics research by saving time, encouraging researchers to reuse already existing, documented and multi-level annotated data as well as tools, to increase our knowledge on languages. Talk 3 = Dealing with Literary Scholars Data and Evidence: the Perspective of the French CAHIER Consortium With digital editions of texts, sources and scientific literature, literary scholars are dealing with unprecedented changes, in particular regarding the publication of the data their analyses are based on. To administer their evidence, they have to tackle two major challenges: on the one hand, the publication of all their data and on the other, its storage. The two aspects cannot be separated. What solutions are available for literary scholars? How are they dealing with these challenges? In the French context, part of the digital humanities data storage problem has been solved thanks to the HumaNum infrastructure: HumaNum provides a GITLAB to store software codes and a technical tool, Nakala, to carry out SSH data storage. Nakala also offers a Uniform Resource Identifierthat unambiguously identifies resources. Thus, one might think that literary scholars papers could be based on these URI and gits, and that it would be enough to have more data papers, with more reproducible experiments and new ways of writing papers. However it is not that easy. Thats why CAHIER has organised three actions in order to facilitate: a) data interoperability by sharing modelsand organising data visibility by providing sitemapsand OAI repositories for projects; b) data storage by helping researchers to model their data in an interoperable format from the very beginning of their project; c) data reuse by promoting data deposit and its visibility. For each action, we have to deal with different challenges because of the complexity of the scientific questions raised in literary research that includes a hermeneutical framework. One way of solving the problem might be to provide highly flexible data models and to develop researchers skills. Thats why we are also investing in training courses, but lack of time is the main obstacle for the researchers. Moreover, if we consider that we could have a pool of experts, where are the journals and data journals that would enable them to publish their results? Talk 4= Towards New Forms of Integrated Publications in Medieval History: the Exploratory Research of the COSME Consortium The construction of digital corpora for medieval sources has always been associated with specific scientific methodologies. These are related to particular historical approaches, such as quantitative codicology 25 years ago, the deep search in Medieval Latin texts in recent years or, very recently, studies on medieval spaces based on GIS or, more broadly, geomatics. Currently, corpora of raw data and corpora of qualified data, metadata/referentials are being built and cross-comparisons are being increasingly carried out. Rather than developing tagged textual/documentary corpora, the search by automatic detection and extraction of named entities from repositoriesmakes it possible to envision specific and multiple enrichments of these corpora in an automatic way as well as their historical exploitation. However, these qualified and requalified corpora are source places that must be associated each time with the published results. As part of the development of next-generation open archives, each publication should have its own metadata-/referential datasets, specific, appropriately qualified corpora and intermediate data. The traditional publishing model must therefore be completely overhauled with the implementation of the necessary permanent, open warehouses capable of preserving and visualizing complex content. Several test solutions are envisaged: the development of the TELMA platform for the electronic edition and publication of medieval sources; the creation of an Overlay Journal dedicated to medieval sources and studies on literacy and medieval writing practices; or the connected and joint development of these solutions. The paper will aim to review the development of these solutions, the advantages and disadvantages and the potential proposal of new complementary solutions. Talk 5= How does 3D Influence Scientific Research and Publications in Digital Humanities? 3D technologies have offered researchers in the Humanitiesnew and effective tools to process, analyse and disseminate their scientific data. As a true research tool, 3D enables one to examine and visualize digital data and also facilitates dialogue and exchange between researchers by providing digital models and visual support to test different hypotheses and confront different documentary resources to find solutions to historical questions that traditional methods of investigation cannot solve. It also raises new questions. 3D makes it possible to present information in a more coherent form and to support a demonstration by making it more comprehensible for the observer. Thus, the 3D solution presented in the publication will clarify the scientific purpose of a text or a speech that might be too abstract without this visual transcription. Thanks to their flexibility, 3D models can be used to follow the historical evolution of an object from cultural heritage or more specifically from an archaeological site: such a digital replica can be updated at will and viewed from different angles. With the online publication of these 3D models, researchers have rapid access to information through an organized database synthesizing all the scientific documentation. Moreover, by this digital medium, researchers can continue their investigation without any need to remain on the site. Furthermore, 3D models can also be used as support for an information system by associating a coherent spatial coordinate system in order to offer researchers all the data associated with a humanities object of study. However exploitation of the large amount of 3D data remains difficult for humanities scholars who are not used to manipulating such data. In order to help the Digital Community, resultats of the ReSeed project, currently funded by the French National Agency for Research and associated with an axis of the 3D-SSH consortium will be detailed by the author. It aims at the development of a new technology: a tool and an interoperable format in order to digitize both historical semantic data and 3D physical objects. ReSeed will implement an ethical code assigned to guarantee the authenticity and uniqueness of the future semantically augmented numerical objects. Talk 6= From Papers to Data in the Open Access Context: Use Cases from OpenEdition Platforms OpenEdition is the French national infrastructure for open scholarly communication in the humanities and social sciences. OpenEdition portal brings together four platforms dedicated to open access digital resources in the SSH: OpenEdition Books; Hypotheses; Calenda. With 20 years of experience and more than 500 open access journals and 7000 books in all disciplines of the humanities and the social sciences, OpenEdition platforms are an interesting observation deck from where researchers needs in terms of linking their data to their publications can be observed. My communication will present several cases of academic publications disseminated on OpenEdition platforms from different disciplinesthat reflects the variety of ways authors and editors try to create links with data whereas no specific feature exist on the platforms. It will be also an opportunity to measure how those actors play with the technical constraints imposed by the existing publishing tools to elaborate an extended scientific argumentation that integrate direct access to data. This communication will be the start of a collaborative project with Huma-Num on that topic, both on a national level and with the framework of SSHOC project. Talk 7= Linked Open Data for Heritage Content: an example of implementation within Persée infrastructure Persée is a digital platform for the digitization, structural markup, online publishing and long-term preservation of heritage content. Persée deals mainly with the collections of old, rare and valuable documents housed by libraries and archives in particular, but not exclusively, academic journals, serials, books, proceedings, grey literature, maps and iconography. The originality of the Persée platform is the accuracy and the standardization of item description and structuringand the use of standards. Currently, the Persée portalprovides open access to archives of scholarly publications in French in the humanities and social sciences, with more than 740,000 documents available. The oldest article dates from 1837 and the most recent one from 2017. This set of documents can be considered from a double point of view: on the one hand, they are published research results that constitute a documentary resource which is still relevant for students and researchers; on the other hand, they are a mass of structured and qualified data that researchers can consider as a digital corpus. The presentation will focus on the methods used by Persée to ensure the usability of these datasets outside the digital library framework and their sustainability for digital humanists. Persée provides web services and implements the methods and techniques of Linked Open Data. Data Perséegathers all the metadata produced by Persée and makes them available in a structured wayaccording to the principles of the semantic web. The mapping with international information systems makes it possible to explore and link not only Persée databases but also data offered by the library community, the scientific communityand other crowd-sourced databases. To ensure quality and relevance, we have prioritized human expertise over automated processes. So, all the links are checked. In addition to that, Persée is involved in a long term preservation programme to guarantee the durability of the links and of the identifiers. "
	},
	{
		"id": 178,
		"title": "\"Building community\" at the National and/or International Level in the Context of the Digital Humanities",
		"authors": [
			"Joffres, Adeline",
			"Priddy, Mike",
			"Morselli, Francesca",
			"Idmhand, Fatiha",
			"Lebarbé, Thomas",
			"Abéla, Caroline",
			"Granier, Xavier",
			"Chayani, Mehdi",
			"Bertrand, Paul",
			"Rodier, Xavier",
			"Parisse, Christophe",
			"Poudat, Céline",
			"Ginouvès, Véronique",
			"Melka, Fabrice",
			"Sinatra, Michael",
			"Château-Dutier, Emmanuel",
			"Camlot, Jason",
			"Sinclair, Stéfan",
			"Del Rio Riande, Gimena",
			"Ricaurte, Paula",
			"Galina Russel, Isabel",
			"Barrón Tovar, José Francisco",
			"Priani Saisó, Ernesto",
			"Grandjean, Martin",
			"Berra, Aurélien",
			"Baude, Olivier",
			"Pouyllau, Stephane"
		],
		"body": " Knowledge production has always act globally, and when it comes to the humanities early networks of scholars can still be traced in their letter correspondence. With the emergence of digital humanities more prominently in the 1970s, research communities have organized themselves in many different ways. The enthusiasm generated by the promises of what was sometimes perceived as a new field were to some extent echoed in new forms of institutionalization, to the point of defining a discipline in its own right. But the enthusiasms was also accompanied by a certain resistance of communities reluctant to introduce digital technology into their field. The term of digital humanities in these earlier days of adopting digital methods into the humanities created an area, a niche, inside which pioneers in Digital Humanities could gain critical mass. Today, where digital methods are far more widely applied, one can observe an almost opposite trend, the abandoning of a specific label and a much broader advocacy concerning all humanities. What remains specific for DH communities is the close alliance between content providers, humanities scholars applying digital methods, and computer scientists linking to new methodological achievements in their field. However, this alliance can express itself in very different forms of national and international organisation, and is far from following a specific model. This panel examines different ways of forming a community among digital humanities scholars and scholars in other fields, and other actors in DH. The contributions span a range from generic ways to design digital research infrastructures in the SSH, over national solutions to supranational coordination The purpose of this panel is to unfold the diversity of the current digital humanist movement, not only to compare, but also to understand what is at stake for the actors involved and what impact the different forms of organisation have on creation and evolution of research communities. We further discuss issues of cohesion and durability. Through the papers presented, we will examine the impact of bottom-up, top-down and horizontal strategies as well as the adoption of hybrid solutionsin the design of research communities. This approach will allow us to put convergences and challenges into perspective and to question the re-compositions at work within SSH communities. This panel will highlight the experiences of SSH research communities from different cultures and organizations rooted at different levels of governance, such as some French communities structured around institutional nodes such as Maisons des Sciences de lHomme, or research infrastructures at the nationalor European level; project based collaboration of research infrastructuresand Canada; and professional networks and transnational associations related to digital humanities. The comparison of the experiences presented will not produce a homogeneous and smooth image but will highlight differences in approaches and organisation. Even it seems nearly impossible to give account of every association that could be representative on a way to build community in DH, the chair of the session will make an introduction with a brief summary of this landscape. That said, besides the geographical aspect that we try to include, another is that we are giving voice to formal and informal associations such as the LatamHD network, that is just at an early stage and that is not yet defined in its goals. We decided to propose several solutions to deal with the diversity of needs and practises inside our communities and we wanted to present some of them to share our experiences and initiate discussions during this panel in order to develop collaborations with colleagues sharing the same kind of constraints. Thus, the objective is to have a broad discussion with the audience to broaden the perspectives to other experiences. This panel aims to contribute to the reflective work in the wider DH context about factors of constitution, consolidation and evolution of its research communities . Talk 1 = Architecting the Digital Humanities The Digital Humanities is a broad church of different communities, each with differing methodologies, approaches to data creation, and data processing; developed organically over time. For cross-community communication and interaction there must be a common form in which to describe the communities and their components. Understanding commonalities and differences is key to the successful building of infrastructures, and especially for distributed transnational supra-community research infrastructures. The creation of international e-infrastructures such as the European Research Infrastructure Consortiumsuch as DARIAH-EU and CESSDAto provide services to broad designated communities means that there is the need to comprehend what form topic specific communities take in order to gather and support them in an efficient manner. This paper will present three architectures, from the abstract to the concrete, created in, and for, humanities e-infrastructures. At the most abstract level is the Reference Model for the Social Sciences and Humanities Data Infrastructures, a deceptively simple high-level model which can be used to model everything from a researcher with their laptop to a distributed ERIC. The second architecture, the DARIAH Reference Architecturewas created to systematically and formally describe contributions to this e-infrastructure. As such it can be used to describe contributed activities and services as diverse as summer schools and conferences to resource creation and data hosting. The DARIAH-RA is built upon the foundation of the RM-SSH. The most concrete of the architectures is that of the European Holocaust Research Infrastructurewhich currently describes the metadata processing and ingest of archival descriptions and controlled vocabularies into an aggregating portal, both manually and automatically. This information architectureincludes process workflows to aid understanding for sustainability as members of the community change over time. All three architectures, due to their nature of describing distributed infrastructure systems and communities, use a common model: the Reference Model of Open Distributed Processing. This provides a framework to describe the architecture of open, distributed, processinginfrastructures; whenever possible ODP-RM uses a formal description technique to specify the architecture, in order to guarantee the consistency and reliability of the description. Talk 2 = Growing Communities in the Arts and Humanities. The case study of the DARIAH-EU Working Groups DARIAH-EU is the European research infrastructure for the arts and humanities. In 2016 it was recognised as an ERIC and it comprises 17 Members and several Cooperating Partners in eight non-member countries. The activities of DARIAH comprise four main strands, namely: 1. training and education; 2. resources, tools and methods made available by and for the research community; 3. policy and advocacy support; and finally, 4. a growing transnational community of researchers. This paper will focus on the fourth aspect and aims at exploring the case study of the DARIAH-EU Working Groupsas a model in which research communities organize themselves, given the boundaries and the assets provided by a research infrastructure such as DARIAH. The DARIAH-EU WG are transnational, grass-rooted, self-organized, collaborative groups which have their roots in existing communities of practice. They form the heart of the DARIAH-ERIC community, but at the same time they maintain the existing ties with theinstitutions where the WG members are based. The creation of new DARIAH WGs follows the need of communities to foster innovative scholarly practices and to provide the infrastructure to support them. In turn, participation in existing WGs is a means to consolidate infrastructure and scholarship in certain areas of research, and to create or reinforce the network of expertise inside DARIAH. The WG level enables an organizational structure which is not just flexible and dynamic, but also driven by feedback and as such it helps DARIAH to be sustainable. Furthermore the value of the working groups lies in the fact they allow a better alignment between research institutions functioning on a national basisand the research interests that emerge in international collaborations - the WGs are therefore able to optimize their own research environment by harnessing both national and international horizons. In addition, the work of the WGs is considered so central in the development of the Research Infrastructure that in 2017 DARIAH-EU established a funding scheme to provide financial support for their activities, including travel to WG meetings, core developments such as the creation of tools, policy documents or dissemination material. This paper will therefore examine the European landscape of the DARIAH WGs, firstly by charting their evolution since 2015 and secondly, by identifying those dynamics of the research community that are the basis for successful collaboration, exchange of information and experiences. This presentation also aims to reflect on what the challenges are in the creation and maintenance of such dispersed communities, and therefore it wishes to contribute to a fruitful discussion with other national and international experiences. Talk 3 = Building Community, the Example of French Consortia Labelled by the TGIR Huma-Num in SSH Based on a review of nearly 10 years, this communication aims to reflect on the factors that led to the construction and development of consortia as the fabric of an emerging digital humanities community in France. The invention of a national infrastructure, partly based on consortia in France by the TGIR Huma-Num, was an original way of responding to the difficulties of the human and social sciences community in keeping pace with the rapid development of the digital humanities. Starting from the needs, uses and practices of higher education and research stakeholders, consortia have brought together communities from different scientific fields around common challenges. Thus, in 2010, the TGIR Huma-Num launched a call for the creation of consortia around disciplines and/or objects, materials or research data articulated around a human and technological device supported by services. In five years, 10 consortia have been proposed and validated by the Huma-Num Scientific Council. The ways in which these communities have been built and the forms they have taken vary, depending on the context in which they emerged and the nature of the research communities they represent. For example, the Consortia Archives des Ethnologues and ImaGEO started from the objectives of processing and promoting the data of researchers whose resource centres and libraries are depositaries. These consortia were thus constituted by focusing on the promotion of collections and their scientific and public mobilization, while other consortia, such as CAHIER, focused on the corpus of authors and their editorial enhancement to give rise to new research, while others are oriented towards a technology and the ensembles created from it, such as the 3D consortium. Still others have preferred to open up to a wide range of documentary typologies and very diversified modes of exploitation, trying to bring together large user communities, such as the COSME consortium dedicated to the study of medieval sources, or the CORLI consortium bringing together linguists and their work on corpora. Each of them has its own mode of governance, internal dynamics and distinct inclusion capacities, but this hybridity has proved to be a strength in working together on issues that cut across all consortia, which involve them in a transdisciplinary way. In this respect, the FAIR principles are real assets for ensuring the coherence of projects - in particular on legal and ethical issues, long-term preservation or data feedback to civil society. Dialogue on the digital practices of communities and the transmission of knowledge exchanged or acquired within consortia is now a real challenge at European and international level. New practices that need to be articulated at different levels are emerging, and consortia are the incubators. Talk 4 = Leveraging Digital Humanities Centers: The Case of the Centre de recherche interuniversitaire sur les humanités numériques in Canada As Neil Fraistat powerfully argues in Matthew K. Golds collection of essays Debates in the Digital Humanities, digital humanities centers have become important laboratories for the application of information technology to humanities research; powerful advocates for the significance of such work; crucial focal points for the theorization of the digital humanities as a field; and local nodes for cyberinfrastructure, or e-science. The Centre de recherche interuniversitaire sur les humanités numériqueswas founded in Québec in the fall of 2013 in order to offer a new structure for over sixty researchers from seven universities working on various aspect of digital culture. These researchers come from various disciplinary backgrounds, primarily in the humanities but with some in social sciences, but they all have in common a theoretical and practical knowledge of digital humanities that, put under one roof, allow to take the full measure of the digital turn that is characteristic of our times. With the series of change to forms and models of publications, the way that information is created, shared, and consulted has undergone some fundamental changes in the last two decades. What is thus required is not only a reconceptualization of a theoretical understanding of digital culture but also the implementation of a series of new tools for disseminating information, for finding it through data mining techniques, for long-term preservation, but also to visualize this mass of data, be it textual, sound-based or visual. These tools should be developed by and for humanities scholars, and be at the same time studied for the way in which they transform future research as well. The CRIHN provides a space for engaging these topics on these two levels along the two axes found at the core of the Centers mandate: Theorizing the Digital and Instruments of knowledge. The first axis focuses on a theoretical framework for understanding the goals and major shifts that have occurred in digital culture, and offer conceptual tools for describing these changes, specifically in the context of research dissemination. The second axis of the centre has as its main goal to assist researchers in transforming in a very concrete fashion the way they create, analyze, visualize, and disseminate humanities research. The CRIHN thus allows a group of researchers to make visible our concept of a platform that can combine discoverability tools with organizational and analytical ones, as well as the modern forms of scholarly dissemination which include an actual social network that goes beyond the use of social media to actually create a live community of researchers that are already involved at the individual level in various projects related to the impact of digital culture on scholarly methods for creating, studying, and disseminating research output. Talk 5 = #LatamHD Latin America is much more than one of the largest and most diverse regions in the world; it is a symbolic construction that broadly covers Mexico and the countries of Central America, South America and the Caribbean. Latin Americas historical, sociocultural, geographical, economic, and political heterogeneity also reflects the organization of communities of practice with different realities and needs. The recent history of Digital Humanities --as Humanidades Digitales, HD-- in Latin America reflects the growing institutional interest and the promotion of initiatives aimed at the professionalization of academics and the opening of programs, curricula and spaces. On the one hand, there is interest in promoting collaboration and cooperation in the region but on the other, there is no consensus on how this could become a regional reality, as every country in the region has different institutional organizations and priorities. Much of the discourse has focused on shared problems, such as obsolete infrastructures, the lack of a grant funding system for the Humanities and disparities in digital literacy among students and scholars. It is important however to also consider shared strengths that could allow the development of the field in the region in order to provide both solutions to our problems as well as innovative and unique knowledge within our complex landscape. Over the past years individuals and associations have discussed possible forms of cooperation in order to build bridges and promote collaboration in Latin America HD. In 2018, during the Digital Humanities Conference held in Mexico City, a meeting was held to discuss the creation of a regional network that could integrate the different experiences that are emerging in Latin America and the Caribbean and continued, some months later, at the HD Conference of the Asociación Argentina de Humanidades Digitales in Rosario, Argentina. These meetings were useful to discuss initial ideas not only about the importance of creating a network but also reflections on what this should look like, what characteristics it should have and how this would differ from other DH networks around the world. LatamHD now faces the challenge of defining a shared ethos that can guide the future of HD in the region towards its own character, taking our situated practices and context into account. There is a strong conviction that the HD must assume a fundamental commitment to impact research into cultural objects from a renewed and critical perspective that may not coincide with the methods and practices of Anglophone Digital Humanities. This presentation will be a state of the art of the HD context in Latin America and the Caribbean and will describe LatamHD as a common initiative with some shared next steps and challenges. In particular we will discuss how LatamHD could follow the minka spirit, in the sense of a pre-Columbian tradition of community and voluntary work with social and reciprocal purposes. For example, the minka may have different purposes for a community, such as the construction of public buildings or helping a person or family, when harvesting or doing other agricultural activities, always with a recompense for those who have helped. The benefit of one should be the benefit of all. Talk 6 = When language structures the community: the case of Humanistica Founded in 2014, the Francophone association of digital humanities Humanistica occupies a special position in the landscape of DH communities. However, the international – and even intercontinental – nature of the association is neither a way of distinguishing itself from most of the other national or regional organizations nor the affirmation of the unity and the primacy of a research culture beyond the borders of France, the country most naturally associated with this language. The reason for this broad spectrum is simply that the association has been created as a grassroots movement on the basis of an already existing community that brought together DH actors from a wide variety of French-speaking areas. It is indeed in 2010, during THATCamp Paris, that a Manifeste des Digital humanities was written. This fundamental text for Francophone DH rapidly saw the number of its signatories reach more than 250 individuals and institutions. If this first stage of structuring laid the foundation for a common discourse within this growing community, it is after THATCamp Lausanne, Florenceand especially Paristhat many expressed the wish to see the collaboration strengthened within an association. It is then at THATCamp Saint-Malothat a first provisional committee was elected, before the official founding at DH2014 in Lausanne. Such an obvious link with the philosophy of these un-conferences, whose participants often come from many disciplinary, professional and institutional horizons, explains the very broad opening of Humanistica. Its first committee included representatives from France, Canada, Switzerland, Belgium and Luxemburg, and was not limited to tenured profiles, but also included doctoral students and engineers. What, then, brings this very heterogeneous community together? And what services can an association provide, on a scale that lies between the traditional institutions of the academic world, national or international infrastructure projects, and global actors? In recent years, together with the growing institutionalization of digital humanities in the French-speaking areas, the number of un-conferences has dropped, making the community harder to grasp. Although its absolute number has increased significantly – nearly 1.400 subscribers to a very active mailing list, institutions gradually integrating digital methods into their syllabi, etc. –, the normalization of DH has changed the meaning of the community, formerly strongly united by its minority status. Moreover, the integration of Humanistica within ADHO in 2016, the creation of the Humanités numériques journal and the recent decision to create an annual event indicate a form of maturity as well as the relative decline of forms of scientific production and exchange that were regarded as characteristic a decade ago. "
	},
	{
		"id": 179,
		"title": "Le Dictionnaire topographique. Une API pour les toponymes anciens français",
		"authors": [
			"Jolivet, Vincent",
			"Pilla, Julien"
		],
		"body": " Un projet éditorial de 160 ans Le Dictionnaire topographique de la France est une ressource de premier plan pour les historiens et les toponymistes : il compile pour la France métropolitaine plus de 1 100 000 toponymes anciens attestés, datés et référencés. Entreprise éditoriale au long cours lancée en 1859 par Léopold Delisle, le Dictionnaire topographique a pour mission de compiler tous les toponymes médiévaux et modernes de la France. En raison de lampleur de la tâche, on opta rapidement pour le principe dun volume par département, le tout devant à terme – et en théorie – être unifié par un index général. Les débuts furent prometteurs : 19 dictionnaires parurent entre 1861 et 1884, dus principalement aux archivistes départementaux. Le mouvement se poursuivit à un rythme plus modéré, à raison de deux à quatre dictionnaires par décennie jusquaux années 1920, avant de connaître un net ralentissement avec la publication de seulement 4 volumes depuis les années 1950. Ce sont aujourdhui 35 départements qui sont couverts, représentant plus du tiers du territoire national métropolitain. Du papier au numérique Léditeur, pour revitaliser cette initiative savante, numérise depuis 2009 les différents volumes, en propose une édition numérique, et en distribue les sources XML. Cette restructuration XML de lédition imprimée a été rendue possible par lhomogénéité des conventions éditoriales assez bien respectées par les nombreux contributeurs au fil des décennies. Le corpus des 35 volumes publiés est intégralement disponible depuis 2018. À cette occasion, une nouvelle application de consultation et de partage de la ressource est développée. Celle-ci nest pas une simple édition numérique. Tirant parti du liage des données à plusieurs référentiels dont celui de lINSEE, elle est conçue avec cette idée que la richesse des dictionnaires est loin dêtre épuisée par leur publication papier et quelle doit rendre possible de nouvelles exploitations : lindex unifié et la géolocalisation des entrées permettent dorénavant de cartographier la densité de certains types de toponymessur le territoire, et ce en diachronie. Grâce au moteur de recherche, les toponymistes peuvent étudier les toponymes selon leur suffixe ou bénéficier de la recherche floue, précieuse pour les contextes historiques de forte variation graphique. Il est aussi possible de lister lensemble des toponymes recensés sur une même commune. Lapplication fournit des permaliens pour lidentification de ces toponymes historiques. Un service est en cours dimplémentation pour faciliter leur identification dans les éditions numériques des documents anciens. Surtout, la nouvelle application entend favoriser le partage de la donnée brute de manière à en rendre possible les réutilisations, via une API documentée et un service de recherche. Une JSON:API pour le partage des données géohistoriques Une API de consultation a été définie pour construire lapplication Web de consultation, de recherche et de géolocalisation des données et pour permettre le partage des données en exposant différents services au public. Le choix de la spécification JSON:API améliore linteropérabilité avec des services extérieurs et favorise la composition de documents avec des ressources liées : avec une requête unique, on peut récupérer lensemble des informations associées à un lieu ancien, tels que son type, ses coordonnées ou les toponymes liés. Limplémentation de la spécification sabstrait du modèle de données sous-jacent qui sen trouve plus facilement modifiable. Enfin, les fonctionnalités de recherche, également exposées via lAPI, sont accessibles à des services extérieurs. Lobjectif de cette API est de favoriser les remplois de cette base de connaissance importante, mais aussi den poursuivre lenrichissement en offrant aux chercheurs une interface pour corriger et enrichir le contenu au gré de leurs découvertes. Les éditeurs scientifiques ont lhabitude didentifier les toponymes anciens dans lapparat critique de leurs éditions. À défaut dêtre accessible et standardisé ce travail de grande valeur est perdu : lapplication doit permettre aux érudits denregistrer et de partager leur travail et ce faisant de contribuer à revitaliser lentreprise éditoriale du Dictionnaire topographique. Le Poster vise à faire connaître cette ressource essentielle. Nous détaillerons les étapes de la constitution de la donnéeet présenterons létat dune réflexion sur le partage et le liage des données géohistoriques, les méthodes et développements associés. Nous souhaitons pouvoir échanger avec les participants de la session sur leurs besoins et pratiquesafin dévaluer la pertinence des choix dimplémentation, si la définition et la documentation de lAPI sont conformes aux attentes des collègues qui souhaiteront développer des applications tierces. "
	},
	{
		"id": 180,
		"title": "Recreating Dante’s Commedia in VR: The Intersection between Virtual Reality and Literature",
		"authors": [
			"Jones, Nicole Madeleine Adair"
		],
		"body": " Since the development of increasingly sophisticated virtual reality technologies, the medium of virtual reality has boomed in recent years, with video, gaming, filmmaking, journalism, and marketing companies exploiting VR for its immersive and interactive potential. At the same time, academic scholars have also begun to see the power and potential of archeology VR, anthropology VR, art history VR, etc. One recent sub-field is literary VR, the adaptation or recreation of a textual work in an interactive, immersive, and three-dimensional space. Examples of literary VR include the NYTimes 360 Video representation of George Saunders Lincoln in the Bardo, and Joycestick, the Boston College gamification of Joyces Ulysses. My own project, and the topic of my poster presentation, explores the potentiality and the stakes of recreating Dante Alighieris Commedia in virtual reality. While literary VR as a sub-field of digital humanities is extremely nascent, scholars have focused on the implicit virtual or real qualities of literary works, as they have also begun to think critically about the process of adapting literature to the VR medium. Notably, Ricci discusses the virtual reality elements present in the corpus of Italo Calvino, while Barolini focuses explicitly on Dante and virtual reality as rhetorical. Broadly speaking, scholars have thought to some extent about the intersection between literature and virtual reality, however, little critical work has been done specifically on Dante VR or on the scholarly process of rethinking literature in terms of its virtual, interactive, and immersive potential. Through an examination in my presentation of the questions that have arisen so far in the development of Virtual Commedia, my own VR project and simulation of the medieval poem, I reread Dantes Commedia for its VR application and focus specifically on the factors that must be considered when adapting literature to virtual reality. These questions include: How does one map a text onto the decidedly physical space of a virtual reality simulation? What happens to the texts narrator? What role does the VR user play within the space of the VR simulation? Does the users so-called perspective converge with or diverge from the original perspectiveoffered in the works textual form? To what extent does or should sound effects, lighting, and physical sensations be made a part of the literary VR experience? Finally, how can literary VR negotiate problems of textual accuracy and authenticity in the adaptation of an original literary work? In this presentation, I argue that a text like Dantes Commedia lends itself especially well to VR technology for three main reasons. First, the incredibly detailed descriptions of the realms of hell, purgatory, and heaven imply a virtual visuality already: readers are invited to imagine themselves walking alongside the pilgrim, through the poets prolific use of rhetorical devices such as enargeia, ekphrasis, hypotyposis, etc. Second, the poems narrative maps perfectly onto the texts space: a VR user would be able to virtually retrace the pilgrims steps in both linear and chronological order, making the physical and textual journey through the realms of the afterlife one and the same. Third, the experience of the Commedia for the most part involves Dante the pilgrims witnessing of the afterlife and its residents. His arrival in each circle of hell, for example, often triggers the speech or actions of the creatures present there. This restricted agency simplifies the process of adapting to VR: the simulation would not rely on user agency, and though interactivity would be emphasized, the user would not be able to make choices to affect the simulations narrative. In this way, the audience of literary VR broadly includes students and academics, Dante readers, and the general public, who may perhaps otherwise never access the poem. My poster thus presents this project in two original ways: theoretically, I rethink the poem through the VR technology lens, thus coming to new conclusions about the Commedias relevance and applicability to the literary VR field; and practically, as I am currently developing the simulation, I report on the real-time process and the questions constantly being raised. Overall, my presentation participates in this new field of critical theory, where literary VR becomes both a medium of interpretive scholarship and a form of art in its own right. "
	},
	{
		"id": 181,
		"title": "Close-Up Cloud: Gaining A Sense Of Overview From Many Details",
		"authors": [
			"Junginger, Pauline",
			"Ostendorf, Dennis",
			"Avila Vissirini, Barbara",
			"Voloshina, Anastasia",
			"Kreiseler, Sarah",
			"Dörk, Marian"
		],
		"body": " Introduction After two decades of steady increases in image resolution through technical advances in image sensors, we are now also witnessing a significant growth in comprehensively tagged image collections. Concurrently cultural institutions have been digitizing their collections, while cultural scholars have been investing considerable efforts into the annotation of images to denote iconographic details and historical context. Despite these developments, existing interfaces to access image collections do not harness the possibilities provided by rich visual details of high-resolution images and detailed tags associated with them. A particularly promising development, however, is the growing research interest in visualization to support the analysis and exploration of cultural heritage data [Windhager et al., 2018]. In this context, art historians are experimenting with digital methods, in particular visualization [Bailey and Pregill, 2014], to explore their potential for expanding the scale and scope of art history [Drucker, 2013; Manovich, 2015]. In these experiments, digital methods tend to be equated with a distanced perspective on the phenomenon [Moretti, 2013] with the result that many visualizations provide high-level overviews that diminish the intricate and intriguing details of individual artifacts [Hochman and Manovich, 2013; Hristova, 2016]. With this research we present an approach towards visualization that is challenging the understanding of overview and detail as something inherently opposed. We introduce a technique that clusters iconographic details of images in order to reveal visual patterns prevalent in a collection. An Overview of Close-ups We worked with a collection of glass plate negatives that were created around 1900 in an attempt to document the inventory of the Museum für Kunst und Gewerbe Hamburgfor publications and for internal use [Kreiseler, 2018]. Since their creation the glass plate negatives have been reinterpreted from internal material to collection objects themselves. Subsequently, they are being digitized and tagged with iconographic terms. Based on these resources we introduce a visualization technique that arranges close-ups into frequency-based collages. The aim is to a) highlight specific details of the collection objects and create an awareness for their iconography, b) represent the thematic and aesthetic patterns across the collection and c) support open-ended exploration at varying granularities. The visual interface is composed of three views, each illuminating different characteristics of the collection, and encouraging a different mode of accessing and appreciating the artifacts. Figure 1: Close-up Cloud Close-up Cloud The initial stage, the Close-up Cloud, provides a high-level overview of the entire collection. It displays a cloud-like collage of close-ups, each representing one occurring tag. Akin to word clouds which vary in font size, the area sizes of the close-up images represent the respective tags relative frequency within the entire collection, i.e., a larger area indicates more occurrences of that tag. Over time the close-ups content changes randomly, constantly creating a different assemblage of depictions. The visual arrangement of close-ups constitutes an abstract-concrete visualization of the collection, which attempts topresent both the texture and the structure of the collection items. The positioning of images is based on semantic similarity. While the initial view does not include any labels, one can reveal the respective tags by hovering over an image. The close-ups of the motifs function as navigation elements, allowing exploration along visual details. Figure 2: Tag view Tag After selecting a close-up, the Tag viewunfolds in a conventional image grid system, displaying all close-ups from the same tag. One can now confidently navigate through the results and select areas of interest. Figure 3: Object view Object In the Object viewthe originating image in its entirety is visible for the first time. It is gradually zoomable and pannable. These interactions can expand the image to fill the complete size of the canvas, and in this way maximize the level of visible detail. A column of tags lists all identified close-ups in the selected object. Hovering over a tag element highlights the corresponding regions in the image by setting non-corresponding regions to be semi transparent. A number next to the tag indicates the overall frequency of the tag across the collection. It links to the respective Tag view, enabling further exploration of tangential parts of the collection. Figure 4: Object view, zoomed in Technical implementation In order to illustrate the core concepts of our technique we have devised a rudimentary web-based prototype using the JavaScript libraries Pixi.js and D3.js. We prepared an exemplary set of 43 data objects. The close-ups of iconographic details were manually identified, cropped and embedded as images in a folder structure. Metadata and coordinates of the close-ups are stored in JSON files that link visual material and tag data. Conclusion We presented a visualization technique for the exploration of digitized cultural collections that proposes a novel approach towards the overview. The centerpiece of our approach is a Close-up Cloud that presents a high-level overview of the collections iconography, while at the same time facilitating close viewing of details. Thus, it enables visual exploration along iconographic patterns across the whole collectionor the details of individual objects. All stages of the interface are interlinked for open-ended exploration at multiple granularities. We implemented the concept as a web-based interface and evaluated the potential of the approach with collection experts and people interested in photography. The feedback that we received suggests that the technique allows both a serendipitous exploration of the collection that requires no prior knowledge, as well as the scholarly examination of iconographic patterns. While the Close-up Cloud is still at an early prototypical stage, the unique integration of symbolic patterns and figurative details may have the potential to bridge distant and close viewing of image collections. For future work, the development of a semi-automatic annotation technique through the integration of computer vision could be a promising approach to visualizing significantly larger collections of various kinds. Acknowledgements We thank the Museum für Kunst und Gewerbe Hamburgfor the collaboration and the Brandenburg Centre for Media Studiesfor funding this research. "
	},
	{
		"id": 182,
		"title": "Digitalizing Old Diary and Reading Multi-layered Everyday Life: A Data Analysis of an Upper-class Elite Man’s Diary (1692-1699) in the Chosǒn, Korea",
		"authors": [
			"Jung, Ji Young",
			"Kim, Jeongmin",
			"Lee, Jungyeoun",
			"Kim, Jiyoung"
		],
		"body": " Research Background and Question How did the everyday life of upper-class elite men look like in 17 th century Korea? How did they establish their position and relationships under Confucian norms, which was the dominant ideology of society at the time? How did they navigate these norms? The current image held by scholars of upper-class elite men of Chosǒn is that of antiquated and solemn scholars who strictly followed Confucian ideology. However, their diaries show us a diversion from these images. This research aims to provide a new understanding of the past by analyzing a diary from the Chosǒn period through DH methodology. Most of the diaries of upper-class elite men were written in Chinese characters and were not translated into Korean, allowing only professional researchers to read them. However, this research will enable public users and scholars from various disciplines to access and analyze this text by first translating the original cursive Chinese Characters into Korean, and then transforming it into Digital data. This research attempts to bridge historical studies and computational methods, increase the depth of the analysis of these types of historical materials, and expand the analytical horizons of the text. By restructuring the real-life diary through text analysis and digitalizing its contents, this research also attempts to open up the possibility to engage with the diary using humanities methodologies. Material This research analyzes the text of Jiamilgi, an 8-year-long diary written by a man named Yun Ihu, who lived on the southern coast of Chosǒn during the 17 th Century. The diary is not only a record of his life, but also of his relationships with family, relatives, and others. Yun Ihu included not only details about his emotions, but also his travels and visits to see relatives; his consumption of medicines and foods; his involvement in land reclamation projects; his property management and construction; and his connections to historical events such as political party strife in the central government. Analysis and the Method of Organizing the Digital Contents The goals of this project are to digitalize the Jiamilgi , implement a visual navigation interface, and create the semantic connections among the digital archives of this Chosǒn historical text. To achieve these goals, we designed a DH methodology, which consists of 8 major steps, including text translation; content structure examination; noun and verb word extraction; concept, class and property, and semantic class relationship definitions, knowledge base construction; and visual navigation interface implementation. Firstly, the original text of Jiamilgiwill be translated into Korean and then constructed into digital contents, which will be made public on the web as Jiam Wiki. Both the original and translated texts will be digitally restructured by date sequence. Secondly, through the content analysis of the original Jiamilgi text, the conceptual elementswill be extracted and based on these, a semantic class model will be constructed and a multilateral data analysis attempted. Our class model will be called the Jiam Ontology, and conceptualize the extracted elements and establish the relationships between these as well as between elements and outside resources such as images, webpages, and ontologies of the subject history. Thirdly, a visualization and navigation system of the Jiam Ontology will be created by using various digital technologies including Graph Database, Python visual libraries, and D3 JavaScript Library. Fourthly, the instance data construction of elements, such as actor-centered places, objects, persons, actions, emotions, and their interconnected analysis, will recreate a historical depiction of everyday life during the time; these will include, travel routes, the contents of letters, the use of gifts, and the roles slaves played in society. Finally, we will attempt to make a textual interpretation to investigate how the frequency of events or the distribution of time intervals can be applied to Complex Systems Theory. In addition, a quantitative analysis of the events, persons, places, and so forth appearing in the daily records of the diary will be performed. Through these attempts, the project will expand the scope of digital humanities research to human dynamics and Complex Systems Theory. Impact of the Research The digital text of Jiamilgi armed with ontology specification, visualization, quantitative analysis, and general interpretation, will offer a new research methodology as a case study of a DH-based diary research, and at the same time, will demonstrate the expandability of existing historical research on the Chosǒn period. By making the results of our work public on the web, we intend to increase the openness of DH researches and promote cooperative research internationally. "
	},
	{
		"id": 183,
		"title": "The Value of Tag Cloud Visualizations for Textual Analysis",
		"authors": [
			"Jänicke, Stefan",
			"John, Markus",
			"Geßner, Annette"
		],
		"body": " Introduction In digital humanities applications, tag clouds are often used as a means of distant reading. By dissolving the structure of texts, thus, splitting it into words, the frequencies of different words, in the following denoted as tags, can be determined. Typically, tag clouds take the most frequent N tags of a text corpus, and by mapping frequency to font size and arranging the tags in a random manner on the screen, the observer gets a quick and intuitive summary of the textual content of the corpus. At least since Wordlehas been offered to the public to generate tag clouds on demand, they enjoy great popularity and are widespreadly used. Nevertheless, there are crucial theoretical problems in the design of tag cloudsthat question their benefit for text analysis tasks. Finding a single word without assistance is hard, long words receive more visual attraction than short ones as they cover more space, and font sizes, thus the frequencies of words, are difficult to compare. Furthermore, a tag cloud usually does not display all words in a corpus, thus, neglecting less frequent words can lead to misinterpretations. In this paper, we evaluate the value of several tag cloud visualization techniques that have been designed to support research tasks in various digital humanities scenarios. Continuing prior works of the co-authors, we base our analysis on the Bible known as the most often read and researched books, thus, well-suited for evaluation purposes. We chose the King James Bible being the most influential English translation. TagVenn Diagrams TagPies have been designed to compare the contexts of different words. For a set of M different terms, TagPies generate M+1 different tag sets for sharedand non-shared vocabulary. TagVenn diagrams are an extension of TagPies aiming to compare co-occurrences more precisely as all combinations of shared vocabulary are considered. Taking three terms a, b and c and their respective sets of co-occurring tags A, B and C as an example, TagVenn diagrams visualize the following tag sets: A\\, B\\, C\\,\\C,\\B,\\A and A∩B∩C. The tags are arranged in a Venn diagram style with a set of colors reflecting the cut sections. As the human ability to distinguish colors is limited, a maximum of four textscan be analyzed with Tag Venn diagrams. A similarity in topics and writing style can be seen looking at the King James Bible: It is known that the books of the four Evangelists have a lot in common with John differing the most from the others. This can clearly be seen by comparing John with Marcus and Matthew with the minimum number for occurrences set to four. Johnand Marcushave significantly less words in common than Mark and Matthew. It is interesting to see here that Matthew has the biggest number of words only used in this book instead of John as might be expected. Especially worthy for further investigation in the close reading were words like truthand trueare frequently used by John, and less than 4x used by the other two Evangelists. In the current version the scholar has to set a minimum number of occurrences, which may not always give accurate results. Then, the diagram might show a word as only occurring in one book of the Bible although other books may also include that word with a too small amount of occurrences. Setting a high number of occurrences will not always be interesting for a researcher since very frequently used words are not necessarily relevant for determining the content of a text. A workaround to avoid unwanted results could be extending the number of stopwords on demand. Also, adding the option to set a maximum number of occurrences would give more opportunities to research different questions. Maybe even considering classes of frequency could bring interesting results and open up an interesting field of research. TagVenn Diagram showing the most frequent words in Mark, Matthew and Johannes MultiCloud MultiCloud has been developed as a flexible approach to show different documents in a merged word cloud visualization. It depicts each document with a colored circle around the word cloud shapeand it applies a force-based layout in combination with a collision detection algorithm to use the available space as optimally as possible. This way, the positions of the tags represent the occurrences in the different documents. Additionally to the position in the layout, the font size represents the overall occurrences in the documents and color saturation indicates how relevant each tag is in the respective text documents. Furthermore, MultiCloud offers the possibility to control the number of displayed tags: Analysts can set both the number of displayed tags that occur in more than one document and the number of displayed words that occur only in an individual document. Both options can be combined and trigger an immediate update of the visualization as depicted in Figure 2. Since MultiCloud uses the largest qualitative color scheme of ColorBrewer, which comprises twelve distinct colors, it supports the analysis of twelve different documents. By scanning the first tag cloud, users get a quick and intuitive overview of the textual content that all the chosen books have in common, like lord and jesus as well as differences in topics. Looking at words only appearing in one of the chosen books shows that the word esausmight be a spelling mistake in book Genesis or a problem of normalization, if the apostrophe is filtered out, for the Biblical name Esau, which is used more often in different books of the Bible. Also interesting is the name abram, which is only used in one of the chosen books, for this is the rarely used former name of the very frequently mentioned abraham. Determining whether a word is relevant for a specific research question depends strongly on the books chosen to compare. The third tag cloud visualizationoffers an interesting overview of tags that describe the seven subparts and the individual documents. Document-related tags, for example, like the city of babylon only being mentioned in book Jeremiahas well as tags that characterize all loaded parts such as glory or prophet can be easily identified. Three different word cloud variations of seven subparts of the King James Bible using: only tags that occur in multiple documents, tags that only appear in the individual documents, and a combination of both. TagSpheres TagSpheres have been designed within a digital humanities project to support the analysis and classification of a terms co-occurrences according to their clause functions. The analyzed term T is placed in the center, and the co-occurrences are placed on concentric discs around it. The depth of the analysis, thus the number of levels, is user-configurable. The farther away from the co-occurring tag in the text, the farther away it is also placed in TagSpheres. A divergent cold-hot color map is implemented to transmit the notion of distance. T is colored red, the tags of the outer disc are colored blue, and a color on the gradient between red and blue is assigned to intermediate levels. Clicking a tag X lists text passages in which T and X have the associated word distance. Choosing the word righteousness with a maximum distance of 6 in all Bible books shows an interesting amount of times this word itself and variations of it are reoccurring in the near proximity: The adjective righteous appears once two words apart, four times three words apart, once four words apart, twice five words apart and twice six words apart. Also, the word itself is repeated several timesas well as its opposite unrighteousness etc. This case shows that the chosen word is frequently being used in a repetitive, sermon-like style of writing, e.g. in Psalms and especially in the Sermon on the Mount in Matthew. Increasing the minimum number of occurrences can massively change the result. Currently, stopwords are omitted, but researchers might want stopwords taken into consideration when trying to detect interesting chains of words like sayings, proverbs etc. It could also be interesting to look for a single word re-occurring in the span of a work and visualizing this or looking for a more flexible span of words between two re-occurring words indicatinghabits of the author. TagSpheres showing the co-occurrences for the word righteousness dependent on word distances from 1 to 6 Summary The three case studies outline that, despite the above mentioned theoretical problems, tag clouds can be–if they are carefully designed–valuable tools to support different research inquiries. As opposed to Wordle, all presented visualizations use tag color and position to express a tags set relations. It was important for the literary scholar in all scenarios to interactively get access to the underlying texts to examine upcoming hypotheses. Furthermore, different parameter sets shall be provided to generate multifarious views on the text in question. "
	},
	{
		"id": 184,
		"title": "A Geo-Sampling Model To Analyse Micro Level Historical Agricultural Production Data For Mid-19th Century Southeast European and Anatolian Regions",
		"authors": [
			"Kabadayi, M. Erdem",
			"Gerrits, Petrus Johannes"
		],
		"body": " With this poster we would like to present and discuss our geo-sampling method devised to make selective and efficient use of a wealth of undigitized, underutilised micro level archival data from the 1840s for five regionsin todays Bulgaria, Northern Republic of Macedonia, and Turkey. While the construction of land use suitability models is a common practice conducted in different fields, an attempt of bringing historical Ottoman archival data together with the spatial methods has not been done before, to the best of our knowledge. In the last two years we have geo-located 1797 villages in five regions shown above. We are working as a research team on this task and by early 2019 we are confident that we will have completed our fifth region, centred around Bitola, in our project, and will reach a total number of around 2,000 villages in our dataset. We have already created weighted Voronoi polygons for all the villages in our five sets based upon their relative population sizes. Below you can see the example for our Plovdiv region. For every village in the dataset we have already manually extracted total number of households and total population from a series of population registers from the 1840s and entered in to our geo-database. For almost all of the villages in the set, we have also very detailed information on agricultural production, recorded per household in an Ottoman empire tax survey from 1845. The data on agricultural production from this 1845 survey include a detailed product mix, total area of cultivation, and total value of agricultural production. Moreover animalskept in households are also listed according to type, number, value, and generated income. Evidently, we are facing a challenging and complex sampling problem to reach a representative sample of villages to be able to extract data and analyse agricultural production and animal breeding to understand dynamics of the primary and most important sector of economic activity in these five regions in the mid-19 th century. To tackle massive number of total villages and huge regional diversity we have divided our regions to four 4 sub-units. Now we are dealing with 20 units and we would like to conduct the same sampling strategy for each of the 20 sub-units. Our geo-spatial sampling model has three major components: 1- Soil depth and soil quality, which can be downloaded for three regions centred around Bitola, Plovdiv, and Ruse from ESDACfor Bulgaria). We do also have soil data for two regions in Turkey. 2- Suitability for cultivation: using available SRTM data with 30 meter spatial resolution, we have created our Digital Elevation Model and using the our devised weighted Voronoi polygons we can analyse the agricultural suitability of our village-polygons based upon their relative positioning according to elevation, slope, ruggedness, and aspect. 3- Connectivity: We have map-mined reliable and extremely detailed information on transport facilities from the Third Military Mapping Survey of Austria-Hungary military map for a large territory covering our three regions from 1900s. Below you can see the extent of Southeast Europe for which we have already vectorised the road infrastructure. We are using a less yet for our sampling purposes sufficiently detailed and already geo-referenced map for two regions in Turkey from 1899. We weighted soil depth and quality with 50%, agricultural suitability with 35% and connectivity with 15% in our model. A preliminary example of Plovdiv region with five categories of suitability can be seen below. In our poster we will operate with our sampling strategy to group village polygons in to five groups of suitability and then we will test the representativeness of our model by conducting a five percent sampling strategy. "
	},
	{
		"id": 185,
		"title": "Embedding Creativity Into Digital Resources: Improving Information Discovery For Art History",
		"authors": [
			"Kamposiori, Christina",
			"Warwick, Claire",
			"Mahony, Simon"
		],
		"body": " Over the past decades, the increase in the use of digital resources and the growth of research conducted in digital environments has transformed academic scholarship. Yet, as the employment of digital resources increases, so does the necessity to understand user behaviour and provide digital infrastructure tailored to the needs of researchers. Through this paper, we aim to explore how the design of digital libraries and resources can be improved to better facilitate information discovery and use in art history; for this purpose, we will look at scholars creative encounters with information and present the implications for resource design. Art history is a highly creative discipline in terms of its interaction with information. Thinking about the beginning of the research process and, therefore, the seeking of the needed information, this is to a great extent linked with the scholars intuition and memory. These two qualities, which are associated with connoisseurship, apply especially to the case where research starts from the examination of the artwork. Brilliant, for example, noted that scholars in the field, after mainly relying on their visual memory to examine a work of art, attempt to search for related information objects. In fact, artworks can often inspire the initiation of the art historical research process through enabling the discovery of the research subject and the generation of research questions; these questions, in combination with the experience of the researcher lead to the searching of the required material. According to Shneidermans Genex framework, which is considered appropriate for understanding creative information work, using information as a source of inspiration rather than just as research evidence constitutes a characteristic of creative disciplines. In the digital age, the abundance of information available through digital libraries and resources offers unparalleled opportunities for art historians to engage creatively with a variety of information objects and gain inspiration for research and teaching projects; for instance, digital images have been found to stimulate the thinking process of scholars in the field. Yet, in order to facilitate creative scholarly processes which can lead to the production of knowledge, information discovery and related phenomena, such as serendipity, which have been found to trigger inspirationneed to be enabled. We argue that understanding art historians information behaviour and needs in terms of accessing and using digital resources is necessary in order to design systems that can positively affect the whole scholarly workflow. This study employed an ethnographic approach to the study of scholarly practices by conducting semi-structured, in-depth interviews with twenty art historians at different career stages, as well as observation of their physical and digital personal research collections in order to develop a sound understanding of their interaction with information at different stages of the research process. We were particularly interested in creating a pool of interviewees consisting of two groups; one where scholars worked on commonly studied areasor employed traditional art historical methodsand another where the topics examinedor the methods employedwere considered less traditional. Identifying any similarities and differences between these two different groups of scholars could provide a better insight into the needs that art historians in different areas of the field have in terms of resources and tools. Accessing and interacting with digital libraries and resources was an important part of our participants daily work routine; in fact, most tended to start their research in the digital environment, an approach which was also found to facilitate serendipity and trigger new ideas for existing and future projects. Several studies have looked into the role of serendipity in scholarly practice and examined whether it can be supported by information systems. For instance, Foster and Fordstudied serendipity in the context of the information seeking behaviour of interdisciplinary scholars and suggested that further examination is needed in order to understand that phenomenon which, as they argued, is […] a difficult concept to research since it is by definition not particularly susceptible to systematic control and prediction. Race and Makriexamined the internal and external factors that facilitate serendipity, some of them often beyond the users control; these included aspects of the users personality, such as curiosity, and issues such as topical knowledge, time, communication or systemic characteristics. It should be noted that the authors highlighted the link that exists between creativity, serendipity and innovation, noting that most of the same factors that encourage or discourage creativity and innovation encourage or discourage serendipity as well. In this research, we discovered that serendipity was more likely to occur during the first stage of research, when scholars attempted to investigate a topic. At this stage, researchers tended to be more open to accidental information discoveries- a personal characteristic identified by Race and Makrias necessary to experience serendipity- and the possibilities to find unexpected information that would significantly affect the research process were greater. On the other hand, during the later stages of research, when creative behaviour was mostly related to the building of the research argument, information seeking behaviour was more goal oriented and the possibilities of experiencing a serendipitous discovery that would have a fundamental effect on the research process were fewer. However, the fact that some areas of research were found to benefit from a larger pool of online resourcescannot be overlooked when considering the possibilities of discovering information serendipitously. This issue, then, generates questions regarding the extent to which information resources available online - even when including secondary material - meet the needs of scholars in the various sub-disciplinary areas of art history, like non-Western art. Actually, the art period that a project was looking at, the geographical focus of its subjector the fact that the topic under investigation may have not been researched before were often connected to issues of availability of resources, conveniently accessible to scholars. To conclude, as part of this paper we will report on the user requirements for designing systems that facilitate discovery, encourage creative use of information and trigger inspiration which will hopefully prove useful to information and other professionals supporting art historical scholarship. Thus, based on our participants reported experiences with digital libraries and resources, interfaces should be simple and the functionalities provided should encourage different types of searching. More specifically, given art historians frequent need to browse content in collectionsand to engage visually with information, digital resources targeted to this group of researchers should enable visual exploration of collections. This could be achieved through allowing users to get an overview of the materialin a collection, providing suggestions for similar content and offering services that facilitate intuitive interaction with information. Apart from that, including related metadata alongside the digital objects in a collection as well as information on the decision-making process with regards to digitisation will enable scholars to make informed decisions when using digital content and gain necessary details for the purposes of their work. Finally, enabling access to digital collections through different means, including the ability to view and download material, is necessary in order to meet scholars evolving need to access and manage material across devices and tools. "
	},
	{
		"id": 186,
		"title": "The Library In The Digital Humanities: Surveying Institutional Practices In The UK And Ireland",
		"authors": [
			"Kamposiori, Christina"
		],
		"body": " It is widely accepted that research libraries play an important role in facilitating academic research and teaching. However, given the technological advances of the last few decades, this role has been continuously transforming; the fundamental changes that information technology has brought to various academic fields, such as the Arts & Humanities, can be regarded as one of the reasons. This paper reports on work conducted to explore the role of research libraries in digital humanities across the UK and Ireland. The emergence of digital humanities, in particular, raises new challenges for libraries. But which are the qualities of this type of scholarship which instigate a change in the support system traditionally provided by libraries to Arts & Humanities researchers? According to Spiro, interdisciplinarity, openness and collaboration are some of the core characteristics of digital humanities scholarship, making it distinct from the work conducted in the context of more traditional Arts & Humanities fields. In addition, scholars in the field increasingly create, use and communicate various forms of digital data in previously unimaginable ways, while libraries are often expected to adapt and respond accordingly. As Vandegrift and Varnerargued, the library can no longer be simply a place to get the right answers or to be directed to the correct resource […]. In the context of the current information and academic landscape, it becomes apparent that a more collaborative approach to the facilitation of scholarship is needed. In fact, library professionals can constitute ideal partners in digital humanities research, offering solutions to several problems that can be met during the lifecycle of a project, such as around the building and maintenance of digital projects and tools or the management of data. Yet, there is little research on how UK libraries engage with digital humanities researchers - most of the studies so far have focused on US libraries- including the degree of their involvement and the models of support or collaboration they employ. Thus, this institutional aspect of the digital humanities field in the UK and Ireland remains largely unexplored. For the purposes of this project, 27 institutions across the UK and Ireland took part in a survey designed to investigate current library practices and opinions concerning the support of or involvement in digital humanities research. Moreover, through further data collection, we looked at specific cases of institutions where library professionals closely worked with digital humanities scholars to understand the role that libraries played at different stages of a research project as well as the circumstances under which different types of collaborations were formed. Generally speaking, our findings showed that many research libraries in the UK and Ireland have started moving from being mere service providers to being active participants in research and teaching. For example, many of the library professionals who participated in the project were actively involved in the co-creation and co-management of tools for research and teaching; these ranged from digital collections and resources to tools used at different stages of the research lifecycle. Additionally, the survey results uncovered the variety of DH activity conducted by scholarly communities at the participating institutions and showcased the range of library services designed to support various projects as well as the innovative contributions made by library staff. Furthermore, in this project, it became possible to examine in detail three different models of library engagement in digital humanities research, currently followed by some of the participating institutions which were highly active in the area. Therefore, the proposed poster, will also provide evidence of the role of research libraries in organising, conducting and communicating DH research; providing specialist DH services and actively engaging in the creation, archiving, curation, and preservation of tools; and proving leadership in the areas of digital preservation and data management. Exploring the different models of engagement that libraries follow when it comes to working with digital humanities researchers, the nature of these professional relationships well as the benefits and challenges they involve will hopefully increase our knowledge about an institutional side of the digital humanities in the UK and Ireland that remains largely undocumented. "
	},
	{
		"id": 187,
		"title": "Attractive, Interactive, Ready for the Web: Visualizing Your Data Using R",
		"authors": [
			"Karjus, Andres"
		],
		"body": " Digital humanities has brought big data and quantitative analytics into the humanities. Great effort has gone into compiling and curating large corpora and databases in numerous disciplines. However, along with big, complex data comes the question of how to make the information accessible for exploration – to the researcher looking for data, to the reader of an academic paper that has made use of the data, but also to the media or the interested lay person browsing the website of a project. Staring at large tables, XML trees or text files of millions of words is rarely useful. Even if a database is made freely available, working with and exploring big data usually requires at least some skill in programming or even in some specialized toolset. This bottleneck effectively bars most lay users and researchers with different skillsets from interacting with interesting and potentially useful data, thus wasting the full potential of a database, the creation of which has more often than not incurred considerable human effort. Visualization of some aspects of the database is the obvious immediate remedy, but static figures often too fall short of providing an informative overview, and only provide selective views of the data. At the same time, the web offers a dynamic medium for publishing graphs that could be interacted with by anyone from researchers to journalists to funding bodies. This workshop teaches participants the basics of R, an immensely powerful and flexible language for data analytics and visualization. We will quickly go through just enough of the programming and explorative data analysis basics before diving into a variety of graphs - for numeric, categorical, textual, GIS, and network data, starting with simple static plots and moving on to creating interactive, animatable, multifunctional figures. We will also look into how to embed such app-like plots into a website, teaching materials, or your next conference slides. We will be making heavy use of modern packages such as plotly, ggplot2, quanteda, visNetwork, and rmarkdown. By the end of the workshop, you will know how to choose a suitable visualization for a given data type, and how to execute it either via traditional static plotting methods or the interactive alternatives. Prior programming experience is not required to participate. "
	},
	{
		"id": 188,
		"title": "Still Waters Run Deep. Including Minority Voices in the Oral History Archive Through Digital Practice",
		"authors": [
			"Karrouche, Norah"
		],
		"body": " While digital archiving practices in the Netherlands in the past two decades have provided better access to oral history collections, the effort has also demonstrated that the voices heard in those oral history projects are predominantly white. This paper argues that the composition of the Dutch oral history archive is in dire need of revision and seeks to generate a dialogue on how to remedy this silence. In a discipline that has traditionally prided itself on its emancipatory potential, ethnic minorities and formerly colonized peoples in particular have received relatively little attention. The reasons for this silence are manifold. First, oral history projects in the Netherlands have mostly dealt with WWII memories, atrocities and trauma victims in particular. Few of these WWII-related oral history collections and research projects have concentrated on events in the colonies and the participation of colonial troops on the battlefield. Others have investigated their interactions with Dutch citizens during the waror the ways in which WWII and its aftermath were experienced in the era of decolonization. Some examples of oral history projects that have successfully honed in on these silences in the Netherlands and the former Dutch colonies are Papuas in diaspora and Het Molukse perspectief in oorlogstijd, two collections that deal exclusively with the Dutch Indies. Although oral histories of WWII have demonstrably become more sensitive to diversity and multiperspectivity, other historical events that may be relevant to the collective memory and mnemonic practices of minority groups with which the Netherlands have no immediate colonial tie, are underrepresented in oral history projects. Second, oral historians in academic settings often struggle with the perception that their method and sub-discipline is highly subjective and thus, untrustworthy as a historical source. As a primary source, an oral testimony of a specific event or a life story is oftentimes attributed a lower status. Two, more recent developments deserve our attention in this regard. Oral history is increasingly being used as a method to gain insight into the ways in which individuals remember, how they selectively engage with the past, and give meaning to their present and future selves. Most oral historians will nowadays not so much regard their method as a means to reconstruct the past through memory. They are far more interested in the construction of identities, their representations and performances. Moreover, scholars acknowledge that the status of original recordings as more truthful sources is changing rapidly. Third, individual researchers who do collect oral histories of post-colonial ethnic minorities in the Netherlandsare often reluctant to deposit and share their archives in scientific repositories, for instance because they have particular ethical and legal concerns and feel unprepared to tackle these. Oral historians are often unaware of the vast array of choices they have within the ruling European privacy legislation. Researchers are reluctant to deposit their oral history data. But oral historys roots are not only to be found in trauma studies and war history. Oral history also emerged in the 1960s in urban spaces where the voices of women, working classes and racial minorities had gone unheard. Historians criticized the dominant top-down approaches to history, as histories that centered on the states and men who governed them. Oral history gave voice to the historically disenfranchised, and was practiced first and foremost at a local, not national level. While in the Netherlands these voices may not be heard through scholarly collecting practices, as listed and explained above, they are certainly available through community archives, which operate at a local level but have neither the means nor the expertise to store and disclose their collections. In many cases, oral history is the only way to gain insight into the specific historical experiences of ethnic minorities in the Netherlands, as these are groups that have been underrepresented in the traditional archive. Sources from less privileged groups in society should be added to our archive as a way to address topics and experiences that are underrepresented in national histories due to a lack of documentary sources. Community archives collect oral histories primarily among underrepresented groups, and first and foremost in order to share them with an audience. In this short paper I therefore advocate a return to oral historys urban roots and focus on identity construction and minority subjectivity. I will do so by elaborating my insights and preliminary analysis of an experimental cooperation between CLARIAH, a large-scale digital humanities research infrastructure project in the Netherlands, and the small community archive Verhalenhuis Belvédère in Rotterdam. This particular initiative aims at connecting local citizens through storytelling, including oral history. The one-year pilot is centered around a collection which voices a diverse group of local citizens and ethnic minorities memories of the era of reconstruction in the city of Rotterdam, and its aftermath. To this end, representatives of the community archive, digital repositories and the digital research infrastructure project closely collaborate. Can we design an oral history collection integration workflow that would help community collection owners to integrate their data in digital repositories and research infrastructures? How can ASR and annotation tools support this process? Which sustainable strategies can we develop in order for these collections to be used and re-used in research? My aim is to generate a dialogue with digital humanities scholars and offer ideas on a template for the further inclusion of community oral history archives in scholarly repositories and digital research infrastructures. In a world rife with issues of diversity and representation, I propose this community-based strategy as a means to navigate these muddied waters, and achieve a more inclusive approach to oral history research in both academic and non-academic contexts through digital practices. "
	},
	{
		"id": 189,
		"title": "Construction of a Corpus of “Christian Materials” for the Study of Colloquial Japanese of the Muromachi Period",
		"authors": [
			"Katayama, Kurumi",
			"Ogiso, Toshinobu",
			"Watanabe, Yuki"
		],
		"body": " 1. Introduction: Christian Materials We constructed a corpus of Christian Materials, which is invaluable material for the study of the Japanese language in the sixteenth to the seventeenth century CE. They comprise documents written by Catholic missionaries who came to Japan for the purpose of proselytization. We chose Feiqe no monogatari and Esopo no Fabulasfrom these materials and constructed a corpus. Feiqe is a digest text of the Japanese epic, the Tale of the Heike. Esopo is a Japanese translation of Aesops Fables. These have special characteristics among the Christian materials: they were written in the Japanese colloquial language of the time using the Roman alphabet with Portuguese spellings. They were written as readers for missionaries to learn Japanese. In order to propagate Christianity, it was necessary for the missionaries to converse naturally with the Japanese people, so they had to study the current colloquial language. Writing the materials using the Roman alphabet made it possible for missionaries to read these books even if they had not learned Japanese characters. Many of the existing Muromachi-period Japanese documents are written in the literary language; the rare materials written in the colloquial language are very valuable. The Roman alphabet spelling reveals information about the phonology of the Japanese language, which is not apparent when referring only to Japanese characters. For example, voiceless or voiced consonants or open or closed o-vowelsare not clear from the Japanese characters. There are no other materials that include these two characteristics. It should be noted that only one single copy of Feiqe and Esopo still exists, housed in the British Library in London. As there is no doubt that these materials are extremely valuable for the study of colloquial Japanese in the Muromachi period, they need to be more widely and conveniently available. Figure 1. Images of Feiqe and Esopo 2. Construction of the corpus One of the features of our corpus is that it has two texts, the Roman alphabet text and the Japanese character text. We referred to the original prints and faithfully converted the original Roman alphabet text to electronic text. Letters like à, ã, and ſ are replicated by Unicode. We also prepared the Japanese character text and encoded it in XML files. We studied tags and document type definitions with reference to TEI P5. Based on that, we selected and added required tags for the structure of Feiqe and Esopo. We also referred to Kawase et al.to design the tag set. We carried out morphological analysis on these XML files. We used the UniDic and MeCab morphological analysis tools to divide the entire text into linguistic units and add morphological information such as lemma, readings, and parts of speech. MeCab is a morphological analyzer based on the conditional random fieldanalytical method that achieves state-of-the-art performance in contemporary Japanese morphological analysis. UniDic is a dictionary for the morphological analysis of Japanese that can lemmatize variations of orthography and word forms. The grammar and vocabulary of Feiqe and Esopo reflect the transition of Early Middle Japanese to Modern Japanese. We therefore used UniDic for Late Middle Japanesefor accurate morphological analysis. This dictionary was developed using the same method as UniDic for Early Middle Japanese. We have added the vocabulary specific to the Feiqe and Esopo to the dictionary and performed machine learning using corpora of the relevant era as training data. As a result of the morphological analysis using this dictionary, the accuracy of the distinction of allomorphsis 0.932. The corpus size is approximately 140,000 words. By utilizing the appropriate dictionary, we were able to keep the manual effort required for correcting errors to a minimum. We then aligned the Roman alphabet text and the Japanese character text counterparts into parallel texts. By morphological analysis using UniDic, morphological information about its pronunciation was added to each word using a Katakana character. For example, the pronunciation of 涙is ナミダ. We romanized each Katakana character using the modern Hepburn system, like ナ into na, ミ into mi, ダ into da. In this way, we generated a modern Roman alphabet text and compared it to the original Roman alphabet text. Both Roman alphabet texts are quite similar, thus we were able to align these texts into parallel texts in an efficient manner. The accuracy of automatic alignment is 0.982. By carrying out morphological analysis on the Japanese character text and aligning both texts into parallel texts, we succeeded in creating a corpus that can access morphological information from both the Japanese character text and the original Roman alphabet text. Figure 2. Comparing the modern Roman alphabet and the original Roman alphabet text 3. Publication of the corpus and link to the image of the original print The corpus has been made publicly accessible through an online search application called Chūnagon. With Chūnagon, it is possible to perform searches that specify complex combinations of different morphological information. The Roman alphabet text is displayed in parallel with the Japanese character text. Another important feature of our corpus is that it includes a direct link to a clear photographic image of the original print in the British Library. Based on a memorandum with the British Library, we received permission to make the photographic image of the original print accessible in the public domain. The image files are one file per page of the original, and the size of each file is one megabyte or less. The photographic image is now on available on the NINJAL website, which is open access. Chūnagon has a link that allows you to access the page that contains the corresponding word. Figure 3. Search results for Chūnagon Acknowledgement: The work reported in this presentation was supported by the NINJAL collaborative research project Construction of Diachronic Corpora and New Developments in Research on the History of Japanese. "
	},
	{
		"id": 190,
		"title": "The Novel And The Quotation Mark",
		"authors": [
			"Katsma, Holst"
		],
		"body": " Introduction Many stories have been told of the novels emergence: the majority string a narrative of development through a handful of texts. More recently, methods in corpus linguistics and topic modeling have determined lexical differences in emerging genres, treating genres as bags of words and counting words to determine large lexical trends. This talk offers a different account of the novels emergence by focusing on format. A preliminary survey of the 18th century literary field locates two formatting features that emerge within the novel as the novel emerges, monopolizing the novel and differentiating it from other genres. These two features are the modern quotation mark and the chapter number unaccompanied by title or table of contents. Here I will focus on the quotation mark, interpreting it as a high-fidelity index of genre emergence in four particular respects. As a visible index, the quotation mark offers a way of intuiting internal changes within the emerging genre. As a recurrent index, the quotation mark offers a way of gauging the pace of the novels emergence. As an evolutionary index, the quotation mark offers a way of registering alternative paths. And as an English index, the quotation mark offers a geographic point of comparison with the emerging French novel. In other words, the quotation mark contains high information about the content, tempo, contingency, and geography of the novels emergence. Whats more, the quotation mark is fully adopted by the 19th century novel, thereby setting, for an extended period of time, the conditions within which the novel continues to evolve. It is a testimony to the significance of the quotation mark that it continues to affect the evolution of the novel—most notably, I will show, by making possible the celebrated novelistic invention of free indirect style. Methods A tendency among editors to modernize punctuation in editions of early modern texts has obscured the history of the modern quotation mark, which can now be reconstructed thanks to the wide-spread digitization of original editions. In order to attribute the modern quotation mark to the novel, I compiled my own corpus for each of the following genres—scientific articles, trial transcripts, literary reviews, novels, poetry, drama, and history—then reviewed these pages looking for unique modes of quotation. Having attributed the modern quotation mark to the novel, I then reconstructed its emergence within the novel by recording methods of depicting dialogue in popular novels from each decade of the 18th century. As of now, these data are compiled by hand because computing within a larger dataset frequently encountered problems. In the case of the Chadwyck-Healey and HathiTrust databases, text-only transcriptions often remove marginal quotation marks and italics, and double quotation marks are sometimes replaced with single quotation marks. This project thus raises questions about how best to encode OCR transcriptions to include formatting elements. Current practices have been shaped by the limitations of OCR and by the demands of corpus linguistics and topic modeling, both of which emphasize lexical differences above formatting ones. In what follows, I hope to demonstrate how quantitative analysis of the quotation mark results in a genuinely new account of the novels emergence, thereby demonstrating the value of developing new methods for preserving format in OCR transcriptions. Plotting emergence As the modifier modern suggests, the creation of the modern quotation mark was not a sudden ex nihilo invention, but rather a gradual but substantial reworking of an earlier species. This earlier species of quotation mark ran along the left-hand margin and was used to denote the transcription of written text. One finds it throughout the 17th and 18th century: in the margins of Philosophical Transactions denoting passages excerpted from treatises and correspondence; in the margins of literary reviews, denoting sample passages from books reviewed; in the trial transcripts of the Old Baily, which reserve quotation marks solely for texts read aloud in court; and even in some early novels, in Moll Flanders and Pamela, where dialogue is left alone and where quotation marks exclusively frame the margins of transcribed letters and notes. Based on my survey of the 18th century literary field, this earlier species would plausibly have remained the dominant model were it not for the novel: for it is in the novel that the quotation mark moved from the margin of the page into the gaps between words, and it is in the novel that the quotation mark was expanded beyond transcribed text to denote the larger grammatical category of direct discourse, written and spoken. I have started reconstructing the emergence of the modern quotation mark within the novel by recording the methods for depicting dialogue in five of the most popular novels from each decade of the 18th century. Within these novels, dialogue could be left unmarked, italicized in various fluid forms, or denoted by one of four forms of quotation mark: what Ive termed marginal-inclusive, marginal-exclusive, endpoint-inclusive, and endpoint-exclusive. Figure 1 plots each novels most frequent form of depicting dialogue. While collapsing much of the struggle and variation within individual texts, Figure 1 nevertheless captures a deep structural shift: the gradual inauguration of the modern quotation mark, which itself advances through three prototypical versions before arriving at the modern format. Figure 1: Evolution of methods for representing dialogue in the 18th century novel. Novels were selected from three bibliographies based on number of editions. The earliest available edition was downloaded from ECCO. Tempo, contingency, content... Figure 1 captures a century-spanning trend: in this case, the gradual overlapping steps of an aggregating consensus. The gradual tempo of emergence suggests that a need is subconsciously intuited in the absence of a clear solution, that the quotation mark is being formed for an emerging content that is not yet fully understood. During a paradigm shift, writes Franco Moretti, nobody knows what will work and what wont. And that is precisely what one sees here: expansive, almost blind, experimentation slowly whittled down to a single solution. Nor was experimentation limited to the form of the quotation mark: novelists also fiddled with purpose, most notably in the ultimately-failed experiment of printing indirect discourse within quotation marks: Gale document number: CW3311810274 Gale document number: CW3311910127 Instances of this practice abound in the 18th century novel, and not without a certain logic: in each case there is an attempt to acknowledge the residue of direct discourse exuding through. Together with the many morphological possibilities, these instances remind us that the novel could have molded a very different type of quotation mark, both in form and function. These were alternatives, not mistakes. If the modern solution seems obvious, then it merely emphasizes how deeply the novel has defined our worldview. And yet one must ask: why did the novel develop the quotation mark in the way it did? The simple answer is because new forms emerge alongside new content, and in this case that emerging content was conversational dialogue...[abridged from abstract]. Free indirect style The quotation mark instituted a genuinely new matrix for structuring storytelling, one into which the oral stories of the past were slowly squeezed. Over the course of the century, narrators and characters begin to fit themselves more and more neatly into the confines of direct and indirect discourse. As a result, novels like Moll Flanders, which seem more like witness testimony, are replaced by novels like Evelina, which seem more like—novels. This is an exceptional ramification of the quotation mark, and nothing illustrates it better than the most celebrated of novelistic inventions: free indirect style.The introductory sentence is clearly a product of the narrator, and Emma clearly speaks the quoted section to herself; but who speaks those many dashed phrases that burst forth in frustration: It was a wretched business indeed!—Such an overthrow of every thing she had been wishing for! The first linguists to recognize this style described it as a blending of what they considered two discrete categories: direct and indirect discourse. So Alfred Tobler noted a peculiar mix of indirect and direct discourse. A decade later, Charles Bally brought the style to academic attention, defining three possibilities of rendering the words or thoughts of a character—direct discourse, indirect discourse, and discours indirect libre—the first two being long known to grammarians, the latter being some new combination of the two. As such definitions make clear, free indirect style depends on a clear distinction between direct and indirect discourse. Yet this very distinction comes into being, not with the ancient grammarians, but rather with the solidification of the modern quotation mark solely for direct discourse. Novels that denote indirect discourse within quotation marks blur this distinctionprecluding the possibility of free indirect style. Only with the modern quotation mark, which created a functional and sustained binary between direct and indirect discourse, does free indirect style become possible. Which is not to say that Bally was technically wrong: the Greeks did make a distinction between direct and indirect discourse. Rather, it is a classic case of a theoretical distinction holding less influence than a functional one, and a powerful example of how a new format can allow a genre to evolve and differentiate in new, previously unthinkable ways. "
	},
	{
		"id": 191,
		"title": "XR in DH: Extended Reality in the Digital Humanities",
		"authors": [
			"Hendery, Rachel",
			"Kasra, Mona",
			"Licastro, Amanda",
			"Ramey, Lynn",
			"Rockwell, Geoffrey",
			"Szabo, Victoria",
			"Kaufman, Micki"
		],
		"body": " Driven by the gaming and entertainment markets, Extended Reality, which includes Augmented Realityand Virtual Reality, continues to expand rapidly in both its capabilities and its consumer adoption. Once the sole province of large installations, rack computers and bulky and expensive headsets, a wide range XR / Augmented / Virtual Reality experiences are now available on a variety of platforms and appliances, whether as standalone, wireless headset units, or mobile apps. A common thread of the use of XR representations and techniques is that they are at once both extremely complex and stunningly intuitive both to render and to interpret. The same paradoxicality can be said of some aspects of digital humanities research. Indeed, the questions that Digital Humanities research and pedagogy address are often highly complex, attributable sometimes as much to their technical difficulty as to the interdisciplinarity of thought they represent. As scholars seek to understand and embrace such complexities, they have begun to use tools that favor experiential, rather than narrative, representations of scholarly work, such as XR. The ability for DH to flourish while comprising such internal contradictions suggests the capabilities of multidimensional technology to distill and refine the essential points of complexity by articulating them in those dimensions. In this manner, multidimensional scholarship seeks to reveal the underlying essence of DH projects by employing rich, deep and immersive experiences in pedagogy, data visualization, modeling and simulation.This panel will present recent research and pedagogy in literature, anthropology, and other humanities subfields using XR. This panel explores the application of virtual and augmented reality technology in digital humanities research and education by a number of DH scholars. Dr. Rachel Hendery will describe work in spatial representations of text and linguistic data in VR, Dr. Lynn Ramey will present on the experience of a number of student groups using Unity and VR. Reflecting on several creative VR projects, Dr. Mona Kasra will explore the process of creating new collaborative, methodological, and pedagogical frameworks that facilitate hybrid research/practice projects in arts and humanities scholarship. Dr. Amanda Licastro will demonstrate the potential of using VR to teach literary analysis in the undergraduate classroom. Dr. Geoffrey Rockwell will look at the design of AR serious games and the learning both in the design and playing and Dr. Victoria Szabo will describe work at establishing evaluative mechanisms and criteria for XR projects. VR for outreach / VR for research Dr. Rachel Hendery In a recently funded project about pre-colonial contact between Australia and the surrounding region, we have begun adapting a virtual reality project we developed previously for public engagement use, to recreate it as a platform that researchers can use to interrogate their data. The earlier version of the project, named Glossopticon had been very successful in museums, university showcases, and other public events, for allowing students or the general public to explore Pacific islands, see a visual representation of the incredible multilingualism of the region, and to hear spatialised audio of the languages in question. The data was drawn from the PARADISEC database, and links to items in the database were automatically saved as the user explored the VR, being presented to them afterwards in a webpage, so in effect Glossopticon functioned as a gamified user-interface to the database. For the new project, which we have named Layered Horizons, we wanted a way to be able to layer data relating to the linguistics, archaeology and anthropology of the region onto a 3D map, and for users to be alerted to relationships between locations that the data suggested might be important, in particular when these suggestions are the result of correlations between the different kinds of data. Through an iterative process involving collaboration between linguists, anthropologists, archaeologists, and designers, as well as successive user-testing phases, we have settled on a more abstract look and feel to the map, with point-cloud landscapes that can be warped and shifted to indicate relationships between place, and controls entirely based on natural gestures, interpreted by the LEAP Motion, rather than relying on controllers of any kind. In this talk I will explain these design decisions, showcase the latest version of the project, and discuss the ways in which the platform has enabled research discoveries so far. Glossopticon Layered Horizons Audience as Performer in Immersive Virtual Reality Experiences Dr. Mona Kasra Immersive media such as virtual reality, augmented reality, and mixed realityare shifting the long and complex relationship between audience and performer, stage and screen, and live and mediated performance. These emerging platforms continue to offer new experiential affordances that bring together performers and audience members in a 360-degree, virtual environment, reimagining the perceptual continuity of live performance. In theater, dance, and other live performing arts, the liveness of the interaction between human performer and human audience is deemed essential as the audience stays in close physical and temporal proximity to the performer at the moment of their performance. In a mediated performance, however, the live interaction between the audience and the virtual performer is achieved only when the audience engages with disembodied digital data in real-time according to the size and configuration of their viewing devices. As Philip Auslander maintains, the idea of what counts culturally as live experience changes over time in relation to technological change. In VR/AR/XR, liveness is realized through digital interaction and participation in the actual performance. By inviting audience members to partake, create, and become performers themselves, immersive experiences afford new experiential control and new sets of possibilities for live engagement. Surrounded by a multisensory and immersive environment, the audience can experience a mediated performance from different perspectives, reposition themselves within the space where the performance takes place, or even contribute to the performance itself. They can become performers, freely moving and interacting with the performance in real-time, using motion capture and sensing systems. This paper explores the ways by which VR, AR, XR redefine the performer-audience relationship by pushing the boundaries of live performance and extending immediacy and intimacy into the virtual space. Drawing on several creative VR project examples varying from immersive dance performances to interactive VR installations, I will examine the process of creating new collaborative, methodological, and pedagogical frameworks that facilitate hybrid research/practice projects in arts and humanities scholarship. I will describe some of the critical outcomes of these projects as they pertain to the ongoing debates concerning presence, liveness and virtual embodiment and reflect on representational, affective, and creative possibilities of immersive media for arts and humanities research and teaching. This talk will also focus on the interdisciplinary nature of the medium and on how multimodal approaches offer new opportunities for practice-led research. Teaching Narrative and Literary Analysis with VR Dr. Amanda Licastro In this presentation the speaker will explicate a series of assignments implemented in an undergraduate course which used Virtual Realitytechnology to teach literature. The aim of the experiment was to use VR in order to guide students through the process of understanding narrative structure and critical themes present in Mary Shelleys Frankenstein. In honor of the 200th anniversary of the 1818 version of the novel, students created VR applications inspired by one of the themes explored in the original work. Throughout the semester students read literary works and criticism such as Jennifer Haleys The Nether, Donna Haraways The Cyborg Manifesto, and Lennard Davis On Normalcy along with viewings of The Stepford Wives and the San Junipero episode of Black Mirror [...] as well as Lincoln in the Bardo, the first VR application adapted from a novel . These experiences culminated in a final project that instructed students to pitch their idea for an educational VR application that could be used to teach a theme from the novel. The proposal had to be clearly related to Frankenstein, with accessible learning objectives backed by literary criticism. Students then worked in groups in create prototypes and a formal proposal for the top three projects. Each group presented their proposal to a local educational VR company, who then choose a winning project to turn into a full fledged VR application. This real world, client-based assignment prepared students from a wide range of academic disciplines to compose in a variety of multimodal genres while engaging in close reading and literary analysis. This presentation will feature the scaffolded assignments backed by pedagogical theory stemming from writing studies and DH scholarship. Sample student projects will be displayed, and selections from student reflection papers will be shared as evidence of the success of this project. VR, Unity, and Student Groups Dr. Lynn Ramey Working with students on research projects involving virtual reality in the humanities classroom has become significantly easier in recent years. Free or inexpensive platforms and countless tutorials aid students with their assignments. Visual scripters allow students to make a VR world with little or no programming experience. VR is rapidly becoming more portable, delivering the experience to any classroom without excessive or expensive technology via emerging platforms and gaming devices readily available in many households. Despite the incontrovertibly rosy outlook for VR in educational environments, obstacles still exist. As projects become more and more complex, more powerful computers with better graphics cards and extended storage are needed to make a smooth experience for users—technology that is not readily available to all students at every institution. While visual scripters can certainly get a reasonable project started, programming skills are imperative for accomplishing the complex goals of certain student projects. Even when the environment has been perfected on a student computer, the steps to build the project for a particular device are complicated and can require specific licensing. Finally, students can find teamwork and communication challenging, and putting together an appropriate team for a VR project involves students with very different skills and interests working together, often outside of class while juggling busy schedules. The communication will conclude with future directions for use of VR in the humanities classroom. What lessons were learned from these classes, and how can we provide a meaningful and reflective experience for humanities students of various backgrounds? What tools were most useful, and what remains on the wish list for educational VR development? Campus Mysteries: Playing with Serious Augmented Reality Games Dr. Geoffrey Rockwell With the release of Pokémon GoAugmented Realitygames have gone mainstream. AR games on mobile devices use the built in GPS to move play out into the community letting players catch items at locations in a treasure hunt style of play. The Campus Mysteries project at the University of Alberta was an experiment in designing AR serious games for learning local history that started in 2009. In July of 1918 the Spanish Flu hit Canada. This epidemic was one of the worst in human history killing between 20 and 40 million people around the world. It came to Edmonton, Alberta in October of 1918 and the University of Alberta closed down for around two months. The Campus Mysteries project created a ghost hunting game for summer students to teach this local history. The game was developed by a team of Digital Humanities, Education and Computing Science faculty and graduate students. It was developed with an AR authoring environment called fAR-Playwhich we iteratively developed along with the games. In this short presentation I will talk about how we evaluated Campus Mysteries game and other AR games developed. The AR version of Campus Mysteries was tested against two control versions, one that used QR codes and one that used paper. The experiment showed the viability of AR serious games for learning, but also showed how the technology can be a hurdle if it isnt used properly. A further limitation is the treasure hunt paradigm which most locative games use. We need an expanded repertoire of understood game genres to better match the issues dealt with. On that note, I will conclude by talking about the development of the game and the development of the authoring environment as themselves a form of playful learning. Evaluating XR: Standards for an Emerging DH Medium Dr. Victoria Szabo Evaluating scholarly work in XR poses a challenge to conventional academic practice in ways that are both familiar to DH practitioners and which amplify some of the biggest challenges to its wider acceptance as scholarly work. This talk outlines some of the Evaluation Best Practices initially proposed in 2013-14 and elaborated most recently in a 2018-19 Institute. Our goal is to augment the general guidelines provided by scholarly organizations like the MLA, CAA, and AHA around digital scholarship to account for the specific challenges XR offers. The contributors to the XR Evaluation Standards project come from a wide range of humanities disciplines, including Art History, Archeology, History, Journalism, Languages, Literature, Media Arts, and Music as well as Scientific Visualization, Computer Science, and Architectural Engineering. Their application interests include historical reconstruction, cultural heritage, digital storytelling, language instruction, representation of fictive spaces, media performance, and data visualization. They work in immersive CAVE spaces, high- and low-end headsets, Cardboards and mobile devices. Content includes 3D models of buildings, objects and landscapes, 360 video, abstract data visualizations, ad hybrid media installations. The group converged around three key meta-principles for evaluation: Integrity, Integration, and Impact. Integrity relates to the ability of the material to interpret and transmit source materials, and the extent to which it surfaces an original argument. Interaction focuses on the extent to which the interactive elements aid in interpretation and discovery. Impact refers to the works originality, the extent to which it affects the conversation in the disciplinary field, as well as for the XR community. Added to these transdisciplinary principles for evaluation are considerations derived from the specific disciplines related to the medium of XR itself: art/design practice, info science, sci viz, communications studies, computer science, cognitive science. To what extent can and should practitioners using such tools be judged according to these specialist standards? Our team suggests a range of potential measures arising out of a more scientific perspective on the use of the technology itself. These include: user interface and experience design, graphic design, response time, accuracy, presence or absence of negative effects, biometric measures of presence in virtual spaces, memorability, comparative analysis with other forms, emotional impact and more. In terms of XR as a humanities endeavor, evaluation standards might include: the extent to which the project pushes against the usual constraints of the medium to signal ambiguity and conflicts, uncertainty, temporal change, the position of the observer, access to underlying data, and visible engagement with a scholarly argument. This dialog between scientific and humanistic communities becomes especially tangible within the collaborative contexts where XR work is being done. Our hope is that by surfacing a range of evaluation measures that draw upon the values of both communities, that we can further a conversation around what XR can do for us in the humanities, and what we in the humanities can do for it as an emerging media form. References Hendery: Pescarin, Sophia et al. 2013. NICH: a preliminary theoretical study on Natural Interaction applied to Cultural Heritage contexts. 2013 Digital Heritage International CongressVol 2: 28 October - 2 November . https://ieeexplore.ieee.org/document/6743760 Thieberger, N., & Barwick, L.. Keeping records of language diversity in Melanesia, the Pacific and Regional Archive for Digital Sources in Endangered Cultures. Melanesian languages on the edge of Asia: Challenges for the 21st Century, 239-53. Thieberger, Nick & Rachel Hendery. 2017. Sonorising and visualizing archive records. 5th International Conference on Language Documentation and Conservation, Hawaii. Full paper available at http://hdl.handle.net/10125/41966 Kasra: Arora, Gabo.. ZIKR: A Sufi Revival. VR film, Sundance Film Festival . Auslander, Philip.. Digital Liveness: A Historic-Philosophical Perspective. PAJ: A Journal of Performance and Art , 34:3-1. Iñárritu, Alejandro G.. Carne y Arena. VR installation, Cannes Film Festival . Licastro: Davis, Lennard. Constructing Normalcy. http://blogs.fad.unam.mx/asignatura/adriana_raggi/wp-content/uploads/2014/05/Davis.pdf. Haraway, Donna. A Cyborg Manifesto. http://www.egs.edu/faculty/donna-haraway/articles/donna-haraway-a-cyborg-manifesto/. Accessed 20 Aug. 2014. Haley, Jennifer. Nether, The. Samuel French, Inc., 2015. San Junipero. Black Mirror. Written by Charlie Booker, directed by Owen Harris, Netflix, 2016. Shelley, Mary. Frankenstein, Second Edition. Edited by D. L. Macdonald and Kathleen Scherf, 2 edition, Broadview Press, 1999. NYTVR application Lincoln in the Bardo Rockwell: Humphries, M. O.. The Horror at Home: The Canadian Military and the Great Influenza Pandemic of 1918. Journal of the Canadian Historical Association / Revue de la Société historique du Canada , 16:1, p. 235-260. Rockwell, G. and S. Gouglas. Experiments in Alternative / Augmented Reality Game Design: Platforms and Collaborations. Seeing the Past with Computers: Experiments with Augmented Reality and Computer Vision for History . Ed. K. Kee and T. J. Compeau. University of Michigan Press, p. 158-175. "
	},
	{
		"id": 192,
		"title": "Extracting Drum Patterns in Traditional Folk Songs Among East Japan",
		"authors": [
			"Kawase, Akihiro",
			"Kuwahara, Miku"
		],
		"body": " Introduction The musicologist Fumio Koizumi analyzed the beats of traditional Japanese musicand found that the strong and weak beat corresponded to the concept of omoteand uraof traditional Japanese music, respectively. He revealed that space has been created for a set of two notes with those front beats and backbeats. In addition, Koizumi classified the rhythm pattern of Japanese traditional music into two types: the freeform Oiwake style and the clear rhythms of the Yagibushi style. Since the Yagibushi style has a rhythm pattern with a constant interval, this style tends to appear in song accompaniments using Japanese drums. However, studies that capture the rhythm patterns of traditional Japanese music theoretically, through empirical analysis, have not been conducted. At present, not only rhythm studies but also studies that capture the melodic characteristics have not been performed either; therefore, it is unclear how the characteristics of Japanese music propagate and transform. In this research, in order to empirically capture the rhythm patterns hidden in traditional Japanese music, we focused on folk songsand analyzed the rhythm of the Japanese drums. This study aimed to compare the rhythm patterns of Japanese drums in the folk music of East Japan quantitatively and to clarify the rhythmic features of each region. Overview of the data We analyzed 589 songs using Japanese drums from the East Japan folk songs published in  Nihon Minyo Taikan. Specifically, we selected all songs using one of the following: oh-daiko, ko-daiko, koshi-daiko, daibyoshi, and taru. Fig.1: Map of East Japan and number of songs used from each region Procedures We converted all 589 songs into MusicXML format and created a music corpus. Musical notes and rests of the drum parts were extracted from each XML data and were replaced with a symbol string. Then, N-gram analysis was executed on this symbol string, and frequent fixed-length patterns were counted. Additionally, by creating co-occurrence networks between notes and conducting network analysis, we clarified the connection of elements constituting rhythm patterns in each region. By comparing the above analysis results with the findings of Koizumi, we clarified the characteristics of Japanese drum patterns in the folk songs of East Japan, which have been passed down over generations. Results and Discussions Comparing the results of the N-gram analysis, we found that the rhythm patterns of Hokkaido and Hokuriku were different from the other three areas. From the results of this summary, Hokkaidos rhythm pattern had characteristics of notes and rests being combined at the same degree, unlike in other areas. In Hokuriku, from the result of the unigram, the pattern with consecutive notes was overwhelmingly high, which differed greatly from other regions. Also, rhythm patterns combining 1/8 and 1/16 notes occurred frequently, indicating that there was a tendency to tick small rhythms, unlike the other three regions of Honshu. These results were similar to features found in chanting and Buddhist music, which strike at regular intervals. It can be pointed out that there is a high proportion of people who believe in Jodo sect and Jodo Shinshuin Hokuriku. The result of the degree centrality for the whole of East Japan showed that 1/4, 1/8, and 1/16 notes were central elements within the drum patterns. This result overlaps with that of Fraisse, showing that 80 percent of the melodies in Western music are occupied with only two kinds of note values. Regarding rests, we found that 1/2, 1/4, and 1/8 rests were high in terms of degree centralities, but that the 1/2 rest was low for betweenness centrality. Therefore, unlike other rests, a 1/2 rest was more likely to have the role of concatenating coherent rhythm patterns than as an actual component of the rhythm patterns in traditional Japanese folk songs. As a future task, we will extend the song data and conduct an analysis for each region of West Japan. By conducting a comparative analysis with the features of neighboring countries, we will develop basic research to demonstrate the relationships, as well as the propagation and transformation of rhythm patterns, between regions. "
	},
	{
		"id": 193,
		"title": "Getting Things Done: Administrative Tips, Tricks, Helps, And Hindrances In Digital Scholarship",
		"authors": [
			"Keegan, Tom",
			"Gehlsen Morlan, Leah",
			"Leonard, Peter",
			"DeRose, Catherine"
		],
		"body": " Occasionally acknowledged and rarely discussed in conference settings, the administrative structures underlying digital humanities work remain an area of discovery for many faculty, staff, and students. While these structures and their attendant physical spaces, staffing, and workflows undoubtedly vary from one institution to the next, there remains considerable overlap between schools in terms of day-to-day challenges and long-term planning and strategy. These shared challenges have traditionally been regarded as something to simply handle in an ad hoc or institutionally idiosyncratic way. For far too long, they have been eschewed as something beyond the pale of scholarly endeavor when, in fact, they are crucial to the development, implementation, and preservation of digital humanities projects. From grant writing to project management to departmental politics, the administrative side of DH work remains shrouded in considerable mystery. Too often questions that begin with How do you deal with… or What do you do when… are relegated to conversations at the hotel bar. In an effort to bring those questions and their various answers into broader conversation and to demystify the work of DH administration, Yale University Librarys Digital Humanities Lab and the University of Iowa Libraries Digital Scholarship & Publishing Studio lead a half-day workshop addressing the day-to-day and infrastructural challenges and opportunities within the administration of digital scholarship centers, units, and initiatives. Designed for staff-side administrators—whether they be a singular Digital Humanities Librarian charged with serving an entire campus or a collection of people working within a clearly defined center—and welcoming of interested faculty and graduate students, the workshop will engage participants in a frank, pragmatic set of discussions and exercises. Topics addressed will include: org charts; working across units and institutions; the administrative politics of turfiness; GLAM collaborations; public engagement workflows; staff agency; and advocacy of others in service to getting things done. Regarding the workshops suitability for faculty and graduate students, we highlight a stark reality in higher education. While graduate students are pursuing a growing number of administrative positions within the digital humanities landscape, instruction in the area of administration is rarely found in graduate humanities curricula. Likewise, on many campuses the work of administration is split between academic and managerial workflows involving both faculty and staff administrators. Elsewhere, faculty may find themselves newly arrived on a campus that provides a degree of administrative support that differs from their previous institution. In each case, understanding the variety of administrative structures and the staff-side considerations underlying them, serves to better prepare all parties for fruitful digital humanities collaboration. In service to these would-be and current administrators, this half-day workshop addresses five main areas of administrative infrastructure and project management, providing attendees the opportunity to share and explore administrative models, project workflows, and institutional networks. Space / Equipment / Architecture / Facilities Workshop leaders will allow time for participants to assess their own space and equipment needs based on their specific organizational missions and campus needs. We will address inherent challenges in the acquisition and development of physical space on campus and explore creative problem-solving processes to address those challenges. Because the workshop is intended for centers of all sizes, we aim to discuss space, both physical and abstract, methods for identifying and obtaining equipment, and how smart use of facilities, staff, and collaborations may allow for solutions to unresolved issues. Organization/ AdjacenciesStaff and institutional size will determine much of the session on internal organization and external opportunities for digital scholarship centers. Because attendee experiences and institutional structures will likely vary greatly, we will lead an informal conversation detailing our own experiences, and then allow for an exploratory brainstorming discussion with participants. Strategies for establishing connections within and across departments, particularly regarding shared services and resource awareness, will be emphasized. Accountability / Client Management / Expectations Once established, digital scholarship centers are typically committed to project development of some kind, including but not limited to application development and pedagogical support and tools. This work is generally collaborative and interdisciplinary, which may lend itself to goal and expectation discrepancies. Development of management processes and distinct infrastructure devoted to this consistent and iterative project work proves efficient and effective. To that end, participants will consider their project needs and priorities, and workshop leaders will discuss various ways to manage and document project timelines, objectives, and client expectations. Lifecycle / Archiving / Preservation / Long-Term Support Determining a completion point for existing projects can be a challenging endeavor, due mainly to the ever-evolving nature of humanities research. Once an end-point has been determined, it is equally challenging to determine a projects lifespan and subsequent appropriate archival treatment. During this section, participants will be asked to discuss plans, preferences, and resources for archiving digital humanities projects. Considerations of institutional capacity, as well as scholarly ownership and how projects follow their PIs will be addressed. Promotion / Marketing / Awareness / Outreach / Engagement Creating campus awareness of digital humanities projects and tools allows for attention to resources developed as a result of digital scholarship, as well as resources available to those who wish to further explore digital scholarship. Dedicated staff and resources arent always available for promotion, particularly in higher education, making it hard to elevate it to a prioritized level. With the assistance of workshop leaders, participants will explore models for promotion of and engagement with digital humanities projects. We will discuss possibilities at all promotional levels, from small- to large-scale. "
	},
	{
		"id": 194,
		"title": "Accelerating DH Education",
		"authors": [
			"Bleeker, Elli",
			"de Groot, Johanna",
			"Kelly, Aodhán",
			"Schophuizen, Martine",
			"Wyatt, Sally"
		],
		"body": " Introduction This workshop will allow DH practitioners to exchange experiences in DH education, and to voice and explore ideas and hopes for the near future. The topic of education is explored, firstly, from the perspective of the pedagogies for DH practices and, secondly, regarding the ways in which DH resources facilitate learning via digital means across broader society. The workshop seeks to create a forum that brings together DH practitioners with specialists in educational sciences and digital innovation. The workshop is open to anyone who is involved or interested in the creation of educational resources in DH. The organisers have made a special effort to include specialists from outside the DH community with expertise in educational sciences and digital innovation. The outcome of this workshop is primarily one of community building, whereby DH practitioners interested in pedagogy can discuss ways to move forward in the near future and to potentially consider the idea of composing a shared strategic agenda on this topic. Background Well over half a century since the beginning of computational approaches to the humanities, digital methods have become more and more embedded in the academic establishment. The primary outputs of much DH work are of course the digital editions, archives, libraries, databases and various tools that, among other purposes, can act as educational resources for the humanities. Additionally, Digital Humanities provides pedagogical resources for its practitioners in an abundant multitude of forms. A clear example of this extreme diversity is the Digital Humanities Course Registry maintained by DARIAH and CLARIN. This list is one example of attempts currently underway to bring this complex web of educational resources into some sort of manageable form, further examples include the online learning platform dariahTeachand the MLAs Digital Pedagogy in the Humanities. It comes as no surprise that such a vast array of educational resources have been developed in DH. The community is characterised by its high level of interdisciplinarity, its commitment to digital innovation and experimentation, and a strong DIY mentality. However, it might also be fair to say that the approach to pedagogy in the creation of such digital tools has been largely ad-hoc. The valuable efforts after-the-fact to map the complex web of educational materials mentioned above opens up the question of what DH would look like with more shared and planned pedagogical strategies. Over the last two years a conglomeration of all research universities and universities of applied sciences in the Netherlands drew up and agreed upon what was called the Acceleration Plan for Educational Innovation with ICT. This four-year plan identifies eight areas in which Dutch universities could benefit from a shared approach to educational innovation on a national level. These include transitioning to digital open teaching aids and materials; and evidence-based educational innovation with ICT. Sessions in this workshop will focus on both the question of openness in DH educational resources and the extent to which pedagogical approaches can be seen as evidence-based. Using this strategic plan as a focal point and inspiration for DH, the workshop asks the question: If we innovate together strategically, using open and evidence-based approaches, can we achieve more as a community in DH education? Topics Topics of the workshop includethe following: DH pedagogy in practice We intend to take stock of existing types of DH educational resources, course materials and pedagogical approaches, focusing largely on the question of how DH practitioners develop their skill sets and on the resources that are available to them. On what grounds can a selection of necessary skills be made, working towards establishing foundational DH skills? At the same time, can we successfully deal with the persistent view that one needs to know everything, from traditional humanities to computer science skills? Furthermore, we consider the pedagogical strategies that are being implemented to teach DH skills to students. This includes identifying the strengths and weaknesses of the current status quo, and looking at the possibility to evaluate the effectiveness of teaching through an evidence-based discussion. Topics may include: DH methodologies Coding and text encoding Programming languages Tools and tool evaluation/criticism Data modelling Interdisciplinary collaborations; collaborative grant writing Open Education This topic explores the question of openness in DH educational resources both from an educational sciences perspective and from the experience of developing and running open DH education in practice. What does open mean in the context of Digital Humanities education? The term open has been applied to a wide variety of contexts, including government, science, data and education. The increased use of the term open has made its meaning become increasingly ambiguous, which could also lead to misinterpretation and unrealistic expectations in terms of outcomes. In other words, open often means different things to different social actors, confirming an early statement of Hylandwho described the field as eclectic. In this part of the session we will address the term in its multiple formulations, zoom in on the aspects that could be of relevance to Digital Humanities education and discuss some of the implications. MOOCs in DH There are increasing numbers of MOOCs being developed by the DH community. Recent examples include those available by the dariahTeach platform, which provides courses on introducing DH, TEI encoding, digital scholarly editions, digitisation and multimodal literacies. Many of the MOOCs from the DH community are developed with a certain consideration and awareness of good pedagogical practices, to the extent that there even exists a meta-MOOC or a MOOC on MOOCs. This part of the session will include guest speakerthat have been recently involved in the development and implementation of a MOOC in DH to discuss their experience about this format and engage in debate around the theories of open educational resources. Agenda for innovation The focus of this topic is to identify potential shared strategies for the future of DH Education. The aforementioned Dutch Acceleration Plan for Innovation in Education with ICT will form one of the focal points and will be discussed in more detail both on a conceptual and content level as an exercise in strategic thinking. By doing this, the aim is to explore if this kind of method could be applied to DH and what that might look like, in other words, how to practically formulate a shared strategy for DH education. It will raise a number of questions for debate, such as: Can we assemble a shared set of key desiderata for innovation in DH education? How can we move towards evidence-based innovation in education? Can we promote a structured rather than ad-hoc approach to developing learning resources, do we need to keep it disruptive, or can some sort of balance be found, and how? Format This workshop will take place over a half-day on the morning of Monday 8 July and will include three sessions. Each session will begin with presentations followed by discussions. The final session will be a summative discussion with a focus on future strategies. Participants are welcome to submit in advance a 5 to 10 minute lightning talk relating to one of the three sessions in an area in which they have experience or expertise. OutcomeThe outcome of this workshop is primarily one of community building, whereby DH practitioners interested in pedagogy can discuss ways to move forward in the near future and to potentially consider the idea of composing a shared strategic agenda on this topic. ContributorsElli Bleeker Elli Bleeker is a postdoctoral researcher in the Research and Development Team at the Humanities Cluster, part of the Royal Netherlands Academy of Arts and Sciences. She specializes in digital scholarly editing and computational philology, with a focus on modern manuscripts and genetic criticism. Elli completed her PhD at the Centre for Manuscript Geneticson the role of the scholarly editor in the digital environment. As a Research Fellow in the Marie Sklodowska-Curie funded network DiXiT, she received advanced training in manuscript studies, text modelling, and XML technologies. Johanna de Groot Johanna de Groot is programme manager of the Acceleration Plan for Educational Innovation with ICT that was initiated by the Association of Universities in the Netherlands, the Association of Universities of Applied Sciences and SURF. Johannas main expertise is in project management and educational policy. After working for eight years as a project manager at VU University, she moved to the Hague to work as a policy adviser on education at the Association of Universities. Currently Johanna works at SURF: the collaborative ICT organisation for Dutch education and research. Aodhán Kelly Aodhán Kelly is a postdoctoral researcher at the Welten Institute in the Open University of The Netherlands and visiting researcher at the Centre for Education and Learning at Leiden-Delft-Erasmus. Aodhán completed a PhD in 2017 at the Centre for Manuscript Genetics at the University of Antwerp on Disseminating digital scholarly editions of textual cultural heritage. His doctoral funding and training was provided by DiXiT, an EU funded Marie Sklodowska-Curie network. Aodhán is currently engaged as a researcher investigating the digitalization of learning and education within the Digital Society national initiative of the Association of Universities in the Netherlands. Martine Schophuizen Martines background lies in Psychology and Learning Sciences, she obtained her bachelor and masters degree at Maastricht University. She is now working as a PhD candidate at the Welten Institute of the Dutch Open University in Heerlen. Her research is centered around the question to what extent Open Online Education is embedded in higher learning institutions. She will mainly focus on the organisationalconditions that lead to success, the effect of Open Online Education on the organisation, and the contribution it has towards the quality of education and educational innovation. Sally Wyatt Sally Wyatt is Professor of Digital Cultures at Maastricht University. She originally studied economics. Her main intellectual affinity is with Science and Technology Studies. For many years, her research has focused on digital technologies, both how they are used by people wishing to inform themselves about health-related issues, and how scholars themselves use digital technologies in the creation of knowledge. She is one of the three coordinators of the Digital Society initiative of the VSNU. "
	},
	{
		"id": 195,
		"title": "“Fear in your Eyes”: Analyzing Threat Perception and Its Influence on Deadly Use of Force by Police Officers against Civilians Using Hebrew NLP Tools",
		"authors": [
			"Keydar, Renana"
		],
		"body": " Overview : This study seeks to explore and empirically substantiate the effects of the gap between objective and subjective threat perceptions of police officers in cases of lethal use of force against civilians. It focuses on a newly accessible corpus of digitized transcripts of police officers who testified before an Israeli State Commission of Inquiry in Hebrew. Combining topic modeling and sentiment analysis to explore subjective threat perceptions of duty holders, we aim to establish a deeper, empirically validated, understanding of serious human rights violations. The study makes an important methodological contribution to the field of NLP for non-English languages and non-Latin characters, by applying, for the first time, newly developed NLP tools for Hebrew on a Modern spoken Hebrew corpus. As such, it also contributes to the diversification of the DH community be opening it to minor languages. Background : Police has extensive powers to use force against civilians. The killing of a civilian is also within the legal and legitimate range of force as long as the police officer acts to protect human life or is in mortal danger himself. Justifying the decision to use lethal force should meet the test of proportionality of the reasonable police officer. While the emphasis in previous studies was on the extent of the objective threat posed to the security forces, mainly by analyzing the characteristics of the event, this study takes a broader approach, both theoretically and methodologically, seeking to understand the subjective threat perception of the officers; feelings - of danger, fear or distress – which are crucial in justifying the decision to use of lethal force against civilians. In order to explore, substantiate and understand the gap between objective and subjective threat perceptions, we aim to examine officers testimonies from the protocols of The Israeli State Commission of Inquiry to investigate clashes between security forces and Israeli civilians during the events of October 2000, in which 13 Arab citizens and a resident of Gaza were killed. The Or Commission held 92 public hearings and interviewed security forces involved in the specific incidents, eye-witnesses, politicians, senior police officers, public figures, and journalists. Therefore, examining the totality of the transcripts of the Commissions hearings will enable us to understand in depth the various aspects of the threat perception that lead to the use of lethal force against civilians. Through an empirical semantic examination of the corpus, we intend to point out the ways in which social constructs that shape subjective threat perceptions and influence decision-making regarding the use of force against civilians. Corpus and Methodology : Our corpus consists of the protocols of the Or Commission deliberationsand the Commissions final report. While the Commissions report is available for online review, the protocols and testimonies are not easily accessible. The methodological contribution of the study : First, it develops and applies tools and algorithms of topic modeling and sentiment analysis to a corpus of Modern Hebrew legal transcripts. Recent years have seen a modest rise in studies using computational tools to analyze legal corpora such as constitutions , US Supreme Court decisions or constitutional debates in the US Congress . The scholarly potential, however, is far from exhaustion. In the Israeli field, such research has to overcome crucial language barriers, as in the current state of the field, there are no ready-made or sufficiently-tested NLP tools for modern Hebrew . Our study is first of its kind to employ NLP methods for empirical legal research in Hebrew. Second, it combines topic modeling and sentiment analysis in order to examine subjective perceptions of threat of duty holders. Applied together, these computational methods make it possible to uncover the hidden semantic themes and to identify the emotional and subjective affects expressed by the various voices in the corpus, thus providing an empirical-computational basis for examining the construction of threat perceptions of the security forces. As such, the research presented here is innovative both in the corpus it examines, and in the methodologies that it implements, in order to establish a deeper understanding of serious human rights violations. "
	},
	{
		"id": 196,
		"title": "Historical Dictionaries as Digital Editions and Connected Graphs: the Example of Le Petit Larousse Illustré",
		"authors": [
			"Khan, Anas Fahad",
			"Bohbot, Hervé",
			"Frontini, Francesca",
			"Khemakhem, Mohamed",
			"Romary, Laurent"
		],
		"body": " Introduction: Publishing Print Dictionaries as TEI-XML and RDF The trend towards the retrodigization of print dictionaries -- especially those considered to have a historical as well as a scientific importance -- has been given a new impetus in recent years thanks to improvements in optical character recognition as well as to developments in the creation and promotion of standards and technologieswhich enable machine actionable versions of such texts to be published and shared much more easily than before. Traditionally the Text Encoding Initiativehas been the favoured standard for the encoding of retrodigitised print dictionaries, but there is now starting to be a move towards the publication of electronic editions of print dictionaries as Linked Data A recent project, ELEXIS, which aims to create a platform for accessing and working with dictionary data, and linking senses together across dictionaries in different languages, works with both TEI-XML and RDF versions of editions of dictionaries.. The prior popularity of TEI for this task reflects the dual nature that digital editions of print dictionaries can often have: namely, both as representations of dictionaries as texts, that is as printed artifacts that follow particular typographical conventions and have specific styles of page layout, etc; and at the same time, as computational resources that serve to make the lexicographic and, more broadly speaking linguistic, information contained in the original texts more accessible for querying and further machine processing. On the other hand one of the greatest strengths of Linked Datalies in its core emphasis on interoperability between datasets, heterogeneous as to subject area and type, through the use of a common semantic model, that of the Resource Description Framework, as well as the use of common vocabularies across datasets. Not only does LD encourage the mutual enrichment of individual datasets by facilitating the creation of extensive links between them, it also gives modellers access to a whole ecosystem of Semantic Web technologies and standards, including several out-of-the-box tools for manipulating and visualising structured data. In addition, formal languages such as RDF, RDFS and OWL, which make up part of the Semantic Web stack, allow us to specify and to elaborate on the semantics of the classes and properties used to structure dictionary data. Modelling a dictionary using RDF requires us to represent the information contained within it as a series of subject-predicate-object statements, which taken together describe a formal graph structure. As a consequence it is much less successful -- which in this case means much less verbose -- than TEI at representing things like the layout and formatting of the original text, or properties relating to the status of the text as a series of tokens, as well as in encoding certain kinds of deeply nested structures. This might suggest that RDF is better suited to describing the more abstract linguistic content of retrodigitised dictionaries, e.g., describing grammatical and semantic information for each entry, and for embellishing this content through links to other salient datasets and vocabularies. However as we will see, there often exist aspects of print dictionaries that although strictly speaking they concern how information is presented in the text -- and relate, for example, to the dictionary as a historical artifact -- and not the lexical information contained in the text itself, are still worth explicitly encoding in RDF. This is both because RDF allows us to make this extra-lexical information more accessible and usable and because it helps to ensure that each RDF version of a dictionary is a more self-contained resource. Modelling Le Petit Larousse illustré In order to flesh out some of the issues outlined in the preceding section, especially from the point of view of elucidating the potential benefits of using RDF as a means of publishing historic dictionaries, we will focus on a particular case study which concerns the French national project, Nénufar http://nenufar.huma-num.fr. One of the main goals of Nénufar is to make different consecutive editions of the illustrated French language dictionary Le Petit Larousse illustré, published during the first half of the 20th century, publically available both via an online interface as well as as downloadable TEI-XML digital editions and as a linked data dataset. So far all of the editions of PLI published between 1906 and 1924 have been digitised, encoded and made searchable. The native digitisation format is TEI, although the encoding adheres as much as possible to the TEI-Lex0 format; the conversion of entries into RDF is currently ongoing. The PLI frequently includes encyclopedic information pertaining to its lexical entries along with the more typical kinds of lexicographic data which means that it possesses a strong socio-cultural and historic interest in addition to its significance as a legacy lexicographic resource; indeed, to some extent it can be considered a hybrid resource, dictionary-encyclopedic. Take, for instance, the entry for the word aviation from three different editions of PLI, from 1906, 1912 and 1915 See the different versions of this entry at http://nenufar.huma-num.fr/?article=3807 . Its interesting to track how changes in the entry reflect contemporary discoveries that were taking place in the field of aeronautics at the time. Here the three successive versions of the entry each contains slightly different encyclopedic glosses. Note also that in the last of the three versions of the entry a reference appears to the lexical entry for the word aéroplane. In this case two aspects of the same entry have changed over the course of different editions: the textual content of the encyclopedic gloss and the appearance of a new reference to another entry. As regards the linked data version of the PLI, we made the decision to include as much of the encyclopedic and bibliographic information from the original dataset as possible and to model the evolution of entries across editions since, as we mentioned in the last section, this helps to ensure that the dataset is relatively self-contained -- and prevents users of the RDF version of the PLI having to constantly refer back to the TEI encoding Go tho the resources section of each Nénufar entry to inspect the TEI xml. , something which would go against the universalising spirit of the Semantic Web -- and also because some of this information is well suited to being represented in RDF. By explicitly modelling the editions in RDF using bibliographic and temporal vocabularies and associating each with a specific year, we can query the data for date-specific information. Furthermore we used the well-known Ontolex-Lemon vocabularyfor publishing lexicon-ontologies as linked data as the basis of our encoding in addition to making extensive use of other standards and vocabularies such as the lexinfo registry, SKOS and DEO http://www.sparontologies.net/ontologies/deo . However these did not always provide the properties and classes we needed and so in several cases we decided to create our own. Note that although at the time of writing the Ontolex-Lemon lexicography module is still in the process of being finalised for publicationwe have tried our best to make sure that we dont define any new classes or properties similar to those likely to be in the former. In order to model links between entries we utilised the already existing class Reference from the DEO vocabulary, and defined a new class DictionaryGloss to represent any written explanation of a lexical element in a dictionary. To reiterate, our intention was to model the changes between PLI editions, and indeed in some cases between reprints of the same edition. We decided to model all the separate editions in one graph, since individual changes between entries in different editions usually werent comprehensive enough to warrant a separate graph per edition, and in addition there were also differences between reprintings of the same edition and we wanted to avoid creating too many different graphs. To this end we created a class PLIDictionary to represent separatePLI editions, along with the object properties, appearsIn, firstAppearsIn and lastAppearsIn to allow us to associate elements with different editions. In our RDF encoding of the PLI, then, we model changes within entries by creating an individual entry component, whether this is a form, sense or gloss, etc, for every change and associating it with one or more PLI editions using appearsIn, firstAppearsIn and lastAppearsIn. We will explain the strategy which was followed using the example of the RDF encoding of aviation. In Figure 1 we represent the entry for aviation and its relationships with its senses. Note that we have added information to the entry regarding its first appearance in the PLI by associating it with the individual 1906_001, which represents the 1906 edition of the work, using the property firstAppearsIn. Each of the two senses of aviation has a gloss apiece neither of which changes over different editions in the example. In Figure 2 we show the three notes associated with the aviation entry each of which has been modelled as an individual of the class DictionaryGloss and each of which is associated with a different edition of the dictionary. We are still looking into the best, read most efficient, way of adding information about what is contained in each edition. The simplest way would be to link each lexical element to each of the editions in which it appears, but this would obviously lead to an explosion of triples. Our provisional solution is to focus on adding version information to the elements that change between versions and linking them together using the Dublin Core relation isVersionOf. Figure 1. The PLI entry for aviation and its senses and their gloss. Figure 2. The PLI entry for aviation and the various versions of the entry note associated with it. Future Work At the time of writing we are carrying out the conversion of the dataset into RDF using the approach outlined above. By the middle of 2019 we plan to make whole of the dataset available both via a SPARQL endpoint and downloadable both in RDF and TEI-XML formats. In the final version we also plan to add links to external conceptual/ontological resourcesusing the Ontolex reference property. "
	},
	{
		"id": 197,
		"title": "Kraken - an Universal Text Recognizer for the Humanities",
		"authors": [
			"Kiessling, Benjamin"
		],
		"body": " Introduction Retrodigitization of both printed and handwritten material is a common prerequisite for a diverse range of research questions in the humanities. While optical character recognition on printed texts is widely considered to be fundamentally solved in academia, with the most commonly used paradigmdating back to 2006, this hasnt translated into increased availability of adaptable, libre-licensed OCR engines to the technically inclined humanities scholar. The nature of the material of interest commands a platform that can be altered with minimum effort to achieve optimal recognition accuracy; uncommon scripts, historical languages, complex or archaic page layout, and non-paper writing surfaces are rarily satisfactorily addressed by off-the-shelf commercial solutions. In addition, an open system ameliorates the severe resource constraints of humanities research by enabling sharing of artifacts, such as training data and recognition models, inaccessible with proprietary OCR technology. Kraken The Kraken text recognition engine is an extensively rewritten fork of the OCRopus system. It can be used both for handwriting and printed text recognition, is easilytrainable, and great care has been taken to eliminate implicit assumptions on content and layout that complicate the processing of non-Latin and non-modern works. Thus Kraken has been extended with features and interfaces enabling the processing of most scripts, among them full Unicode right-to-left, bidirectional, and vertical writing support, script detection, and multiscript recognition. Processing of scripts not included in Unicode is also possible through a simple JSON interface to the codec mapping numerical model outputs to characters. The same interface provides facilities for efficient recognition of large logographic scripts. Output includes fine-grained bounding boxes down to the character level that may be used to quickly acquire a large number of samples from a corpus to assist in paleographic research. Kraken implements a flexible output serialization scheme utilizing a simple templating language. Templates are available for the most commonly used formats ALTO, hOCR, TEI, and abbyyXML. While including implementations of all the subprocesses needed in a text recognition pipeline, most functional blocks can be accessed separately on the command line, allowing flexible substitution of specially optimized methods. A stable programming interface allows total customization and integration into other software packages. Recognition Network architectureThe recognition engine operates as a segmentation-less sequence classifier using an artificial neural network to map an image of a single line of text, the input sequence, into a sequence of characters, the output sequence. The artificial neural network employed is a hybrid convolutional and recurrent neural network trained with the CTC loss functionthat reduces training data requirements to line-level transcriptions. Regularization is mainly provided by dropoutafter both convolutional and recurrent layers. User intervention in determining training duration and model selection is largely eliminated through early stopping. Specialized networks, e.g. for particularly complex scripts, can be assembled from building blocks with a simple network specification language although the default architecture shown in Figure 1 is suitable for the vast majority of applications. Processing of dictionaries and library catalogues with extensive semantic markup such as italic, underlining, and bolding, is also possible through specially prepared training data. Layout Analysis and Script Detection Sample output of the trainable segmentation method. Krakens layout analysis extracts text lines from an input image for later processing by the recognition engine. Apart from a basic segmenter taken from OCRopus a trainable line extractor is in the process of being implemented. Full trainability of layout analysis is of utmost importance to a truly universal OCR system, as text layout and its semantics varies widely across time and space, e.g. hand-crafted methods for printed Latin text are unlikely to work reliably on Arabic text or manuscripts with extensive interlinear annotation. The trainable layout analysis module consists of a two-step instance segmentation method: an initial seed-labelling network operates on the whole page labelling the area between baseline and mean of each line. As the output of the network is a probability of each pixel belonging to a baseline it is binarized using hysteresis thresholding after smoothing with a gaussian filter. The binarized image is then skeletonized and end point are extracted with a discrete convolution. Finally, the vectorized baseline between the endpoints is rectified and a variable environment calculated based on the distance of connected components from the labelled area is extracted. The seed-labelling network is a modified U-neton the basis of a 34-layer residual networkpretrained on ImageNet. Preliminary results on a page from a publicly available dataset of Arabic and Persian manuscriptscan be seen in Figure 2. Script detection, the basis for multi-script support in the recognizer, is implemented as a segmentation-less sequence classification problem, similar to text recognition. Instead of assigning a unique label to each code point or grapheme cluster we assign all code points of a particular script the same label. The network is then trained to output the correct sequence of script labels. The output sequence is then used to split the line into single-script runs that can be classified with monolingual recognition models. Original and modified ground truthResults Mean character accuracy Standard deviation Maximum accuracy Prints Arabic99.5% 0.05 99.6% Persian Mid-20th century printing 98.3% 0.33 98.7% Syriac Late-19th century printing in Serṭā form 98.7% 0.38 99.2% Polytonic Greek Late-19th century printing 99.2% 0.26 99.6% Latin98.8% 0.09 99.3% Latin incunabula99.0% 0.11 99.2% Fraktur99.0% 0.31 99.3% Cyrillic 99.3% 0.15 99.6% Manuscripts Hebrew Midrash Tanhuma, BNF Héb 150 96.9% - - Medieval Latin Josephus Latinus, Bamberg 78 with augmentation 98.2% - - Sample output of the script detection on a bilingual French/Arabic page. Note that Eastern Arabic are always classified as Latin text Kraken has been used on a wide variety of writing systems, achieving uniformly high character accuracy. Sample accuracies for a diverse set of scripts spanning across multiple centuries of printing are shown in Table 1. As a special use case we evaluated recognition of text and emphasis in a mixed English and romanized Arabic library catalog on a training set of 350 linesresulting in an averaged CER of 99.3%over 10 runs with 95.38% CER on cursive and text with increased spacing. When using only emphasized text accuracy as the stopping criterium mean accuracy rises to 99.03%. "
	},
	{
		"id": 198,
		"title": "Branding East Asian Cultural Studies By “Opening” Access To Research Resources, Research Groups, and Know-Hows",
		"authors": [
			"Kikuchi, Nobuhiko"
		],
		"body": " 1. Introduction Kansai University Open Research Center for Asian Studiesaims to build digital archives from Kansai universitys East Asian collections and promote East Asian cultural studies. We were selected for one of the research branding projects granted by a government from FY2017 to FY2021, and our mission is to brand East Asian cultural studies through digital humanitiesprograms. In this paper, we will explain our project concepts and current status. 2. Particularity and openness of our East Asian cultural studies KU-ORCAS wants to become an international research hub for East Asian cultural studies through the use of digital archives. It should be noted that East Asian cultural studies in this context does not only cover specific national frameworks such as Chinese history or Korean cultural research, but also a trans-border aspect. This is because our university has a long history of East Asian Cultural Interaction Studies and this field has been the central theme of research. The word trans-border has two meanings. The first is in the context of the national research framework and the second is in the context of academic research fields. Therefore, we must assume that users include those whose research themes are based on cultural relationships across national and/or regional boundaries within East Asia and users who do not have any expertise in East Asian cultural studies. Hence, we adopted an openness policy to make our collection and resources available to a wide variety of people. 3. Materials provided by our digital archives Before explaining the aforementioned concept of openness, we want to describe our digitization plans. Our digital collections are roughly divided into three groups. The first group is comprised of pre-modern resources translated into Asian languages such as English-Chinese dictionaries, Chinese grammar text books written in English, and missionary reports. The second group consists of the Hakuen Bunko archives that were formerly owned by Hakuen Shoin, a famous and influential Chinese school in Osaka from the Edo period to the early Showa period. It was also one of the origins of our university. In addition, the Osaka Art Collections from a local art school in the Edo period that has almost been forgotten will be added to the second group. The third group is composed of materials such as excavation data and drawings relating to ancient Asuka and Naniwazu studies, which have been promoted by our university. The above collections include data from ancient Osaka to modern Chinese history. They show that our research areas and time period are very broad, and hence, our DH programs must embrace a wider vision. 4. Three Concepts of Openness and an Open Platform KU-ORCAS will provide the collections from the standpoint of the three concepts of openness and an open platform. The first concept of openness is the provision of open access to research resources. This refers to the digitization and free provision of materials in our digital archives. In particular, we provide metadata as CC0 and give our collection data the Public Domain Mark. Our university is a member of the International Image Interoperability FrameworkConsortium; hence, we release images complying with IIIF standards. Figure. 1: Sample Image of Our Digital Archive The second is the broadening of research groups. This encompasses cooperation with researchers outside our university, i.e., academic societies, educational institutions, and citizens. In particular, we plan to develop a crowd-sourcing system for transcribing digitized materials through which citizens can easily participate in our research. The third is the opening of research know-hows. We will build a website to disseminate technical know-how and information accumulated through the construction and operation of the digital archives. On site, users will be able to exchange knowledge regarding how to use the data from our digital archives. Furthermore, we will provide instructional YouTube videos that explain how to use our data and create research portals for users to look for DH tools and software related to East Asia cultural studies similar to the DiRT directory. Finally, our open platform will employ the above three concepts of openness and provide a global search engine portal for East Asian IIIF collections. 5. Future Plan We have provided the digital archives which is Kansai University Digital Archive since the last March. In the next fiscal year, we will further expand the collection to include a movie brochures collection in Shanghai under Japanese rule and Chinese ancient wooden character datasets to promote DH research in East Asian cultural studies in Japan. "
	},
	{
		"id": 199,
		"title": "Index Cards and the Analog Humanities. A Media Archaeology of Cultural Studies in Poland",
		"authors": [
			"Kil, Aleksandra Maria"
		],
		"body": " A self-reflexive turn in the digital humanities, also referred to as the third wave, prompts us to critically examine interlocking media and epistemic changes, seeking answers to the intriguing question, posed by Lorraine Daston, of how humanists know what they know. Addressing this issue would entail identifying the ways in which humanist thinking dependson technocultural infrastructures, as well as laying out remediations, affordances, literacies, and shared norms and values pertaining to making knowledge in the humanities. Adopting a media archaeological perspective – in which insights of past and often obsolete technologies inform our understanding of the contemporary media age and vice versa – enables us to analyze the epistemological implications of both the digital and the analog. This paper will explore a yet unexamined archive of paper index cards created and used by Stanisław Pietraszko, the pioneer of cultural studies in Poland, and propose a media-specific and infrastructure-oriented account of knowledge-making in the humanities, especially in the Polish postwar reflection on culture. In examining the artifact – dated back to the early 1950s – and revolving practices as revealed by interviews, narratives, and manuals, the following questions relating to technicality, instrumentality and creativity in the analog humanities come to light: How is knowledge crafted with a slip-box? What does this apparatus consist of? What is the genealogy of its format and standardization? Why were scholarly index cards resisted or criticized by some and, on the other hand, what made the cards so eagerly adopted by others? What kinds of humanistic work was recognized as valuable thinking, as opposed to mundane, auxiliary labor? And, finally, can index cards affect the style of thinking, foster or even enforce any specific way of reasoning? Building upon Lisa Gitelmans idea of paper knowledgeand Markus Krajewskis study of Zettelkästen as a storage medium and a universal paper machine, I investigate index cards as a knowledge-making device. Apart from its more obvious applications in bibliography and libraries, index cards can also be understood as a part of academic infrastructuresand) or epistemic surroundings) typical of the humanities, especially in the fields such as philology and history. I am interested in what can be exposed through examination of the cards about intellectual work in the humanities, especially in the context of postwar Poland. While seeking to grasp a local context, my analysis encompasses a broader view of the tool, referring to working habits and file cards of the luminaries such as Roland Barthes, Niklas Luhmann and Claude Lévi-Strauss. Framing my research in a media archaeology perspective, I propose to read the paper card index alongside and against digital media. A general trajectory of my thinking is threefold and goes fromthe digital humanities and their critical insight into knowledge devices and epistemic infrastructures tothe analog humanities and their uses of paper technologies andback to the digital again, looking into how the index cards have been repurposed by electronic media. This project is media archaeological in several respects. Firstly, it sets out to explore the materiality of scholarly production of Pietraszko, revisiting old and purportedly discarded technologies of intellectual labour. In a worms eye view it concentrates on snippets, sketches and scribbles. Additionally, technological change is not viewed here in terms of imminent progress, i. e. boxes of paper slips, deemed an instance of cultural theorists mindware, are not simply the ancestors of the cutting-edge software employed nowadays by scholars. Last but not least, the nature of such an endeavour is in fact an-archaeological), since an interest in Pietraszkos legacy is not particularly strong even in the Polish humanities and could be revived by this study. I adopt a notion of the analog humanities, aiming to use it in a more rigorous, theory-laden sense rather than simply in a metaphorical way, usually signaled by the quotation marks around the term. I would like to instill a kind of reflexivity or a retrospective, pre-posterous logic in it. As Jonathan Sterne argues, the analog humanities refers to a nexus of methodological, technological, and institutional conditions across the humanities that have only come into clear focus in retrospect and in this sense the term serves as a rhetorical before. Elsewhere, in his entry on the analog in the Digital Keywords he claims that we should return some specificity to the analog as a particular technocultural sphereand not treat this category as a blurry term denoting everything being not digital. Therefore, my paper follows a similar logic seeing the analog humanities as a concept facilitated by the digital humanities and identifying a concrete technocultural condition of scholarship. At the same time, I understand index cards as a constitutive element of the analog humanities, which is not everything that preceded the digital age, but rather a media ecology of print, typewriters, sound recordings, transparencies, overhead projectors, copiers and microfiches, etc., used in the humanities, especially in the second half of the 20th century, to which my case study points to. "
	},
	{
		"id": 200,
		"title": "Measuring Bias in Aggregated Digitised Content: a Case Study on Google Arts and Culture",
		"authors": [
			"Kizhner, Inna",
			"Terras, Melissa",
			"Rumyantsev, Maxim",
			"Khokhlova, Valentina",
			"Demeshkova, Elizaveta"
		],
		"body": " Large cultural heritage aggregators, such as Europeana and Google Arts and Culture, collect metadata and images from cultural institutions. They provide a single portal that introduces cultural heritage from around the world to the public. Selecting images and artifacts for these aggregators is an outcome of curatorial decisions, enlarging an art canon, building a cultural capital, and providing an infrastructure for a corpus of art history imagesthat is critically important for the research in Digital Humanities. However, are such portals indeed a representative and balanced collection, the foundation for objective humanistic study and judgement? In this paper we argue that diversity, although present in GA&C, is too narrow to support our hope that it can act as a corpus of digital art history images. Our evidence proves that the digital corpus amplifies biases within the arts world towards western culture. Methodology The source of GA&C collections Our analysis is based on the full collection of two-dimensional images with metadata available on GA&C web site, excluding videos and street view panoramas. All our numeric estimates below are based on Collections. We accessed collections via Places option in the GA&C menu and identified the size of the collections submitted to GA&C and the country of the collection holders. Russian collections in GA&C GA&Cs collections from Russia are relatively smallsupplied by 49 institutions. We have identified geographical location of all 49 GA&C contributors from the Russian Federation and for the 32 from them that are listed in the official museum registry of the Russian Ministry of Culture and are currently tabulating genres, periods and authorship of the 2,802 images that these museums supplied to GA&C. This is a limited share of the artifacts held by over 2,000 Russian museums that altogether hold about 60,000,000 objects in the main part of their collections. French collections in GA&C We identified 87 French collections that we used as a control group and compared them with the list of over 1,200 French museums downloaded from the web site of the French Ministry of Culture. We found 19 museums from the list of the Ministry with 4,477 images of objects. We understand that two countries can be hardly enough to find out how representative GAC is and further research is needed to cover a larger sample of countries. Results Is Google Arts and Culture a balanced corpus? Western museum collections have traditionally been biased in approaching art objects in the frames of western aesthetic/cultural conceptsand the bias continues in new digital aggregators. Table 1 shows that the majority of images come from the top five countries at the time of writing this paper. It demonstrates that a vast majority of providing cultural institutions come from the United States, United Kingdom and the Netherlands. The largest collectionis the LIFE Photo Collection, New York, with 4,5 million images. The fifteen countries to follow the top five are represented with a much smaller number of objects. However, all the countries from the list of the United Nations are represented in the aggregator either as countries with institutional collections or countries tagged as the places of origin of cultural objects. Our analysis shows that 123 countries out of 195 nations are not represented in the aggregator through their collections but can be referred to as the places of origin. Table 1 The number of records with images for the institutional collections representing the places related to the names of countries in GAC. The images from the top five countries collections account for 93% of images Country No of images, institutional collections % of total USA 4,713,779 82 % United Kingdom 334,558 5.8 Netherlands 170,201 3 % Italy 98,225 1.7 % South Korea 52,214 0.9 % Other countries337,198 6.6 % Table 2 The images from the top four collections account for 88% of images Collections Country, city Number of images, institutional collections % of total LIFE Photo Collection USA, New York 4,403,372 79.3 The Natural History Museum UK, London 298,804 5.4 Rijksmuseum Netherlands, Amsterdam 164,510 3 The Strong National Museum of Play USA, Rochester 72,556 1.3 Other collections Other 615,113 11,1 Do smaller collections represent provincial museums to demonstrate diversity? Our results demonstrate that although the aggregator can be considered a representative corpus at the scale of nations, it is by no means a balanced corpus. We show that provincial museums take a small portion of the museums represented by the aggregator for the two countries in the study. The paper will provide the distribution of genres, periods, geographical regions and artists when the study is completed to be presented at Digital Humanities 2019. Conclusion We demonstrate that GA&C is a representative collection of images as it includes at least two images from the cultural institutions of every country recognized by the United Nations. However, our results indicate that five countriescontributed the largest share of images. Our hypothesis is that GA&C tends to work with the museums that are either easier to work with due to common legislation structures, or similar attitudes and languages, those museums that are more accessible in terms of opening their collections or with private museums that are interested in making their collections known. Further research is needed to provide evidence in support of this assumption. Introducing cultural heritage that was previously difficult to access seems to be the task that new platforms perform fairly well. However, the case of GA&C demonstrates that this digital corpus tends to reinforce traditional biases of western curatorship. This results in important consequences for large scale research in Digital Humanities when obtained patterns are skewed towards western aesthetic and cultural concepts. It may also prevent the general public from building a cultural capital based on cultural diversity. "
	},
	{
		"id": 201,
		"title": "QuoteSalute - Inspiring Greetings for Your Correspondence",
		"authors": [
			"Klappenbach, Lou",
			"Dumont, Stefan",
			"Neuber, Frederike",
			"Philipp, Luisa",
			"Pohl, Oliver"
		],
		"body": " Spice up your e-mail with open access research data quoteSalute aggregates salutesfrom various openly available digital scholarly editions of letters based on the encoding of the TEI‑element <salute>. The project website hosts a corpus of curated salutes, so they can be copied into an e-mail with a single button press. Thus users can quote historically important persons and use these quotes in their daily correspondence. The project is available as part of the web service correspSearch which aggregates metadata of various scholarly editions of letters. The complete source codeis accessible on GitHub. Furthermore, templates as well as an extensive documentation are provided, so other projects can quickly incorporate their own data into the corpus of salutes. Subject matter and contents Salutations were and are essential elements of written communication, especially in letters. They serve as an indicator of the relationships that the letter writer has or believes to have with the recipient. Thus the mark-up of salutations is extremely important when encoding letters digitally. The guidelines of the Text Encoding Initiativehave long been offering a corresponding encoding option by providing the <salute>‑element. In the context of quoteSalute exactly those <salute>‑elements are automatically extracted and subsequently transformed into the projects TEI‑XML‑based interchange format. During an additional manual curation process, duplicate entries and inappropriate content are removed. The remaining data is enriched with semantic information to enable users to filter the corpus for: politeness; gender of sender and recipient how the grammar of the salute suggests; and language. The curation process has shown that the use of the <salute>‑element is not yet common practice in digital scholarly editions of letters, although it provides important insight into the relationship between correspondents. Editors are encouraged to use the <salute>‑tag more in their daily work. The web application is implemented by using HTML, Bootstrap and the JavaScript framework VueJS. The salutes are stored in an eXist‑db instance. A random quote is fetched by performing asynchronous requests to the database server, query the eXist‑db using XQuery and serialize the response as JSON. Currently, the corpus contains 981 salutes originating from 14 different scholarly editions of letters. All incorporated editions have licensed their data under a compatible free Creative Commons license. Relevance In the light of the conference topic Complexities quoteSalute demonstrates how complex research data can be provided in a creative and easily accessible way without losing ties with the original scholarly resource. Since each quote is accompanied with a link to the letter of its respective edition, digital editions should enjoy greater popularity the more broadly quoteSalute is being used. The project aims to open up new contexts for the use of research data from the Digital Humanities and to promote the public presence of cultural data and research results. In September 2018 quoteSalute was awarded the DARIAH-DE Digital-Humanities Award. »I have the honour to be with the highest consideration, Sir, your most obedient & most humble servant« John Pickering to Wilhelm von Humboldt, 27.11.1827 Wilhelm von Humboldt - Spachwissenschaftliche Korrespondenz https://wvh-briefe.bbaw.de/Brief?section=all&id=447 "
	},
	{
		"id": 202,
		"title": "Prototypes as Proto-Theory? A Plea for digital theory formation",
		"authors": [
			"Kleymann, Rabea"
		],
		"body": " Introduction In the discussion about the status of Digital Humanitiesas a scientific discipline, the question of theory formation is often raised. Confronted with the accusation of a lack of theoryand the call for more theoryThe text passages of the German research quoted above are translated by the author of this submission. , an excessive theoretical debate can be observed. On the one hand, due to the heterogeneity of their objects as well as their tasks, DH are positioned somewhat beyond theory formation. On the other hand, theory is being temporally separated from DH. The proclamation of an end of theory heralds a post-theoretical era, while at the same time affirming a pre-theoretical status of DH. Either way, DH paradoxically appear to lie before, after, besides, or beyond theory formation. Instead DH seem to focus on the activitiesor doing research. In the course of this shift towards digital practice, the development of prototypical software is becoming increasingly important. Transferred from informatic software development, a software prototype reduces a future system complexity to certain subcomponents and exhibits them assuming a descriptive or prospective potential. Software prototypes are understood as methodically and theoretically modelled digital artefacts. In addition to the functions of exploration, explanation and understanding already established in computer science, software prototypes are regarded in other discursive contexts as arguments, provocation, mediators of performance and criticism. Ruecker/Galey even speak of prototypes as theory. The premise of this presentation is that software prototypes seem not only to gain epistemic status, but also to address the need for still outstanding, or arguably obsolete, theory formation. This presentation will show to what extent software prototypes can be understood as a proto-theory of DH. Thereby, the presentation not only puts forward the theory-building effects of a software prototype, but also argues that the software prototype is a unique form of theory design. With proto-theory I propose a theoretical form that is constituted not in text, but in the form of the software prototype. So, the presentation is intended as a contribution to the attempts to re-measure the humanities as a place of advanced theory formationand to explore the scope and limits of theory formation based on software prototypes. First, an understanding of the concepts of prototype and theory will be presented. Afterwards, the theoretical merit of the software prototype will be determined. On the View of Identity and Difference – The Concept of the Prototype While the notion of a software prototype is highly prevalent in recent DH research discourse, an examination of the concept of the prototype beyond an informatic comprehension is overdue. Derived from the Greek prototypon Urbild, the composite consists of  proto the foremost, first, most significantand  typos stroke, impactformative form, outline, shape, pattern. Two determinations of the prototype can be distinguished: From the perspective of cognitive psychology, the term prototype is introduced in its standard use as an abstract entity that represents the typical characteristics of an extensional category as the best example. The affiliation to a category is determined in relation to the prototype qua analytical quantification of characteristics. To be distinguished from this is a morphological determination of the prototype, as it represents Ludwig Wittgensteins family resemblance among others. Using the example of the term game Wittgenstein explains that not all members of a familyare united by a single common defining characteristic, but by a complicated network of similarities that overlap and cross. Wittgenstein defines family resemblance as followed: I cannot characterize these similarities better than by the word family similarities, because in this way the different similarities that exist between the members of a family overlap and cross: Growth, facial features, eye colour, gait, temperament, etc. etc. - And I will say: the games form a family. Belonging to a category is established by similarity relationships. Theories as approaches: Negotiating contingency and complexity The concept of theory as a mental viewing or scientific consideration refers to explicit, elaborate, ordered, and logically consistent category systems that serve to describe, research and explain the facts of their respective object areas. Theories address an experience of deficiency that is expressed in the difference between theory and world: Theories are an effect of the inaccessibility of the world in such a way that they substitute and thus compensate for accessibility as theory. Providing an approach to the phenomena, theories pursue management of contingency. Given a certain degree of abstraction of the theoretical statement, which applies to several, possibly even all, individual phenomena of a certain type or provides a model for the phenomena, theories have explanatory and prognostic functions. Furthermore, the theory formation is subject to the assumption that the complexity of the manifold phenomena can be traced back to superordinate, general principles. The term complexity is under-defined. In the humanities, Niklas Luhmanns determinationbecame particularly relevant: Complexity is number and diversity of the relations that are possible according to the structure of the system between the elements are. Recently, James D. Proctor and Brendon M. H. Larsonhave described complexity as a placeholderfor the unknown. Theories enable areduction of complexity by unifying the perspective and creating communicable observation effects. The distinction between totality and complexity, which plays a role especially in the discussions of German idealism, is also linked to this: The more heterogeneous experiences a theory can unite among itself and bring to a coherent understanding, the greater is the range of the theory. But this also goes hand in hand with the idea that totality takes precedence over complexity. When it comes to literary studies, theories not only constitute a specific approach to the object of literature by reflecting conditions of production and reception [...] as well as [...] constitution and [...] functions, but they also generate the object as an epistemic object of their observation in the first place. Methods differ from theories in that they rather describe the way of proceeding in a target-oriented way. Theories can therefore provide a set of generaland specificmethods. Exemplary for hermeneutic theory is the hermeneutic circle or the system-theoretical re-entry for systems theory. Moreover, theories represent a specific literary text genre. Since theories appear primarily in the form of written texts and therefore follow a linguistic logic, they are subject to a specific textuality that is constitutive, but not reducible, for the shaping of a theoretical knowledge. On the theoretical merits of the software prototype Against the background of the outlined prototype concepts and the function oftheories, the question now arises to what extent software prototypes can meet theoretical requirements. The development of prototype software is first preceded by a process of data modelling and the production of an ontology. In the classification procedures for obtaining classes, properties, and relations, theoretical knowledge already sediments itself, such as the idea of the text or meaning. Beyond the theoretical pre-configuration, however, the difference between the experience of observing and being observed that is constitutive for theory reveals itself in the software prototype. The software prototype represents an approach that is communicated and experienced interactively. As a reduced and compressed model of a set of mostly methodical operations, the software prototype often fulfils the synthetic and integrating performance of a [...] view. Individual facts or activities are linked, which then appear to be prototypical, i.e. formative and exemplary, as part of a larger coherent explanatory context. More precisely, the software prototype represents theory as a functional structure and thus assumes an exemplary function. It stands not merely for something else, but prototypically and materially for something from which it itself [promises] to be a part separated only for demonstrative purposes. Contrary to the thesis of Rockwell/Ramseyon Thing Theory, it is not about the concrete materialization of theory as an objective object that substitutes the textual form of theory. For example, the abstracts of the DH Conference 2018 contain not only numerous ideas of software prototypes See DH 2018, Mexico City Abstracts: There are 25 mentions of software prototypes, understood as prototypical software, on pages 51, 55, 67, 115, 130, 137, 141, 178, 238, 250, 253, 397, 451, 470, 481, 483, 505, 531, 534, 537, 542, 559, 570, 584, 592. , but also formulations on explicative and prognostic performance as well as a form of validation. Examples of mentions of software prototypes with reference to the scope of epistemic achievements: typical episodes in the domain of a rudimentary experience, networks are illustrated, reflect upon the next steps in the project, considering the implications it may have on the data schema, making it worth simply trying it out rather than lengthy discussions about the value of proceeding, establishes confidence and commitment, Gaps in desired functionality immediately become clear through interaction. The precarious epistemic situation shows itself oscillating between the notions of the software prototype as a figuration of evidence and as a temporalized and spatialized form manifesting theory. For the software prototype dissolves the sharp distinction between the world of things and the world of signs. Rather, the boundaries merge smoothly, so that at the same time the difference-theoretical foundation of theoryis questioned in favour of an interactive experience. In contrast to the textual theoretical form, the subject, which wants to gain access through theory, is apparently co-constituted by the interactive orientation of the software prototype. This is followed by the question of subject constitution in textual and technical theoretical forms. In addition to proving the theory-generating effects, the question arises as to the form in which the theoretical knowledge appears in the software prototype. The visual order of the software prototype in the form of a graphical user interface is not committed to the textual aesthetics of the linear. Thesystematicity is replaced by a topological and temporary unfolding of the theory. The software prototype seems not only to represent theoretical knowledge, but visuality and interactivity generate a surplus in which new knowledge emerges. This interferes with empirically proven and hypothetically speculative knowledge. If we now consider the plurality of software prototypes in DH research from a distanced perspective, the individual software prototypes can be regarded as members of a family in Wittgensteins sense. In this way, no systematic theoretical structure is created, but rather the dynamics of an open and expanding network. Conclusions The discussion about the epistemic status of the software prototype not only questions the self-conception of DH asscience and as research infrastructure The aspect of research infrastructure includes considerations about interdisciplinary team structures, project constellations as well as research funding and financing. One of the questions under discussion is why resources are used for the production of software prototypes. , but locates its growing body work on theory formation firmly back into the centre of the humanities. What does a contemporary literary theory look like that situates the individual facts in the context of digitality? Proto-theory promises to be an answer that implies a fruitful starting point for follow-up research. The growing family of software prototypes in the DH, if understood as a theoretical form, need to be thoroughly considered, contextualized, and critiqued. The tasks for the DH community are therefore to experiment with the legitimation of different forms of theory, to discuss the differences and barriers between textual and technical forms of design, and thus to contemplate this hybrid network of theories. "
	},
	{
		"id": 203,
		"title": "Digital Edition and Analysis of the Mediality of Diplomatic Communication - Habsburg’s Envoys in Constantinople in the mid-17th century",
		"authors": [
			"Koch, Carina",
			"Würflinger, Christoph",
			"Huemer, Anna",
			"Brunner, Lisa"
		],
		"body": " Research Question The FWF FWF - Austrian Science Fund, https://www.fwf.ac.at/en. -funded project about the mediality of diplomatic communication examines the exchange of information of Habsburgs diplomats in Constantinople with the imperial court in Vienna. Written sourcesfrom the 17 th century are the basis of the study. They provide a deep insight into problems of peacekeeping, intercultural and interreligious coexistence, consequential conflicts and strategies to solve them. However, the information is not objective; discourse traditions, personal interests, the self-portrayal of the agents, as well as transmission conditionsare of importance. In summary: the media had a great impact on information and knowledge transfer between Habsburgs diplomats and the Viennese Court. Moreover, the media determined what people learned and knew about the Ottoman Empire. Thus, the computer-aided analyses of the selected sources is conducted from a media-scientific viewpoint. The study will provide a better understanding of the constructed knowledge about Ottomans and the Islam inEurope, which has still an impact until today. Digital Edition The travelogue and the correspondence, as well as research results and analyzes, will be open access and publicly available as a digital edition The digital edition will meet the evaluation criteria for digital scholary editions by the Institut für Dokumentologie und Editorik, the Quality criteria for electronic source editions of the Porta Historica Porta Historicaand the Minimum Standards for Electronic Editions of the Association for Documentary Editing. . The edition will offer different perspectives Digital methods create a transmedial representation of text. On transmediality of text, see: Sahle, Sahle. on the texts, which include not only the digital facsimiles and the transcriptions, but also semantic enrichments and visualizations to uncover the ways of knowledge transfer and discourse traditions. Additional historical background information provide the contextualization of the sources. The historical critical edition of the travel report and letters will be realized in TEI-P5 TEI, Text Encoding Initiative, http://www.tei-c.org. and follow wherever possible the DTA-Basisformat DTA-Basisformatis a guideline for historical texts, recommended by CLARIN-D and the DFG . . A detailed encoding definition for research specific annotations will be documented in TEI ODD ODD stands for One Document Does it all. It is a TEI XML-conformant specification format that allows customizing TEI P5. . It includes major transcriptional observations, text structuresand historical interpretation. Especially the latter annotations get identifiers to interlink text passages within the sources. By merging the specific events, persons or places direct comparisons and analyses of the reports can be made. The travelogue will also be described with the methods of the manuscript-description module of the TEI TEI Modul Manuscript Description, http://www.tei-c.org/release/doc/tei-p5-doc/en/html/MS.html. . Besides, the digital resources are described in METS METS. Metadata Encoding and Transmission Standard, http://www.loc.gov/standards/mets. for the representation in book viewers and system-internal in Dublin Core DC. Dublin Core Metadata Initiative, http://dublincore.org. . The data model of the letters also include the TEI extension correspondence description TEI element <correspDesc>, http://www.tei-c.org/release/doc/tei-p5-doc/en/html/ref-correspDesc.html to enable data exchange with the web service CorrespSearch correspSearch. Search scholarly editions of letters, https://correspsearch.net, see: Dumont. . Semantic Web technologies are used for data enrichment: SKOS SKOS. Simple Knowledge Organization System, https://www.w3.org/2004/02/skos/. representations of concepts for indexing and to integrate the data in the Linked Open Data context, the project will make use of public identifiers from international authority files e.g. GND. Gemeinsame Normdatei, http://www.dnb.de/DE/Standardisierung/GND/gnd_node.html, VIAF. Virtual International Authority File, http://viaf.org, GeoNames, https://www.geonames.org, Deutsche Biographie, www.deutsche-biographie.de. . Based on the encoding, content analyzes for a comparison of the content in the two media types and visualizations will be realized. This will include for example space-time visualization of the named locations from the itinerary or the scenes of the events described, evaluations of categorized passages to reveal dominant topics, or network analyses to identify main actors and their relationships to each other. Part of the data will be experimentally processed with natural language processingtools. For example, Named Entity Recognitionto identify spatial information or persons, n-grams and part-of-speech tagging for analyses of the use of words and word frequencies. The outcomes should help to uncover plagiarism or help answer the question of how information is conveyed and what opinionshave been expressed about an entity, in particular in connection with the characterization and description of persons or the self-presentation of the writers. It is experimental because applying natural language processingmethods and tools to historical texts is problematic, especially because of a lack of standardized orthography. Digital Repository The described analysis, processing and presentation as well as the archiving will take place via an in-house digital research data repository GAMS GAMS - Geisteswissenschaftliches Asset Management System, https://gams.uni-graz.at.. It is an OAIS OAIS. Open Archival Information System, http://www.oais.info. -compliant Asset Management System for the management, publication and long-term archiving of digital resources from all disciplines of the humanities. The infrastructure is based on the open source software project FEDORA FEDORA. Flexible Extensible Digital Object Repository Architecture, https://duraspace.org/fedora. and is continuously further developed since 2003. T he citability of all data objects in the system is guaranteed with persistent identifiers based on the Handle System Handle System, https://www.handle.net/. and the reusability as well as the compatibility of the research data, the data models and the transformation scripts by the use of data standards. "
	},
	{
		"id": 204,
		"title": "Towards Constructing An Ecosystem for Digital Scholarly Editions of East Asian Historical Sources: With the Focus on the TEI-Markup of the Engi-Shiki",
		"authors": [
			"Kokaze, Naoki",
			"Nagasaki, Kiyonori",
			"Hashimoto, Yuta",
			"Kokaze, Ayano",
			"Goto, Makoto"
		],
		"body": " The Text Encoding Initiative has long been the defacto standard for constructing digital scholarly editions of humanities as the interoperable data. As beginners often worry about the high cost of learning TEI, those seeking to disseminate TEI commonly provide markup guidelines. Compared with European sources, however, there are fewer projects to create TEI documentation for East Asian materials. Therefore, it is necessary to construct basic texts with the TEI and distribute them internationally because they function as the basis for research on East Asian culture and history. This poster presents the importance of creating a TEI documentation for East Asian sources through the markup project of the Engi-Shiki and demonstrates the documentation itself. The Engi-Shiki is a 50-volume work compiled between in 907 and 927 C.E. The first ten volumes are Imperial Shinto regulations, and the last 40 are codifications of criminal and administrative law. This abstract explains why the Engi-Shiki matters in the context of East Asian studies, and describes the contents of the TEI documentation. There are three reasons why the Engi-Shiki has a further implication for East Asian studies. Firstly, as the general characteristics of East Asian text, the Engi-Shiki contains Chinese characters which flow vertically, double-line annotations contained within a single line, written in Kuzushiji —classical calligraphic renderings of Japanese text—without the space for word-separation, and Chinese numerical values with the non-International System of Units for measurement. All of these features require customized schemes when processing East Asian texts by computers. Secondly, the documentation will be referencable for various markup projects, because the Engi-Shiki has several styles of texts. For example, the one is the Shito ritual prayer Norito 祝詞 with the classical Chinese grammar, and the other is the bookkeeping entry containing the explanation of administrative law. Concerning the bookkeeping entries, it has already been clarified the compatibility between the Engi-Shiki and the Transactionography —the markup strategy of historical financial records—. Therefore, the TEI documentation of the Engi-Shiki offers the high versatility for other projects. Thirdly, the Engi-Shiki has useful connections with Chinese history. Because the Engi-Shiki was detailed enforcement of the Chinese ancient administrative codes 令, it helps us to understand how the Chinese Tang 唐 administrative rules were used in Japan. Primarily, the Engi-Shiki provides information on the division of administrative counties based on their socio-political power, so it has something valuable with such geographical modeling of administrative regions as some researchers in Kyoto University investigated. Although this poster focuses on the Japanese source, we will address to reach out scholars whose interests lie in East Asian history and culture, as a prospect. As for creating the documentation, we need to determine how profoundly and informatively we encode the texts. According to Best Practices for TEI in Libraries, our markup project applies the deepest Level 5. The contents are the following: 1. Metadata Description 2. Encoding the Overall Structure 3. Detailed Markup according to Your Research Interests 4. Dealing with Multi-Lingual Texts 5. Scheme Customization 6. Examples of the Display and Applications Among them, the Scheme Customization is crucial for East Asian texts. Notably, Japanese texts often contain the parallel embedded small-font size textswhich explain the pronunciation of a word. A TEI ODD file for Ruby, created by Kiyonori Nagasaki, is now available at http://www.dhii.jp/nagasaki/tei_all_ja_20190417.xml. Finally, the target audiences should be specified. They areEncoders of the Engi-Shiki,Research institutions that create TEI files of East Asian pre-modern materials mainly in Japan,DH scholars utilizing TEI data, andpeople who analyze data after the end of our project. The documentation also works as a traceable record of research which describes what kind of intention we have had during the markup. Though it may be a too magnificent notion in the academic field, we are willing to make efforts toward a digital ecosystem. Once we have provided the documentation in the interoperable way where scholars can edit, develop, and share the contents, it can be said that this documentation function as a platform of the digital scholarly edition. In the presentation at the conference, we welcome critical feedback on building a research platform, such as connecting the relevant researchers and institutions and maintaining the data. "
	},
	{
		"id": 205,
		"title": "Living apart together: Research across Repositories",
		"authors": [
			"Kollatz, Thomas",
			"Grüntgens, Max"
		],
		"body": " Nowadays most Digital Editions are created or at least distributed in TEI-XML as scholarly research data. In epigraphy EpiDoc: TEI XML for epigraphic documents is the subset of choice. While consistency within an encoded corpus or collection of sources is enforced via schema files, experience has shown, that different scholars for sensible scholarly reasons decide to encode similar phenomena slightly differently, moving back and forth between a more flat structured as well as data centered approach on the one hand and a more narrative as well as verbose approach on the other hand. RDF as high level data model enables researchers to make statements about scholarly resources as well as their constitutive parts and the relations among them. These RDF statements themselve are formulated in a way abstract enough to transcend the individualized usages essential to TEI XML encoded resources. Thus RDF as a bridge technology provides the means to mirror more individualistically encoded TEI XML resources onto an abstract metadata layer of more easily processable data points, Grüntgens, Schrade, 61–62); also compare Ciotti, Tomasi, Ciotti). Two challenges are associated with RDF from a Humanities point of view: Firstly, statement extraction may degenerate into complex data wrangling tasks done once more by every single scholar. A solution for this may be provided by web services like Torsten Schrades XTriples that centralize functionality and provide a relatively low-threshold approach: triple extraction via XPath and triple building via a basic XML configuration file. Secondly, most RDF ontologies are either seen as being to complicated for low-threshold statement building for individual scholarly activity in the Humanities or as capturing the scholarly domain in question not adequately enough. This challenge may be remedied by advising scholars to first build flat statements for explorative in-house research. These basic statement patterns, however, have to be none the less formalized and operationalized in regard to their scholarly approach. Thus, after constructing a prototype, the scholars may subsequently lift the prototypical extraction on to a distributable level by means of inclusion of complex ontologies, 194–195). The poster will demonstrate a workflow for easy prototyping of cross-corpus research questions by means of RDF-Lifting from TEI XML via XTriples. The material used comes from two epigraphical collections: DIO and EPIDAT, Kollatz). Both repositories make medieval inscriptions of the German City of Worms available. DIO provides Christian inscriptions in German and Latin, EPIDAT provides inscriptions from the Jewish cemetery Im Heiligen Sand, one of the oldest Jewish cemeteries still in existence in Europe. Both repositories provide EpiDoc: TEI XML for epigraphic Documents. Therefore XML can serve as a starting point for RDF-Lifting with the XTripels webservice. As a minimal prototype, we want to approach gender distributions in both collections. Information about persons is contained in the data of both repositories. Figure 1. Gender attribution implicitely given in the sex-attribute within the person-element; full record: http://www.steinheim-institut.de/cgi-bin/epidat?id=wrm-1253-teip5The transformation of the implicit semantics of XML given into the explicit semantics of RDF is done by means of the XTripels webservice, 54, 56). XTripels has proven itself to be an effiecient and easy-to-use tool to crawl XML-resources and extract RDF-statements. The configuration is based on XPath expressions. Figure 2. Configuration of the XTripels webservice As soon as the data of both repositories are available as RDF triples, the RDF data as a whole is stored in a triplestore, and queried via SPARQL. Figure 3. SPARQL-Query The result of the above simple SPARQL-query shows that gender distribution in both repositories differ significantly: The proportion of inscriptions about men and women are equally balanced in EPIDAT, whereas in DIO inscriptions about women are outweight by inscriptions about men.. Figure 4. Simple gender distribution in DIOand EPIDAT. Visualization realised with RawGraphs http://app.rawgraphs.io 0=unknown; 1=male; 2=female; 9=not defined Repositories EPIDAT – research platform for Jewish epigraphy: DIO – Deutsche Inschriften Online | German Inscriptions Online http://www.inschriften.net "
	},
	{
		"id": 206,
		"title": "XML2RDF – Extracting RDF Statements From XML Resources With XTriples",
		"authors": [
			"Grüntgens, Max",
			"Kollatz, Thomas"
		],
		"body": " The tutorial focuses on scholars, who want to acquaint themselves with a low threshold — generic, simple yet powerful, and explorative — workflow of modelling, extracting, processing, and publishing of structured datafrom heterogeneous XML-sources by means of XPath. The instructors assume basic acquaintance with XML and XPath. Number of Participants 12 persons. Special Requirements Own laptop with an XML-Editorand a modern web browser. Focal points Triple-Statement-Extraction from heterogeneous XML-Resources, Basic-Research-Workflow. Introduction RDF is an abstract and high level data model and RDF statements themselve are therefore to such an extent abstractly formulated that they inevitably transcend the data heterogeneity ingrained in individualized notations and practices essential to XML encoded resources. Printed as well as digital scholarly editions in this regard for the most part oscillate between more data centered and more narrative centered verbose approaches. RDF may thus function as a bridge technology providing the means to mirror several heterogeneous XML resources onto one common abstractdata layer of interconnected as well as outward pointing data points, that is easily and efficiently queryable and visualizable. In this way RDF and LOD are extending the capabilities of XML resources without cluttering of the underlying documents or imposing of external restrictions for the encoding scholars, Grüntgens, Schrade, 61–62); also compare Ciotti, Tomasi, Ciotti). One of the main focus points of the workshop is thus to make it clear to the participants that it is pivotal to make an effort to recognize the implicit semantic potential already inherent in human-readable XML-resources, e.g. in the form of markup like, 54, Ciotti, Tomasi, pars. 46–49). This potential accordingly [has] to be transformed into explicit semantic annotationsto make the data usable for Semantic Web approaches [...]. On the whole, the process of translating XML to RDF is therefore mainly focused on the determination of general statement patterns, which can then be applied to and extracted from all resources of the data set in question., 56). Relevance In the above mentioned context, many scholars may well enough know about the obvious advantages of supplementing their XML resources with an additional RDF layer and nevertheless may find it difficult to bridge the gap between XML and RDF. A common obstacle is in this regard, that statement extraction from XML may deteriorate into overly complex or semi-manual data wrangling. Additionally, most ontologies, like CIDOC-CRM, are — or rather may at least be perceived as being — too complicated and intricate for a first low-threshold explorative approach to statement extraction in particular and the structured data ecosystem in general. The tutorial thus wants to mitigate both hurdles by teaching how to utilize T. Schrades XTriples web service for RDF statement extraction). The web service provides a low-threshold approach to statement extraction from any well formed XML resource — be it a full-fledged REST-API serving TEI-XML or the entrails of a word processor file like .DOC, .ODF, or .FODT — via XPath and triple building via simple XML statements. With the aid of a configuration template written in simple XML scholars are quickly enabled to build first statements for prototypical research that are flat— e.g. not build upon a complex ontology — and in this regard to gradually acquaint themselves further with the extended toolbox around RDF. These flat patterns, however, have nevertheless to be properly formalized and operationalized in regard to the research question. Thus, after having worked out a prototype, the scholar maysubsequently lift the prototypical extraction on to a distributable level by incorporating more complex ontologies into the statements described in the configuration template hereby providing the semantic interoperability needed, e.g. for LOD, 194–195). Tutorial The tutorial will start with a short introduction into the basic assumption behind the RDF data model, a short overview of a workflow in regard to statement extraction with XTriples, and a quick recap of the basics of XPath. The tutorial will then use research data provided by the API of the scholarly edition Der Sturm: Digitale Quellenedition zur Geschichte der internationalen Avantgarde). The edition provides several APIs that provide data about encapsulating persons, places, works, letters, and files. The participants will form small work groups, evaluate the APIs contents, and will henceforth formulate whiteboard-friendly flat and verbose statements about the resources, e.g. not corresponding to an established ontology. Subsequently the participants will inscribe these statement patterns into the XTriples configuration file in order to automatically extract the desired statements by means of form-style POST requests. The participants will subsequently reiterate the process of adding statements to the configuration, extracting, and subsequently evaluating the statements. When asound basis has been achieved, the participants will make a final extraction and then import their RDF-XML files into the web based RDF4j triple store, where the separate graphs will be merged automatically. Then the participants will be introduced to the basics of SPARQL. Subsequently the groups will — instructed by the instructors — perform some simple queries to become acquainted with SPARQL and the corresponding query endpoint. Further more complicated queries that aggregate information in a tabular form, may be executed from saved queries, written earlier by the lecturers, in order to facilitate the process. The aggregate will then be visualized with the web based service https://rawgraphs.io. To make it clear: This sections focal point lies not on performing the most intricate queries, but rather on showing in a lucid way how and based on what kind of technological foundation a simple aggregation and subsequent visualization of RDF data may be performed, thus clearly outlining the conceptual side of the workflow as a whole. Outline Time Topic Mode Note 10:00–10:30 Short introduction to RDF and the basic structure of an extraction workflow via XTriples; very short recap of basics of XML & XPath. lecture Slides; XPath cheat sheet 10:30–11:45 Modelling statements and writing of the XTriples configuration file. First extractions. Group work; supported by the instructors Via web based Form-style POST request on 11:45–12:00 Final statement extraction Group work 12:00–13:00 Lunchbreak 13:00–13:15 Import into Triplestore Group workWeb based RDF4j-triplestore provided by instructors. 13:15–13:45 Basic querying of Triplestore via SPARQL endpoint Group workSPARQL-endpoint provided by instructors; SPARQL cheat sheet 13:45–14:00 Visualization via rawgraphs.io Group work"
	},
	{
		"id": 207,
		"title": "Detection of Scenes in Fiction",
		"authors": [
			"Gius, Evelyn",
			"Jannidis, Fotis",
			"Krug, Markus",
			"Zehe, Albin",
			"Hotho, Andreas",
			"Puppe, Frank",
			"Krebs, Jonathan",
			"Reiter, Nils",
			"Wiedmer, Nathalie",
			"Konle, Leonard"
		],
		"body": " Narratology usually defines the event as the smallest unit of a narrative, but computational literary studies has problems to come up with a workable definition and operationalization of event for its purposes. This has proven to be a serious obstacle for research on narrative texts, because similarity of plot is an important aspect for many operations, for example genre classification. Recent operationalizations like syuzhetcapture the emotional arc but not the plot itself. But it is quite obvious even to non-narratologists that a main component especially of conventional fiction consists of parts which contain a lot of direct speech and detailed descriptions of actions. These parts are usually called scenes and in our context they can be understood similarly to plays as a division [...] during which the action takes place in a single place without a break in timeincluding the aspect that it is a part of a play, movie, story, etc., in which a particular action or activity occurs. A narrative text doesnt consist of scenes alone. It can be seen as a continuous alternation between summary and scene and usually also has some ellipses - that is events not told - and pauses , for example in descriptions. Genette, who proposed these four classes, famously based his definition of scene on the aspect of time alone: scene, most often in dialogue, which, as we have already observed, realizes conventionally the equality of time between narrative and story. It is worthwhile to point out that this equality is only a kind of conventional equality between narrative time and story time, with a lot of leeway. This reduction is useful in the context of his theory, but as the basic component of plot we need a definition which is more geared towards the representation of action and which includes more information about action than just time. So we start out from the fuller understanding of a scene outlined above. We are interested in a segment of the discoursof a narrative which presents a part of the histoirein such a way that a) time is more or less equal in discours and histoire , b) place stays - more or less - the same c) it centers around a particular action, and d) the character configuration is - again: more or less - equal. We understand our panel as a first step towards modeling, annotating and automatically detecting scenes as such multi-dimensional phenomena. In this first step we will look into a) approaches for manually annotating scenes, b) the relation between just one aspect, character or time, with scenes and c) a machine learning approach for the detection of scenes. We start our research with a simpler form of literature: dime novels. Obviously the next step after detecting scenes would be to label them in an informative way, which would allow to see them as part of a plot, but that is beyond the scope of this panel. Segment Annotation The high complexity and context-dependence of the notion of scene makes manual annotation and automatic detection difficult. Establishing manually annotated data, however, is a stepping stone and prerequisite for developing automatic detection systems. Intersubjective annotation of narrative segments is difficult to achieve. In a series of experiments, Reiterreports Fleiss κ scores of below 0.3 for non-expertsand below 0.5 for students of literary studies. The only setting that led to high agreement in Reiteris one in which the segmentation task is coupled with the task of summarizing the text. In this section of the panel, we will report on annotation experiments that implement the idea in Reitermore thoroughly and are currently conducted. The experiments are carried out on a corpus of dime novels. Dime novels have been selected for compatibility and comparability with the other sections of the panel. The aim in the experiments is to create segment annotations with high inter-annotator agreement, measured with boundary edit distance. All experiments are carried out with two annotators, recruited from undergraduate students of literary studies. After being trained, the general workflow is: Read the entire text Read a summary of the text Align each sentence of the summary to a segment of the text In a second stage, annotators first undergo an additional training round, in order to ensure that the summaries focus on the text. For this training, we will use short stories that do not belong to either corpus under investigation. The annotators are also instructed that thequality of the summary does not matter. After having written a summary for a narrative text, the annotators will – as before – link sentences/clauses of their summary with segments in the original text. Technically the experiments are done with the annotation tool CorefAnnotator. While it is not designed for this task, it allows annotating arbitrary text spans and since it is developed by one of the authors, problems can be addressed internally. We will also remove all typographical/visual segmentation indicatorsfrom the texts. Next to an analysis of the annotation procedure, we will investigatethe relation between the summary-based segments and scenes as discussed above. It can be expected that a number of segments created by the annotators are not scenes in the above sense, but contain e.g., narrative summaries. An interesting question then is, whether the identification of scenes could be modeled as a two stage process – first segmenting the text, followed by a classification of the segments. Character constellations as indicators for scenes - a narratological model In drama, the smallest unit is characterized by the change of character. It therefore seems obvious to transfer this directly to narrative texts. In the following, the problems that arise with a simple implementation of this concept to narrative texts are described. The starting point of our considerations is a model of reading in which the reader builds a mental model of the situation based on the text and world knowledge. Each new sentence contributes to enriching, supplementing and possibly correcting this model. At the beginning of a chapter that is not explicitly marked as a continuation of the previous chapter, there is information that marks time, place and characters. This can be done explicitly in the mode of a description, i.e. without the beginning of the narrated time: Where Soho is darkest, there lies the SHOCKING PALACE.or as part of the narrative of the time by naming the characters and places: Hello, Joe!, I said to the old Henderson who manages the New York FBIs armory.. In the last example, there is the somewhat more complex situation where the I refers to the fictional author of the novel series, FBI agent Jerry Cotton, and has to be resolved by the reader accordingly. The once established situation can then be supplemented by further information, at the same time knowledge about the situation can and must often be used to resolve the character references. It is not easy to model this process because it would require a distinction to be made between a mention of a character and a reference to a character present. Such a complete understanding of the text cannot be achieved at present, but it can be approximated by setting a counter to a value n in each figure reference - assuming a reasonably functioning coreference resolution - and then reducing the counter in each subsequent sentence until it reaches 0, and removing the character from the situation model. A special feature at the beginning of a chapter must be taken into account: It may take a few sentences - k sentences - until the situation model is fully established. The values for n and k must be determined empirically, but preliminary investigations suggest that the values are text type-specific, i.e. are by no means the same even in the case of the dime novels. How do we determine the end of a scene? This is far more challenging based on character alone. We use two indicators: First we track words which point to someone leaving the group, like ‚weggehen, ‚sich verabschieden usw. Secondly, we take an update of the situation which introduces new characters beyond the k sentences as an indicator of a new scene. Temporal Changes as Scene Indicators This section of the panel explores the identification of scenes with regard to time. The broad conception of scene presented above defines scenes as segments of a narration displaying, among other, a more or less equal time . From an operationalizing point of view, this poses two problems:What does equality mean?What does time mean? Equality of time is a graded feature of scenes as the more or less in the definition underlines. Additionally, the Genettian concept of equal time, i.e. scene as corresponding coverage of the time of the narrated and the time of the narrating, heavily depends on the not adequately operationalized concept of events. As a consequence, equality of time cannot be operationalized in a straightforward manner. For the detection of segments that are coherent in terms of time, detecting the borders of these segments is the better viable approach. Therefore, the focus on changes of time as indicators for segment borders seems more promising. Regarding the operationalization of time, the analysis of temporal expressions and tense would be an obvious, but somewhat reductionist approach. From a literary studies point of view, many phenomena in narrations are related to time. These need to be considered in a time-based approach to scene detection. The presented approach tackles this by building on two prior narrative theory-based studies. In Gius, time phenomena in narrations have been operationalized and analyzed manually for more than 40 categories capturing tense, temporal expressions, order, duration, frequency, the temporal relation of the narrator to the narrated and temporal perspective. These categories go well beyond linguistic temporal information and were subsequently automated by Bögel et al.for German literary texts. The detection of temporal signals was modelled by adapting and extending HeidelTimefor the detection of expressions with temporal information considered relevant for narratological studies. Bögel et al.modelled order changes as approximation to anachronies. The detection of points in narrations where the orderchanges reached a comparably high F1 score of 81,4%. The 21 deployed features comprised temporal signals, aspects of tense, direct speech and structural features as well as features for the detection of narrative levels. Occurrences of anachronies in Gius et al.and manually annotated scenes. These so called order switches can also be used as approximation to scene detection. This approachIt tackles both equality of timeand the segmentation task. A closer examination of the annotated textssuggests that the number of contained anachroniesexceeds the number of scenes. Nevertheless, somethe borders of scenes seem to correspond to a border of an anachronic text segment. Therefore, anachronies will be examined further as possible can be regarded as , though often more fine-grained segmentations of narrative texts connected to within scenes. Here, whereby anachronies encompassing more than one paragraph will be are considered candidates for scenes. Additionally, the relation between other temporal phenomena annotated in Gius et al.and scenes will be analyzed and the relevance of these temporal phenomena for the detection of scenes will be discussed. Anachronies with >= 1 paragraph extent Scene Segmentation as a Classification Problem In this section we will talk about our strategies to detect scenes using machine learning for this new task. Previous talks in this panel underline the importance of character constellation, narrative time and the fictional location for the definition of a literary scene. These form three scene detection markersassumed to heavily correlate with scenes. We therefore propose to use a two-stage approach to detect scene borders: First detect the sdm s and then use these to detect scene borders. Lacking manually labelled datasets, we used 70 Jerry Cotton novels, where segment borders are marked with ****. Manual inspection has shown that these segments are more coarse-grained than traditional definitions of scenes: they do not mark all changes between scenes, but at every segment marker a new scene begins. A second dataset was manually annotated: we randomly sampled 1000 paragraphs from 100 novels from TextGrid and labelled them as to whether or not they contain one of the sdm s, that is change in character constellation, time or place. By using FastTextfor document classification, we reached an F1-score of about 60% only on the class sdms, showing that this classification can be done using standard methods. To show the influence of sdm s on scene detection, we first attempted to detect scene borders in Shakespeares dramas, where the sdm s are explicitly given. In dramas, scene markers are also directly found in the text, enabling us to extract scene annotations automatically. In order to train our network, we removed the scene markers as well as the words  exeunt  and  exit  . For the prediction of scenes, we used a neural network-architecture consisting of two LSTM encoders and a prediction layer, depicted in figure 3. More specifically, we encode four sentences on each side of a position in the text as context using separate BiLSTMs and ask the model to predict whether or not a new segment begins at this position. We encode the LSTMs input using word2vec-embeddings pre-trained on a large corpus of German novels. The output of the LSTMs is then fed into a fully connected layer to perform the classification. We tested this model on both Jerry Cotton and Shakespeares dramas. As anticipated, the model performed well on dramas, where sdm s are explicitly present: we reached an F1-score of 62%, with high recall of 93% and moderate precision of 46%. This shows that our system captures almost all scene changes, but tends to predict too many scenes, on average splitting every scene into two. For Jerry Cotton, the system performed poorly, reaching an F1-Score of 17%. This confirms our expectation that sdms are immensely useful for the detection of scenes and thus combining our two stages, the detection of sdms and the classification of scenes using a neural network, is a promising approach to this task. Furthermore, while manual labeling of scenes is a task with notoriously low inter-annotator agreement, we noticed that labeling paragraphs for sdm s could produce good inter-annotator agreement and enable training classifiers for a very helpful first step in a scene detection pipeline. Neural Network architecture to detect scene changes. An instance is created between two sentences and encoded using a context of four sentences before and after the sentence. Both contexts are embedded using a separate BiLSTM, which are concatenated and after an additional fully connected layer a prediction layer decides whether a scene change occurs. "
	},
	{
		"id": 208,
		"title": "Thematic complexity",
		"authors": [
			"Jannidis, Fotis",
			"Konle, Leonard",
			"Leinen, Peter"
		],
		"body": " 1. Introduction Readers of literary texts have an impression of its complexity, and professional readers like literary critics often use if not the term, but the concept as a basis for their evaluation of literary texts. Recent attempts in literary studies to explore the notion had difficulties agreeing on a definition, a fate which we try to avoid by relying on a very general understanding of complexity: the number of elements and the number and quality of their relations. When we are talking about texts these elements can be words, syllables, metaphors, intertextual relations, the syntax of sentences, themes and topics etc. In this perspective text complexity becomes a multi-dimensional phenomenon. We should emphasize that we are talking about a set of textual features and not about the difficulty for a reader to process the text, which is the domain of cognitive reader studies. Our attempt in this paper to describe a useful approach to thematic complexity in fiction is part of our ongoing research on the complexity of fiction. In Jannidis et. al.we looked at measures of vocabulary richness and syntactic complexity and applied them to the same collection of popular genre novels and the collection of literature of renowned German writers we are analyzing in this paper. Surprisingly with the exception of sentence length there is no single measure which allows to distinguish consistently between these two collections. This study now looks at thematic complexity. Obviously there is no limit to the themes and topics a novel can deal with, but an infinite amount is difficult to measure. So we use the mixture of genres in documents as a proxy for thematic complexity, and we measure this mixture using topic modeling and Zeta to describe the genres. 2. Cooperation with the German National Library The analysis is based on the digital texts delivered to the German National Library. The DNB collects, archives, records and makes available to the public the media works published in Germany since 1913 as well as the German-language media works published abroad. Since 2006, the collection of media works published online has also been one of DNBs tasks. DNBs holdings currently comprise of more than 5 million digital objects, including some 900,000 e-books, around 1.5 million e-journal editions and about 2 million e-paper editions. In addition to the extensive physical inventory, DNB users have a growing pool of born digital objects at their disposal. The cooperation with the DH communities is a strategically important continuation of DNBs range of services. One aspect is the support of selected cooperation partners through the provision of even extensive corpora, primarily from born digital objects such as e-books, for carrying out automatic analyses in the DNB labs on site. 3. Corpus Our corpus holds 9000 novels from 13 pulp fiction genres and high brow literature novels from prize-winning authors. Novels per genre in corpus 4. Methods We aim to compute thematic complexity on the mixture of genres in documents. Since genre labels for documents are provided we will use these to identify word fields covering stereotypic themes, topoi or motives in genres. For this task, we use Zetaand LDA. In a second step, the words are used to measure a distribution of genres in each novel. 4.1 Zeta Originally, Zeta is used to determine which words are well suited for distinguishing two groups of texts, for example, texts by two authors. However, we are only interested in which words are characteristic of a group. This is why we compare a homogeneous group of texts of one genre with a heterogeneous group to represent the entire corpus. The zeta scores of word x for each genre are multiplied by the frequency it appears within a document. To represent a document by a single vector, the average scores overall words are computed. We will use variants of zeta as listed by Schöch et al., see Table 1. Table 1: Variants of zeta fromdocument proportions relative frequencies no transformation log2- Transformation no Transformation log2- Transformation Subtraction sd0 sd2 sr0 sr2 Division dd0 dd2 dr0 dr2 4.2 Topic Model We use the LDA implementation Malletto compute Topic Models with 50, 100, 150, 200 and 250 topics by 1000 iterations over our corpus. We use the same stopwords as in the zeta pipeline. For each topic a genre distribution is calculated from genre labels and this distribution is used to infer the distribution of genre in documents via their share of topics. Because every topic contains all words and every document inherits all topics to some extent we set a threshold of 5% proportion for topics in documents to be taken into account. From Topic Model to genre distribution 4.3 Complexity Both approaches create a representation of documents as vectors with one dimension for each genre. To measure thematic complexity, we will use the Gini-Index. This measure of dispersion is commonly seen in economics to quantify inequality in income. The values for the Gini-Index lie between 0, which indicates that all incomes are equal or in our context that all genres contribute equally to this text, and 1, which means that only one person has any income or in our context that only one genre contributes to the document. But in this form a higher value would actually mean lower complexity, so for better readability, we will use the inverse Gini-Index, so a lower value indicates usage of mainly one genres vocabulary and a higher one the occurrence of words from different genres. 4.4 Evaluation There is no way for us to evaluate thematic complexity scores directly because creating test data by humans would be quite time-intensive and of uncertain success. Instead, we will use genre classification as a downstream task with our document representationas input feature, to at least evaluate the genre distribution in documents, on which thematic complexity is based. 5. Results Evaluation results of genre classificationshow best results for LDA with 150 Topics. The following results, except the most distinctive words in table 3, are all based on this setup. Table 2: f1 scoreof genre classification with logistic regression Zeta Variant sd0 dd0 sd2 dd2 sr0 dr0 sr2 dr2 f1 .86 .62 .86 .83 .72 .68 .73 .77 LDA Topics 50 100 150 200 250 f1 .86 .88 .89 .86 .86 Table 3: Most distinctive words per genreRegional Western High Lit. Medical tavern cows salute mountain rescue ride gun pasture bridle so-called jewish earthphilosophy surgery jealous eat sob Figure 3 shows the transformation of document vectors consisting of the 14 genre proportions into a 2-dimensional representation. The calculation was done with Uniform Manifold Approximation. Umap transformation of genre distribution Figure 4 shows the distribution of genres in genres. Naturally the most prominent genre in each genre is its own class. But it shows that there are - as expected - strong intersections between royal, medical, family and love genres. Genre proportions heatmap Figure 5 shows the change in genre distribution of a mixed genre like western with erotic elements compared to its straight main genre, in this case western. Label for these crossover genres are provided by publishers. Changes in genre distribution of mixed genres relative to main genre Inverted Gini-Index of novels per genre Figure 6 shows the inverted Gini-Index, which we propose as a measure of thematic complexity. High Literature, Science Fiction and Crime form a leading group, Love and Adventure novels show high diversity and the war genre seems to be the most monothematic group of texts. 6. Discussion and future work We have shown two approaches to calculating genre distributions, with LDA doing slightly better in downstream evaluation than Zeta. Based on this results, we were able to use the Gini index to calculate thematic complexity. Our assumption that high literature is constituted from a broader spectrum of subject areas than pulp fiction genres is confirmed. Because Zeta and LDA are so close to each other, it seems promising to us to combine both methods, for example, to initialize GuidedLDA with Zeta seed words. An intermediate product of the calculation with Zeta are word vectors whose dimension represents the distinctiveness of a word for a genre. Considering this as a naive word embedding it seems almost obvious to redefine the whole workflow by learning word embeddings together with genre classification task. "
	},
	{
		"id": 209,
		"title": "A tool for multifaceted analysis of the Old Polish New Testament apocrypha",
		"authors": [
			"Bohdanowicz, Karolina",
			"Borowiec, Karolina",
			"Cieplicka, Anna",
			"Kozak, Michał",
			"Rojszczak-Robińska, Dorota",
			"Wytrążek, Justyna",
			"Ziółkowska, Olga"
		],
		"body": " The presented tool – web search engine – is being created as part of the project funded by the National Science Centre in Poland: Origins of Polish language and religious culture in the light of the medieval Apocrypha of the New Testament. A universal tool for the study of Polish apocryphal texts. It will serve for multifaceted, multidisciplinary analysis of all saved Old PolishNew Testament apocrypha. The religious Polish language from a historical point of view constitutes the most important variety of Polish – it formed the basis for the shaping of the Polish literary language. These texts are fundamental not only for the history of Polish culture, but also for the literature and language of the East Slavdom. This is also the most extensive body of the Polish mediaeval writing – the New Testament apocrypha consist of more than 2000 pages of manuscripts. Unfortunately, those texts are largely inaccessible or poorly accessibleor available only in excerpts. Moreover, the editions remaining in circulation are not sufficient to conduct in-depth research. This is true for not only philological, but also religious or cultural studies. Due to their complexity and diverse character the above mentioned texts require a digital way of presentation. Consequently, one of the aims of the project is to develop a tool enabling fully interdisciplinary and multifaceted studies. This tool will be an advanced search engine with the functionality of comparing results based on a meticulously developed database, including, among others, Latin sources, Slavic contexts and the employed themes. New transliterations and transcriptions of these texts will be prepared for the needs of the search engine, taking into account the latest research on them and Old Polish language in general. Due to the fact that the project engages not only philologists, the engine will meet the needs of researchers specializing in other fields: Including sourcesor themes should help to conduct traditional historical-lexical interpretations. Owing to this, it will be possible to analyse these texts in the context of the studies concerning orality and literacy, vernacularism and also the art of memory. Indication of the texts used by the Old Polish writers will also enable the reception of theological works of selected Church Fathers in medieval Poland. The compilation of themes will make it be possible to carry out an analysis of theological awareness or spirituality of particular writers and their way of understanding the described events. It will also be possible to examine the customs connected with commenting biblical texts and to deepen knowledge about biblical studies in mediaeval Poland. Indication of parallel locations will enable research on the medieval creative process, it can also be a starting point for further comparative studies of Slavonic apocryphal narratives. An additional help for non-philologists users will be a grammatical comment – another functionality facilitating understanding of the Old Polish text. The already existing tools for morphological analyses of natural languages are not suitable for the analysis of the Old Polish language. The application created for the needs of the project will facilitate annotation of grammar forms. The web portal designed in this way will be a modern research tool, answering the needs of not only philologists, but also theologists or culture experts. It will be implemented in Java programming language with the usage of Play Frameworkand in JavaScript programming language with the usage of the React library. The system will import apocrypha in the form of MsWord files and automatically transform them to TEI P5 files. Next these files will be stored in the relational database PostgreSQL associated with the portal. These texts will be simultaneously indexed in many ways in the search engine Apache Solr, due to this, the portal will offer an advanced search, not only by a phrase, but also by words taking into account the lemmatization of Old Polish language developed in the project. Other above-listed relationships, like sources or themes will also be stored in this database and indexed in Apache Solr. The portal will provide user-friendly browsing and searching according to these relationships as well as user-friendly views for comparing matching parts of texts. "
	},
	{
		"id": 210,
		"title": "Topography of Character's Body: a Case of Russian Children's Literature",
		"authors": [
			"Maslinsky, Kirill",
			"Vidyaeva, Alexandra",
			"Dodonova, Ekaterina",
			"Kozhevnikova, Yulia"
		],
		"body": " This poster presents quantitative data on the representation of characters bodies in the corpus of Russian childrens literature, visualized as a series of body heatmaps. This visualization is inspired by the research on the topography of social touching in different cultures. But unlike physical touch data in that study, our literary topography represents what parts of a characters body the authors pen is allowed to touch. The central question of the research is how the selectiveness of authors in describing their characters bodies is related to the demographic features of characters, such as gender and age. Our data reveal gender differences in the representation of female and male characters, point out differences in the representation of adult and child characters, and provide comparative material for the study of character embodiment in literary fiction. Corporeality is a fundamental attribute of fictional characters. In literary criticism, references to characters face and body features as a means of characterization have a long tradition of study under the rubric of literary portrait. Yet only recently, with the advances in computational methods and literary corpora appeared quantitative data that suggest large-scale regularities and historical trends in the representation of characters bodies. These trends can be related to transformations both in society and in literary taste. For instance, in the corpus of 19th century English novels the frequency of body parts mentions was observed to grow over time, as the urbanization progressed and the attention of the authors shifted from direct and evaluative characterization to physical details of characters. Presumably, representation of characters bodies depends on social stereotypes. Plenty of research confirmed that depiction of male and female characters in childrens books is strongly affected by the normative view of gender roles in a society, even in contemporary literature that is supposed to be progressive in terms of gender. The usage of words denoting body parts was also shown to be associated with characters gender and to change with time on a large scale in the corpus of English-language fiction. We are not aware of any similar quantitative work on Russian childrens literature. The prominence of children as characters in childrens literature allows us to address the question of differences in the representation of bodies of children and adult characters. We base our research on the corpus of about 500 works that includes prosaic fiction for children and youth written by Russian authors during 1930—2000s in broadly realistic genre, which is a sample of available digitalized texts. To obtain frequency data on the references to characters bodies in the corpus, we have to compile a list of words denoting body parts and to locate the occurrences of these words that refer to certain characters. First, we automatically extract the list of characters using procedure similar to that of BookNLP, adapted to Russian and with some modifications. We then build a series of classifiers to select only human characters, and detect their gender and age category. Finally, references to body parts are attributed to characters with the use of syntactic parsing and coreference resolution. The resulting data allow to build heatmaps for various groups of characters, and to test statistical hypotheses about their differences. Our research is still work in progress, but preliminary results show that the representation of characters bodies varies by gender. However, gender differences were not the same for children and adult characters. Comparative analysis of contexts in which body parts of different categories of characters are mentioned, showed that some frequency peaks correspond to the formulaic features in portrait descriptions. For example, boy characters very often scratch their napes. The primary contribution of our work is that we present empirical data on the representation of characters bodies in the Russian childrens literature, that allows to quantitatively explore the association of normative images of gender and childhood with its literary representation. These data may serve as a comparative material to the existing quantitative research in this field in English and American literature. Furthermore, the analysis of contexts of body parts mentions suggests the existence of formulaic features of character portraits that may be seen as a part of literary tradition. Our heatmap visualization is meant to draw attention of researchers in digital humanities to the prospects of further comparative and historical research of body as a specific aspect of literary character. "
	},
	{
		"id": 211,
		"title": "Traditional Methods Of Textual Criticism Vs. Juxta Commons: A Study Of One Poem Existing In Many Versions",
		"authors": [
			"Krasnikova, Anna"
		],
		"body": " The poster presents a study of Uljalaevshhina, famous soviet poem by Ilya Selvinsky. A work with a complicated history, existing in many versions, it was not, paradoxically, studied up to present. Our aim was to reconstruct the history of Uljalaevshhina using traditional methods and digital instruments. After the search of archive documents and published texts, we have created a corpus of 36 handwritten, typewritten and printed versions of the entire poem or its parts. The corpus includes all the versions of Uljalaevshhina known today, and many of them were found and studied for the first time. Collating the texts and using the facts of the biography of Ilya Selvinsky we have managed to reconstruct the chronology of the versions. Another aim of our work was to define and establish the level of differences between the versions of Uljalaevshhina. To achieve it we used, besides the critical analysis of the texts, the quantitative methods: form the simple count of words and lines to the creation of frequency dictionaries of the versions. We used the frequency dictionaries to explore, analyze and compare the so-called artistic worldof the texts. As a result we propose to divide the versions in the following groups: we think that the versions of the poem of the 1920s–1930s represent two redactions; in the late 1940s – early 1950s Selvinsky has written a new redaction, that exists in 4 variants and was never published. In 1953 the author has created a new text with the title Uljalaevshhina that had not much in common with the previous ones. Nevertheless the author insisted that it was the same poem, and even indicated the same and only date, 1924. But the results of our research shows that Uljalaevshhina written in 1953 and first published in 1956 has to be considered a new work. The form of the new Uljalaevshhina differs a lot from the form of the old one: for example, the number of lines of the text of 1950s is 1,5 times bigger than the number of lines of the old text, and only 15% of the lines of the new poem are presented in the old one. Comparison of the systems of the personages, the plot structures and the frequency dictionaries reveals that the differences in the content are numerous and significant: for example, 60% of the most frequent nouns and adjectives of the new Uljalaevshhina – and that is to say the ideas and images – do not coincide with the most frequent nouns and adjectives of the old poem. As a digital system of comparison of the texts we have used Juxta Commons, created by the the Applied Research in Patacriticism group at the University of Virginia. The system allows to collate up to 15 versions of the text and offers three types of the visualization: the Heat Map that displays a collation of all the witnesses in the comparison set against the current base document; the Side-by-Side view that displays two witnesses at a time; and the Histogram that shows the overall rate of change across the witnesses. The Edition Starter option allows to see and save the results of the digital collation as a traditional critical apparatus. As an experiment we collated 15 versions of the two fragments of the poem: those that had migrated, at least partially, from the old Uljalaevshhina to the new one: the first chapter and the fragment that in various versions appears as the second, the third or the fifth chapter. The change indexes calculated by Juxta reveal the same dynamics and the same clusters that were proposed after the use of traditional methods. At the same time the experiment helps us to see all the factors that are necessary to consider for the correct and intelligent collation by the digital system: the punctuation, the system of diacritics invented by the author, the type of quotation marks and so on. The poster discusses advantages and disadvantages of the visualizations of Juxta for the work of a textual critic. "
	},
	{
		"id": 212,
		"title": "Visualization of Big Data Phonetics",
		"authors": [
			"Kretzschmar, William",
			"Stanley, Joey"
		],
		"body": " We received funding from the American National Science Foundation to conduct automated formant extraction of stressed vowels, as they occur in sixty-four audio interviews conducted in the Southern US, about 370 hours of speech. The resulting data set contains about 2 million vowel tokens. After extraction, we processed the formants in order to reveal their distributional patterns, overall in the South and in a number of regional and social categories of speakers. Our Big Data phonetics thus represents a challenge to generalizations made on the basis of smaller data sets with few tokens per speaker. Now, with automated tools, we can record complete sets of formants from a data set instead of just a few examples, and our observation of how phonetic realizations actually occur for individuals and in populations can reform traditional understandings of phonetic systems. We have found that the distributional patterns in our phonetic data follow the predictions of complexity science: realizations of each vowel in each group will occur in a nonlinear pattern where a few types of realizations are very common, some types are moderately common, and most types are rare. These distributions, call them A-curves, are scale-free, in that the same A-curve pattern will appear in every subset of the data, whether for the overall set, or for any regional of social subset, or for any individual. Our very large data set, Big Data in a humanities setting, shows that the complex system of human speech, generated by massive numbers of interactions between speakers, creates reliable distributional patterns that linguists can use to make better predictions about how language is produced and perceived in populations. In this paper, we describe a new tool for visualization of all of our Big Data phonetic results, called the Gazetteer of Southern Vowels, available to the public at http://lap3.libs.uga.edu/u/jstanley/vowelcharts/. The site was built in Shiny, a web application framework for R. With Shiny, users can utilize the computational power of the R programming language without having to learn R or install it to their computers. Shiny was designed to make interactive web apps. GSV exploits the framework to supply traditional F1/F2 plots of our phonetic data, and also to supply point-pattern F1/F2 plots that take advantage of a standard tool of spatial analysis. A key feature of GSV is a range of user-selected display features as applied to user-selected vowel types in specific environments, and used to display results from user-selected groups of speakers. GSV works with the 2 million vowel measurements from the forced alignment and automatic format extraction of our NSF grant. Our modifications of forced alignment and automatic formant extraction processesare described in Olson et al 2017, but our methods are not the subject of this paper. The traditional F1/F2 plots of GSV show user-adjustable plots with associated counts and statistical information. Fig 1 shows a plot of the fleece and kit vowels from eight speakers in Georgia, with ellipses for the 95% confidence intervals and means marked with labels for the vowels. Fig 2 shows the same data, now with display of the individual tokens as points. The statistical information is shown in Fig 3. Figure 1 Figure 2 Figure 3 These figures show the difference that Big Data can make for phonetics. Before automatic formant extraction, it was normal for phoneticians to make perhaps 10 measurements of a vowel, and then to take the mean of those measurements as the target location for realization of a vowel. Using Big Data we have hundreds of tokens instead of 10, and their distributional patterns are far more widely dispersed and the mean location, while we can still calculate it, does not appear to represent a very useful generalization about realization of a vowel. Figure 4 Figure 5 With GSV, we can drill down into such distributions at will. Fig 4 shows the tokens from just one of the Georgia speakers. Fig 5 shows the tokens from the same speaker, now only in environments before a stop consonant. Unstressed tokens can be included at will. As Figs 4 and 5 illustrate, the means differ for the different environments for this speaker, and similar differences occur for each speaker and between all the speakers in the database. The flexibility of the visualization calls into question easy assumptions about targets and the amount of variation for phonetic segments. The point-pattern displays of GSV address the usefulness of statistical means and confidence intervals. Fig 6 shows the Georgia tokens for fleece, Fig 7 the tokens for kit. Figure 6 Figure 7 The ellipses have been retained, but the new elements consist of a square grid applied to the F1/F2 space, and shading by density. The darkest grid units have the densest occurrence of tokens, down to clear units with lightest density. Figs 6 and 7 show that there are several units with high density, not a single target unit, and that the means of the ellipses do a poor job of representing how people usually realize their vowels. As for the traditional plots, with GSV it is possible to drill down to single speakers and environments with the point-pattern plots. In addition to the speaker and vowel summaries provided with the traditional plots, the point-pattern visualization provides charts of the frequency rankings of the grid units. Fig 8 shows the ranking for the Georgia fleece tokens, and Fig 9 the kit tokens. The rankings both clearly form A-curves, and this status is confirmed by the Gini Coefficient at top right in each figure. As Kretzschmardiscussed, Gini Coefficients for normal distributions are always below 0.2, while A-curve, nonlinear distributions are always much higher. The coefficients here, near 0.5, match what we expect to find in our Big Data phonetics results. The point-pattern visualization thus has proven, with every chart at every level of scale in the dataset, that the phonetic realizations of human speech match the prediction of complexity science for nonlinear A-curves. Figure 8 Figure 9 We look forward to more and better digital visualizations for our Big Data phonetics, such as a new visualization in 3D phonetic space over time now in development, as in Figure 10. Figure 10 The visualization looks something like a sheaf, which shows either a relatively straight or a curvilinear movement over time. We use five measurements for this purpose, and plot a line between the measurements. Here, the visualization shows 25 tokens of /i/ in the word three and 25 tokens of /ɪ/ in the word six, in blue and green. The base of the visualization is a normal F1/F2 plot, so it shows that the origins of the tokens are relatively dispersed and the blue origins overlap with the green. Over timemany of the tokens are straight, but some are quite bent, either diphthongal or perhaps measurement errors. When we develop the sheaf method we will be able to see and evaluate how duration affects the realization of vowels. The visualization of phonetic duration in 3D will be an essentially new way of looking at phonetic data. This challenge to traditional notions in phonetics would not be possible without effective digital methods. Part of the story is forced alignment and automatic formant extraction, which makes it possible to generate Big Data in phonetics. An equally important form of digital assistance is the visualization tool, GSV, as prepared with Shiny, which allows the analyst to see how Big Data is distributed in F1/F2 space. Traditional F1/F2 plots can begin the analysis, and innovative implementation of a spatial analysis technique in phonetic space offers a striking new interpretation of the way that vowels are realized. "
	},
	{
		"id": 213,
		"title": "VinKo: Language Documentation Through Digital Crowdsourcing",
		"authors": [
			"Kruijt, Anne"
		],
		"body": " The region of Trentino-Alto Adige and South Tyrol in Italy is a multilingual border region which is home to a large number of linguistic varieties and languages. Here dialect varieties of the Romance type, like Venetian, Lombard, and Ladin, are spoken side-by-side with Germanic Tyrolean dialects. Additionally, there are two Germanic minority languages, Cimbrian and Mòcheno, and the national languages of the area are Standard German and Italian. The languages in the region exist in a constant state of language contact and language change, influencing and being influenced by neighboring varieties and the national languages. The grammatical systems of the Romance and Germanic languages families have profound differences, yet influence each other in environments of such long-term intensive language contact. Description and analysis of the languages and varieties of a complex linguistic environment will provide valuable insights into the processes of multilingualism and language contact. This project aims to collect data from this linguistically complex region with innovative data collection methods through a digital platform. VinKois an online platform for the collection of oral data from non-standard languages through crowd-sourcing. The platform is designed to collect oral data for the varieties spoken in the region of Trentino-Alto Adige. Speakers are asked to contribute data by filling out questionnaires on phonology, morphology, and syntax in their own local variety. One of the most original characteristics of VinKo is that it records oral responses, rather than written ones. This method of data collection has many advantages. Not only do speakers answer more spontaneously in a spoken register, but oral data also retains important features of spoken language, like prosodic properties. Additionally, it solves a myriad of problems which speakers of primarily oral varieties face when writing their local language in a digital environment. Many minority language and dialect varieties do not have a standard orthography in which to write their language or might need special characters not found on a standard keyboard. On the VinKo platform, data collection is done through a simple interface and in a quick manner, which facilitates easy collaboration with speakers and speech communities. The primary goal of the project is to provide further insight into the different aspects of multilingualism and microvariation. The contact between varieties of different language families, Romance and Germanic, makes this region particularly interesting for applications of the platform. For example, so far it has been used to acquire a complete inventory of obstruentsand to examine the syncretism of case on pronouns. Investigation into non-standard varieties can reveal a lot about the evolution of grammar, which research into standard languages with prescriptive traditions cannot. This type of microvariation research would be practically impossible using the traditional methods of field work, e.g. one-on-one interviews or paper questionnaires, since there is such a high level of variation and diversity in the area. The platform is still undergoing development and hopes to achieve the following things in the near future. First of all, the area for which VinKo is configured has to be enlarged beyond the borders of the Trentino/Alto Adige region, especially to the Veneto region, and data needs to be collected for underrepresented or missing varieties. Secondly, additional data collection methods, including but not limited to questionnaires, have to be developed and employed. Lastly, but perhaps most importantly, the data collected through the platform must be given back to the community in a meaningful way. Collaboration with local communities and active engagement of speakers with the data will play a crucial role in the larger aim of the project. Minority languages and dialect varieties are experiencing increasing pressure from standard languages, and in many regions, speaker numbers are dwindling. Having an online presence increases the visibility and heightens the prestige of a variety, and it can play an important part in the maintenance of the mother tongue. Increasing the local identity and pride by increasing awareness of the own language can be an important contribution for improving life conditions and slowing down out-migration from remote rural areas. The platform can help to actively engage speakers with not only the language, but also with other speakers and the local cultural institutes. The digital nature of the data collection encourages collaboration between speakers of different generations, as young and old speakers can bring their own specialized knowledge of language and new technology together. The platform straddles the balance between academic and community goals, combining them to the benefit of both parties. It sets itself apart from existing language atlases and data bases by collecting oral data and providing community resources. It is also distinctly different from community focused platforms aimed at language teaching and cultural revival, by adding an academic purpose. The platform is open-sourced, which makes it possible for other low-resourced regions to benefit from this development in the future. This talk aims to discuss and introduce the VinKo platform as a linguistics fieldwork tool and crowd-sourced data base, and hopes to inspire fruitful discussion with scholars with similar research aims, projects, or interests. "
	},
	{
		"id": 214,
		"title": "Calculating Sameness: Identifying Image Reuse In Early Modern books",
		"authors": [
			"Kräutli, Florian",
			"Valleriani, Matteo",
			"Lockhorst, Daan"
		],
		"body": " Our research is concerned with the dissemination and transformation of scientific knowledge across Europe. The basis of our investigations forms a corpus of, to date, 343 books that have been printed between 1472 and 1650. We assembled the corpus around a specific text: the Tractatus de Sphaera by Johannes de Sacrobosco. This 13th century treatise on cosmology describes the spheres of the universe according to the geocentric worldview. Up until the 17th century it has been repeatedly published as part of university textbooks. In these the treatise is included in original, commented or translated form, and accompanied by other texts that were seen as relevant for the study of cosmology from disciplines such as medicine, astronomy or mathematics. As many of these textbooks were part of the mandatory curriculum at European universities, we regard their contents as representative for the scientific knowledge that was being taught and seen as relevant at the time of publication of the books. We extract several markers from the individual books that form the material evidence of our research. In addition to bibliographic data such as publishers, printers, date and place of publication, etc., we identified for every book the content structure: which texts it contains and, if applicable, wether the texts are commented or translated versions of existing texts. In doing so we can not only identify how the content of the books changed and – by extension – how certain disciplines gained and lost in importance, but also which publishers might be responsible for certain changes. The books also contain various types of visuals: diagrams, illustrations, decorative elements, etc. In the same way as texts, these visuals can offer insights into the kind of knowledge that is being distributed. By identifying and analyzing recurring images, we can evaluate the success of certain imagery. If we find the same images being used by different printers, for example, that might be telling of one printer being influenced by another, or even indicate a physical exchange of wooden printing blocks. In this paper we present our approach for analyzing the more than 16.000 illustrations that we have annotated in our corpus. We employ an image hashing algorithm for identifying recurring images and existing visualization tools for analyzing the results. As the algorithm we use is independent of the visual material and – unlike machine learning algorithms – does not need to be trained, it can readily be used on arbitrary image collections. As part of this paper we will offer the entire analysis and visualization workflow for others to reuse. The algorithm organises the images into groups of same or similar ones. While most of the groupings are correct, some diagrams may incorrectly be assigned the same groupProcess The images we analyses are being manually annotated by a team of student assistants using the Mirador IIIF viewer and classified as either Content Illustrations, Initials, Frontispieces, Printers Marks, Title Page Illustrations or Decorations. They are stored in RDF as annotations on the digitised pages of the books, along with the remaining metadata that we gather in the project and store according to a CIDOC-CRM data model in a Blazegraph database. For processing, the respective regions of the original pages are downloaded to a local machine via a IIIF API. We want to identify which of the illustrations and diagrams appear several times in our corpus of books. In other words, we want to organise the total set of images into groups that are duplicates or near duplicates of each other. Duplicate detection in a set of digital images can be achieved through an Image Hashing algorithm, as proposed by Venkatesan et al.. A hash function takes an arbitrary sized input and deterministically produces an output of a fixed size, the so-called digest. For an introduction to hash functions see Knuth. In order to identify images that are not duplicates but variations of each other, a particular type of image hashing algorithm is required. A perceptual image algorithmis designed to take an arbitrary image as input and produce a digest that bears a deterministic relationship to the input image. We use the difference hash, or dHash, algorithmin an implementation for the Python programming language. The algorithm works by scaling down and converting the input image to grayscale, and then produce a digest based on each pixels difference in brightness to its neighboring pixels. The similarity between two images can then be expressed as the difference – the Hamming distance– between two digests. For analyzing the output of the algorithm we make use of a tool initially developed to visualize a collection of coins. The web app, which is freely available on GitHub, allows us to visually inspect the entire set of images and evaluate quality of the identified groupings. Results After experimenting with different values for the threshold and rescaling resolution we found the ones that are optimal for our particular image collection, resulting in match found for 66% of the images in our set. We use a two-pass process in which the images that have not been matched in the first pass are processed again using adjusted settings. The resulting image groups have given us immediate new insights into the reuse of images within this particular corpus of publications. We were able to discriminate clear patterns in the use of certain images, such as, which images have been published throughout the print history of our corpus – there are two – as well as which images have changed context from one text to the other – only one. In contrast to more sophisticated methods of image analysis using pre-trained neural networks, the method used works in a transparent fashion and independent of the material at hand. Only two parameters need to be adjusted – threshold and scaled image resolution – to optimise the output for a given image collection. The algorithm as well as the visualisation tool employed here can be applied on any given image collection. Visualizing the identified image groups against date of publication reveals patterns in the use of certain visuals. The large groups of images at the top represents those that have not been assigned a group. Visualization tool: Coins by Flavio Gortana, "
	},
	{
		"id": 215,
		"title": "Quantitative Analysis of Character Networks in Polish XIX and XX Century Novels",
		"authors": [
			"Kubis, Marek"
		],
		"body": " Introduction From qualitative work of Morettion Shakespeares plays and Chinese novels, through quantitative works on the 19th century English literary fiction by Elson et al.and Jayannavar et al.to the investigation of dynamic plots of German plays by Fischer et al., the analysis of social networks induced from literary works became a valuable tool in digital humanities research. This paper presents a study on induction and quantitative analysis of character networks inferred from Polish novels. The corpus gathered for this study is an order of magnitude larger than the collection of novels used by Elson et al.and Jayannavar et al. It contains primarily novels from the second half of the 19th century and the first half of the 20th century. The main goal of this paper is to present novel results on systematic differences between the 19th century and 20th century Polish prose with respect to the collected corpus. The two by-products of this research are: The development of fully automatized, quantitative pipeline that leads from raw Polish text to the set of testable hypotheses. The reproduction of the observations of Elson et al.and Jayannavar et al.on a larger, more demanding corpus of a different language origin that contains both the 19th century and 20th century works. Figure 1. Main characters of Lalka by B. Prus Data collection In order to build the corpus I utilized two digital libraries that offer texts in Polish. The Polona digital librarywhich is maintained by the National Library of Poland offers digitized copies of printed books. I have managed to fetch from Polona around 3000 volumes that are in public domain and are available online in a form of OCR-ed text. The multi-volume editions of novels fetched from Polona were merged resulting in ca. 2300 complete pieces of literary fiction. The second source of texts for the corpus is the Wolne Lektury librarythat focuses on school readings and offers carefully revised electronic editions of books that are in public domain. Around 230 novels were available for download from Wolne Lektury at the time of writing. In order to make the corpus representative I decided to select exactly oneedition of every novel that has authorship attributed, resulting in 1555 unique pieces of work. Due to the sparsity of available data I restricted my attention to novels created between 1800 and 1945, obtaining 1443 volumes in result. Since the focus of this research is on Polish novels only I have selected from the corpus the books that have Polish origin according to the catalogue of the National Library of Poland. Thus, the corpus used in this paper consists of 930 novels. Before feeding the texts from the corpus into the network induction procedure described in the following section some preliminary processing is required. In case of Polona texts word segmentation errors introduced by OCR have been fixed by a custom normalization script. Furthermore, since the part-of-speechand named entity recognitiontaggers used for the network induction are trained on a corpus of modern Polish language, I have applied a diachronic normalizerin order to contemporize the Polona texts for the purpose of improving the part-of-speech and named entity recognition accuracy. Finally, the novels are split into paragraphs by another script. The texts from Wolne Lektury are checked for errors and contemporized by library editors before publication, hence beside splitting them into paragraphs in accordance to the XML schema no further processing is required. Figure 2 . Novels in corpus by decade Network induction Beforethe conversation network can be inferred from a novel, the text has to be passed to the annotation pipeline that appends additional data necessary for the network induction procedure. The annotation pipeline splits the paragraphs of text into sentences and tokens. Then, the text is lemmatized with help of the Polimorf dictionary.Afterwards, the text is annotated by POS and NER modules that were trained using the manually annotated 1-million word subcorpus of the NKJP corpus. The final step of the annotation pipeline is the detection of dialog boundaries. A script with hand-crafted rules that take into consideration possible shapes of beginnings, endings and internal paragraphs of dialogs is used for this purpose. The script failed to extract dialogs from 23 books, thus the networks have been inferred for 383 19th century novels and 524 20th century novels, respectively. The network induction procedure iterates over dialogs identified in the novel. The dialog turns are surface parsed in order to detect speakers and distinguish them from other named entities that are referenced in the dialog, but do not talk. Since characters can be referred in text in different ways, E.g. by their names, surnames, first names, diminutives, honorifics. the detected speaker mentions are passed to the entity resolution module which is responsible for assigning a common identifier to all mentions of the same character on the basis of the dialog history and the plot. Finally, alink is created in the network for every pair of identified speakers that participate in the dialog. Figure 3. Annotation pipeline Results Elson et al.made a distinction between characters and speakers. I decided to focus the study on speakers only since the NER module identified many entities that are not relevant to the conversations such as historical figures. Plus some false positives that definitely are not named entities. The conversation networks induced from the entire corpus have 34characters on average. The mean number of dialogs is 191 and the average number of conversation links between characters is 80. The average number of conversations that a character is involved into Average node degree according to graph terminology. is 2.28 . I have run the Walktrap community detection algorithmand found the mean number of communities to be 4.68 and the average size of a community to be 5.68. Excluding communities of size 1 that are found by the walktrap algorithm. Network metric 19th century 20th century All character count 36.87 ±27.48 31.48 ±23.56 33.75 ±25.42 dialog count 182.60 ±168.07 196.27 ±158.08 190.50 ±162.42 link count 89.56 ± 109.23 73.16 ±94.08 80.09 ±101.03 average degree 2.35 ±1.81 2.23 ±1.52 2.28 ±1.65 community count 5.13 ±3.49 4.35 ±2.83 4.68 ±3.14 community size 5.83 ±2.94 5.56 ±2.55 5.68 ±2.73 T able 1. Network propertiesandreported that the number of characters in the novel is correlated with the properties of the inferred network. As can be expected the same observation holds for the conversation networks induced from Polish novels. The number of characters is strongly correlated to the number of dialogsand the number of conversational links. Furthermore, the number of communities is strongly correlated to the number of characters. In contrast toI found the number of characters to be positively Instead of negatively. correlated to the average number of interlocutors that a character hasand the average community size. This result may be due to the definitional differencebetween conversational links used in this paper and interaction links used by Jayannavar et al. Network metric 19th century 20th century All dialog count 0.74 0.73 0.73 link count 0.84 0.88 0.86 average degree 0.35 0.54 0.44 community count 0.83 0.83 0.83 community size 0.40 0.46 0.43 Table 2. Network metrics correlated with character number Having the corpus that contains comparable number of the 19th and 20th century novels, I decided to check if properties of networks change systematically between the centuries. Since network metrics are not normally distributed, Normality of all network metrics discussed in the paper is rejected according to Shapiro-Wilk test. I have used the Mann-Whitney test to verify the hypothesis that it is equally likely that a randomly selected novel from the 19th century subcorpus has a lower or higher value of the network metric being tested than a randomly selected novel from the 20th century subcorpus. This hypothesis has been rejected in case of the character number, dialog count, link count and community count metrics and maintained in case of the average degree and average community size. These results suggest thatthe prose of the first half of the 20th century became richer in dialogue, but at the same time focused on smaller sets of characters. Hypo-thesis 19th century median 20th century median p-value 0.95 conf. interval character count 30.0 26.0 0.00098 [2.00, 6.00] dialog count 132.0 161.5 0.00671 [-34.0, -6.0] link count 54.0 43.0 0.00370 [3.00, 15.00] average degree 1.9 1.9 0.68892 [-0.14, 0.21] comm. count 4.0 4.0 0.00020 [0.000068, 1.00] comm. size 5.2 5.0 0.26845 [-0.117, 0.49] Table 3. Network metric change between centuries Final Remarks The systematic differences between the 19th and 20th century Polish novels presented in this paper are interesting on their own, but they can also initiate further computational investigations of the character networks. One problem that is not covered by this study and should be examined in the future is the impact of indirect speech on the structure of character networks. Another issue that should be taken into consideration is the verification if the proposed hypotheses still hold in presence of the constantly growing corpus of digitized literary works of Polish origin. Polona claims to add up to 2000 digital objects such as books, photographs and postcards on the daily basis. "
	},
	{
		"id": 216,
		"title": "The Emerging Paradigm of Bibliographic Data Science",
		"authors": [
			"Vaara, Ville",
			"Ijaz, Ali",
			"Tiihonen, Iiro",
			"Kanner, Antti",
			"Säily, Tanja",
			"Lahti, Leo"
		],
		"body": " Introduction Library catalogues contain rich, albeit potentially incomplete information on historical trends and shifts in knowledge production. Their research potential has been debated for more than 50 years. Large-scale harmonization and analysis of these data collections has provided new ways to investigate classical research hypotheses in book history and intellectual history. Whereas the potential biases in the data, arising for instance from variations in data collection practices over time and place, have to be carefully considered in the analysis, a data-driven approach provides a uniquely large-scale view and a supportive role in research. Despite related earlier work, a systematic research use of bibliographic metadata has proven to be challenging. The lack of scalable solutions for improving and verifying data quality and completeness have formed major bottlenecks for large-scale analysis. Our team has recently proposed the concept of bibliographic data science in order to overcome some of these challenges. Here we provide an overview of the ongoing attempts to develop a broader research line that focuses on the development of targeted analysis methods in this research area, and specifically demonstrate the advantages of fully open bibliographic data science. A number of specific case studies and applications of this approach are included in the proceedings of this conference. Data and methods Bibliographic data scienceis an emerging paradigm in the digital humanities. It aims to improve the overall data reliability and completeness through systematic harmonization, error correction, and enrichment of missing information, thus greatly enhancing the research potential of bibliographic collections. It derives from the paradigms of open science and data science, and incorporates best practices from these fields, including reproducible analysis, open source code, and open collaboration models. We have recently integrated metadata across four large bibliographies and altogether 2.64 million harmonized entries in the period c. 1500–1800from the Finnish and Swedish National Bibliographies, the English Short-Title Catalogue, and the Heritage of the Printed Book database. Compared to the earlier efforts, our automated approach is uniquely scalable and comprehensive in terms of data integration and quality monitoring. Furthermore, it is combined with systematic expert curation and research use that allow us to detect shortcomings and inconsistencies that are historically relevant but challenging to observe by automated means. As such, our newly implemented methods exemplify the application of augmented intelligence in digital humanities data curation and analysis. Case studies Large-scale bibliographies are seldom available as open data. This has formed a bottleneck for the development of new data processing and analysis methods as their usability and value is very limited and restricted to only those research groups who have access to the same or similar data collections. This is in contrast to some other data-intensive fields such as bioinformatics, where considerable data resources are openly shared by national and international research organizations, and algorithmic tools to access and utilize them are being routinely developed and shared widely within the research community. The National Library of Finland has, however, released The Finnish National Bibliography in an openly licensed, machine-readable format. This combination of open data and open analysis workflows allows us to demonstrate the opportunities of fully open bibliographic data science. We have previously estimated the long-term development of book formats, which reflects shifts in reading habits and public communication over time. One example is the observed changes is the rise of the octavo format, which supersedes other printing formats during the eighteenth century, in parallel with a systematic decline in the use of Latin and a growing share of published books printed in vernacular languages. Here we complement such case studies by demonstrating the challenges and opportunities in opening the complete analysis workflows, and show how this establishes the overall methodological basis for the more specific case studies that are being presented in this conference. Conclusion Bibliographic data science is renewing research in digital humanities in general, and in book history in particular. Related data harmonization efforts include for instance the Collections as Data project, which has promoted generic research use of data collections in digital humanities and related fields. In contrast, our work explicitly focuses on the specific research area of early-modern knowledge production and intellectual history. Our focus is therefore more specific than in generic data science projects. The scalability of the work over additional metadata fields and different time periods can pose remarkable further challenges which could be partially addressed by focusing on specific topics of interest, such as variations in authors, language use, publishing networks or document materiality, as is often the case in pragmatic research projects. Our vision includes combining the harmonized metadata with full-text collections such as the ECCO, and studying how the materiality of printing is related to developments in newspapers. When combined with a proper quality control, such data-driven approaches have potential for wider implementation in related studies in the digital humanities. Hence, the contribution of this work is not merely in the development or application of new algorithms or exploration techniques, but in demonstrating their wider potential in advancing the methodological basis of the field. Funding This work was supported by the Academy of Finland [grant number 293316]. "
	},
	{
		"id": 217,
		"title": "OpenTermAlign : Interface Web d’alignements de vocabulaires archéologiques hétérogènes.",
		"authors": [
			"LAMÉ, Marion",
			"PONCHIO, Federico",
			"PITTET, Perrine",
			"MARLET, Olivier"
		],
		"body": " OpenTermAlign est un outil en ligne permettant de faciliter lalignement et le suivi de la qualité scientifique et sémantique des contenus entre un vocabulaire issu dun jeu de données avec le vocabulaire dun thésaurus normalisé, vocabulaires que nous appellerons respectivement terminologie source et terminologie cible dans le reste de ce document. Cet outil a été mis en oeuvre dans le cadre des collaborations et des développements du Consortium MASA pour accompagner la communauté des archéologues dans la publication de leur données scientifiques selon les FAIR principles , et en particulier pour rendre les données archéologiques interopérables. Créé en 2014, le Consortium MASA est labellisé par la très grande infrastructure de recherche Huma-Num . Le Consortium rassemble plusieurs équipes et institutions françaises dans le champs de larchéologie. Au niveau européen, le consortium MASA diffuse ses outils, parmi lesquels OpenTermAlign , au sein du programme H2020 ARIADNEplus , dont le partenaire ISTI réalise le développement. Dans ce cadre, les outils dalignement actuels entre terminologies spécialisées se distinguent plus par le type de service rendu aux communautés impliquéesque par leurs différences entre leurs fonctionnements respectifs, fonctionnement qui se révèlent relativement proches. Ainsi, ces outils dalignement terminologique, dans la chaîne de production et de publication de la connaissance scientifique en ligne, offrent différentes modalités de travail selon les terminologies prises en considération et les objectifs à atteindre. Il savère possible daligner une terminologie source vers une terminologie cible déterminée, vers des terminologies de structures homogènes, vers des structures différentes. En revanche, il nexiste pas doutil intermédiaire pour faciliter lalignement entre des terminologies peu ou pas structurées. Ces dernières se présentent sans organisation standardisée de la connaissance tout en étant fortement contrôlées, dun point de vue scientifique et sémantique, par les producteurs de données eux-mêmes. Au sein dOpenTermAlign, ces producteurs de données doivent coordonner, valider et travailler explicitement la qualité sémantique et scientifique de ces données, sengageant de fait dans une démarche à la fois scientifique et numérique. Ces terminologies sont ici produites par des archéologues avec une certaine liberté linguistique. Ces caractéristiques hétérogènesgénèrent une complexité encore non traitées par les outils dalignement et qui concerne potentiellement de nombreux jeux de données en archéologie. La figure suivante représente les diverses fonctionnalités des outils dalignement en fonction du type de terminologie. Fig.1 Usages de différents outils dalignement en fonction de différents modèles dorganisation de la connaissance Notre outil OpenTermAlign propose de gérer le flux de ce travail dalignement, travail rendu complexe de par lhétérogénéité structurelle et dusage des terminologies, en sappuyant sur six pratiques pour assurer la qualité scientifique des données :employer les mêmes fonctionnalités que les outils traditionnels dalignement,assurer une compatibilité et une continuité technique avec dautres terminologies et les outils dalignement qui existent déjà,accompagner de la façon la plus simple possible le travail et le dialogue entre les différents acteurs,simplifier lappariement pour lexpert du domaine, et enfinpermettre lélaboration, la coordination et la validation dune sémantique partagée explicite entre la terminologie source et la terminologie cible. Cette élaboration doit aboutir à une situation définitoire ainsi appelée, car elle ne peut, à cette étape, accepter que trois optionset représentée dans linterface par une zone définitoire où se déroule une coordination sémantique à valider par lexpert ;vérifier le positionnement entre les éventuelles structures et, ipso facto , la sémantique afférente aux structures. La bonne pratiqueimplique delimiter les actions dalignement confiées à lusager. Celles-ci sont ramenées à quatre étapesdurant lesquelles un seul choix est requis parmi quelques propositions assertives, pré-sélectionnées chaque fois que possible. Cette même pratiqueimplique deproduire une table de correspondancecomposée dune table de concordances terme à termeet des résultats de lalignement. Les différentes combinaisons possibles à chaque étapesont ramenées à sept situations décrites ci-après :le constat dune incohérence entre le terme et la définition dans la cible,la situation définitoire doit être harmonisée,Les deux termes sont à discuter,les termes et la situation définitoire sont à discuter,le positionnement du terme source dans la structure cible crée une difficulté,la définition de la source est à préciser,Aucun problème. Fig. 2 Table de correspondance Ces sept situations se répartissent en quatre actions déquipe autour du travail dalignement afin de faciliter la prise de décision, la redistribution des tâches et la mise à jour automatique des bases de données sources et lenrichissement semi-automatique de la terminologie cible. En outre, linformation état de lalignement se retrouve aussi bien dans la table de correspondance que dans la notice de lalignement lui-même. Ces quatre actions sont les suivantes :Réélaboration - concerne une situation trop problématique pour être maintenue en létat,Concertation avec les équipes de la terminologie cible - regroupe un ensemble de problématiques sémantiques concernant la terminologie cible,Concertation avec léquipe de la terminologie source - regroupe un ensemble de problématiques nécessitant une réflexion scientifique en amont,Consensus - regroupe les situations validées à savoir alignementou proposition denrichissement avec un nouveau candidat. Fig. 3 Notice denregistrement de lalignement pour le mot abside Loutil a été testé avec Une terminologie source composée dun cluster de mots de 600 termes en provenance de quatre bases de données archéologiques différentes. Vers une terminologie cible spécialisée servant de référentiel pour larchéologie francophone. Il sagit du thésaurus PACTOLS. Celui-ci, débuté en 1987, référence aujourdhui plus de 50 000 concepts, organisés en sept domaines, chacun disposant dun URI ARK. Il respecte la norme ISO 25964 sur les thésaurus multilingues et sur lorganisation et linteropérabilité des thésaurus. Lalignement avec le méta-thésaurus Backbones, Geonames et Wikidata est un travail en cours. La structure du thésaurus cible permet de solliciter lexpert sur un nombre de choix limité aux seules quatre étapes. Par exemple, à létape 1, le terme peut être candidat dans la cible, convenir à la cible, être synonyme de la cible, être incompatible avec la cible, être associé à un positionnement sans candidater. Les combinaisons exploitables retenues dans ce test précis sont au nombre de trente-deux. Sur les 600 termes alignés, 300 candidats sont proposés à lenrichissement des PACTOLS. Sur les 300 termes restants, environ 70% ne posent pas de problème particulier et les 30 % restant nécessite des consultations déquipe pour les ajuster, les préciser et aboutir à un alignement satisfaisant et fonctionnel aussi bien sur le plan sémantique que technique. OpenTermAlign , est un fork d Heterotoki et a été adapté aux exigences des archéologues en permettant de conserver et de caractériser de la terminologie source en langue ancienne, en moissonnant les traductions des termes dans les pages Wikipédia correspondantes dans les vingt-quatre langues, en interagissant avec lendpoint du thésaurus archéologique cible, en installant en local ce même thésauruset en moissonnant ses identifiants ARK. Loutil a été mis à lépreuve avec dautres terminologies cibles de type thésaurus offrant les mêmes garanties techniqueset les fonctionnalités permettant lalignement dun même terme vers plusieurs vocabulaires cibles sont à létude. Notre outil sappuie sur une configuration GUI standard, est en accès libre et produit ou participe à produire des données respectant les principes FAIR. Linterface web repose sur des technologies standard du web : php / MySQL, JavaScript. Limportation de la terminologie source se fait au format CSV ou SKOS. La navigation dans la terminologie cible est possible selon deux modalités :limportationdans un triplestore - particulièrement utile pour les thésaurus ne disposant pas dun endpoint opérationnel.Accès directe via lAPI. Lexport quant à lui advient au format csv ou SKOS. "
	},
	{
		"id": 218,
		"title": "Database Aesthetics and Ergodic Ephemerality: Remediating the Scrapbooks of Edwin Morgan",
		"authors": [
			"Moynihan, Bridget",
			"Armoza, Jonathan",
			"Lang, Anouk"
		],
		"body": " As forms of digital textuality proliferate, genres such as the periodical and the scrapbook have come into view as productive lenses through which to understand the relationship between digital texts and print culture artefacts. Observing that the process of digitally remediating a periodical can lead to the emergence of surprising affinities between the two categories, Sean Latham suggests ways in which such remediation makes it possible to see periodicals as forming a bridge between print and digital media. Similarly, Ellen Gruber Garvey points to the connections between scrapbooks and contemporary information management practices, arguing that just as individuals in the present day manage digital abundance with favorites lists, bookmarks, blogrolls, RSS feeds, and content aggregators, so nineteenth-century readers channelled the flood of information with scrapbooks which allowed them to save, organise, and reprocess this information. These reflections on how the affordances of particular print culture genres produce particular reader behaviors chime with Johanna Druckers call to think about what a codex book does rather than focussing on what it is: to understand it in terms of what is known in the architecture profession as a program constituted by the activities that arise from a response to the formal structures and, through this process of denaturalizing, come to better understand its specific structural and technological features. In this paper we describe our digital prototype, Working from Scraps, which remediates a 200-page subset from the scrapbooks of Scottish Poet Makar Edwin Morganheld at the Glasgow University Library Special Collections. A sample two-page spread from one of the scrapbooks is provided in figure 1, and a screenshot of this prototype is given in figure 2. In considering how our prototype engages with some of the questions above, we seek to address the gap in the scholarly literature around the specific utility of scrapbooks as objects to think through questions about the relationship of material texts to their digital analogues, to digital textuality more generally, and, we will argue, to other genres such as database and narrative. Remediating a set of scrapbooks as a relational database and a digital interface provides the opportunity not just to denaturalize a scrapbook and consider how its elements relate to one another, but also to better understand what is involved in traversing the text, to repurpose a phrase of Espen Aarseths from his discussion of ergodic literature: the effort required to tie image to text and carry out other meaning-making activities. Constructing a database-backed digital prototype from a set of fragile scrapbooks which cannot be moved from the archive has the obvious benefit of bringing these artefacts, and the period of twentieth-century life which they meticulously capture, to a wider public than those individuals who are able to consult the artefacts in person. From the perspective of digital humanities, however, such a remediation has additional value in shedding light on the interplay between database form and more conventional book historical and literary critical categories. Drawing on art historian Ervin Panofskys characterisation of linear perspective as a key symbolic form of the modern age, Lev Manovich proposes that the database can be understood as a new symbolic form of the computer age, one which offers a new way to structure our experience of ourselves and of the world. If digital interfaces present us with an endless, unstructured array of images, texts, sounds, and other kinds of data records, Manovich maintains, it makes sense to model these records as a database, with the further consequence that we then need to develop a poetics, aesthetics and ethics of this database. The Working from Scraps prototype seeks to advance this project of working through what database aesthetics, poetics, and ethics might look like, but with the additional imperative of engaging questions around materiality, ephemerality and copyright restrictions which are not ordinarily so prominent in such discussions. When deforming a set of scrapbooks into a digital prototype, opportunities to learn arise from the process of thinking about the underlying data structures. Latham, borrowing terminology from Aarseth to describe the digital archive The Modernist Journals Project, describes how creating a digital edition of a periodical involves first identifying its textons—the discrete parts which form its basic units—and then determining how those might be assembled into scriptons—the unbroken sequence of information that is delivered to the user on the screen. This identification was one of the core conceptual tasks involved in the construction of the Working from Scraps prototype, and it included the following challenges: As the scrapbooks were created between 1931 and 1967, and thereby fall squarely into the copyright black hole of the twentieth century, the time and financial costs of copyright clearance for the thousands of third-party materials prevented us from simply displaying digital facsimiles of pages and the clippings constituting them for our users. Information about each page needed therefore to be communicated legibly enough that a user who had never seen the page at all could understand it. This activity, borne of practical necessity, quickly became ekphrastic and raised specific identification questions: How much description should be provided? To what level should a scrapbooks visual components be broken down: pages, clippings, or other units? How could we recuperate some of the visual presence of the clipping components through non-verbal means, so as to not rely solely on this ekphrastic practice? As databases aim to provide machine-readable data which can be queried by users, they present an imperative for a degree of standardization. With scrapbooks, however, as with so many other real-world objects, such standardization can be difficult to produce, and is not always desirable to impose. Chronological data present one example of this problem. If a date is provided either by Morgan or as part of a clipping in his scrapbooks, the form in which it is given varies: as a year, a month and a year, or as a precise date. How should such dates be standardized, if at all? How would such standardization account for theundated items? And does this standardization erase the messiness of the scrapbooks presence? A similar challenge arose around the task of quantifying elements from the scrapbooks that were not inherently quantitative, for example finding ways to register how clippings are layered on the scrapbook pages. Our database uses a z-index format in which 0 denotes something drawn directly onto a page, 1 a clipping pasted onto a page, 2 a clipping pasted on top of another clipping, and so forth. However, an occasional foldout clipping with clippings pasted onto the back of the fold disrupted this schema, as it was unclear whether the attached clippings should be coded as 1, as 2, or as something different altogether. Difficulties such as these betokened an additional challenge: to avoid presenting the scrapbooks too thoroughly through quantitative lenses such that description, natural language, images, and so forth would be obscured. In presenting Working from Scraps as an exemplary prototypical object for working through these and other challenges arising from the remediation of print culture artefacts into data structures and digital textual forms, we also offer our interface as an apt illustration of Matthew Kirschenbaums point that platform studies and editorial studies share significant common ground, concerned as they both are with the material substrates through which a text takes shape and acquires meaning. In addition to revealing a wealth of detail about twentieth-century life, Scottish culture, queer identities, technological developments and a wide range of other topics, the Morgan scrapbooks and their digitally de/reformed versions seek to contribute to the conversations around how a readers asynchronous, contingent, nonlinear encounter with the textmay be remediated, and how the modelling of unruly print culture objects can be used to interrogate our assumptions around what constitutes a literary text. Fig. 1. Double page spread from Edwin Morgans Scrapbook 1, pp. 227-8. Made available by the Glasgow University Library at www.flickr.com/photos/uofglibrary/4545010659/in/album-72157623915217656/ . Figure 2. Screenshot of the Working from Scraps prototype, showing different views on the data contained in the clippings. "
	},
	{
		"id": 219,
		"title": "Using Data Visualization to Explore International Trade Agreements",
		"authors": [
			"Ford, Oliver",
			"Serrano, Esteban",
			"Du, Xinyu",
			"Lang, Anouk"
		],
		"body": " This poster explores what can be learnt by applying different data visualization methods to a corpus of 450 preferential trade agreements, gathered and structured into XML format by the ToTA: Text of Trade Agreements projectand available at https://github.com/mappingtreaties/tota. As lengthy legal documents, these agreements present an interesting challenge to data mining and visualization: they contain a large amount of boilerplate and have a high degree of similarity to one another, with the devil in the detail, so their interpretation by human readers requires significant trade and legal expertise. There is, moreover, currently relatively little crossover between computational analysis and the domain of international trade, so the ToTA corpus opens up an opportunity for discerning which computational methods have the most potential to drive forward research at the interdisciplinary intersection of legal research and digital humanities. Our research questions centered on two main areas. First, what kinds of relationships between countries can be discerned by examining the text of legal documents that regulate economic interactions between those countries? And second, what is the relationship between the documents themselves, especially concerning how earlier trade agreements may have influenced those that followed them? In addressing these questions, we employed a number of different data analysis and visualization approaches: Topic modelling and visual rendering of the similarity of documents based on topics within them demonstrated that trade agreements tended to cluster along regional lines, with some chronological patterning. Figure 1 demonstrates how the trade agreements can be broadly separated along continental lines into Europe, CIS and East Asia, East Asia and Central America, and North America and Oceania, with the nations in the Commonwealth of Independent States engaging predominantly only with each other. Network visualizations were used to show the proximity of different countries based on the trade agreements in which they were involved. Figure 2 gives an example of this: a network representation of the countries who signed agreements with each other in the period 1989-1998. This graph produced some expected clustering along the lines of geographical regions, for instance the former Communist Bloc nations such as Ukraine, Georgia, Kazakhstan and Tajikistan, whose position between the nodes representing the EU, the EC and EFTA on the one hand, and the node representing the Russian Federation visually embodies the way these nations were caught between two power blocs in the period of geopolitical realignment following the Cold War. It also represents some less obvious similarities, for instance the clustering of smaller political entities such as Andorra, San Marino, the Palestinian Authority and Finland. Creating a number of network visualizations using chronological slices of the data enabled us to represent change over time, and using these it is for instance possible to see trends such as the gradual rise in prominence of Asian nations such as Malaysia, Korea and Vietnam in the years from 1999 to 2016. A word2vec word embedding model was trained on the text of the agreements. Despite the high proportion of boilerplate, the model successfully clustered terms from the same domain, for example import/export, progressive concernsand bureaucracy. Figure 1. Visualisation of trade agreements clustered by topic demonstrating regional similarities and showing change over time. Figure 2. Network visualization of countries which signed trade agreements between 1989-1998. "
	},
	{
		"id": 220,
		"title": "Finding Visual Patterns in Artworks: An Interactive Search Engine to Detect Objects in Artistic Images",
		"authors": [
			"Lang, Sabine",
			"Ufer, Nikolai",
			"Ommer, Björn"
		],
		"body": " Summary Objects are essential to artistic images as they reveal a persons identity or profession; repetitions then testify to their popularity, reveal processes of reception and artistic relations. Thus it is crucial for art historians to find objects in images. Digitization has produced large image corpora, but manual methods proof to be insufficient to analyse these collections; the collaboration between art history and computer vision provides methods and tools, which enable a comprehensive evaluation of images. The paper presents a user-oriented search engine for object retrieval, thus assisting with art historical research. After presenting specific requirements for retrieval systems, the paper introduces the engine, exemplifies a search and shows qualitative results. We include critical remarks on existing tools and possible issues, which arise when working with artistic data. Figure 1. The detection of objects is crucial for art historians to study reception processes or artistic relations. Here, the skull is shown in artworks of different genres, styles and techniques throughout time. Image by the Computer Vision Group, Heidelberg University Object detection and retrieval have been core tasks in computer vision; results are obtained by using hand-crafted featuresor learning-based features, favorably used in recent years due to the rise of CNNs. Works used a template-based detector to find gestures in manuscriptsor additional curvature information of objects to improve detections. A discriminative model based on parts and aggregated compositions was further utilized to propel object detection and scene classification. The success of CNNs has triggered research on how objects can be localized more precisely in images, also using region-of-interest proposal networks. Networks were then used to detect objects in artworks, establish visual links between paintingsand find patterns in art collections by adapting a deep feature to this task. These works emphasize the communitys interest in using computational approaches for object detection; however, suggested methods have rarely been implemented in publicly accessible systems and thus cannot be used by art historians in practice. Project description We developed an interface and underlying search engine for object detection based on the workflow and specific requirements imposed by art historical research. Requirements were observed first–hand and formulated by computer scientists and art historians and refer to the handling and functions of retrieval systems. The following aspects were identified as crucial: the interface must be intuitive to use, allow for an interactive experience, is accessible from the outside and provide the possibility to study large and diverse image collections. Systems must be applicable to diverse data, across various media or styles, enable a visual search – this is essential since most images have incomplete, false or missing metadata – and allow to search for entire images and object regions in images. The latter is of relevance to art history because objects provide more information about a depicted person or hint to a specific iconography: the lion as an attribute of Saint Jerome is just one example. The search process should be performed fast, enabling a free exploration of the data. Figure 2. The figure shows the initial page of the search engine for object retrieval, where all available collections are displayed. To search for regions, the user can select an existing dataset or upload a new one. Image by the Computer Vision Group, Heidelberg University So far, search engines mostly allow a text or entire image search: ArtPI 1., developed by Ahmed Elgammal and team, uses deep learning methods to perform aforementioned tasks and a recognition of style, artist and genre. The Oxford Painting Search 2. by Oxfords Visual Geometry Group, enables a text, color and structure-based or entire image search, but does not allow to search for image regions. Replica,offers a text, entire image and region-based search, however, only in a given dataset of mostly Venetian art 3. While other systems fulfill some requirements, they are only partly sufficient for art historical research. It was our objective to develop a system, which considers all listed requirements, focusing explicitly on object retrieval to assist with a formal and semantic analysis. Figure 3. The system enables an object search: here a region in a portrait of James Timmins Chance by Joseph Gibbsis selected with a bounding box and defined as the search query. Image by the Computer Vision Group, Heidelberg University Introducing the search engine The search engine and corresponding interface was developed in collaboration between computer scientists and art historians, thus considering technical possibilities and art historical requirements. The final engine offers to search for entire images and regions in large datasets. A learning-based approach is used, where an exhaustive search is performed using CNN features to find identical and most similar regions to a user-defined query. The process solely relies on visual input and can be described as follows: after uploading a new or selecting an existing collection, the user selects an image and marks, for example, an object with a rectangular bounding box. This allows to find images of a specific subject, which requires the presence of certain objects, or to study form developments over time and space. The search process is triggered; underlying algorithms operate on CNN features, which demonstrate enormous potential for processing and analyzing large datasets in an unsupervised manner. In contrast to HoG features, used for retrieval tasks in the past, features extracted with CNNs also contain high level information about semantically abstract conceptsand encode context information, hence are more suitable for object detection. After the search has terminated, Figure 4. Retrieval results can be viewed from distance, showing more context information, and in close-up. Images by the Computer Vision Group, Heidelberg University results are displayed in another window with decreasing similarity. The search engine not only detects identical but also similar regions; finding variances of a motif is relevant, when art historians aim to reconstruct reception processes of a particular object. Other functions add to the usability of the interface: the addition and access of metadata, storage of favorites and alternation between a close-up and distant view of images and retrieved regions. The layout of the interface supports an easy, intuitive navigation through the search process, where each function aims to simplify the workflow for art historians: the simultaneous view of selected favorites, for example, allows for a comparative analysis. Figure 5. Shows the obtained results, based on the query, which is displayed in the top left corner. Results are arranged with decreasing similarity and show that the system was able to retrieve similar regions to the selected part. Image by the Computer Vision Group, Heidelberg University Figure 6. Shows retrieval results for a queryin a dataset of street art. The engine was able to retrieve similar regions to the selected part. Image by the Computer Vision Group, Heidelberg University The system has proven its applicability to diverse datasets, such as medieval prints and pre-modern paintings, addressing different research questions. How conventional is the representation of specific objects?shows that algorithms were able to retrieve the motif of a hand holding a letter in a challenging dataset of pre-modern paintings. Results indicate a great conventionality, mostly showing portraits of seated men, holding a letter in the right hand, while the left is put loosely on an armrest. Variations are shown in the second and third row, emphasizing that the system also detects variances of motifs; retrievals one and three of row two disregard the pose, the former also displaying a different subject matter. How popular are hats in street artworks?shows search results for the query hat obtained from a dataset of street art: images highlight that Brazilian street artists OsGemeos often use hats in different shapes and colors for their yellow figures. Eventually, the tool enables a quantitative and qualitative analysis of the data: one might study the formal development of an object over time in a large dataset or fine-grained similarities between objects during a limited time period. Since computer technologies allow to study large image collections in a short amount of time, scholars can explore image sets first and formulate their research questions after they have assessed the structure and content of the data. This is not possible with traditional methods because it is too laborious. So far, a dataset for content-based retrieval in artworks does not exist; therefore we collected a dataset of 1101 historical paintings consisting of various media. We compare retrieval results obtained by our and a HoG-based model. Quantitative results are provided in Table 7 and a qualitative retrieval example is presented in Figure 8. Besides introducing the retrieval system, the paper includesremarks on existing tools and possible issues, which arise when working with art data; some of which have been mentioned, such as only allowing for a text or entire image search. Systems are then often challenged by unknown, often pre-modern object categories, such as medieval clothing, buildings, swords etc., because most networks are trained on ImageNet, a database which was collected without artistic consideration containing only modern object categories. Also deformations of objects and visible brushstrokes, due to the respective style, further challenge algorithms.illustrates a failure case: the abstract style and the use of ImageNet features, which were used to retrieve a hand in a dataset of modern, abstract portraits, lower the performance of algorithms. Additional issues arise from missing or incomplete metadata or bad-quality reproductions. Table 7. A comparison of precision accuracy for top k retrievals using our and a HoG-based model. We calculated the mean precision of 8 queries from different object categoriesConclusion The paper presents a system for object retrieval to analyze large image collections, thus assisting with art historical research. It enables a quantitative and qualitative evaluation, supports a form and semantic analysis, allows to study reception processes and artistic relations on large scale. The paper lists requirements for search engines, which were formulated by art historians and computer scientists, and illustrates how these were implemented. Eventually, we provide search examples and results and point to possible challenges when working with art images. The retrieval system is available from the outside: users do not need to install it but can access it online. Figure 8. Qualitative example with our search engine. We show top 12 retrievals for one query from four annotated categories. Notice that our system was able to retrieve objects correctly. The search was performed in the dataset consisting of 1101 historical paintings. Image by the Computer Vision Group, Heidelberg University Figure 9. Shows an example of an erroneous retrieval performance; the abstract style and ImageNet features lower the performance of retrieval systems. Image by the Computer Vision Group, Heidelberg University Notes 1. Link to the search engine ArtPI, developed by Ahmed Elgammal and team, https://www.artpi.co/2. Link to the Oxford Painting Search by Oxfords Visual Geometry Group, http://zeus.robots.ox.ac.uk/artsearch/3. Link to the Replica Search Engine by the École Polytechnique Fédérale de Lausanne, https://diamond.timemachine.eu/"
	},
	{
		"id": 221,
		"title": "A Techno-Human Mesh for Humanities in France: Dealing with preservation complexity",
		"authors": [
			"Larrousse, Nicolas",
			"Marchand, Joel"
		],
		"body": " Nowadays, as the use of digital data for research in Humanities has become the norm, researchers are dealing with a huge amount of data. As a consequence, the risk of data loss is increasing. Another difficulty is to provide full access to this flood of data to users often located in distant areas. These problems can no longer be addressed individually by researchers or even at a laboratory level: it is therefore necessary to use a technical infrastructure with specific skills to provide stable preservation services. Huma-Num https://www.huma-num.fr/about-us , the French national infrastructure dedicated to Digital Humanities, was looking for a way to address these challenges. The main goal was to deploy a technology that would be readily usable and transparent for average users. Scalability was mandatory considering the rapid evolution of the mass of data, and the system should be, ideally, distributed to ensure better security. Besides these purely technological requirements, we also had some political and organizational concerns. We wanted to delegate the close relationship with users and local administration to an existing robust network of regional centres. By doing so, we expected a better appropriation of the proposed solution and also an enhanced capacity to respond to their specific needs. This paper will present the implementation of a preservation system in France, branded Huma-Num-Box, which aims to address all the above-mentioned goals. Then, we will give some feedback about this experiment and actions for the near future. A technical choice We all know that researchers do not really take preservation into account during the research data life cycle, especially at the early stage, but are more accustomed to making copies on a local device. Accordingly, the first goal was to provide a device as simple to use as a local hard drive. Moreover, we operate in a classical server landscape and we need to be able to access these data on servers using different technologies. We had substantial experience with IRods software https://irods.org/  which was very efficient but not really user-friendly to say the least and not totally tailored to some servers technologies. After some research, we decided to go for Active circle https://www.oodrive.com/products/save/active-circle-storage-archiving-solution/ , a software edited by a French company. Here are some reasons why we made this choice: - It uses standard hardware which can be recycled and new hardware can be added with no re-replication - The file system is natively distributed and provides all the protocols we desiredfor users and servers - You can delegate user management to each node relying on a classical LDAP system - You can mount a share as local and you can share data between nodes - For each set of data, you can easily decide on the policy you want to apply- All the steps of the data life cycle are integrated in a single tool- We knew that the support was very reactive. This software seems to be perfectly adequate and even offers more than we expected but the choice of a commercial software was not an easy one. One downside is the cost: the cost of the licence is far from being marginal and is related to the data size. A Human mesh Huma-Num provides services at a national level, but relies on a network of 23 regional centres, called Réseau des MSH http://www.msh-reseau.fr/les-msh , to pass on information about its services and in return get some feedback. This network has been around for 20 years and each centre incubates research projects and provides local services. It therefore seemed logical to set up our nodes in some MSH. Each node is associated with a technical correspondent who manages local accounts and shares under the supervision of Huma-Num. His/her role is also to ensure links with local system administrators and to perform the administrative tasks. Feedback and the near future After two years, we have a mesh of nine nodes geographically distributed all over France used by 500 users with 100 data shares. We now host around 500 TBs of data, which was quite unexpected so we were forced to expand the system. We were able to save endangered archaeological data located in remote Ecoles Françaises à lEtranger. We also discovered a set of dataof very important contemporary historical archives which did not have a single backup because of the lack of local resources. So, we can say that its a success. A nice side effect is that we built a logical network above the national network https://www.renater.fr/?lang=en to connect our nodes: this private network for SSH is ready to be used for future services. We now consider that this service is mature enough to make it available at a European level via the ERIC DARIAH https://www.dariah.eu/ infrastructure. However, the use of the mesh is very uneven. Some nodes are quite empty, and we decided to use them as backups for other nodes. This means that there is probably more work to do to convince users of the benefits of using it. In order to address this, we organize meetings, called Huma-Num-Bar, to inform communities about our services: these meetings are broadcasted and archived. We also do a MSH Tour to interact directly with potential users. The purpose of this paper is to demonstrate that providing a technology alone is useless: the key to success is user uptake. You absolutely need to rely on a network of expertise to explain the project and to make it work on a daily basis. We also learned that it takes time: from installing an intrusive machine inside an existing network until its use by researchers and engineers, every step comes with different difficulties. The main one is the real complexity of coordinating such different categories of actors involved in this project. "
	},
	{
		"id": 222,
		"title": "“Un Manuscrit Naturellement ” Rescuing a library buried in digital sand",
		"authors": [
			"Larrousse, Nicolas",
			"Jacobs, Christophe",
			"Jacobson, Michel",
			"Kagan, Gilles",
			"Marchand, Joel",
			"Masset, Cyril"
		],
		"body": " Manuscripts in a digital necropolis This story really began during the Middle Ages, with the creation of manuscripts by copyist monks Un manuscrit naturellement. In 1930, Félix Grat a French archivist paleographer, experimented with a sophisticated camera to create microfilms of manuscripts in order to make them widely available and facilitate their study. This resulted in the creation of IRHT Institut de Recherche et dHistoire des Textes. See https://www.irht.cnrs.fr/?q=en , an institute devoted to fundamental research mostly on medieval manuscripts. In 1979, an agreement was signed with the Ministry of Culture and IRHT to digitize all the manuscripts stored in French public libraries. This corpus is among the largest digitized medieval sources: the work is still in progress! The original digital copy was stored on Huma-Nums Huma-Num is the French national infrastructure for humanities which provides mostly digital services. See http://www.huma-num.fr/about-us infrastructure: it was made of files for the manuscript pages encoded in TIFF format representing a huge volume of data, around 40 TBs distributed in 2 million files. The fantasy of digital immortality is widely shared, but in reality, digital resources are highly fragile. Even if we are able to store them safely in a readable format, they prove to be totally unusable if we dont provide related information to understand their content and their organization. In short, over many years, we have built a very safe digital necropolis progressively covered by layers of digital sand rather than a clean organized library. The context As the original material of this set of data is clearly part of French cultural heritage, it is important to conserve both the manuscripts themselves, some of which are no longer physically available for consultation due to their poor condition, and the scientific work already done on them. It was becoming a matter of urgency to take action as the memory of the project began progressively to disappear mostly because of the numerous changes in human resources. We have an institution in France, the CINES Centre Informatique National de lEnseignement Supérieur. See http://www.cines.fr , dedicated to the long-term preservation of research data. But the cost of preservation in our case was too high. In the meantime, the French National Library had successfully converted part of its resources from TIFF format into JPEG2000, thereby reducing the data to one-third of the original size, and prices at CINES had fallen dramatically. It now became financially reasonable to consider the preservation with the CINES using the JPEG2000 format. We therefore decided to begin the preservation project. Dealing with the data abundance and imperfection The first need was to sort and organize the huge number of files: deleting files is a serious decision to take. We made a copy of the corpus on the new storage system, and began to clean the data. We kept track of every single step in order to be able to go back over all the changes. Eventually, we succeeded in getting rid of one million files, mainly technical files and redundant images. The file tree also required some transformations as it was based on library names, and of course some of them had changed over this long stretch of time. We also needed to solve traditional encoding problems and special characters in both file and directory names. The analysis of the technical metadata also showed that some files were missing. A further check showed that some other files were empty or corrupted. All these files were manually checked, and some were regenerated from a copy. The next step was to transform all the files into JPG2000 format. The goal was to ensure that the transformation was technically correct but also that the image was still human-readable and of good quality. The result of this test workflow on a small sample of pictures showed that different TIFF encodings caused many errors and that it was again necessary to carry out a manual check on some files. It took us no less than two years just to do this part of the work. Documenting data Then, it was time to retrieve the corresponding metadata from various technical and scientific databases. During this process, we discovered that for some manuscripts, the identification number was not correct: it was again necessary to re-adjust the file tree. To encode metadata, we chose a mix of different standards encapsulated in METS format to describe all the metadata available: TEI for scientific metadata and XMP for technical stuff. Lastly, we had to abide by French law on archives, which complicated matters even further. At last, we were able to create nice packages compliant with the needs of the CINES platform based on the OAIS Open Archival Information System. See https://fr.wikipedia.org/wiki/Open_Archival_Information_System model. What we learned To achieve this project, it was necessary to assemble a team of people with very different skills and backgrounds: it was not easy, to say the least, to make this team operate smoothly! We had on board Huma-Nums system administrator and also people from IRHT, the database experts to take care of all the relevant metadata and the manuscript photographer who also happened to be the living memory of the project, and last but not least some archivists. Its impossible do this kind of work if you dont have access to a proper infrastructure: we had around 80 TBs to deal with, and we also needed computing power to proceed with the format migration. You cant use the same approach with millions of files as you do with a standard corpus. The person who was the memory of the project was the key whenever decisions needed to be taken: this is due to the complexity of dealing with material created over a long period of timeby different persons. "
	},
	{
		"id": 223,
		"title": "Repetition And Popularity In Early Modern Songs",
		"authors": [
			"Lassche, Alie",
			"Karsdorp, Folgert",
			"Stronks, Els"
		],
		"body": " This study explores the relation between repetition and popularity in Dutch historical songs. Previous studies on song lyrics have shown that contemporary songs stand a greater chance of reaching #1 of the Billboards hit chart when their lyrics are more repetitive. This preference for repetitive structures is a well-known cognitive bias, yet little is known about whether similar preferences were at play in historical popular song lyrics. The current study aims to address this question by quantitatively modelling the relationship between popularity and various forms of repetition in the lyrics of a large-scale collection of historical songs and, subsequently, relating our findings to observations in the modern era. While we acknowledge the effect of musical repetition on a songs popularity and the way this can affect our results, we focus in this study on textual repetition. As our object of scrutiny, we investigate a large sample of historical song lyrics from the Dutch Song Databasehosted at the KNAW Meertens Institute. This database contains data of over 175.000 descriptions of Dutch songs, from the Middle ages up to the twentieth century. We focus on material from 1550-1750 – since the 17th century was characterized by Grijpas the golden age of the Dutch Song – and analyze a sample of approximately 22k song lyrics and available metadata. All songs have been encoded with TEI compliant XML, which provides both metadataand the actual lyrics of a song. Investigating the interaction between popularity and repetition in historical songs poses two important challenges. The first challenge is to establish a ranking of songs reflecting their contemporary popularity: after all, what is the historical equivalent of the modern Billboards hit chart? We solve this problem by approximating an early modern hit chart, in which the popularity of a historical song is defined as the interaction of several variables that affect the popularity of a song. Inspired by studies of Farmer and Lesseron the structure of popularity of early modern print sources, we define the popularity of a song as the interaction ofthe number of reprints of a song in a fixed time period) andthe geographical distribution of places of print of a song, as a reflection of the either local or wide-spread popularity of a song. The second challenge involves measuring repetition. Repetition in text can be measured on various dimensions, such as words, lines, letters, consecutive onsets and n-grams. In this study, we quantitatively estimate a songs degree of repetitiveness using a variety of information-theoretical measures. More specifically, we employ different methods of text compression, such asthe Shannon Entropy andthe Lempel-Ziv-Welch-algorithm. Drawing inspiration from prior work by Alexander, we measure repetitiveness of words using the Shannon Entropy, which estimates the degree of uniformity in a message, and can be expressed as follows: where p represents the relative frequency of item i in collection A with n items. To control for differences in document length, we use the normalized entropy: The second compression method is the LZW algorithm, which, put simply, incrementally encodes 8-bit dataas fixed-length 12-bit codes. We compute the LZW score for song fragments as the number of characters in fragment x divided by the number of codes used to encode x. Using the above-described compression methods as predictors, we model the relationship between popularity and repetition with regression models. Being at the intersection of several disciplines, the current study aims to contribute to literary and computational research, but also gives insight in the appreciation of the human brain for repetitive and nonrepetitive patterns. "
	},
	{
		"id": 224,
		"title": "Attributions Of Early German Shakespeare Translations",
		"authors": [
			"Lassner, David",
			"Baillot, Anne",
			"Coburger, Julius"
		],
		"body": " Introduction Since it was first printed, the translation of Shakespeares plays edited by August Wilhelm Schlegel and Ludwig Tieck has been re-edited many times. A major reference in the first half of the 19th century, it is still regarded as a groundbreaking translation and referred to today. While there is little doubt that Schlegel translated the first edited plays, L. Tieck did not work out the edition of the final volumes by himself, but delegated the main translation work to his daughter Dorothea Tieck and Wolf Graf von Baudissin. This paper investigates the contribution of the three actors involved in this joint translation project. Machine Learning methods are used to analyse the plays and translations in order to gain quantitative insights into what may seem a peculiar authorship setting but was quite usual in the context of the 19th century. The method proposed here is hence likely to improve our understanding of co-creation conditions in the 19th century at large. Stylometric investigations of collaborative translations to identify translators has already been analyzed by Rybicki and Heydel, who could show that Burrows delta features were able to distinguish between the different translators of novels by Virginia Woolf into Polish. Based on D. Tiecks statement of the repartition of the plays we start with the setting shown in Figure 1. Since the manuscript of the raw translation is now lost, the sole material this paper can base its analysis on is the Shakespeare edition and the first German edition. We have no material traces allowing to easily discriminate between what D. Tieck translated, what W. Baudissin translated, and what L. Tieck corrected in the translations. We investigate two questions: firstly, the goal consists in defining the roles and tasks of the three translation partners, especially for scenes where D. Tieck and W. Baudissin collaborated. The second point of interest is to shed light on the cooperation mode between father and daughter Tieck. In contrast to authorship attribution, translators are aiming at preserving the style of the original text – the traces of the translators should therefore be even harder to identify. This paper presents a novel approach to use methods such as Burrows delta in the multilingual context, to compare translation styles and attribute translators. Plays were written by Shakespeare and were translated by either D. Tieck or W. Baudissin. In some plays they collaborated. All translation drafts were then discussed in common, including L. Tieck. Method The first two experiments deal with the question of the individual translation properties of D. Tieck and W. Baudissin, while the third experiment assesses the question of L. Tiecks contribution. The data layout and the analysis steps of all experiments are shown in Figure 2. The English corpus is retrieved from First Folio, for the German corpus, TextGridwas used. Throughout the experiments, spacyfor preprocessing and pyphen for syllable counts are used. In the first experiment, solely based on the German material, translation-stylistic characteristics are to be found that discriminate the translator. In addition to Nearest Neighbors on Burrows deltathat was used by Rybicki, Bag-of-N-Gram features and also pre-trained word vectors using the Fasttext modelwere used and classified by a Support Vector Machine with RBF kernels. Cross validation was used to find good hyper parameters using sk-learn. In the second experiment, we use the trained classifiers of Experiment 1 on the collaborative works of D. Tieck and W. Baudissin. We compute the predicted class of each scene individually and try to examine who the major translator of each part of the translation was. This explorative experiment enables us to concentrate on scenes for which the classifiers tend to agree, which we then manually evaluate. In Experiment 3, cross-language features are compared with respect to its translator. As shown in Figure 2, the first step for analysing the translation is to map the corresponding scenes, to be able to identify deviations on scene level. During the translation process, the scene boundaries were not always preserved and in order to compare intervals of the same contents, an automatic mapping of scenes is performed. Afterwards, two different features on scene level, namely the richnessand the number of syllables per line, and Burrows deltaon play level are compared. Data layout for all three experiment settings. The first experiment evaluates the possibility of classifying translators based on textual features of the translation. Experiment 2 explores the unknown parts of the corpus with the trained classifiers of Experiment 1. Experiment 3 parallelizes the corpus of the English and the German version and investigates the influence of each of the collaborators. Results Experiment 1: Classify translator scenes in validation set As shown in Table 1, the individual classifiers on scene level show decent performance. Burrows delta, however, does not show convincing results. For further improvement, we combined the classifiers by filtering scenes for which all scene-classifiers agree. This results in a smaller test setbut also in a dramatic performance boost. For this subset of the test set, our combined classifier is on average performing with a precision and recall of ≈.93. Overall, the classifiers perform better in identifying scenes by W. Baudissin. Table 1: Scores on held-out test set for various features and groupings. For classification of N-Gram features and Word Vectors, an SVM with RBF Kernel has been used. The Support row denotes the number of scenes in the respective class. Parameters have been optimized using grid search and 5-fold cross validation. For Burrows delta, a Nearest Neighbors Classifier has been used. The optimal number of features for the delta has been cross validated. Method Burrows Delta Word N-Grams Char N-Grams Word Vectors Combined Classifiers Grouping Play Scene Scene Scene Scene D. Tieck F1 .5000 0.6216 0.6486 0.7952 0.8947 Precision .5000 0.6765 0.7059 0.7674 0.9444 Recall .5000 0.5750 0.6000 0.8250 0.8500 Support 2 40 40 40 20 W. Baudissin F1 0.6667 0.7705 0.7869 0.8496 0.9474 Precision 0.6667 0.7344 0.7500 0.8727 0.9231 Recall 0.6667 0.8103 0.8276 0.8276 0.9730 Support 3 58 58 58 37 Weighted average F1 0.6000 0.7097 0.7305 0.8274 0.9289 Precision 0.6000 0.7105 0.7320 0.8298 0.9306 Recall 0.6000 0.7143 0.7347 0.8265 0.9298 Support 5 98 98 98 57 Experiment 2: Classify translator scenes in the collaboration set In Figure 3, the translator attribution for the collaborative scenes are shown. Additionally, we exploit the finding of Experiment 1 that our classifiers performance is boosted when they are combined. In Viel Lärmen um nichts, fourth act, first scene the highest agreement for D. Tieck, in Der Widerspenstigen Zähmungfirst act, second scene the highest agreement for Baudissin is observed. As it turns out, the two scenes are exceptionally long scenes with 302 and 264 speeches respectively, although the mean number of speeches per scene over the whole German corpus is only ≈118.7. The length of the scene may give the classifiers more features to distinguish the translators. The scene from The Taming of the Shrew alternates between Verses and Prose which may have given the translator the chance to underline their characteristic style. The scene from Much adoe about Nothing has a much more coherent rhythm which possibly fits D. Tiecks translation style better. This figure shows the average score of all scene-level classifiers of Experiment 1 to attribute each scene to D. Tieck or W. Baudissin for the two plays in which they collaborated. Experiment 3: Identify Contribution of Ludwig Tieck In Figure 4, the results of the cross-language comparison are shown. Points in all panels that are close to the diagonal do not deviate across language. The richnessof the scenes stay very close to the diagonal, however the majority of points is slightly below the diagonal. The original is slightly richer in the sense of our measurement than the translation, but there is no difference across translators. The median syllables per lineof the translation deviates quite significantly in that the German version often uses more syllables per line than the English version. D. Tieck stated in her letter that she also translated Sonnets even in a play that was otherwise translated by W. Baudissin. Because of this statement we originally expected D. Tieck to follow the number of syllables of the original more strictly. This expectation is also in line with the findings of Experiment 2 where most classifiers agree on D. Tieck as the translator in a scene with a coherent rhythm. However, the findings ofcannot verify this hypothesis, because the deviation exists for both translators. In, the points visualize Burrows delta between the two plays in English, the vertical position is the Burrows delta of the respective pair in German. Each data point for which both plays are translated by the same person is color-coded accordingly. Interestingly, the green points are almost exclusively below the diagonal, with only a few exceptions for plays that already exhibit a small delta in the English version. This indicates translations by D. Tieck move closer to each other and thus may incorporate a more consistent style. Three different features that compare original texts and their translations across languages. For each panel, the horizontal axis corresponds to the original version in English, the vertical axis corresponds to the German translation. The richness featureshows little deviation in both languages. The Syllables per line featureshows deviation in the translation for both translators and the Burrows featureshows deviation especially for one translator: D. Tieck. Forgaussian noisewas added to the points to visualize overlapping points. Also, in, a few outliers are not visualized. The points inare grey if both plays were not translated by the same person. Conclusion We proposed an ensemble of translator attribution methods that result in a very high performance on scenes where they agree. We show a significant improvement over state-of-the-art methods for translator attribution. This combination of classifiers is used to suggest translators for scenes where the true translator is unknown. A close reading of the scenes revealed distinct characteristics that could explain the decision of the classifiers. We thus argue that this method likely found scenes where the majority of translation work can be attributed to the proposed translator. A novel approach of comparing the material in the source language and the translations yield the result that D. Tieck has a more distinct style in her translations. With regard to the daughter-father relationship this can be seen as a literary independence from her father. Also, it could be observed that there is a translation system on which the three collaborators agree. In that, we identified candidate features that could signal a contribution of L. Tieck. For further analysis we plan to include original plays by L. Tieck in order to identify distinct characteristics that further narrow down his contribution to the translation. We also plan to include additional cross-language features that characterize a distinct style of W. Baudissin. "
	},
	{
		"id": 225,
		"title": "Annoter Des Contenus Audiovisuels: Récit D’une Collaboration Entre Montréal Et Paris",
		"authors": [
			"Lavorel, Marie",
			"Bourgatte, Michael"
		],
		"body": " "
	},
	{
		"id": 226,
		"title": "The Living Archives Of Rwandan exiles And Genocide Survivors In Canada: Une Nouvelle Façon D’explorer, Sur Une Plateforme Numérique, Les Récits De Vies De Survivants De Violence",
		"authors": [
			"Lavorel, Marie"
		],
		"body": " "
	},
	{
		"id": 227,
		"title": "Migration and Biopolitics in Cultural Memory: Conceptual Modelling and Text Mining with Neural Word Embedding",
		"authors": [
			"Leavy, Susan",
			"Greene, Derek",
			"Wade, Karen",
			"Meaney, Gerardine"
		],
		"body": " Abstract This paper explores the cultural representation of migration and the biopolitics of contagion and disease represented in a digital corpus of literary fiction from the British Library. This work is part of a project examining the shifting representation of migration, ethnicity and contagion in cultural memory. A curated subset of the British Library Digital Corpus was examined using techniques from artificial intelligence and text mining. Concept modelling with neural word embedding revealed complex relational dynamics between societal views of migration, ethnic identity and contagion that question prevailing theories. Thematic lexicons were generated with word embedding to mine the corpus for excerpts of text that capture these conceptual relationships and enable critical analysis. This bridging of digital analysis and close reading sets out a methodology whereby patterns identified in corpora with artificial intelligence techniques may be critically evaluated through close reading of the text. Keywords: migration, contagion, biopolitics, word embedding, text mining, literary fiction Introduction The complex relationship between societal views of migration, ethnicity and concepts of contagion and disease are explored in this paper through neural word embedding and text mining. This research is part of a project examining the representation of migration, ethnicity and contagion through the analysis of a collection of 45,000 digital texts from the British Library, primarily dating from the late 19th century. In order to explore the cultural representation of migrants, this paper focuses on their representation within literary fiction, which comprises 16,426 texts of the digital collection. Given the largest communities of migrants to Britain during the late 19th century were Irish and Jewish, this paper focuses on their portrayal in relation to prevailing concepts of contagion, disease and migration. Lexicons of terms associated with the migration and the biopolitics of contagion were generated with neural word embedding. Thematic lexicons are learned associations between terms in the corpus and a set of seed terms corresponding to a concept. The dynamics of the relationship between concepts in the corpus were modelled and explored with t-SNE visualisation and measures of semantic distance. Through modelling how the concepts of migration and disease and contagion, real or imagined, were related within the corpus, patterns emerged that revealed a complex conceptualisation of contagion and the nature of its association with Irish and Jewish migrants. Excerpts of texts capturing the interaction between concepts of migration, ethnicity and contagion were extracted using text mining based on thematic lexicons, developed with word embedding. This bridging of neural word embedding methods with text mining demonstrates how complex conceptual relationships may be identified in text and critically examined through close reading. Related Research Migration and the Biopolitics of Contagion Cultural attitudes towards migration have traditionally been associated with fear of contagious disease. Poverty induced migration from Ireland to Britain during the famine has been cited as generating a fear of transmission of contagious disease. The conflation of issues of migration, ethnicity and contagion is evidenced by the fact that tuberculosis was identified as the Jewish Disease despite the fact that the mortality rates from the disease in London were lower for Jewish immigrants than their counterparts. Recent research, notably Samuel Kline Cohn  s work on the history of epidemics, has produced a more complex and nuanced understanding of the relationship between fear of contagion and suspicion of migrants, based on a much broader historical and cultural archive than heretofore. This research addresses this by applying digital methods to support the systematic study of the relationship between concepts of migration, disease and contagion. Concept Modelling and Text Mining Machine learning has been used in digital humanities research to generate thematic lexicons for a range of purposes, including detecting language change over time, extracting social networks from literary texts, sentiment analysis, and semantic annotation. In developing domain-specific vocabularies, neural word embedding can be particularly effective. The exploration of topics in text through visualisation of neural word embedding models has been applied in automated text analysis systems. However, challenges have been identified in bridging patterns uncovered through visualisation and semantic similarity analysis with close reading of texts in digital humanities research. This paper addresses this issue by using thematic lexicons developed through neural networks to explore the relationships between concepts in text and also as a basis for mining excerpts of text. Methods In this work, thematic lexicons are developed using neural word embeddings, and then visualized using the t-SNE algorithm. The word embedding algorithm used here is the popular word2Vec approach, which generates real-valued, low-dimensional representations of words based on lexical co-occurrences, as identified by sliding a window over documents in a corpus. Lexicons representing key thematic strands in the dynamics of bio-politics and migration were developed by using word embeddings to uncover terms that are semantically related to an initial set of seed terms. The resulting expanded thematic lexicons were used to model concepts within the corpus, and also to uncover excerpts that capture relationships between key concepts in the text. The dynamics of relationships between thematic lexicons and their positioning within the entire corpus were modelled using a t-SNE visualisation approach. The t-SNE method allows the visualisation of high-dimensional data, such as word embedding models. The conceptual structure was explored through an interactive embedding projector in TensorFlow platform. Observed patterns proposed the nature of the relationship between ethnicity, migration and concepts relating to contagion. These patterns were evaluated through the analysis of the cosine similarity of word vectors in the embedding to quantify the semantic distance between concepts. Table : Seed terms for thematic lexicons Texts relating to the key themes listed in Table 1 were uncovered based on the use of the lexicons described above. Specifically, excerpts of texts were extracted if they contained one or more words from a given lexicon. A sample of top words from the lexicon representing the concept of contagion is provided in Table 2. Table : Sample of top terms from thematic lexicon related to contagion Findings and Conclusions The findings of this research uncovered a dynamic between concepts of race and migration that challenge prevailing theories about the attribution of threats of contagion to Jewish and Irish immigrants. Contrary to expectations, analysis of the corpus with neural word embedding did not support a link between race and concepts pertaining to contagion. While Irish and to a lesser extent, Jewish communities were described as themselves being disease, a fear of transmission of disease to British people was not systematically evident in the corpus. Religion and new political ideologies, rather than ethnicity itself, show a stronger association with a threat of contagion and ultimately disease. Figure : Visualisations of conceptual model of migration and biopolitics A striking pattern evident within the clustering of concepts in the t-SNE visualisation was an absence of proximity between the lexicon of disease and those representing either Irish or Jewish identity. However, the term exterminate, a term used in relation to the extermination of disease, was aligned with elements of the Irish lexicon. Cosine similarity analysis demonstrated a stronger association of extermination with ethnic identity, and particularly the religious aspect of that identity, than with disease itself. Excerpts of the texts which were identified as containing this term alongside elements from the Irish or Jewish thematic lexicons, suggested a conceptualization of some migrants as disease to be exterminated, rather than presenting a threat of contagion. Figure : Semantic distanceof concepts in text While most lexicons appeared clustered within the model, the concept of contagion was more dispersed within the t-SNE visualisation. Contrary to expectations, an overall close relation between concepts relating to contagion and Jewish or Irish identities was not evident. However, the aspect of Irish identity pertaining to religion was aligned with elements of the concept of contagion. Cosine similarity analysis of contagion and Catholicism, the nearest neighbours from the Jewish identity, along with the key themes of poverty, migration and immorality, was used to evaluate the semantic distance between concepts. This indicated a stronger association between Catholicism and the concept of contagion than the Irish as an ethnic community or migrants in general. Similarly, the aspects of Jewish identity that pertained most strongly to religion were associated more closely with concepts of contagion, suggesting that religion rather than ethnic identity itself may have had a stronger association with concepts of contagion. Table : Excerpts capturing conceptual relationships between ethnic identity, religion, and contagion Figure : Heatmaps indicating cosine similarities between thematic lexicons and ethnic identities Excerpts of text capturing concepts associated with contagion along with Irish and Jewish identity were extracted to critically evaluate the patterns identified in the word embedding model. Close reading of these revealed a fear of contagion of religions and political ideology. Tracking back to these excerpts also demonstrates a belief that foreign religion could induce disease. Future narrative analysis will examine the extent to which opposition to intermarriage with these groups used the imagery of infection. Conclusion This paper investigated key themes pertaining to migration and the biopolitics of contagion, and uncovered conceptual relationships and excerpts of texts from a collection literary fiction from the British library that question prevailing theories pertaining to the historical association of perceptions of migrants with fear of contagion. Insights regarding the association of the religious aspects of the ethnic identity of immigrants and concepts of contagion in Britain, particularly in the 19th century, were uncovered using neural word embedding. Critical analysis of these complex conceptual patterns was enabled though mining the corpus based on thematic lexicons derived from the embedding models. In bridging artificial intelligence and text mining approaches in this way, this research merges both digital and traditional forms of humanities research. "
	},
	{
		"id": 228,
		"title": "Which Services for User Participation? Representing Cooperation and Collaboration in a Participative Digital Library",
		"authors": [
			"Leblanc, Elina"
		],
		"body": " This poster will present the cooperative and collaborative services defined by a French-Italian Digital Libraryfounded on the principles of public engagement Public engagement is intended as the myriad of ways in which the activity and benefits of higher education and research can be shared with the public. Engagement is by definition a two-way process, involving interaction and listening, with the goal of generating mutual benefit. . Cooperation and collaboration are often confused with each other, but our experience leads us to distinguish them in order to better achieve the purpose of our project. We define a DL based on the public engagement model as a participative DL. Such a library offers a set of services that encourages expert and lay users to work together to enrich pre-existing content and disseminate their knowledge, all the while acquiring new skills. Public engagement within a DL can take different forms, defined by the ways users are encouraged to contribute. First, the public engagement can result in a cooperative participation. Users perform a task that helps the DL to enrich its collection, but during the realization of this task the DL in return passes on knowledge to them, as is done with the citizen sciences, such as the Old Weather project https://www.oldweather.org . According to a cooperative model, only the results are mutualized; the DL defines the way users can contribute, that is to say in a way that can potentially benefit both library and users: the DL gains new content and the users, new skills. If cooperation is often the way of public engagement that is encountered in DLs, a second model is the one of collaboration. With this latter, the library and the users co-construct services and digital contents. It is the results and the working process that are mutualized and shared among the participants– as seen in the The Social Edition of the Devonshire Manuscript https://en.wikibooks.org/wiki/The_Devonshire_Manuscript or in many projects developed within the Cambridge University Digital Library https://cudl.lib.cam.ac.uk . We have applied those principles to our DL, which aims at becoming a participative digital library, where the users are both readers and authors of the content they access. To define our participative services and to allow the coexistence of cooperation and collaboration within the same interface, we relied on an exhaustive state of the art of participative services in digital heritage projects and on the results of a user study. This user study took the form of a questionnaireand interviews, centered on the behaviour of users on DLs, as well as on the notion of participation. The poster will focus on the collaborative and cooperative services we have chosen for Fonte Gaia, thanks to this user study and this state of the art. Our collaborative services allow users to carry out digital projects focused on Italian studies, with the help of the DL managers: digital scholarly editions, edition of ebooks, creation of reading paths or digital exhibitions. The scholarly community behind our DLsupports users in the realization of their project, from the definition of common objectives and working methods to the publication on our DL. They work together at each stage of the project. These projects are carried out in the back-office of our DL, which is based on Omeka S https://omeka.org/s/ . Some of these user-driven projects, such as virtual exhibitions, rely on standard tools. Others, such as digital editions, are currently developed using external tools, and simply linked to the DL. Collaboration thus leads to a diversification of user intervention spaces: users are not only associated with the front-end interface, but also with the back-end interface. At the same time, we have defined cooperative services, for instance the adding of tags, of bibliographical references or of comments, all of which performed in a controlled way. This will allow users to share their knowledge with others, while being initiated to a specific librarian skill. For instance, in the case of participatory indexing, we want the user to add keywords from controlled vocabularies: on one hand, it will ensure a harmonization of the contributions; on the other hand, it will initiate users to librarians indexing methods and help them better understand the vocabularies used by librarians, in order to improve their further research on the DL. The contribution process of cooperative services is entirely defined by the DL: those services will therefore be only accessible to the users from the front-end interface. The development of these services is ongoing and is itself based on a collaborative process. Most of the services are developed by our DLs transdisciplinary team. Other services will benefit from a special treatment, such as the indexation, which will be the subject of a hackathon in March 2019. Through a hackathon, participants with varied professional backgrounds will experiment several solutions to design a cooperative service in line with the librarians missions, as well as with the principles of public engagement. "
	},
	{
		"id": 229,
		"title": "L’Environnement Sonore en tant que Ressource Culturelle pour les Selk'nam et les Yahgan : de la Terre de Feu au Cap HornUne étude pluridisciplinaire des lieux, des espaces et des fonctions humaines et sociales des paysages sonores",
		"authors": [
			"Lemasson, Lauriane"
		],
		"body": " Les lieux, les premiers habitants, la colonisation et le contexte actuel Les territoires de ces recherches, partagés entre lArgentine et le Chili, ont été exclusivement peuplés pendant plusieurs millénaires par : -les Yagán. Leurs territoires ancestraux sont repérables grâce aux amas coquilliers. -les Selknam et les Hausharrivés du nord et habitants de lîle de Terre de Feu exclusivement. La colonisation intensive de ces régions sest déroulée après 1880. Des chercheurs dor, missionnaires, chasseurs de phoques et éleveurs dovins ont participé aux persécutions, assassinats, déportations, concentrations, importations de maladies, et viols, générant la diminution catastrophique de ces populations et dénormes répercussions sur leurs cultures. Malgré l invisibilité dont ils ont été alternativement victimes et acteurs après cette période, leurs cultures se sont transformées et la mémoire collective a traversé les décennies. Une étude sonore pluridisciplinaire Depuis 2011, mes recherches interrogent les représentations des lieux et espaces de lenvironnement sonore, dans ses fonctions humaines et sociales. Elles intègrent létude des sons dans les domaines de la musicologie, lacoustique, lethnologie, lhistoire et la géographie. En quoi le paysage sonore peut-il témoigner dune culture et accompagner ces peuples dans leurs reconstructions patrimoniales? Comment mettre en évidence limportance des sons et redonner du sens à des territoires? Le premier son des lieux: la toponymie En croisant les sources, en localisant des lieuxet en travaillant avec des membres de ces peuples, jai pu créer une base de données de plus de 2000 toponymes en langues selknam et yagán. Les cartes générées mettent en évidence lactuel vide toponymique: beaucoup de lieux ne sont pas nommés. Le vide actuel sexplique en partie par la colonisation rapide, la disparition massive des premiers habitants, et le peu dintérêt possible pour des lieux privatisés parfois depuis plus dun siècle. Lanalyse montre que les toponimes indigènes restantssont très majoritairement situés dans des lieux ayant servis de refuge: les zones montagneuses et forestières de Terre de Feu, et les environs des missions anglicanes. Malgré certaines modifications, de nombreux lieux portent toujours des noms choisis par des acteurs de la colonisation et des assassins reconnus. Paysages sonores et acoustique Les paysages sonores Lanalyse croisée des archives sonores et écrites avec les paysages sonores collectés lors de plusieurs missions a entre autre permis de mettre en évidence limportance des sons en tant que: - sources dinspiration pour les chants et les mythes- explication des lieux et des évènements- support du chaman pour prouver et/ou exprimer sa force. Lacoustique du Kloketen/Hain Grâce au Laboratoire dAnthropologie et dArchéologie du CADIC, l étude acoustique dun lieu de rituel selknam attesté a été réalisée sur le site Ewan 1. Il dispose toujours dune hutte cérémonielle construite au printemps 1905 pour le rituel initiatique des jeunes hommes nommé Kloketen ou Hain. Les tests réalisés permettent de décrire une morphologie particulière mêlant absorptions, résonances et phénomènes déchos et corroborant les exigences du déroulement du rituel décrites dans les écrits scientifiques. Compte-tenu des distances, du déroulement du rituel, de sa disposition et de son orientation selon les vents dominants, le choix du lieu requiert des propriétés acoustiques spécifiques permettant par exemple d amplifier ou absorber les manifestations sonores. Travaux en cours Analyses sonores Lors dexpéditions, une cinquantaine de tests acoustiques de lieux a été réalisée et attend dêtre exhaustivement analysée. Cartographie et communication L absence de nombreux lieux selknam et yagán dans les bases de données des instituts géographiques argentins et chiliens complexifie la réalisation des cartes et contraint à un traitement long des données pour localiser les lieux. Ces cartes à vocation participative seront mises en ligne. Contexte actuel Les témoignages protéiformes sur le génocide Selknam, recensés lors de ces travaux et associés à létude des lieux, situent les évènements listés en amont. Le contexte et ceux-ci imposent dapprofondir les recherches associant les sources écrites et la mémoire collective des Selknam et de la population en général. "
	},
	{
		"id": 230,
		"title": "Theorising the Spatial Humanities",
		"authors": [
			"Murrieta-Flores, Patricia",
			"Gregory, Ian",
			"Liceras-Garrido, Raquel",
			"Bellamy, Katherine",
			"Martins, Bruno"
		],
		"body": " The Spatial Humanitiesis a discipline that emerged after the spatial turn, going hand-in-hand with the democratization of geographic information technologies. The interest in locating geographical, vague and imaginary space and place in History, Archaeology, and Literature has opened a new world of possibilities in humanities investigation, providing the opportunity to revisit traditional questions from new and innovative approaches. Usually set in a highly interdisciplinary scene, research making use of spatial analysis and other disciplines such as computational linguistics and natural language processing have led to the development of tools, techniques and methods that can answer questions that were impossible to address before. In doing so, however, the focus of the SH has not only been the development of new methodologies and tools for analysis, but also the creation of a theoretical understanding of multiple constructions of space through time, culture and/or geography. This session is concerned with the use of Spatial Technologies in Humanities research, aiming to explore and discuss modern geospatial techniques and their contributions to Humanities, as well as the underpinning theories behind them. Therefore, the session has three interconnected objectives. Firstly, it will carry out an overall examination of current geospatial technologies, methods and approaches in the Digital Humanities. This will be a valuable exploration of the use of GIS and other technologies across diverse disciplines, the associated research tools and methodologies, and the pros and cons of these. Secondly, it will aim to look at different ways of visualising and understanding data within a spatial environment, from GIS-based methods of modelling space and time, to a variety of other techniques, including graph theory, deep mapping, and more. This aspect of the session will have the purpose of showcasing novel developments and methodological innovations in the Spatial Humanities including Linked Open Data, Natural Language Processing, and Machine Learning, amongst others. Bringing the first two aims together, the final aspect of the session will be concerned with the discussion of the theoretical underpinnings of spatial technologies and their use in Humanities research. The session aims to bring together researchers from all disciplines engaged in geospatial innovation, both within and beyond the humanities. This includes scholars from disciplines such as History, Geography, Literary Studies, Linguistics and Archaeology, as well as GISc, DH, Computational Linguistics and Computer Science. The discussion will revolve around six presentations where critical aspects of the SH will be examined, as well as a theoretical overview of the subject, and new pathways to explore in the future will be also discussed. In doing so, the session will include the debate of concepts such as chorography or biographies of landscapes, and it will reflect the ongoing interest in new ways of visualizing spatial data, in terms of not only time, but also representing constructions of multilayer and interactive space. 1. Re-thinking Geographic Information Systems for the Digital Humanities Carmen BrandoThe interest for GIS approaches to support digital humanities has grown as Humanities scholars have integrated this methodology into their practices. Beyond geo-visualisation, GIS provides with a reasonably easy-to-use toolbox for performing queries and analysis, and customising cartographies according to semiological practices. To make the most out of GIS, information must be structured relying on a layered-based data model which imposes the definition beforehand of rigid categories, a limitation that can find its origin in database systems. GI science has analysed this issue and states that formalizing semantics of GI is indispensable. Modelling of Humanities data relying on ontologies is challenging. Nevertheless, Linked Dataapproaches allows for gaining in expressivity by reusing and extending ontologies and thesaurus thereby to create graph-based knowledge bases. Also, semantic historical gazetteersare a simple mean to bring structured knowledge into sources during annotation. Here, gazetteers must be understood as much more than a list of names and coordinates, they are instead KB including historical facts, useful for interlinking datasets online. Such paradigm can be further generalised to become the next generation Historical GIS, graph-based and interconnected. Furthermore, GIS assumes that geography, as stated by Andrews, can be treated as a set of specific points, connected into paths or polygons where appropriate. Such assumptions lead to consider that geometries and location prevails over semantics and that place represents an isolated entity without semantic connections. On the contrary, a place evolves and interacts in time with other entities through relationships that one can semantically qualify, and all this information can always be traced back to the source, a key issue in historical research and Literary studies. CIDOC-CRM and its spatiotemporal extension, promote such paradigm for documenting cultural heritage sources. Only time will tell how scholars adopt this formalism. Yet today, spatialising texts represents a complicated task for humanists as they must tag, disambiguate and geolocate named entities. Several NLP pipelines facilitate the transition from texts to maps and link to KB, but still tool adaptation is tedious and time-consuming. GIS was not in fact built for handling texts or any unstructured content, and it may better if it stays that way, however further effort is need to facilitate synergies between multiple methodologies. We, spatial humanists, may contribute to re-think GIS by considering the aforementioned challenges. Do we wish to move on that direction? How can we move forward? In my presentation, I intend to elaborate on these questions based on ongoing project experiences. References Andrews, T.. Wait - When, Where? Difficult Answers to Simple Questions About Historical Time and Place, Proceedings of the 2nd Workshop on Corpus-Based Research in the Humanities, 25-26 January, Vienna, Kuhn, W.. Modeling the Semantics of Geographic Categories through Conceptual Integration. Proceedings of GIScience 2002. LNCS, vol 2478. Springer, Berlin Paris, P-H., Abadie, N. and Brando, C.. Linking Spatial Named Entities to the Web of Data for Geographical Analysis of Historical Texts, JMGL, 13:1: 82-110 2. Historicising the Spatial Humanities: The Antiquarian Foundations of Some New Ways of Knowing Christopher DonaldsonThe Spatial Humanities is often characterised as a new or emergent area of scholarly activity. The reasoning underlying such characterisations is sound insofar as it identifies computational technologies and methodologies as essential to Spatial Humanities research. In this paper, however, my aim will be to historicise the Spatial Humanities by considering its relation to the pre-disciplinary approaches of early modern antiquaries. More than just drawing this connection abstractly, I shall also demonstrate it practically by drawing connections between current trends in the use of linked open data, GIS, and deep mapping and antiquarian conventions such as hyper-annotation and extra-illustration. I shall use these examples to adduce evidence in support of my overarching contention: namely, that the Spatial Humanities represents less a new development than a recasting and elaboration of older models of spatially informed knowledge creation. A key point of connection that I shall develop in this paper is the link between Spatial Humanities research and the tradition of chorography. Chorography originated in classical antiquity as a category of research focussed on inventorying and collating, at length, the features, character, customs, and manners of particular regions. This practice, combining as it does elements of history, geography, ethnography, and natural philosophy, was adopted and adapted by early modern scholars as a way of characterising the intensive investigation of specific localities. A key work within this context was William Camdens Britannia, which, with its subtitle a chorographical description, exerted a powerful influence over the subsequent development of antiquarian research in Europe and consequently informed the scholarly culture out of which an interest in space as a paradigm for humanities research has emerged. Like much current research in the Spatial Humanities, chorography constitutes a process of accounting for the trans-historical character of a region as a landscape persisting through time, and therefore as one populated by markers and memories of the past that exert an abiding influence on perceptions of the character of that region in the present. Chorography, in other words, is a practice that seeks to push beyond the shallow time-depth of conventional planar maps to explore the evolved character of places through time. Thus, not unlike much recent research Spatial Humanities research, chorography is an analytical and descriptive process guided by an awareness of the interplay of past and present on the way in which a given location, locality, or region is perceived, experienced, and understood. Accordingly, as I mean to show, although modern Spatial Humanities scholarship is defined by an engagement with computational resources, its methods and aspirations are broadly complementary with those of much older scholarly traditions. Acknowledging this connection, as I shall argue in conclusion, offers a salient reminder of the need for those working in the Spatial Humanities to maintain a reflexive openness to the multiple – and, at times, competing – constructions of space that define the study of human civilisation and society. 3. A Spatial Humanities approach to integrate History and Literature: insights from a decade long experience researching Lisbons past Daniel AlvesGeography has always been a fundamental element in historical analysis. In other disciplines of the Humanities, such as Literature, for example, studies on the representation of space have been occupying an increasing interest. While it is true that Geographic Information Systems and other tools of the Digital Humanities have facilitated what has been termed the spatial turn, a process of reconfiguration and revaluation of the use of space in the social sciences and the humanities, this dynamic is, however, prior to the introduction of digital tools in humanistic research. Perhaps the most relevant reference in the field of historical studies is Henri Lefebvres pioneering work, La production de lespace, 1974. In the case of Literature, among several possible references, see Gabriel Zorans article, Towards a Theory of Space in Narrative, 1984. Although Literature is thought and used as a source for History, and several works point to a necessary interdisciplinary vision in the study of the past, integrating fiction with historical sources, there are not many works that add to these two dimensions a spatial perspective. The use of GIS and other tools and methods of extraction, visualization and analysis of geographic information has strengthened this perspective. In the case of Urban History and Literature about Lisbon, there is still an immense potential to be explored, despite almost a decade of work on these subjects for the 19th and 20th centuries. Studies on the use of urban space in literature, on urban musical landscapes, on political sociability in cafes or on memory and space of revolutionary events have already been carried out, allowing now to question the effective contributions of these tools and the interdisciplinarity that they enabled in the historical and literary knowledge about the city. Is it possible to integrate a fictional view on the social space of a city and the historical knowledge about the human activities that unfolded in the same space? How can knowledge about urban history contribute to the study of literature and its evolution over time? To what extent can GIS enhance or limit these two perspectives? We will discuss these issues based on a body of evidence on the work carried out in recent years, particularly those resulting from the Atlas of Literary Landscapes of Continental Portugal and the Lisbon Retail Trade History projects. 4. Linked Data, Data Enrichment and Public Participation for Deep Mapping: the Case of de Krook QuarterPiraye Hacıgüzeller, Fien Danniau and Christophe VerbruggenDeep mapping has served as a promising concept in digital humanities in recent years building on the unattainable yet inspiring ambition of recording and representing everything you might ever want to say about a place. In our contribution to the session, one of our aims will be to explore recent developments in deep mapping within spatial humanities. Subsequently we will aim to to lay out major technological and conceptual elements for deep mapping in spatial humanities in general and historical deep maps in particular. Specifically we will make three major points. Firstly, we will suggest that deep mapping in spatial humanities would considerably benefit from the creation of dense linked data ecosystems through introduction of new standard vocabularies for linking information on, for instance, main places, events and people related to history of small places. Secondly, we will provide a critical review of automatic data enrichment pipelines that are used in digital humanities research and how they can be implemented while creating historical deep mapping. Finally, we will place public participation as a vital part of the process of making historical deep maps and put forward some public participatory mappingand GISstrategies that can be effectively employed to involve the public in historical deep mapping projects. We will argue that focusing on these elements in deep mapping applications, will enhance the flexible architecture and open-endedness of deep maps, enrich their content and shift the focus from the end resultto the processin spatial humanities and, as such, significantly contribute to the realisation of the deep mapping-related aspirations prominent in spatial humanities today. We will be illustrating these ideas further by presenting a new deep mapping project initiated and coordinated by Ghent Centre for Digital Humanities at de Krook Quarter of Ghent. Since 2017, de Krook is the architectural iconic city library and digital innovation centre in the city of Ghent. It is located in a neighbourhood that holds several iconic and cultural institutes and theatres each of which offer well-documented histories and heritage. Attracting three times the expected amount of visitors, de Krook is a new public place in town, fulfilling a range of functions and rapidly becoming a new identifier of the neighbourhood. The engagement by the citizens, the city council and the university in this place-in-the-making offers an interesting testbed for participative, digital and analogue, methodology and for the concept of deep mapping as a way to captivate the historical as well as the currently shifting dynamics of the place. References: Bodenhamer, D., Corrigan, J. and Harris, T. M.. Deep maps and spatial narratives. Indiana: Indiana University Press. Pearson, M. and Shanks, M.. Theatre/archaeology : disciplinary dialogues. London; New York: Routledge. 5. Spatial History between Maps and Texts: Lessons from the Eighteenth Century Katie McDonough, Ludovic Moncla, and Matje van de CampRichard White described the work of the Spatial History Project at Stanford in terms of the combination of collaboratively-built, digital visualizations and a conceptual focus on space. Visualizations refashioned opaque and unwieldy evidence so that it could become part of the historians narrative. More recently the Digital History and Argument white paper stressed the utility of mixing computational methods–text, network, and spatial analysis–to strengthen arguments. It also highlights the bias lurking under the cover of technologies we depend on in digital history. Our Spatial History of the Encyclopédie project models the combination of text and spatial analysis, building new Natural Language Processing tools to create spatial data about places named in printed reference books. It contributes to advancing historical arguments about the ways that Europeans created and shared spatial knowledge between the Renaissance and the French Revolution. As a result, it also challenges assumptions that historians have made about spatial metadata. These metadata, collected in books and now in digital gazetteers, derive from strategic, imperialist, and elite place-naming practices that have until very recently erased local toponymic traditions from the encyclopedic tradition. We will share our most recent work to analyze European traditions of spatial information documentation. Learning from Diderots Encyclopédie–the eighteenth-century bookend to the explosion of reference works in the Enlightenment–we can begin to understand why gazetteers from the nineteenth-century onwards illuminate certain geographies and ignore others. We have created a geoparser that does not prioritize geolocation as the end goal, allowing for less specific spatial definitions and maintaining links to textual content that aids in disambiguation and relative location. This contributes a key resource to the community of geohumanities scholars interested in building early modern-specific, often locationally-ambiguous place authority records. It also makes the provenance of spatial data transparent. This is the first step towards two much needed transformations in spatial history practice: 1) awareness of gazetteers sources and 2) access to temporally-specific spatial data, or, data about places that is chronologically similar to our evidence. Finally, we will discuss approaches to making visualizations of the Encyclopédie data that refer to source texts reused in Encyclopédie articles. Spatial History of the Encyclopédie copes with unwieldy material, but does not stop at visualizing this evidence from the past. It is also not only conceived of as a means to interpreting eighteenth-century geographical discourses and knowledge collections practices. At its core, this is a project that tackles the ethics of doing spatial history, indeed, all geohumanities work. In excavating the history of the European gazetteer tradition through the lens of the Encyclopédies geography content, we illuminate the distant origins of data sets like Wikipedia and Geonames. References: White, R.. What is Spatial History? The Spatial History Project. https://web.stanford.edu/group/spatialhistory/cgi-bin/site/pub.php?id=29. Arguing with Digital History working group.. Digital History and Argument, white paper, Roy Rosenzweig Center for History and New Media. https://rrchnm.org/argument-white-paper/. 6. Towards a spatial linguistics? How far can recent methodological developments in spatial humanities be practically applied for the geo-visualising dialect data? Jacques Van Keymeulen, Veronique De Tier, Roxane Vandenberghe and Lien Hellebaut, Sally Chambers and Piraye Hacıgüzeller, Jesse de Does, Katrien Depuydt and Tanneke Schoonheim, Netherlands) Geo-visualisation of data is fundamental to dialect research. Dialectologists have been creating and interpreting language maps for many years. Language maps enable dialectologists to present data in a clear and understandable manner. Particular examples include:objective maps to display the raw language data prior to interpretation,isogloss maps which use lines to mark borders between variants andsymbol maps where for example, the same dialect words are used in different geographical locations. However, do the recent developments in the spatial humanities, represent new opportunities for linguistics research? Can the geo-visualisation of linguistics data move beyond flat cartographic representations towards dynamic, interactive spatial visualisations of geo-linguistic phenomena? If spatial humanities techniques can be repurposed for linguistics research, to what extent can they be practically implemented? The aim of the Database of Southern Dutch Dialectsprojectis to aggregate and standardise three databases of the Brabantic, Limburgian and Flemish Dialects into one database for the Southern Dutch Dialect area. Now that the integration of the three dictionaries is underway, the project team can turn their attention to exploring how to geo-visualise this integrated dialect dataset, using state-of-the-art web-mapping techniques. Originally, dialect maps were created MapInfo. The symbol maps were generated on the basis of Kloeke codes, unique codes for places and hamlets in the Dutch language area, included in the dialect data. While this system works perfectly well for the offline creation of dialect maps, the current set-up does not meet the online needs of 21 st Century dialectology. The project team is therefore looking for a new solution to geo-visualise the integrated dataset of the Southern Dutch Dialect Area. The aim of this contribution is to: a) explore recent developments in the spatial humanities and in particular their application to linguistics research, b) explore how geo-visualisation of linguistics data, and in particular dialect data, can move beyond flat cartographic representations towards dynamic, interactive spatial visualisations of geo-linguistic phenomena and c) explore in how far the currently available technologies could provide practical implementation solutions for the Database of Southern Dutch Dialects. References Ghent University, Department of Dutch Linguistics. Dialectkaarten. https://www.dialectloket.be/tekst/dialectologie/dialectkaarten/Ghent University, Department of Dutch Linguistics. Database of Southern Dutch Dialects. https://www.ghentcdh.ugent.be/projects/database-southern-dutch-dialects-dsdd"
	},
	{
		"id": 231,
		"title": "Training NLP Models for the Analysis of 16th Century Latin American Historical Documents: Tagtog and the Geographic Reports of New Spain",
		"authors": [
			"Murrieta-Flores, Patricia",
			"Liceras-Garrido, Raquel",
			"Favila-Vázquez, Mariana",
			"Bellamy, Katherine",
			"Campos, Jorge",
			"Cejuela, Juan Miguel",
			"Martins, Bruno"
		],
		"body": " The latest advances in Text Mining, Natural Language Processingand Machine Learninghave opened a world of possibilities in processing massive amounts of texts that would otherwise take a lifetime to read and investigate, enabling researchers to revisit old sources from a new perspective. The purpose of this poster is to present a) the annotation model created for investigating topics related to economy and society in 16 th century New Spain; and b) the use of tagtog, an online tool forautomated annotation, to create/curate the resources required for developing NLP tools capable of supporting historical research, through statistical learning. Figure . Ameca Reportannotated with tagtog 1. Gathering information from New Spain The project  Digging into Early Colonial Mexico: A large-scale computational analysis of sixteenth-century historical sourcesexplores advanced computational techniques in conjunction with spatial analysis methods. Using a Big Data approach, the project focuses on analysing a 16 th century corpus known as Las Relaciones Geográficas de la Nueva España, consisting of documents related to New Spain, an area which encompasses present-day Mexico and Guatemala. These accounts include textual and pictorial information that portray the colonial situation of the domains ruled by the Spanish Crown, describing the life of their inhabitants and the state of the territories five decades after Spanish arrival. This corpus contains around 2,800,000 words compiled from answers to the RGs questionnaires which were conducted between 1577 and 1585, covering topics including economy, resources, environment, traditions, geography, government, military organisation and language, among many others. 2. Annotating the RGs WITH Tagtog Analysing the multiple facets contained in the RGs requires a combination of techniques from NLP, ML and Text Mining. In order to identify, extract and analyse such information, our project partnered with tagtog, a collaborative text annotation tool for adding metadata to text, improving processing, querying and classification. In this particular case,automatic annotations were produced by teaching tagtog the linguistic nuances of our domain. This tool supports two learning methods: built-in ML and dictionaries. Patterns were defined using dictionaries and a domain-specific ML model was trained to annotate the RG corpus automatically. Tagtogs interactive interface was used to perform initial annotations, and for curating the results of the ML model. Figure . Entity types. Please note that the colours correspond to those used in Figure 1 We first created an annotation model with a set of 40 tailored entity labelsbased on data relevant to our research questions with the aim of: a) training the machine to automatically recognise topics we are interested in analysing; b) improving versatility when exploring the text; and c) expanding the scope of techniques such as Named Entity Recognition that are only able to identify proper names. Training the machine in this way will make it possible to extract information and answer questions, such as: What were the main health issues and remedies in New Spain? What were the most common resources and how were these distributed? How did Spanish and indigenous populations view and interact with the physical environment? The RGs present many challenges for NLP and ML. Firstly, there are complexities associated with a corpus written in 16 th century Spanish, especially considering that many techniques used to analyse large corpora have been developed and tested using modern English texts. Secondly, the corpus is multilingual. The reports are peppered with words in different native languagesalongside Spanish. Finally, as it is a questionnaire, the topics discussed are very diverse. Tagtog has allowed us to overcome some of these problems by designing our own annotation model, taking into account that annotations must be accurate and plentiful in order to represent the various contexts in which entity references can appear. As it is also a multi-user tool with an easy-to-use interface, it guarantees anyone could participate and annotate full documents. Beyond entity annotations, tagtog also supports different annotations that can be used to enrich existing annotations and increase their accuracy: at entity leveland/or document level. The tools flexibility also minimized the time required to teach tagtog to annotate relevant information automatically. However, there is still much to be done. We need to carry out more experiments with this tool, comparing, for instance, its performance between different documents. The challenges we face in this endeavour offer us the opportunity to test existing techniques from NLP, ML and Corpus Linguistics, with the aim of improving and developing these tools for use in the humanities. "
	},
	{
		"id": 232,
		"title": "Conveying Uncertainty in Archived War Diaries with GeoBlobs",
		"authors": [
			"Liem, Johannes",
			"Goudarouli, Eirini",
			"Hirschorn, Steven",
			"Wood, Jo",
			"Perin, Charles"
		],
		"body": " GeoBlobs visualize uncertain and ambiguous spatio-temporal data derived from handwritten War Diaries from the First World War, documenting the story of the British Army and its units on the Western Front. We propose a design space that shows the possible variations of GeoBlobs and how these variations can communicatecertainty visually. Handwritten military diary, GeoBlob design space, GeoBlob example map with pictograph overlay. Data, Uncertainties, and Challenges Since 2014, The National Archiveshas undertaken the digitization of more than a million analog diary pages, using an interactive crowd-sourcing platform. During this process, essential information about military units, including labels for casualties, unit strength, weather, everyday army life, soldiers names and grades, military activities at the front and non-military activities behind the line, have been tagged and annotated. However, the transcription process introduced uncertainty on many levels due to missing records or pages, misspellings, illegible passages or lost diaries. Some uncertainties derive from the particular type of data that has been gathered, the circumstances of its creation, and its post-processing. These may be the result of human errors: the British soldiers who wrote the diaries made misspellings, faithfully transcribed, while the crowd-workers also introduced new typos, leading to further ambiguity regarding names of people and locations. Or they may be intrinsic to the historical record, as where spatio-temporal uncertainty arises when several places are mentioned on one single day in the diary. Close reading is then required to determine in which order the places were visited by the troops or whether a place was mentioned as a troop location or for other reasons. These types of uncertainty may lead to ambiguous geographic coordinates during the geo-referencing process. As part of the collaboration between The National Archives and City, University of London, we developed GeoBlobs, an abstract representation of spatio-temporal data, which visualizes uncertain spatio-temporal data derived from the handwritten military diaries. GeobBlob Design Space GeoBlobs offer one possible solution to the problems that arise from uncertainty in the data. GeoBlobs are an abstract representation of moving entities on a map with uncertain positions. Instead of showing a unit at a given point in time, GeoBlobs convey an unordered estimation of the possible locations over a temporal window using enclosed shapes, or blobs. To this end, we apply heuristics to weigh each location within the temporal window. At the representation stage, it is possible to specify dynamically whichsets of location are considered to form the GeoBlobs. Different form and style parameters can influence the visual appearance of GeoBlobs and their semantics. The use of multiple GeoBlobs allows us to compare units and show different probabilities. Animation or overlays communicate additional context like military activitiesor non-military activities. For example, three degrees of spatial certainty are represented by three superimposed GeoBlobs. Using the proposed design space, one can vary the visual attributes to convey uncertainty. For example, the shapes and their outlines can have different opacity or different levels of sketchiness or blurriness. While a crisp shape edge conveys certainty, a sketchyedge expresses a higher degree of uncertainty. Rather than offering a direct mapping between numerical probabilities and visual representations, the design space shows the possible variations of GeoBlobs and how these variations can communicatecertainty visually. Application Examples The GeoBlob project aims to reveal stories of the soldiers day-to-day life behind the lines, which will lead towards a narrative visualizationfor communicating the life behind the trenches that cannot be found in our history books. A first approach we present is a comic-like sequence showing the day-to-day life of 3 battalions of the 3rd Division during WWI, which is communicated through text, GeoBlobs, and Isotype-like activity overlays. We further developed an interactive, web-based prototype allowing to track and explore the activities and whereabouts of units over the course of the war. A non-spatial map layer shows the distribution of a troops activities behind the line and at the front within a given time range. In future work, we plan to enrich the prototype by adding more data e.g., about single soldiers mentioned in the diaries. GeoBlobs also apply to areas where the movement data is not uncertain but has a high density, or where the coverage area is of interest like sports data visualization. "
	},
	{
		"id": 233,
		"title": "I-Media-Cities: A Digital Ecosystem Enriching A Searchable Treasure Trove Of Audio Visual Assets",
		"authors": [
			"Scipione, Gabriella",
			"Guidazzoli, Antonella",
			"Imboden, Silvano",
			"Trotta, Giuseppe",
			"Montanari, Margherita",
			"Liguori, Maria Chiara",
			"Caraceni, Simona"
		],
		"body": " Introduction: Digital Cultural Ecosystems The paradigm of ecosystems for digital cultural contents, the so called DCEs, can facilitate and foster the process of democratization of knowledge. This process started in the 1990s with the first applications developed by means of ICT tools applied to Cultural Heritage. Open digital frameworks allow the access and the enrichment of cultural digital resources enabling new researches and studies. Moreover, these systems can bring cultural content to different audiences in innovative ways and include citizens in the process of content enrichment. W ith respect to digital Cultural Heritage, citizens should be more than consumers ViMM Manifesto full version, ViMM project, retrieved on 3 April 2019. . The audience participation should be improved at an active level, implementing user-oriented perspectives. The same principle was at the basis of the I-Media-Cities project, with its aim of involving both researchers and the general public, not only as users of the platform, as mere performers of queries, but, instead, as active contributors by tagging and further enriching the digital contents delivered by the platform. The I-Media-Cities project The I-Media-Cities Horizon 2020 project is the initiative of 9 European Film Libraries, 5 research institutions, 2 technological providers and a specialist of digital business models to share, access to and valorise audiovisualcontent about history and identity of European cities. A huge quantity of fictional and non-fictional AV worksha s been made available for research and creative purposes, describing cities in all aspects, including physical transformation and social dynamics. The platform, with its dynamic processing pipeline and automatic video analysis tools, enables the enrichment of the moving images meta-data. By allowing also manual and automatic annotations of the A/V content the platform creates a new digital ecosystem. Fig. 1: IMC Movie Processing Pipeline The innovative elements of the I-Media-Citiesplatform are not limited to conveying the audio/visual contents of nine archives to a single collector and access point. Once collected, the A/V material is elaborated with a series of algorithms that automatically enrich it with a good set of meta-data. Nowadays it is a common activity for people to contribute to the enrichment of web contents with various kinds of data, in particular through social media. However, this manual activity is an extremely laborious process; especially when it regards, as in the case of the IMC project, the annotation of videos at shot or single frame level. Automatic meta-data, on the other hand, while not yet achieving the accuracy of a manual enrichment, is able to add a great deal of information. To this end, IMC has integrated a set of tools provided by Fraunhofer for the automatic analysis of video and images. These algorithms provide information on the quality of video, identify camera movements, such as zoom or pan, segment videos in various scenes, and identify and recognize objects and various elementsthat appear in the videos of IMC platform. The object detection tool, thanks to deep learning techniques, is able to identify the presence, for example, of people or means of transport or architectural or urban elements, differentiating between man or woman, adult or child; points out trams or carriages or bicycles; a square or a fountain, and so on. Figure 2: Visualisation of the object detection activity. Green detects objects with a confidence higher than 80%, yellow between 50 and 80% and red lower than 50%. This is a particularly interesting and complex procedure, which has to manage the analysis of sometimes scarred films digitized several years ago and dating back up to the end of the 19th century. Figure 3: IMC platform: video content item page displaying manual and automatic meta-data. Tags are colour coded. Since the automatic tools still retain a certain margin of error, a sophisticated online frame by frame analysis tool has been set up for the manual correction of inaccuracies on the shot detection, particularly useful on older videos. Figure 4: IMC platform: geo-localised search results The result of this process is the creation of data that are transformed into semantic data, directly understandable by people as well as by computers. The IMC meta-data model summarizes the descriptive meta-data from the archives, and then uploaded directly with the audiovisual materials, based primarily on the CEN EN19507 standard; and the data generated once the material has been loaded onto the platform. As mentioned, these meta-data can be the result of an automatic or manual enrichment process, modeled using the W3C Web annotation Data Model. In I-Media-Cities a semantic search engine was developed for processing the requests coming from the user interface and, by analysing all the available meta-data, it provides researchers and citizens with the search results. The meta-data system includes different types of annotation associated with different details: original meta-data of the video or image, providing information at the level of the whole content; automatic annotations at the level of a single segment of the video; manual annotations at the level of the single segment of the video, tags and geotagwith which the content has been enriched. Table n.1 - Contents delivered to IMC platform up to February 2019 In particular, the automatic and manual annotations, being linked to the single segment of the video, allow a collection of scenes belonging to different videos where the same element can be traced: for example the tramways and traffic of the early 1900s. Figure 5: IMC platform: search by term The automatic annotations guarantee that all the audiovisual material is analysed and annotated homogeneously, at least on a common set of aspects, while the manual annotations provide an enrichment of detailed information, also in the form of textual notes, bibliographic references, links to other similar material, both internal and external to I-Media-Cities. At the moment, there are 59,457 manual annotations; 422,123 automatic annotations; 6,411 geotags for 1,091 different places. The performance of the IMC platform rely upon the High Performance Computing resources of Cineca, which allow the use of the most suitable hardware architectures for the different analysis algorithms applied. In order to adequately present the contents of the research, particular attention is paid to the development of the visual interface, which also allows to perform searches starting directly from the map on which all the AV contents are geolocated, delimiting the selection through a time bar. In relation to these aspects, a user experience evaluation process is applied, in accordance with the Agile methodology, which pervades the project. The Agile methodology implies an iterative approach, with several phases of development: check, correction, check and development. Table 2: Agile methodology flow chart During the first phase User and System requirements were gathered by breaking the project and its requirements down into little parts of user functionalities, called user stories and use cases, and prioritizing them. Each Film Heritage Institutionand research partner provided a list of vision items and user stories that were categorised and grouped in use cases by the Coordinator of the project and Cineca staff. During this process, the technical partners detailed each use case with more technical information, such as technological enablers. A series of training sessions and a set of living document were curated in order to enable this Agile process among researchers and users and make them aware of what was already available and feasible. Living documents are continuously updated, following both the development of the project and the state of the art of the different areas on which the project insists. The final phase of the project foresees the opening of the platform to the wider public, and not only to researchers. The enrichment of contents can therefore take advantage of crowd-sourcing, obviously subject to a check by researchers and archives on all the information added Among the metadata sent by the archives and associated to each content there is a right status value, selected from the following list: - In copyright - EU Orphan Work - In copyright - Educational use permitted - In copyright - Non-commercial use permitted - Public Domain - No Copyright - Contractual Restrictions - No Copyright - Non-Commercial Use Only - No Copyright - Other Known Legal Restrictions - No Copyright - United States - Copyright Undetermined All contents, whatever right status they have, can be visualised by everybody, even by the general users group. The archives, depending on the right status, can upload low resolution versions with watermarking. . The results of the research can be presented by archives and researchers as virtual exhibitions, set up in a 3D Web environment within the platform itself. Conclusions I-Media-Cities meets two main requirements of Cultural Heritage sector: gathering information of different types and from different sources and enriching the original information with annotations, automatic or manual ones. Each multimedia content, image or video, is inserted in a process in which the asset is associated with the original meta-data belonging to the archive and, then, it is enriched with automatic meta-data extracted by different algorithms and further enhanced by manual annotations. In the next phase of the project a particular attention will be devoted in improving the general public user experience. The citizens will be able to visualize the public domain content of the archives, search, browse, annotate A/V content and share their discoveries. "
	},
	{
		"id": 234,
		"title": "More Than Just CG: Storytelling And Mixed Techniques In An Educational Short Movie",
		"authors": [
			"Guidazzoli, Antonella",
			"Bellavia, Giovanni",
			"De Luca, Daniele",
			"Delli Ponti, Francesca",
			"Farroni, Federica",
			"Liguori, Maria Chiara",
			"Chiavarini, Beatrice",
			"Imboden, Silvano"
		],
		"body": " Introduction Il piccolo Masaccio e le Terre Nuove is a short animated Computer Graphics video explaining the origins and the history of San Giovanni Valdarno, a city of foundation. The video was created for the Museo delle Terre Nuove, which offers an overview of the phenomenon of the New Towns born in Tuscany, Italy and Europe during the Middle Ages. The main character of the video is a young version of the painter Masaccio, born in 1401 in Castel San Giovanni, the earlier name of San Giovanni Valdarno. Tommaso roams the city during a siege and, on top of the tower of Arnolfos palace, meets the Vicar, ruler of the town on behalf of the city of Florence, who explains him the rational rules underlying the creation of San Giovanni;;. In order to communicate to the visitors a unique experience, combining education and entertainment, the video exploitsof a series of very different techniques, such as: live shots, taken also by drone; Computer Graphics, with photogrammetric, procedural and geometric 3D modeling, crowd, cloth and particle simulations and a digital library for the vegetation of the time; watercolours and 2D drawings, made also with a tablet device; digital videos taken from Google Earth. Figure 1 - Watercolours painted for the video Besides CityEngine, used for the procedural modelling of San Giovanni, and PhotoScan, used for some materials and minor details of the buildings, the work was realised in Blenderin an Open Source pipeline. Figure 2 - Procedural reconstruction of San Giovanni Fig 3 - Simulating a crowd of soldiers Figure 4 - A view of the city This contribution presents an overview of the solutions adopted in the development of a CG educational video in order to both attract more visitors to the museum and inspire them with the desire to deepen its topics. Computer Graphics and videos for Museum communication The use of documentaries to present cultural contents in museums is not new. A short video allows a fairly rapid rotation of the spectators and is generally well compatible with pre-existing museum displays. Over time, real-life documentaries have been joined by 2D educational cartoons and, later, by Computer Graphics videos. The latter, however demanding, allows us to reconstruct, re-contextualize, explain and show different physical perspectives and make landscapes and events of the past much better understandable;;, enabling also the insertion of particular graphic elements into reality, as in the application of the Tate Britain Museum. Usually, Computer Graphics is used to show architectural reconstructions. However, the emotional narration is gaining ground, increasingly introducing storytelling, also for interactive applications, such as CrazyTalk, created for the Thyssen-Bornemitzsa Museum. Figure 5 - Tommaso on top of Arnolfos tower Since they are cheaper, actors are preferred over animated characters. In virtual setsor alongside CG integrations, to show landscapes or reconstructions of the past. Storytelling with 3D animated characters is still limited because of its high costs and challenges, even if, thanks to tools such as Cookie FlexRig it is possible to create them quicker than before. As stated by Vosinakis, a key requirement for digital characters is credibility, that is more a coherence of this character within the world it is situated in, than a high degree of realism in its realisation. Believability involves the characters behaviour as expressed by gaze, facial expression, gesture and posture, making the creation of a character even more complex and time consuming. However, the use of characters inside a narration offers a significant contribution for capturing the audiences attention and entertain it. Characters, atmospheres, dialogues and history, can all transform a passive activity into something more engaging, trigger the visitors interest, empathy and imagination, leading them towards a successfully entertaining experience. Vosinakis remembers us to ...focus on the expressive side of the digital characters, even by incorporating non realistic elements, rather than striving for accuracy and realism. However, when working at a communicative product for a museum, it is very important to keep a good level of philological accuracy. All these elements were central while developing Il piccolo Masaccio, implying a great commitment to philologization, for example in choosing and modeling clothes, and, at the same time, the adoption of something peculiar and cartoonish for attracting the viewer, such as the large eyes of Tommaso or its hair, stained with painting. Figure 6 - Distribution of the vegetation on the map Directing a CG educational video Il piccolo Masaccio is a peculiar educational animated movie, both for the multiple techniques and for the different visual solutions adopted. The video explains how the New Towns of the fourteenth century, such as San Giovanni Valdarno, were born and on which principles and rules of political and architectural design were based. However, teaching through an animated video can risk of not reaching the goal because an excess of didacticism reduces its appeal. A varied visual poetic was used, therefore, to give more rhythm to the story and avoid a boredom effect. For example, the cold and precise lines of the digital houses were totally replaced by irregular and sweeter lines. Even the colours of the façades of the houses were made softer and warmer, inspired by the same frescoes by Masaccio and his contemporaries, such as Masolino. Surfaces and colours are never clean, but dirty, with darker patches to suggest the idea of real walls, scratched sometimes or affected by damp and mould. Figure 7 - Creating a credible town of the Renaissance To give more emphasis to the film and to further soften the 3D model of the city, we resorted to a very blue sky with white clouds, almost wisps of whipped cream, reminiscent of Hayao Miyazakis skys. Camera movements were also devised in order to be more independent from a classic navigation of a 3D model. In addition to 3D computer graphics, it was deemed appropriate to use 2D graphics. This choice enabled the explanation of geometric principles in a simpler way. Furthermore, we used hand-made drawings with ink and watercolours to lighten the narration and enrich the film with visual suggestions, which culminate when Tommaso, inspired by the Vicars narration, draws in an infinite and imaginary space a series of ideal cities. Live shots were adopted in order to link past and present and to highlight the continuity of history, where everything we now see is the result of what was conceived, designed and built in the past. Figure 8 - Study of characters expressions Conclusions Creating a 3D world requires a significant amount of time and resources, especially compared to a real-time shooting or, even, a 2D movie, which could lead to similar results in terms of communication. The 9 minute video Il piccolo Masaccio e le Terre Nuove required the effort of a team of six person for one full year; some small contributions of high-school students; the help of 2 video-makers, for live footages, and of a professional animator. In the end the repository hosted 15,3 GB of Blender files and textures and 11 GB of high resolution frames. The rendering is in Cycles, the Blender unbiased rendering engine, and for the low resolution version and its work in progress 72,000 core hours were used; 65,000 core hours more for the high-res version. Renderings were launched on the Cineca Blender Render Farm, scripted ad hoc for this project with the new SLURM scheduler on the supercomputer Galileo 360 Compute nodes, each with 2*18-core Intel Xeon E5-2697 v4 @ 2.30GHz - 128 GB RAM. The consistent effort became in time a co-production, with Cineca supporting the economic possibilities of the Museum. However, Computer Graphics also provided a higher level of philological accuracy, thanks to the multiplicity of solutions available, such as procedural modelling tools or material or vegetation libraries, plus a better support to the directors creativity. Besides, once the 3D assets have been built, they are reusable in further applications. But, above all, CG and storytelling can be a powerful attractor towards a young audience, used to high quality productions. The movie was premièred at the museum during the National Families at the Museum Day on October 2018. A questionnaire was submitted to the younger audience and it collected praises from everybody. Asked to draw the favourite character from the video, the kids privileged Tommaso, but also the vicar and Tommasos mother got their share of attention. From then on, the repeated submission of questionnaires to the audience will help in fully understand the capacity of Il piccolo Masaccio e le Terre Nuove in transmitting the desired contents and attracting more visitors to the museum. "
	},
	{
		"id": 235,
		"title": "Onto Word Segmentation of the Complete Tang Poems",
		"authors": [
			"Liu, Chao-Lin"
		],
		"body": " Introduction The research community agrees on the influences and importance of the Tang Tang is a Chinese dynasty that existed between 618CE and 907CE. poems for the studies of Chinese literature and linguistics, and we have seen discussions about the Tang poems in the community of digital humanities. Chinese is a language that does not place clear markers, e.g., spaces in alphabetic languages, between words in the texts. To study the Chinese contents with digital tools, words are more meaningful units than characters, although using characters is also possible. Unfortunately, it remains difficult to algorithmically segment words in classical Chinese. Chinese word segmentationis a key step for computational processing of Chinese texts. CWS for modern vernacular Chinese has achieved good results in international competitions and in practical applications. It is relatively easier to obtain data to train software for CWS of modern Chinese, but there are no known good sources of labelled data for CWS of classical Chinese yet. In fact, even segmenting classical Chinese texts into sentences is an ongoing research topic. We report results of our attempt to segment words in the Complete Tang Poems. CTP is a representative and arguably the most famous collection of Tang Poems. To study CWS for CTP, we must acquire poems that are segmented by domain experts. At this moment, we use only regulated pentametric octavesand regulated heptametric octavesof seven prominent poets. Notes:Regulated pentametric octaves and regulated heptametric octaves mean五言律詩and七言律詩, respectively.An octave poem has eight lines.The poets are Yuan Zhen, Li Shangyin, Li Bai, Du Mu, Du Fu, Bai Jyuyi, Wei Yingwu. The segmentation task was achieved by five assistants who have university-level domain knowledge in Chinese poetry. We have to ignore two poems because they include very rare characters that cannot be handled in our programming environment. We also dropped nine poems when our annotators believed that the poems could be segmented in multiple ways or when our annotators were not sure how to segment them. At this moment, we have 2433 segmented regulated octavepoems. 1427 regulated pentametric octaves and 1006 regulated heptametric octaves There is a popular belief, among experienced researchers and readers of classical Chinese poems, about the word boundaries in the RPO and RHO poems. A sentence in an RPO poem contains five characters. They can be segmented into two patterns: 2-2-1 or 2-1-2, where 2-2-1 indicates that a sentence is segmented into a two-character word, a two-character word, and a one-character word. Similarly, a sentence in an RHO poem can be segmented into two patterns: 2-2-2-1 and 2-2-1-2. We have 19464 lines in the 2433 RO poems, and found that 96.5% of the lines followed the aforementioned expectation. An octave poem has eight lines, so we have 2433×8=19464 lines. Table 1 shows the details. The most common exceptions are due to place or person names in poems, and, in such cases, we observed 2-3 or 2-2-3 patterns. They represent 1.6% of the lines. Table 1. Frequencies and percentages of the most frequent patterns in 19464 lines Weighted Pointwise Mutual InformationThe simplest version of a CWS problem is to determine whether we should segment a sequence of three characters, say XYZ, into XY-Z or X-YZ. Here, an English letter represents a Chinese character. XYZ could represent 依山盡 in 白日依山盡, which is a line in one of Li Bais poems. Our question is whether we should segment XYZ into XY-Z or X-YZ, i.e., should we choose 依山-盡 or依-山盡. We compute the PMI for the competing candidates, and choose the bigram that has a larger PMI. Namely, we choose XY-Z if PMIis larger than PMI. The PMI of a bigram, AB, is defined below. Normally, we use labelled data in machine learning research for training. However, we have only 2433 segmented poems, and will use them to evaluate our methods for segmentation. Hence, we cannot use 2433 poems for training, and have to train PMI values with other poems to segment the lines based on domain knowledge or heuristics. Consider a line, JKLMNOP, in an RHO poem. An RHO poem contains seven characters, and, again, we use an English character to represent a Chinese character. Although we are not sure of the correct segmentation, we can assume that this line may follow either the 2-2-1-2 or 2-2-2-1 pattern as we explained above, and record the occurrences of JK, LM, NO, and Por the occurrences of JK, LM, N, and OP. Since we are not really sure of the correct pattern for a line, we can only assign different weights to JK, LM, N, NO, OP, and P based on certain assumptions. We can then use the weights for unigrams and bigrams to estimate the probability values. In this running example, we are more confident that we will see the occurrences of JK and LM than the occurrences of NO and OP, so it is reasonable to assign larger weights to JK and LM than to NO and OP. Under the current assumption, KL is unlikely to form a bigram, but we may choose to assign a small weight to its occurrences. This can be done in many different ways, and we will report technical details in DH2019. We can compute the PMI for bigrams with 38580 CTP poems that contain only five or seven characters in their lines. Since there are less than 7500 distinct characters in CTP, we hope that having more than 2.22 million characters in 38580 poems will provide statistics about the PMI values with reasonable reliability. We do not use the poems of the poets whose poems are in our segmented poems for training PMI values because we use the segmented poems as the test data. Hence, it is possible to encounter unseen unigrams and bigrams at test time. In these cases, we adopt a basic smoothing procedure to estimate the unseen instances. Segmentation with Weighted PMI We can measure the quality of segmentation decisions in different ways. The most common measurement is the precision rate, recall rate, and F 1 measure. Denote the segmentation decisions for a line as an array of either Nor P. Consider a seven-character line. There are six positions to place segmentation markers between the characters, and the correct decision for a line of 2-2-2-1 pattern is NPNPNP. An array of NPPNPP will produce a 2-1-2-1-1 pattern, and resulting in 2/4, 2/3, and 4/7 in precision, recall, and F 1, respectively. Among our 2433 segmented poems, we can find 2009 poems that contain only lines that conform to the four patterns. These four patterns are 2-2-1 and 2-1-2 for regulated pentametric octaves and 2-2-2-1 and 2-2-1-2 for regulated heptametric octaves. For these patterns, the segmentation problem is boiled down to choosing either 2-1-2 or 2-2-1 for RPO poems and either 2-2-1-2 or 2-2-2-1 for RHO poems. Hence, we expect to achieve more favorable results when we use these 2009 poems as the test data. Using this prior information in our segmenter, we produce only NPPN or NPNP decisions for RPO poems and NPNPPN or NPNPNP for RHO poems. Running our segmenter with the 2433 poems, we achieved 89.6% in F 1. When experimenting on the 2009 poems, we achieved 90.3% in F 1 , which is listed in line in row  4 patterns + PMI in Table 2. Table 2. Quality of Segmentation for different combinations of strategies and datasets. Taking this factor into consideration, our segmenter can segment a pair of lines in the same pattern. We achieved 89.9% in F 1 and recovered 85.7% words for the 2433 poems. Statistics for this method of segmentation is shown in row  4 patterns + PMI + parallelism in Table 2. The resulting improvements in F 1 and word recovery are not very impressive by adding parallelism into consideration. We have to consider an even stricter measurement: the percentage of perfectly segmented poems. Considering the number of correct decisions needed to perfectly segment a poem, this measurement is very challenging. After including the parallelism factor, we raised this percentage from 14.2% to 21.5% for the 2009 poems. Concluding Remarks and Recent Progress with Word Vectors Table 2 summarizes the experimental results that we have discussed and observed in experiments that considered different combinations of segmentation methods and types of test data. Using the weighted PMI scores and adopting appropriate domain knowledge help the segmenter achieve better results. Our results are based on 2433 poems of seven famous poets. It is intriguing to replace the PMI scores with the cosine similarity that we can compute with the word vectors, but we only observed some poor results in few preliminary explorations. More recently, we have increased the amount of labelled data significantly, and were able to apply deep learning, including the LSTM unitsin our classifiers for the CWS task. With the increased data, we have boosted the performance noticeably, and we shall discuss these latest results at DH 2019. Acknowledgements The research was supported in part by contracts MOST-104-2221-E-004-005-MY3 and MOST-107-2221-E-004-009-MY3 of the Ministry of Science and Technology of Taiwan and in part by projects 107H121-06, 107H121-08, 108-H121-06, and 108H121-08 of the National Chengchi University. The segmentation task was carried out by five assistants who major in Chinese Literature. We thank Yi-lin Dai, Nai-An Fu, Wei-Ting Huang, and Shuo-Feng Tsai of the University of Taipei and Yu-Ching Song of the National Taipei University. "
	},
	{
		"id": 236,
		"title": "Toward Building Chronicles from Biographies in Local Gazetteers: An Application of Syntactic and Dependency Parsing",
		"authors": [
			"Liu, Chao-Lin",
			"Chang, Wei-Ting",
			"Zheng, Ti-Yong",
			"Chiu, Po-Sen"
		],
		"body": " Overview Inspired by the increasing influence of China Biographical Databaseto historical studies, researchers are building biographical databases for JapanJapan Biographical Database: <https://www.network-studies.org/#!/>. , Singapore Singapore Biographical Database on Chinese Pioneers: <http://www.nlb.gov.sg/resourceguides/category/singapore-bio-db-on-chinese-pioneers/> , and Taiwan Taiwan Biographical Database: <http://tbdb.ntnu.edu.tw/>. . We extract information from various types of historical documents, and organize the extracted information for further queries and analyses. In CBDB and the Taiwan Biographical Database, data are stored in relational databases. We report experiences for producing a chronical listing for accounting an individuals life. In the Chinese community, this is called 年譜 /nian2 pu3/ or 年表 /nian2 biao3/. The chronicles record main achievements of individuals in selected years. The statements summarize yearly achievements, and are usually brief and informative. To this end, we locate descriptions about an individuals yearly work from source biographies, and aim to produce shorter descriptions for the chronicles. One may accomplish the latter sub-goal with some different techniques including statistics, extractive summarization, abstractive summarization, and sentence compression. We focus on our experience in sentence compression that utilize parsed structures. Results of empirical evaluation suggest that we might produce reasonable assisting software for compiling chronicles, but fully automatic compilation of chronicles without large human annotated data can be challenging. Data Sources and Main Ideas We use the texts in two volumes of biographies which belong to the official gazetteers that were published by the Taipei city government, and we refer to this collection as STCG henceforth. The biography part of STCG includes more than 479 thousand of Chinese characters and punctuation for 319 persons. The biographies have between one and five pages. Although the contents of biographies differ, many of the statements in the biographies follow a typical style We have opportunities to contact the editorial board of the gazetteers, and was confirmed that the authors did agree on a format that they should follow whenever reasonably possible. , so it is possible to extract statements about yearly accomplishments from those relatively long biographies. Figure 1: a constituency tree Figure 2: a dependency structure We then process the statements with constituency and dependency parsers. We use the Stanford package that includes several components, including constituency and dependency parsers. <https://nlp.stanford.edu/software/> Figures 1 and 2 show the output of the parsers for the statement  張先生擔任臺灣電力公司的銷售經理 An English translation for this Chinese statement is  Mr. Zhangserved asthe sales managerof the Taiwan Power Company. We chose not to show the constituency and dependency trees for this English translation mainly because the syntactic grammarsof Chinese and English are different. Showing the constituency and dependency trees for the English translation might not really help reviewers and readers who do not read Chinese to appreciate the meaning and applicability of the trees. , respectively. These figures were produced by the Stanford CoreNLP 3.9.1 GUI. The constituency tree shows the syntactic relationships among the words, and the dependency tree shows the functional relationships among the words. In Figure 1, we can see that the constituency parser recognizes  張先生 as the main noun phraseand  擔任 as the main verb of a verb phrase. In Figure 2, the dependency parser identifies  張先生 and  銷售經理, respectively, as the subjectand the objectof the main verb. Given a dependency structure, we may design a computational procedure to shorten the original statement while attempting to keep the core information of the statement. For the above example, if we can identify the main verb in the dependency structure, keep the nsubj and the dobj nodes and the associative compound:nn nodes, and drop all other nodes, we will produce 張先生擔任銷售經理 The English translation of this compressed sentence is  Mr. Zhang serves as the sales manager. , which is a good compressed expression of the original statement. Empirical Evaluation To evaluate the effectiveness of the main ideas, we implemented a prototype for creating personal chronicles from a given biography. Extracting Statements about Yearly Achievements The first step is to extract statements that begin at a time stamp and end at another time stamp. For instance, we can extract the statements between In 1931 and In 1932, and assume that the statements are about the biographees status in 1931. Although this assumption might not work for all genres of texts, we actually extracted 4948 such statements from our corpora. Our current method for identifying time stamps needs to be strengthened. Not all time expressions include numbers, e.g., next year and two years later. We need to build a full-fledged capability to identify all time stamps and extract the statements perfectly. Figure 3: Distribution of sentence lengths Segmenting and Sampling Chinese Sentences Although modern Chinese texts use a punctuation mark for ending a sentence, such a Chinese sentence may actually be separated by few Chinese commas, and these separated segments may correspond to multiple English sentences. Treating statements that are delimited by Chinese commas and/or Chinese periods as sentences is a common practice. We analyze the numbers of characters in the sentences, and Figure 3 shows the distribution of the sentence lengths. We chose to work on the sentences that have between 7 and 20 characters, and ignored those that have no verbs or have multiple verbs in the current study. Sentences that are very short do not need to be shorted, and, normally, parsers cannot correctly handle very long Chinese sentences yet. Sentence Compression using Heuristics We can employ and evaluate heuristic rules for the compression task. An intuitive rule is to start from the main verb in a sentence. We reserve the nsubj and dobj nodes that connect to the main verb in a dependency structure. Keep the amod, compound:nn, and tmod nodes that are with the nsubj or dobj nodes. Keep also the auxpass, case, and neg nodes because they all carry crucial information, e.g., auxpass indicates the passive voice. Definitions about the dependency labels are available at <https://nlp.stanford.edu/software/dependencies_manual.pdf>. Alternatively, we compute the word vectors for the words in the corpora with Gensim. Gensim: https://radimrehurek.com/gensim/: window_size=5, vector size = 250, CBOW For typical simple sentences, we could find the top level NP and VP, and identify the main verb and the main noun for the main VP in a constituency tree. We calculate the average vector for these core words, and select a next word to include to optimize the resulting cosine similarity between the average vector and the sentence vector of the original sentence until the similarity reach 0.95. Evaluation Among the shortened sentences, we randomly chose ten samples for a specific length of sentences, and judged the appropriateness of the sampled sentences ourselves. We repeated the evaluation five times, and accepted 317 instances out of 700 test sentences when we tried the first heuristic. Figure 4 shows the average acceptance. We list some pairs of the original sentence and the shortened sentence for reader reference. Accepted:,,,,,Failed:,,,,,We repeated the same procedure to evaluate the effects of using the second heuristic. Figure 5 shows the average acceptance when we asked three different persons to judge the accpetance of the compressed sentences. Figure 4. Average acceptance rates of the shortened sentences for the original sentences of varying lengthsFigure 5. Average acceptance rates judged by three personsAcknowledgements The research was supported in part by contracts MOST-104-2221-E-004-005-MY3 and MOST-107-2221-E-004-009-MY3 of the Ministry of Science and Technology of Taiwan and in part by projects 107H121-08 and 108H121-08 of the National Chengchi University. "
	},
	{
		"id": 237,
		"title": "Dishes on the menu: Turning Historic Menu into Menu Network",
		"authors": [
			"Li, Hui"
		],
		"body": " Introduction The term restaurant, in Oxford English Dictionary, is defined as a place where people pay to eat meals that are cooked and served on the premises. This word first appeared in 1806, and origins from Latinand French. The term menu, is defined as a list of dishes available in a restaurant. This word first appeared in French in the 1830s, and origins from the Latin word minutus. Historical menus provide abundant information about changing regional tastes, the ingredients of popular dishes, the arrangements of different meals, and fascinating stories behind the menu, to name but a few. These menus constitute a significant aspect of culture of diet. Recently network analysis has been applied in the fields of digital humanities, e.g., kinship analysisand correspondence network, to explore a variety of relations among different entities effectively and efficiently. However, research upon the modeling, measurement, and analysis of menus network is still at its very beginning. In this paper, we make particular efforts to answer the following questions: can we model the menus network in such a way that we can analyze interesting evolving patterns of cuisine? How can we integrate temporal, geographical, economic, and textual information into the network? In this paper, we aim to propose a menu network that closely resembles todays social network based on the metadata and content of menus. It is our hypothesis that such a menu network provides a more holistic view of that period of time than what would be perceivable through single menu sheet. Literature Review Historic menus convey detailed information about the trends, styles, culture, and history of diet. An increasing number of libraries and institutes, e.g. New York Public Library, Los Angeles Public Libraryand Cornell University Library, devote to digitizing the menus collections and making them accessible to not only academic, but also the general public who are interested in the history of dining. Facilitated by digitization, historic menus embody emerging academic interests from fields such as history study, linguistics, literary study, and nutriology. Researchers are curious to explore more beyond the words on the menu. For instance, Jurafsky et al.investigated the origin of Sushi and employed state-of-art statistical methods such as linear regression to compare the differences between the language used by low-cost restaurants to expensive ones on their menus. Turnwald et al.used log-likelihood ratio to reveal that restaurants described healthy dishes with significant less appealing words but more health-related words. However, these studies didnt propose formal modeling of historic menus. Chahuneau et al.built a statistical model to predict price and sentiment for restaurants based on menus and customer reviews. However, they didnt integrate temporal and geographical information in their model. Menu Networks In this section, we present the definition of a menu and propose our menu network model, respectively. Definition 1. Menu . A menu is represented as a tuple m = { r, l, t, f, p} . r ∈ R denotes the restaurant name. l ∈ L denotes the restaurant location and t ∈ T specifies the date when the menu was used. f ∈ F specifies a dish on the menu. p ∈ P corresponds to the dish price. Definition 2. Menu Network . A menu network is represented as an undirected graph G =, in which nodes V correspond to menus and labeled edges E correspond to the co-occurrence of dishes on the menus. Each node has associated attributes {| r ∈ R, l ∈ L, t ∈ T, fd ∈ F}. we define a dish function fd : V → F as an attribute for each node to mark the dishes on each menu. r ∈ R corresponds to the restaurant that created a menu. l ∈ L corresponds to the location of the restaurant and t ∈ T denotes the date that a menu was used. Each edge has associated attributes {| d ∈ N , f ∈ F, p ∈ P}. Considering that there might be more than one dish appearing on both menus, we use an index number d to differentiate each element in the edge attribute set. For instance, if there are k dishes appearing on both menus i and j, the corresponding edge attribute is: {,,...,}. The number of triplets in the set corresponds to the number of dishes occurring on both menus. "
	},
	{
		"id": 238,
		"title": "Visualizing Networks of Artistic Ideas in History Paintings in the Seventeenth-Century Netherlands",
		"authors": [
			"Li, Weixuan"
		],
		"body": " Existing studies have demonstrated the potential of network analysis in extracting and visualizing complex information from large historical datasets. Art history, however, still dwells in the early stages of exploring how network methodologies can benefit its scholarship. The only applications of network analysis in art historical research often equate network to social network and borrows concepts and theories from the social sciences. Digital art history calls for new concepts tailored to artistic networks in order to account for the developments of art in all its complexity. This research conceptualizes a novel art historical network of ideas inscribed in the iconography, connecting artists not through social ties but shared subject matters. Through visualizing this conceptual network, this study tries to answer the following questions: how are artists connected through their choice of subject matters? Have the patterns of such connections changed over time? And more importantly, is the choice of subject matter related to the choice of painting style, location of residence, and timing of entering the market? To answer these questions, this study draws on large digital collections of Netherlandish paintings to construct a dynamic network model of iconography. This research foregrounds history paintings produced in the Low Countries between 1575 and 1700, an established genre which witnessed dramatic iconographical changes during this period. The results of this study demonstrate the spatial and temporal evolution of the artists network of ideas, revealing the changes in the structures of artistic interactions and the diffusion patterns of subject matters within the artist community in different cities. Data This research takes advantage of the large online collection of paintings of the Netherlands Institute for Art History. https://rkd.nl/en/explore/images This database describes over 150,000 paintings within the scope of this study. In the absence of a mature image recognition technology that is able to discern a great variety of complicated subjects of the paintings, this research uses the iconographical notation system ICONCLASS, a hierarchically ordered system of unique alphanumeric classification codes with which most images in the RKD have been processed and tagged. Iconclass is a classification system designed for art and iconography. It is the most widely accepted scientific tool for the description and retrieval of subjects represented in imagesand is used by museums and art institutions around the world, such as the Rijksmuseum. More details see: http://www.iconclass.nl/ History paintings in the RKD database are distinctively labeled with ICONCLASS often referring to a specific scene or story, like Adoration of shepherdsand Diana discovers Callistos pregnancy. I further link the RKDImages to the comprehensive biographical database, ECARTICO, to supplement the paintings with the biographical information of the artists. ECARTICO is a comprehensive collection of structured biographical data concerning painters, engravers, printers, book sellers, gold- and silversmiths and others involved in the cultural industries of the Low Countries in the sixteenth and seventeenth centuries: http://www.vondel.humanities.uva.nl/ecartico/ The active period of the artists from ECARTICO is used to introduce the dimension of time to examine the evolution of popular subjects and to evaluate the strength of connections among generations of artists. Method The network of ideas draws a connection between two artists through a mutual subject matter; the strength of the tie between them is measured by the total number of iconographical overlaps in their oeuvre. The record of a painting in the RKDImages identifies a link between the painterand the subject. One painter can link to many subjects as he painted various scenes and one subject matter can tie to numerous painters who depicted it. Putting all the works of art and artists together, a two-mode network is constructed, which consists of two types of nodes: artists and subject matters. The two-mode network of persons and subjects are then collapsed into two one-mode networks by converting one of the node types to an edge or link between nodes of the other type. For artists who picked a particular subject matter, the subject becomes the conceptual edge connecting the two artist nodes, while a single artist who depicted two stories would be the edge between those two subject matters. In other words, the two-mode, artist-subject network is projected into 1) an artist-artist network, in which the link is constructed by the subject matters artists mutually depicted; and 2) a subject-subject network in which the link is drawn by the artists who painted both themes. I resort to graph theory to examine the nature and the properties of the network of ideas. Among the measurements, modularity is particularly relevant to art history as it helps us to identify clusters of artists that are more likely to paint the similar subject matter or groups of subjects that are more often co-existent in a single painters oeuvre. Modularity can help understand whether or not the thematic choice is related to the choices of other aspects of their work and life. Results and Discussion Changes in the network of ideas over time Artists are grouped by the year they entered the market following Rasterhoffs periodization of the art market in the early modern Netherlands and the network of ideas is constructed among artists in each period. Rasterhoffintroduces the three periods of the painting industry and art market in the early modern Dutch Republic: 1580-1610 as a period of transition; 1610-1650 as an expanding market; and finally, 1650-1800 as mature market for paintings and other cultural goods. A simplified version of the networks is visualized in Figure 1, showing the iconographies shared by at least three artists. Nodes in Figure 1 are sized by weighted degrees and are colored by the modularity classes. Table 1. Metrics of the network of subject matter by time period # of edges #of Iconclasses Avg. Degree Degree centrality Avg. Clustering coeff. Avg. path length Modularity 1575-1610 89 35 5.09 0.53 0.75 2.26 0.39 1610-1650 237 75 6.32 0.62 0.64 1.90 0.38 1650-1700 380 57 14.58 0.40 0.73 1.97 0.33 The structure of networks shown in Table 1 indicates an expansion in popular subject matters between 1610-1650, coinciding with higher interconnectivity seen in the network of artists. Astonishingly, although a greater variety of subject matters have been explored and popularized by the generation of masters who entered the market between 1610 and 1650, their successors fell back to a shrunken pool of subject matters and restrained their repertoire, which is reflected in the reduction of the number of nodes. This observation of the popularity of a limited number of subjects and the virtual absence of others in the evolution of the network of subject matters verifies the limited repertoire art historians suggested. Furthermore, a structural change is observed in the evolution of the network across time periods: the connections among the subjects depicted by painters entering the market after 1650 are more evenly distributedand the network is less centralized, meaning the painter was likely to pick among the same pool of subjects instead of venturing into new iconographies.1575-16101610-16501650-1700 Figure 1: Network of subject matters by time periods Remarkably, the clusters of subjectsdo not speak to the categories modern art historians use such as mythology, Old and New Testament. Rather, one can find a mix of all three in one cluster. The clusters in Figure 1 relate more closely to the visual resemblance of the scenes and the skills required to paint them. For instance, both the Destruction of mankindand Marriage of Peleus and Thetisfrom the same cluster, depict scenes that are usually composed of a large group of nude figures in an outdoor setting. Geographical variation of taste and connection The network of ideas infers a certain degree of locality, as observed in the size and structure of the network constructed by artists who mainly worked in the following artistic centers: Amsterdam, Haarlem, and Utrecht. It means the concept of local schools did exist before 1610 but gradually dissolved in the following decades when the market was more integrated and painters living in different cities could keep abreast of each other reflected in their choices of subject matters. Table 2. Metrics of the network of artists by location # of edges #of painters Degree centrality Avg. Clustering coeff. Avg. path length Modularity Amsterdam 140 43 0.48 0.59 2.24 0.17 Haarlem 29 16 0.87 0.68 1.93 0.08 Utrecht 43 18 0.68 0.64 1.87 0.10AmsterdamHaarlemUtrecht Figure 2: Network of subject matters by city The networks of ideas in Amsterdam, Haarlem, and Utrecht demonstrate unique characteristics: Amsterdam had higher modularity which loosely corresponds to different styles co-existing at the same time; Haarlem had a centralized network and a schism between generations, and Utrecht enjoyed the most densely woven networks of both artist and subject matters. The choice of subject matters shows certain preferences in each city, with Amsterdam venturing more into the new subjects popularized by innovators from Rembrandts teacher, Pieter Lastman, to Rembrandt, Haarlem demonstrating the legacy of Mannerism, and Utrecht embracing both old and new fashion in its oeuvre. Amsterdam has the largest network among the three artistic centers. The high average path length and the relatively low clustering coefficient indicate a more hierarchical structure around key painters and the flow of artistic knowledge is likely to have trickled down from the great masters to their circle, then to the lower segments, instead of diffusing among the minor masters. Amsterdams network is also the least centralized among the three, suggesting that several masters formed their relatively independent circle of interaction. "
	},
	{
		"id": 239,
		"title": "Knowledge Representation: Old, New, and Automated Indexing",
		"authors": [
			"Logan, Peter M",
			"Greenberg, Jane"
		],
		"body": " While historical documents are common sources of digital humanities collections, many digital projects still do not use controlled terminologies to represent their content and aid search, discoverability, and use. There are often pragmatic reasons for this. The art of identifying appropriate descriptive terms is a valuable skill. Unfortunately, too few DH projects have access to information specialists who can index their documents for them. We are addressing these challenges with the Nineteenth-Century Knowledge Project, an ongoing initiative to create the first standards-compliant digital version of historical editions of the Encyclopedia Britannica. The sheer scale of the project precludes human indexing, because it would take an estimated six-to-eight years to read through all of the entries. Instead, we use an innovative method to add automatically generated content metadata using linked open terminologies and the HIVE-approach. This method has allowed us to experiment on the optimal controlled vocabulary to use for indexing historical documents. Our presentation will focus on the results of this experiment. HIVE stands for Helping Interdisciplinary Vocabulary Engineering, and allows for standardized, linked open vocabularies to be used for automatically indexing digitized text. The Knowledge Project demonstrates how large corpora can use automated indexing and still garner the benefits of controlled terminology. As part of the undertaking, we need to optimize the fit between the material and the controlled vocabulary we choose to do the indexing. What is the best vocabulary to use? We will compare different outputs generated from current and historical controlled vocabularies. The question we are trying to answer is whether a historical vocabulary that was current at the time of publication produces significantly different results than the present-day Library of Congress Subject Headings. The topic is critical for the Knowledge Project in particular. We are examining a 120-year span of historical editions, with an eye on identifying changes large and small in the construction of knowledge over time. Researchers know that knowledge changed dramatically between 1790 and 1910, and part of that change was a shift in attitudes about what counted as official knowledge and what did not. As social beliefs change, so too do the cultures ideas about what matters and even about who has the authority to define knowledge. In the nineteenth century, that authority decisively shifted out of the hands of religious authorities and into those of scientists and professionals. The history of the earth changed, from a narrative based in scripture to an evolving narrative dependent of the discoveries of geologists and biologists, like Charles Darwin. Social beliefs at the time also define the selection of articles; there were few topics specific to women, as we would expect in a society that devalued their contributions. And social beliefs also shaped the content of the articles that were included: those on India and Africa reflect a colonizers perspective and represent indigenous people in ways we recognize as offensive stereotypes. Britannica was the most authoritative general reference source in the English language in the nineteenth century, largely because it faithfully represented the idea of knowledge at the time, in the most comprehensive fashion possible, and so it documents for us today the many problems inherent in the nineteenth-century concept of knowledge itself. This historical collection is not what we would call knowledge today, and that is exactly the point. It serves as a viable stand in for what constituted knowledge in a previous century, and thus provides us with an important data set to explore the changing structure of knowledge over time. Within this corpus, we can trace the emergence of the twentieth-century concept of knowledge. But to do this accurately, we need to identify the older structure of knowledge and preserve its internal integrity as a historical object. Example of knowledge organization in 1728. Chambers, E., Preface. Cyclopaedia, or, An universal dictionary of arts and sciences, 2 vols., London: Knapton, 1728, p. ii. Like other comprehensive representations of knowledge, older vocabularies are also socially constituted, rather than neutral categorizations of knowledge topics. The Dewey Decimal Classification illustrates this principle well; in its early formation, the only two categories for African Americans were slave and colonial subject. This left no way to indicate books that had been authored by African American writers, and so it mirrored the same racial stereotyping present in the nineteenth-century Encyclopedia editions. Both are products of the same society and so both embody parallel issues in representing knowledge as a system. In theory, by imposing a controlled vocabulary from the twenty-first century onto older historical materials, we are distorting the historical structure of knowledge and muddying the waters, precisely when we need to see most clearly the difference between current and historical concepts. In theory, using a historically-appropriate vocabulary will generate metadata that better captures the older structure of knowledge by not translating it into the terms of current, equally contingent vocabulary. There is little evidence that this theory has ever been tested. We are running a study on these entries using the HIVE automated keyword generator with two controlled vocabularies. The first is the current Library of Congress Subject Headings. The second is the controlled vocabulary created by Ephraim Chambers for his Cyclopaedia. This was the ontology used by the Britannica for its third edition of 1790, so it is the ideal vocabulary for the material. In this short presentation, we would like to review the experiment, compare the automated index terms output from the two vocabularies, and present our findings on the real-life consequences of indexing older materials with historical vocabularies. "
	},
	{
		"id": 240,
		"title": "Simulating Historical Flows And Connection. The Artistic Transfer During The 15th To 16th Century In The Iberian Peninsula.",
		"authors": [
			"Ferreira-Lopes, Patricia"
		],
		"body": " Introduction The Late Gothic periodwas a phase of transition in Europe – with social, political, economic and cultural changes. Within this framework, Europe was the scene of a significant amount of mobility of artists that in some way materialised the production of architecture without borders: a Pan-European stylecapable of reproducing and adapting models in different places. This paper will present the project ArTNet Analysing artistic transfer network. A social and spatiotemporal study of Late Gothic architectural production in the Iberian Peninsula which was designed to identify, record and analyse artistic transfer network transcending the building scale to better understand the process of Late Gothic architecture production in the Iberian Peninsula. An integral view bringing together several factors is being studied by multiscale models, combining HGIS and Graph model, and analysis. The Iberian Peninsula was chosen for analysing the artistic transfer due to its spatial and historical characteristics: 1) it is a sufficiently extensive area, so it provides a suitable sample of data; 2) there was considerable communication and a great number of exchanges between Spain and Portugal during the Middle Ages; 3) during the transition to the Modern Era, the so-called Reconquest influenced the territorial consolidation and the architectural production; 4) Spain and Portugal were at this period conquering American and African territories, which increased their capital power and a new constellation of cities emerged providing more activities related to building construction. The five research objectives of the project are: RO1. To digitally register the artistic transfer in the Iberian Peninsula with all the available resources, creating a new and open digital model; RO2. To analyse the movement of artists through space and time. These track lines changed over time, reflecting also the political, economic, social and cultural evolution of the Iberian Peninsula. The study of the artists activities can show interesting patterns, especially when this took place between the main centres. In addition, we expect to discover the different levelat which these movements have occurred to also provide the analysis from the building scale; RO3. To evaluate, from the heritage point of view, the systems and subsystems that make up the Late Gothic in relation to landscape, cities, infrastructure and the most significant architectures; RO4. To formulate hypotheses through simulations to analyse transfers of artists due to incomplete or undocumented records. By doing so, the project will be able to deal with gaps in the historical data and offer new interpretations and insights; RO5. Diffusion of the methodology and the results, increasing open access to the historical data created and gathered. In terms of scientific research in Architectural History, this project is providing new methodological concepts that will help to better understand the process of the Late Gothic architectural production. Methodology ArTNet uses a data-driven approach backed by techniques borrowed from GIS, Graph theory and data visualisation aimed at addressing the objectives outlined above. The methodology designed for this project is based on previous projects. The methodology is divided into four linked phases: M1. Documentation of the artistic transfer . This is the main base of the database. All the information availableis being selected, digitised and processed in order to convert analogue formats into digital ones. M2. E-database Model. While doing M1, a preliminary phase to structure the data as an event-based model was conducted and tested . Implicitly ArTNet therefore assumes that an event is the temporal entity that will interconnect artefacts, agents, time, buildings, techniques, etc. M3. Inter-sectorial data connection . This project also contemplates that the data should be open and interconnected with Open Scientific Data Repositories and Institutional Databases, such as the Instituto del Patrimonio Cultural de España and the Direção Geral do Patrimonio Cultural de Portugal. M4 . The impact of artistic transfer in the architectural production. Two digital models will be considered: a spatiotemporal model capable of organising the information from the E-database in layers, a geographically-integrated historical GIS model; and a Graph modelbased on an abstract model in which the main elements are the relationships, a relational-integrated historical model. In M4 we were able to: i) create working models to simulate the movements of artists; ii) analyse the density of events and patternsin space and time; iii) capture, analyse and visualise the influence of artists at the buildings where they were active; iv) run ego, centrality and clustering; v) provide models capable of testing hypotheses and also of generating new ones by simulation. These analyses offered new perspectives for understanding the phenomenon of the Late Gothic and re-evaluated its dynamism in light of the interaction between agents. Discussion Although in early stages, ArTNet has already provided new ways to research the artistic transfer by the simulation of new hypothesis and visualizations. The inputted data is still limited by the sources consulted and treated so far, making it difficult to achieve a more wide-ranging model. While ArTNet, as a project, is focused specifically on the artistic transfer in the Iberian Peninsula, collaborators have already identified cases that connects with other European countries. One of our goals moving forward is to test and design a semantic web in order to adjust and expand the data structure for the research community. Fig.1 Simulation of a flow hypothesis. Studying the route made by Juan del Castillobetween Seville and Lisbon in 1508. Fig. 2 General of the Graph model. "
	},
	{
		"id": 241,
		"title": "Computer Assisted Curation of Digital Cultural Heritage Repositories",
		"authors": [
			"Lorenzini, Matteo",
			"Rospocher, Marco",
			"Tonelli, Sara"
		],
		"body": " Introduction The objective of metadata curatorship is to ensure that users can effectively and efficiently access objects of interest from a repository, digital library, catalogue, etc. using well-assigned metadata values aligned with an appropriately chosen schema. However, we are often facing problems related to the low quality of metadata used for the description of digital resources, for example wrong definitions, inconsistencies, or resources with incomplete descriptions. There may be many reasons for that, all completely valid, e.g, in many cases those who host a digital repository have few human resources to work on improving metadata, and often data providers are not themselves the metadata creators. In this paper we present our ongoing work aiming at defining computable metrics to assess metadata quality and automatize metadata quality check process. State of the art The curation framework developed by Bruce and Hillmannis considered as a benchmark in the pursuit of quality assessment of digital repository. This framework defines seven dimensions to measure the quality of the metadata: Completeness , Accuracy , Conformance to Expectations , Logical Consistency and Coherence , Accessibility , Timeliness , Provenance . In the digital cultural heritage domain, these dimensions are fundamental for the evaluation of metadata quality. The evaluation helps various curators to systematically identify metadata problems, and could be straightforwardly applied to Europeana Digital Library https://www.europeana.eu/portal/en or the Ariadne http://www.ariadne-infrastructure.eu project as well. From the literature analysis it can be inferred that the problem of metadata curation is one of the most debated topic in the domain of humanities. Few attempts) were made to automatically compute quality metrics: however, they either focus on one dimension or are specific of some repository or metadata schema/profile. Proposal for automatic quality check The solution we propose is a framework which aims at automatically checking metadata quality of a repository along the dimensions proposed by Bruce and Hillmann. It would also enable reporting to metadata curators a detailed analysis of the metadata situation and suggestions for possible improvements. To develop such framework, two main activities have to be carried out: To define metadata quality metrics, capturing the status of the metadata both at object leveland, aggregated, at repository level; To define algorithms to compute the aforementioned quality metrics, andreturn suggestions on how to fix low-quality metadata; We already identified some strategies on how to perform these activities along some of the dimensions proposed by Bruce and Hillmann, and in particular: Completeness : computed by statistical approach; ratio of filled fields with respect to metadata profile. The resulting value is a real number between 0 and 1: the closer this value is to 1, the more complete the metadata description of the object. Accuracy : computed using Natural Language Processing applied to description field; several linguistic featuresare extracted from the description text, and, using part of the gold standard data as training set, a binary classifier will be trained to distinguish between good and bad descriptions based on such features. Each description is represented as a vector of features. Logical consistency & Coherence : computed using semantic web technologies; leverage ontological background knowledge on artists, objects, dates, art periods, etc., to assess the coherence of metadata values); enforce use of reference ontologies / vocabularies on controlled metadata. To validate the quality of the developed metrics and algorithms, we will rely on a gold standard dataset manually collected from the available repositories. Part of this dataset of good quality data, will also be complemented with bad metadata quality objectsto support the training of the envisioned supervised approaches. Work done so far We collected an initial gold standard dataset of 100 high-quality records, in terms of metadata description, from the repository of the italian digital library Cultura Italia http://www.culturaitalia.it/ , spanning different object typologies, to test the effectiveness of the metrics and methods to be developed. We started implementing the metrics and methods for the completeness dimension. The idea is to count the available metadata, dividing this number over all expected metadata. More precisely, to capture that some metadata may be more importantthan others when estimating the completeness of an object description, we associate a weight to each metadata: e.g., compulsory fields may have weight 2, while non-compulsory ones weight 1. Hence, the completeness ratio results by dividing the sum of the weights of the available metadata over the sum of the weights of the expected metadata. The resulting value is a real number between 0 and 1: the closer this value is to 1, the more complete the metadata description of the object. We computed the completeness ratio for all the ~2,500 records of the Palazzo Pitti dataset in CulturaItalia. Fig. 1 shows a breakdown of the records in 5 interval categories according to their completeness ratio, while Fig.2 plots the frequency of each specific metadata in the dataset. Fig.1 Percentage of completeness ratio Fig.2 Frequency of the metadata elements Conclusion Considering the amount of digital archives, problems related to metadata curation becomes evident. Reasons may be different: There is no curation task force, the metadata curation activity is delegated to the content providers or the metadata curation activity is made by hand. The development of an automatic process will enable the curators to not only obtain snapshots of the quality of a repository, but also to constantly monitor its evolution and how different events affect it without the need to run costly human effort. This could lead to the creation of innovative applications based on metadata quality that would improve the final user experience. "
	},
	{
		"id": 242,
		"title": "Czech Literary Bibliography: Database Mirror of 250 Years of the Modern Czech Literature",
		"authors": [
			"Malínek, Vojtěch"
		],
		"body": " The aim of the proposed poster is to present the datasets and the current DH-related projects of the Czech Literary Bibliography research infrastructure, which has now been continuously operated for more than 70 years under methodology developed over the long term. The CLB comprises a set of bibliographical and other specialized databases processing research information on Czech literature. These datasets are updated on an ongoing basis and systematically reflect poetry, fiction, cultural journalism and specialist texts on Czech literature and literary life in the Czech lands. CLB activities centre around their compilation, systematic utilization and further ongoing development. Having used the internationally recognized standardized data format MARC21 and the set of persistent identifiers, the CLB data are available for interconnection into national and international networks for the exchange of research information. In the last few years a significant movement towards software tool development and data-driven research can be observed in the CLB. The parameters of the CLB bibliographical databasesmake them the most extensive specialist bibliography in the Czech Republic and one of the most complex sources of literary-scientific information in Europe. During 2014 the CLB successfully underwent an assessment by the Ministry of Education, Youth and Sports of the Czech Republic and was included with effect from 2016 in the Roadmap of large Czech infrastructures for research, experimental development and innovation. Having nearly finished the protracted conversion of single databases, the CLB is just starting to use the modern VuFind discovery system as a central gateway to its datasets. Simultaneously, the CLB has significantly broadened its scope since 2012 by processing large specific corpora of the data: the bibliography of the 1945–1960 period, samizdatand the internet. This poster will summarize the most important CLB projects related to the field of DH and describe the main CLB datasets. Namely: The Retrospective Bibliography of Czech Literature – a database based on an original card catalogue that chronologically covers the 1770–1945 period with 1.6 million entries; currently available in the specialized RETROBI system; The Contemporary Bibliography – a database for the period since 1945 with approx. 600,000 entries; particular parts of this database include the Database of Czech Literary Exile and the newly established databases of Czech Literary Samizdat and the Czech Literary Internet; The database of Czech Literary Figures – a database recording key biographical details of approx. 37,000 individuals involved in Czech literary life, with the emphasis placed on systematic verification of data e.g. at registries and archives or in the form of questionnaires; The poster will also provide information on two main CLB projects within the field of DH research. These are: Czech Literary Internet This ongoing project is being implemented over the 2017–2021 period. Research activities undertaken as a part of this project include the following:bibliographical excerption from the Czech internet – the ongoing treatment of individual selected literary websites and portals, as well as information on the articles there and individual authors for the CLB databases. Every record will include a link to the original web-page and – if possible – to its archived version. A preliminary estimate anticipates at least 30,000 new entries for the period since the late 1990s.development of tools to analyse bibliographical data – a set of superstructural analytical and statistical tools will be prepared to enable the end-user to visualize the selected bibliographical data clearlyRETROBI The RETROBI specialized system has been put in place within the CLB database for the digitization and online presentation of the card catalogues, developed at CLB in 2010–2012 and currently presenting online the enormous Retrospective Bibliography of Czech Literature 1770–1945. This system enables full-text and semistructured queries in the OCR representations of the original cards and includes a number of advanced user functions and utilities for administering and editing data. It has also been successfully deployed at other institutions and its further development is also possible, particularly with regard to the potential conversion of Retrospective Bibliography data to a MARC21-format structured database). Funding: Activities of Czech Literary Bibliography Research Infrastructure are supported by the Ministry of Education, Youth and Sports of the Czech Republic. "
	},
	{
		"id": 243,
		"title": "Modelling a Catalogue: Bilingual texts in Tuscan Middle Ages (1260-1430)",
		"authors": [
			"Mancinelli, Tiziana",
			"Montefusco, Antonio",
			"Bischetti, Sara",
			"Conte, Maria",
			"Macchiarelli, Agnese",
			"Bolognari, Marcello"
		],
		"body": " Introduction This poster introduces a domain-specific ontology built for a catalogue conceived for the study of multilingualism in a medieval context. The catalogue is a final outcome of a systematic investigation from a five-years project of various literary documents which circulated simultaneously in more than one language in Tuscany, and especially Florence, between the mid-13 th century and the beginning of the 15 th century. The conceptual model for this catalogue may serve as a model for the study of similar processes in other language regions. How to make a model for social history of medieval translations? At no time in the history of the West has translation played a more vital role than in the Middle Ages. Translation of texts was an important cultural universe in Italy and throughout Europe. Translation from Latin to the vernacular– Italian, French, etc. – was only one possible way to enlarge the audience for an important selection of texts. At the same time, a number of texts were transferred from the vernacular to Latin, and finally, a smaller group of works were conceived and written in two languages. The multilingual landscape of Tuscany and Florence in particular, presents an ideal corpus for a case study to understand both the social role of translation in a Medieval context and the development of Humanism. Consequently, thanks to a huge amount of data about translation, circulation of texts and cultural actors in such a unique context, this project instigates new questions about the intellectual history of the Late Middle Ages: What is the social role of translation? Who are the translators? In what way have bilingual texts been conceived, and disseminated? The challenge of data modeling of translations and their manuscripts in the Middle Ages The so-called Biflow ontology models the intellectual approach taken by the researchers investigating this corpus of bilingual texts. It will contribute to a better understanding of the circulation of texts and the evolution and the transmission of knowledge in Italy and in Europe. The Biflow ontology maps out two main realms that are clearly distinct from each other: firstly, the linguistic and literary aspects; secondly, the material and historical characteristics of the manuscripts in which the texts are present. Built in RDF/OWL, this tool can also be expanded to all standards of Semantic Web. In order to handle the heterogeneity of all these features, we define classes and properties that extends other formal ontologies already in existence such as FRBR, FOAF and CIDOC-CRM. The model also defines further specific classes and properties related to manuscripts inspired by the manuscript model of Biblissima project. The Biflow ontology allows us to model important aspects of traditions of text and manuscript production and their interrelationships, as well as the prosopography of the people involved as authors, translators or copyists. Within the catalogue entries, there are many possible interpretations of the use of geographical place names and dates - phenomena covered by CIDOC-CRM. The modeling begins with the dossiers compiled by the researchers. Each dossier concerns a text and its tradition of translation and dissemination in various manuscripts. The ontology models this by defining a work class with two main subclasses: manuscript and subwork – corresponding to the two realms. Subwork is defined by a language, translator, etc. and, most importantly, with a property called derivedFrom which maps out a genealogy of subworks. As in the Biblissima model, the class manuscript, which comprises a variety of elements, is a single instance of a subclass of frbroo:F4 manifestation singleton. This defines unique physical objects and is connected to the subwork class. The poster will not only present the Biflow ontology and cataloguebut also examples of the dossiers that have already been completed. "
	},
	{
		"id": 244,
		"title": "Cutting the Gordian Knot: Sustaining Digital Scholarly Editions",
		"authors": [
			"Mandell, Laura C",
			"Tarpley, Bryan",
			"Brown, Susan",
			"Laiacona, Nicholas",
			"Moore, Shawn",
			"Pratt, Lynda"
		],
		"body": " Recent work on digital scholarly editions by Elena Pierazzo, Hugh Cayless, and Joris van Zundert and Peter Bootdescribe the complex array of choices confronting editors of digital scholarly editions who wish to preserve their work beyond the life of current platforms, tools, and even standards. Here we describe a preservation model developed by the ARCScholar Digital Publishing Cooperative during tenure of a planning grantawarded to Texas A&M by the Andrew W. Mellon Foundation. Digital preservation is really two-pronged: libraries are working on the technological infrastructure, in the broadest sense, for maintaining assets, while creators and publishers of digital editions must use established standards in order to prepare the content so that it can be accessed and used via this infrastructure. ARCScholar is doing the latter. The Advanced Research Consortium or ARCconsists of a community of scholars that began to approach digital aggregation in 2003. In that year, Jerome McGann organized a steering committee to launch his Mellon-funded project, NINES, or the Networked Infrastructure for Nineteenth-century Electronic Scholarship. NINES served as a model for launching 18thConnect.org — focused on the preceding century — and the Mellon-funded MESA community, the Medieval Electronic Scholarly Alliance. This group of field-specific scholarly communities and online-finding aids needed to create an overarching organization that would make available for other groups the technical model spearheaded by NINES, provide infrastructure and programming support, and negotiate on behalf of all the groups with vendors that might contribute data. ARC is that organization. ARCScholar, a subsidiary of ARC, is a digital edition publishing cooperative focused on helping publishers of digital editions, both large, institutionally backed publishers and scholar who have published their digital editions on the Internet without institutional support. Our goal is to make those digital editions interoperable and preservable in the most findable way possible, which is in keeping with the mission of ARC. During our planning grant, we created a process for preserving digital editions in university libraries digital asset management systems. The model has been developed at Texas A&M which has a Fedora 4 repository. We are creating a preservation model based upon on one edition, the Digital Donne Variorum. The Variorum Edition of the Poetry of John Donne, edited by Gary Stringer, won a prize from the Modern Language Association in 2015-2016, and the press release explicitly mentioned the value of Digital Donne. The Variorum, both print and digital, was created using primarily assets held by the Cushing Memorial Library at Texas A&M University. The Digital Initiatives Group at TAMU-L created a pipeline for ingesting the TEI files of the edition that is stored in their instance of Fedora 4. Next, the metadata librarians are in the process of creating a catalog record of the digital edition that will appear not only in the local library catalog but in other library catalogs that use EBSCOs Discovery System. The record will contain a) a link to the current, live digital edition; b) a link to the downloadable TEI file and its schema; and c) a IIIF Manifest of the manuscript page images available on a IIIF server: Figure 1. We will be working with Cushing directly so that the IIIF Manifest references the page images digitized by TAMU-L which reside on their IIIF server. This would allow future researchers to access the page images along with TEI transcriptionsso that, once the Digital Donne website no longer exists, users can still access the edition through software such as Mirador: Figure 2. Although enabling access to Digital Donne via EDS and eventually WorldCat does make the edition findable outside of its home institution, more is needed. Findability is critical to digital preservation: the more that digital content is accessed, used, and downloaded, the higher the probability of effective preservation. In order to maximize discoverability, access, and use, we also need to go beyond making metadata available. Future researchers may not know that the information they need is connected to authors, titles, dates of specific digital editions. In other words, the information needed in the future may be part of the content of a digital edition that is not exposed by its metadata. Fedora 4 natively exports RDF Triples: Resource Description Framework statements about the object which render the digital edition findable via the semantic web. However, the triples automatically generated by Fedora 4 reveal metadata only. We are setting up a SPARQL endpoint for ARCScholar that allows depositing much more detailed RDF, exposing the content of the edition as well as its metadata. For instance, in the case of The Collected Letters of Robert Southey, published by Romantic Circles, we would like future generations of scholars to be able to find letters that British Poet Laureate Robert Southey wrote to Humphrey Davy a scientist, creator of the Pneumatic Institute, and developer of electricity. Beyond the life of any particular platform, the research embedded in scholarly editing would then be preserved for and discoverable by future generations. "
	},
	{
		"id": 245,
		"title": "A Robot’s Street Credibility: Modeling authenticity judgments for artificially generated Hip-Hop lyrics",
		"authors": [
			"Manjavacas Arévalo, Enrique",
			"Kestemont, Mike",
			"Karsdorp, Folgert"
		],
		"body": " Introduction Recent years have witnessed an impressive surge in the interest for Natural Language Generation. Advances in neural language modeling in particular have boosted the capacities of computational text generation systems, resulting in increased realism of generated text and the capability to generate text in a variety of genres or styles. Aside from scholarship, text generation is currently triggering a significant deal of attention in the arts, for instance, with the emergence of artist communities such as botnik.org. While the limited semantic coherence of generated text remains a worry, the grammatical correctness and stylistic qualities of these artificial texts are remarkably convincing. Goal The current study set out to study how well human readers are able to distinguish between authentic and synthetic text fragments, with an interpretive focus on the linguistic cues that readers seem to rely on in their authenticity judgements. As a case study, we turned to the domain of Hip-Hop lyrics, known for its explicit content and typical rhythmical delivery. Through a large-scale experiment in the form of a serious game, we crowdsourced authenticity judgments for original and generated rap lyrics. Through statistical modeling of the resulting database of authenticity judgments, the present study aims toenhance our understanding of the cognitive processes at play in the perception of authentic and synthetic artifacts in cultural production andimprove text generation systems. Methods We have compiled a large corpus of Hip-Hop lyrics from the Original Hip-Hop Lyrics Archive, an online archive of Hip-Hop songs. This corpus, amounting to approximately 38M tokens in about 64k songs, was preprocessed in a pipeline that included line and stanza detection, tokenization, syllabification, and grapheme to phoneme conversion. These data were used to train a neural language model, which makes use of recurrent connections to model longer-term dependencies. With the help a LM, a sentence of n words, represented by words w_1 to w_n can easily be generated by following the generative process expressed in Equation 1: During generation, we sample from the output distribution at each step inputting the token sampled at the previous step, which can also be a single character or syllable, depending on the sequences the model was trained on. We trained three kinds of LMs at different levels: a character-level LM, a syllable-level LM and a hierarchical LM, the latter being similar to a word-level LM in that it decomposes the probability of a sentence into the conditional probabilities of its words but, additionally, it decomposes the probability of each word on the basis of its characters. Additionally, we also experiment with a conditional LM variant to each of the three models, which controls for sentence length and final sentence phonology. Figure 1 shows examples of generated text. Figure 1: Generated samples from the experiment randomly extracted from different difficulty bins. Models correspond to character-level, syllable-leveland hierarchical. The trailing + indicates a conditional model. Serious game At the heart of this study lies a crowdsourcing experiment carried out at a popular music festival. In this context, we collected authenticity judgments from participants in a serious game relating to how well participants could distinguish between authentic and artificial fragments generated by one of our models. In order to efficiently communicate the games purpose in the media, we publicized this experiment as a so-called Turing test, although the description below will make clear how the set-up is markedly different from the imitation game which Turingoriginally proposed. Each game took the form of a series ofquestions, each of which had to be solved within a time limit of 15 seconds. The questions randomly alternated between two kinds: Type-A: presented with an authentic and an artificially generated fragment, the participant had to decide which fragment was authentic. Type-B: presented with a single fragment the participant had to decide whether the fragment was authentic or generated. Type-B questions involved less reading but only presented participants with a single fragment, meaning that participants were unable to compare two fragments. Each game allowed the player to answer at least 11 questions and the player was awarded one point per correct answer. After the first 10 questions, the game switched into a sudden death mode, allowing the player to continue until a false answer was given. The length of fragments was randomly varied between 2-4 lines. We ranked pairs of generated and authentic texts in terms of difficulty. Pairs were then collected into bins of increasing difficulty. After 5 questions, the questions presented would be sampled from the next, more difficult bin. Figure 2: Example of a type-A question in the games interface. Modeling authenticity Each fragment for the game was enriched with a set of linguistic features that were deemed to be relevant in modeling the difference between authentic and synthetic Hip-Hop. These included morphological, lexical and syntactic characteristics, which we refer to as cues. Most of these can be argued to capture some aspect of the linguistic complexity of the fragments. Using these features, we fit a Bayesian logistic regression modelwith participants as random effect against the following two response variableswhether the text is authentic or generatedandthe actual participant authenticity judgment. Additionally, we also study and control for authenticity perception biases, learning effects and linguistic cues learnt to be exploited by participants to solve the game. Results We restrict this discussion to our most salient results and refer the reader to the presentation for further details. On average, the authenticity detection accuracy was above chance level, with participants correctly answering 60.5% of the time. With 58% median accuracy, participants performed slightly worse on questions of type-B than of type-A, suggesting that the task becomes harder in the absence of a reference point. In addition, we observed marked differences in the authenticity detection accuracy for the three aforementioned Language Models. Hierarchically and word-level generated fragments were markedly harder to detect than character-level fragments. As shown in Figure 3a, there is considerable evidence of a learning effect in both question types, especially towards the beginning of the game. Importantly, the learning effect must be explained differently depending on question type. By design, any learning effect in type-A questions can only involve accuracy of the original fragment. For type-B questions, however, the learning effects seems to reflect a shift in bias towards generated to bias towards original, as can be seen in Figure 3b. Figure 3: Marginal effects with 95% credible intervals of trial number for both type-A and type-B questions. Marginal effects plots showing the effect of trial number on guess accuracy for Authentic and Generated in type-B questions. To estimate the objective feature importance, we perform a logistic regression analysis, with as dependent variable whether a text was original or generated, and as predictors the linguistic cues; we performed the same procedure to model the perceived authenticity of text fragments. The odds ratios show interestingsimilarities in feature importance between the objective and the perceived authenticity. The average depth of syntax trees, for instance, suggests that generated text fragments have considerably less complex sentence constructions and this was clearly picked up by participants as well. Interestingly, Figure 4b shows that humans easily overestimated the positive weight of some feature types—e.g. the portion of politically incorrect words—, indicating that humans underestimated a machines ability to produce foul language. These observations point to specific aspects for future text generation research to improve on. Figure 4: Log-Odds ratios for objectiveand subjetivefeature importance. "
	},
	{
		"id": 246,
		"title": "Book Formats and Reading Habits in Early Modern Europe",
		"authors": [
			"Marjanen, Jani",
			"Roivainen, Hege"
		],
		"body": " Introduction: Reading and the materiality of the book The late eighteenth century entailed a rapid change in reading and writing books. Rolf Engelsingfamously suggested a reading revolution took place in the second half of the century in which the public gradually switched from reading a few key works, such as the Bible, intensively and repeatedly, to reading extensive amounts of literature by different authors. The breakthrough of extensive reading and the ensuing exposure to a broader spectrum of ideas is linked to the political mobilization of people, but assessing these links requires better knowledge of when and where reading habits changed. Information about changes in reading habits is available only through individual statements about changes in reading. To trace changing practices of reading, we have focused on one aspect that facilitated a transformation in reading, that is, the changes in the material outlook of books produced in the early modern period. Earlier research has suggested that smaller book formats, in particular the octavo format, became more popular in the eighteenth century and thus coincide with the changes in reading habits. Smaller formats cannot be assumed to be a direct result of changes in reading habits or vice versa, but the size of books does make a difference in how people read. Smaller books could be easily transported, carried in a pocket to places where individuals could read in solitude. Larger books were more appropriate for reading out loud to an audience by a desk or from a piedestal. To properly assess the change in the material dimensions of books and other print, we turned to bibliographies as a large-scale data source for early modern publishing. Materials and methods Our statistical analyses are based on four large bibliographies, which allows for more reliable and more detailed studies on the change in book formats than what has been possible before. While bibliographies have been compiled to provide as good coverage of the publication record as possible and thus provide an undervalued data source for the exploration of print culture, they have so far been used only to a limited degree. Bibliographic information, such as authors, titles, publishers, languages, publication places, publication years, and book formats, have been used to conduct analyses. To deal with gaps, inconsistencies, bias and ambiguities in the data, we have extensively harmonized selected metadata fields of the Finnish and Swedish National Bibliographies, the English Short-Title Catalogue, and the Heritage of the Printed Book databasewhich is a compilation of 45 smaller, mostly national, bibliographies. Data handling was mainly carried out in R and Python using dozens of data science packages. Altogether, these bibliographies cover 2.64 million harmonized entries from the investigated period. In terms of coverage, the ESTC, the SNB and the FNB provide the best possible dataset covering printed material for the early modern period in Britain, Sweden and Finland. The HPBD has a larger geographical scope as it covers most of Europe, but its level of coverage varies depending on the source bibliography. Results and discussion A statistical analysis of changes in book formats, their shares in titles and in paper consumption, shows clearly how the octavo format became more popular in Europe toward the end of the eighteenth century, but also indicates that the development was uneven in the sense that the timing and speed of the development varied according to location. The Swedish caseshows a clear rise in the production of octavo books in the second half of the eighteenth century and a decline of the larger quarto format. In the British case, a similar trend is slightly earlier, but there is also an increase in the production of the smaller duodecimo format, indicating an overall shift towards smaller books. In the Finnish case the trend is only visible in the nineteenth century, indicating a slower development in the European periphery. Despite unevenness in the data, the HPBD shows a similar trend for the whole of Europe, but zooming in on individual cities shows remarkable regional differences – for example, most German cities show a growing trend, whereas Spanish cities have a much stronger presence of larger book formats during the century. Overall, capital cities, commercial centres and university towns tend to have slightly different profiles. "
	},
	{
		"id": 247,
		"title": "Humanities Centered Design Features: Emergent Serendipity with HuViz",
		"authors": [
			"Martin, Kim",
			"Miya, Chelsea",
			"Brown, Susan",
			"Murphy, Shawn"
		],
		"body": " Introduction The Digital Humanitiescommunity has made several attempts to foster serendipitous experiences through digital visualization tools. Some toolsand StackLife) visualize bookshelves, hoping to produce the aha moments that frequently occur in libraries and archives. Visualization tools developed specifically for exploring linked data,,, demonstrate the possibilities for serendipity that arise when information is semantically connected. Recently, the research team behind FERASAT, an interface for exploring linked datasets, identified design elements that enhance the potential for making unexpected connections. These directed attempts at serendipity design reflect the importance of the accidental acquisition of information to humanities scholars. Sometimes, however, the aspects of visualization that lend themselves to serendipity are themselves discovered accidentally. This paper describes HuViz, the Humanities Visualizer, a digital tool for visualizing semantic relationships and ontologies represented using the Resource Description Framework. Though not originally conceived as an environment to foster serendipitous experiences, user studies conducted over the past two years indicate that it does just that. Moreover, unlike FERASAT, aimed at STEM and the social sciences researchers, HuViz was built with humanistic inquiry in mind. We here demonstrate how HuViz aligns with features identified in library and information scienceliterature as fostering serendipity. We conclude by introducing pending features designed to enhance its serendipitous potential. HuViz: Background and use Figure 1. The HuViz Shelf OrlandoVision, the precursor to HuViz, was prototyped in 2010 as a tool to display extracts from The Orlando Projects textbaseas a series of interconnected nodes in a graph, allowing for a new method of exploring this large dataset. Initial experiments confirmed that such tools open up new hermeneutical pathways. The next generation of the tool, known as HuViz, grew out of the recognition of its potential for much wider applications beyond Orlando. In its current state, HuVizs browser-based, interactive interface allows for the exploration of semantic relationships and ontologies represented in RDF. The shift positions HuViz to be compatible with Linked Open Datafrom across the Web. HuViz users begin by selecting an RDF datasetand related ontology. The initial visualization is a circular shelf containing all the nodes found in the dataset. There are two ways to interact with the data in HuViz. The first method, for users unfamiliar with ontologies, is to simply drag a node into the center of the shelf. Once this original node is released, any nodes that are connected to it by one or more predicates will follow it into the graph, resulting in a small network of interconnected nodes. Additional nodes can be dragged into the centre of the graph or stage to explore more connections, and unwanted nodes can be dragged back to the shelf. The Command Panel at the right of the shelf offers the second set of affordances. This Panel allows users to select groups of nodes by class, edge/predicate, or set and perform operations on groups. Some familiarity with ontologies is desirable for using this second option, though either method of interaction lends itself to information exploration. Figure 2. Network appears after one node is dragged into center. User-testing User testing defamiliarizes a tool for tool makers, alerting the team to aspects of design and functionality that are only visible to an outsider, making this an important step in its development. Testing brought our attention to the value researchers place on serendipity as well as to the ways in which HuViz facilitates unplanned discoveries. We conducted 23 user tests with graduate students, senior scholars, library professionals, and members of CWRC projects. The tests took the form of a combined survey-tutorial, beginning with the simple task of building a network using the drag-and-drop technique, gradually working up to more difficult, directed queries. Participants explored datasets generated from Orlandos textbase of women writers, including a dataset centred on Ada Byron and another of famous cookbook authors. Participants could generate networks based on the node type, discovering, for instance, various jobs held by Jewish novelists. Finally, by clicking on links between nodes, users could see the texts from which the data was generated, learning for example about the lesser-known connection between Ada Byron and Charles Dickens. In both the survey responses and the post-test interviews, we found that users valued the ability to contextualize relationships, to draw upon their own expertise to customize the dataset, and to follow their own exploratory pathways through the data. This feedback helped to inform the next stage of the tools development. Figure 3. The HuViz Command Box Serendipitous elements of design Thudt et al.highlight several design features that they believe encourage serendipity, drawing on several LIS articles. These include: Multiple visual access points; Highlighting adjacencies; Flexible visual pathways; Enticing curiosity; and Playful exploration. Since 2012 a number of studies in LIS and DH have highlighted features of digital tools that encourage serendipity. Ridge et alused the concept of play to guide their development of Serendip-o-matic. Khalili et al, in describing FERASAT, created a table of 12 design features related to serendipity. Two features relevant to our work are tools that support background knowledge and user contextualization and supporting convergent and divergent information behaviour. As HuViz was not designed with serendipity as a primary goal, the research team did not have a similar list of features that support serendipity as objectives during development. However, user tests indicate that several design features actively support serendipitous information encountering as described in the literature. These design features are briefly described below: Playful interaction and enticing curiosity HuViz is unusual in providing the ability to switch between two modes of inquiry, each of which lend themselves to serendipitous discovery in different ways and are also connected to the affordance of having multiple visual access points. The hands-on mode encourages spontaneous, intuitive interaction, which in turn invites users to further explore both the dataset and the capabilities of the tool. Indeed, during testing, participants would frequently deviate from the set tasks and start experimenting on their own. Their tendency to go off script suggests that most found working directly in the graph not only intuitive but enjoyable, moving from simple play to curiosity about other affordances of HuViz. Convergent information seeking The Command Panel interface is less intuitive, requiring additional practice in order to familiarize oneself with the various functions. In this sense, it lends itself to more focused and purposeful, rather than exploratory investigation. At the same time, the ability to manipulate the dataset in more complex and sophisticated ways, which comes with higher levels of expertise, creates new opportunities for unexpected findings. Martin and Quan-Haaseshow how chance discoveries often incorporate an element of intention. Similarly, the HuViz Command Panel lets the user narrow their searches through related material, fostering serendipitous experiences through more directed forms of inquiry. Divergent information seeking HuViz also allows for multiple ways to diverge from the task at hand, allowing users to come across related information that they were not expecting. Simply dragging a node onto the stage also moves in all nodes connected within one degree in as well - immediately opening up users to a series of relationships. Following the edge from one node to another, users can see the relationship between these two nodes, and can continue dragging in or activating more nodes to expand their inquiry. In addition to this, once they find an edge, or relationship between nodes, that catches their interest, they can click on the edge to display a pop-up box to provide contextand a link to the source of the data. Contextualization of information Martin and Quan-Haasefound that historians were reluctant to use digital search strategies because of a lack of contextual information. HuViz provides context by providing snippets of information to users when they click on the edge connecting two nodes, highlighting a portion of the original text from which the relationship was extracted. For further information, users can click through to the Orlando textbaseand expand the context further. Figure 4. Once an edge is clicked, a snippet box appears to show context. Conclusion and future steps Our user testing has had two main results: inspiring us to push forward with HuViz and motivating us to further foster serendipity, leading to the development of several new features, which we will briefly demonstrate. These include optimizing HuViz to work with the Web Annotation data model, expanding the history function with a path option, and responding to SPARQL queries. "
	},
	{
		"id": 248,
		"title": "Ontologies for Linked Data in the Humanities Sponsored by the ADHO LOD SIG",
		"authors": [
			"Brown, Susan",
			"Martin, Kim",
			"Drudge-Willson, Jasmine",
			"Drucker, Johanna",
			"Tennis, Joseph T",
			"Shaw, Ryan"
		],
		"body": " Scholars, researchers, and advocates of linked open data from the humanities and GLAM community are invited to participate in a half-day workshop, Ontologies for Linked Data in the Humanities, to take place prior to the start of Digital Humanities 2019, probably July 7th, in Utrecht, Netherlands. We are soliciting short papersrelated to ontologies. Those accepted will be circulated in an open-access online collection in advance of the workshop.Selected contributors who are attending the workshop will be invited to give short presentationsthat will form the basis for discussion and debate. Please note: If you are unable to attend DH2019 but would still like to contribute a paper to this open source resource, please fill in the form at the link below. This link will remain open after the workshop call is complete to collect further contributions. Potential topics include: Ontology re-use, including the choice of whether to import or cherry-pick terms Evaluations, applications, and implications of upper-level ontologies for humanities scholarship Application and extension of standards such as Web Annotation, IIIF, and CIDOC-CRM to solve challenges of digital representation or to support novel forms of analysis Strategies for combining ontologies Challenges of linking incommensurate vocabularies or ontologies Modules, suites, and other strategies for promoting the uptake of ontologies in large or interconnected domains Principles of ontology organization and architecture Tensions between simplicity and complexity or nuance Revision, management, versioning, and usage of dynamic ontologies Impacts of particular ontologies on discoverability, visualization, or analysis Reviews or overviews of generic tools for managing, versioning, or publishing ontologies Paper submissions are due May 13th , after which the conference program committee will review them and notify potential participants, with a schedule of presentations, no later than June 7th. If you plan to participate in the workshop either as attendee or presenter, you will need to register for it through the DH2019 conference. The 2017 workshop resulted in this collection: Advancing Linked Open Data in the Humanities . We intend to publish the 2019 collection of short papers in a similar format in advance of the workshop. Short papers should be submitted using this form , which asks you to provide a link to your paper and confirm your willingness to make your submission part of an openly available online resource with a CC-BY-NC licence. Looking forward to hearing about the exciting work everyone is doing with Linked Open Data! "
	},
	{
		"id": 249,
		"title": "Lessons Learned in a Large-Scale Project to Digitize and Computationally Analyze Musical Scores",
		"authors": [
			"McKay, Cory",
			"Cumming, Julie E.",
			"Fujinaga, Ichiro"
		],
		"body": " SIMSSAis an ambitious project that aims to unite, under a single framework, the ability to: Transform images of musical scores into searchable digital symbolic representations using OMRComputationally extract meaningful statistical informationfrom symbolic music files Use machine learning and statistical analysis to conduct musicological research using this data Create a framework for searching symbolic scores based on both metadata and musical content Make the resulting information and tools easily accessible to other researchers Much has been accomplished since SIMSSA was first presented at DH, but we have also made missteps along the way. Both our successes and failures have provided insights applicable not only to the specialized fields of MIRand digital musicology, but also to the digital humanities in general. This paper is intended to share our experience with the DH community. The proper construction of datasets is one area of central importance. Too often, researchers simply combine digitized data as is, from whatever sources are available, or digitize data themselves without first developing a carefully considered workflow. This can lead to erroneous conclusions, as false patterns may be observed based on inconsistent dataset construction practices rather than on the underlying information itself. Alternatively, meaningful patterns may be obscured by datasets that fail to capture essential information. We encountered such problems when we carried out research on regional differences between Iberian and Franco-Flemish Renaissance music: individual transcribers had encoded note durations differently, so rhythm was correlated more with the transcriber than with the underlying music. Problems can also be introduced during the encoding process, as we observed when commercial score editing software confused the encoding of slurs and ties. We therefore developed a set of best practices to help avoid bias when constructing datasets from historical documents. Selection of data is also essential. Results can be compromised if a dataset does not represent the full range of relevant instancesor contains uneven class distributions. For example, we observed in machine learning-based research on composer attributionthat, if we did not carefully prepare our data, trained models would sometimes perform classifications based on genre rather than compositional style, since the number of masses and motets were not evenly distributed across composers. Judicious application of machine learning is another central area of interest. Most current research emphasizes deep learning, where models are trained on huge datasets, with relatively generic pre-processing. This contrasts with more traditional approaches where training is performed on hand-crafted statistical features that quantify specific qualities of domain interest, or where sub-systems sequentially process data in stages following a pre-defined workflow. Deep learning and more traditional alternatives each have different strengths and weaknesses, which one must understand before choosing which to utilize. The current emphasis on deep learning is understandable, given its widely documented success in many domains. We found, for example, that our OMR performance improved substantially when we switched from a traditional architecture to a deep learning framework that directly processes pixel windows. Deep learning also has important limitations, however. Its need for huge training sets can be a serious limitation when dealing with historical data with limited extant instances, even when clever data augmentation techniques are used. This problem became apparent in our research on automatically harmonically analyzing chorales, for example. Another important limitation is that deep learning, despite recent advances in model transparency, still often results in black-box classifiers. In contrast, feature-based systemsproduce searchable data and directly accessible insights into how features differentiate classes in ways that are meaningful to domain experts. This can be even more important to DH research than class label outputs themselves. As an illustration of the potential advantages of a feature-based approach, we used 1497 features extracted by our jSymbolic softwarenot only to train models that could correctly attribute the music of Renaissance composersand identify Renaissance genreswith high accuracy, but also to gain novel musicological insights into which musical characteristics statistically differentiate these classes. We also empirically tested expert predications about musical style in these studies, 63% of which were found to be inaccurate. There is a particular need for such testing in the humanities, as there are many generally accepted assertions that have never been validated empirically. We also used the jSymbolic features to provide content-based supportfor composer attribution confidence levels proposed by Rodin and Sappbased only on historical evidence. This is a nice example of how computational and traditional humanities research can complement one another. It is also essential to consider issues associated with making research data, software and results available, useable and attractive to other researchers in the humanities, including those not yet accustomed to computational approaches. As noted by Wiering, one must consult domain experts about what they need, rather than imposing decisions on them. Other priorities include: clean and consistent interfaces; clear and extensive documentation, including tutorials; adoption of open accepted standards; compatibility with diverse data formats; facilitating extensibility for other researchers; and careful consideration of data and software in the context of intellectual property laws. The better DH researchers become at facilitating the sharing of data and software, the better we will be able to directly compare techniques and results across research groups. This will in turn permit experimental repeatability and validation, as well as encourage iterative refinements across groups. Researchers will be better able to explore data in new ways and subject long-standing assumptions to empirical validation, arguably the two greatest benefits computational approaches bring to the humanities. We believe such steps will further expand the already excellent research underway in the digital humanities. "
	},
	{
		"id": 250,
		"title": "DH And The Evolving Monograph",
		"authors": [
			"McKee, Sarah E."
		],
		"body": " In 2014 the Andrew W. Mellon Foundation launched a new initiative to fund explorations of long-form digital publishing in the humanities. That same year a small group of Emory University faculty began a conversation, with a planning grant from Mellon, about the state of humanistic scholarship, and in particular how the monograph might evolve or adapt alongside the digital humanities. Their findingsserve as the foundation for the Digital Publishing in the Humanitiesinitiative, a five-year experimentto support faculty authors engaged in digital monograph publishing. The DPH initiative shares its charge with a cohort of sister Mellon projects, including the Digital Publications Initiative at Brown University, Greenhouse Studios at the University of Connecticut, and Publishing Without Walls at the University of Illinois. Each project takes a unique approach in its offerings to faculty authors. At Emory, DPH provides support to individual faculty working on long-form humanistic scholarship that seems well suited to the digital environment, and it does so in real tenure/promotion clock time. While Emorys T&P guidelines include a memorandum that recognizes the significance of digital scholarship in the humanities, and affirms the importance of assessing this scholarship fairly and carefully in decisions of faculty tenure and promotion, many faculty and administrators continue to rely upon printed monographs as the safest route to advancement. The DPH initiative, then, consists not only in supporting individual publications but also in fostering conversations that promote wider acceptance of digital scholarship among humanities faculty. We also invite academic press editors to join these discussions, in hopes that the DH and publishing communities might collaborate to produce born-digital works of robust scholarship that stand alongside printed monographs in both quality and import. The DPH initiative is based at Emorys Bill and Carol Fox Center for Humanistic Inquiry, an interdisciplinary space that has encouraged thoughtful and rigorous intellectual work for nearly two decades. During those years, as researchers developed the tools, theories, and methodologies that comprise todays digital humanities, the open access movement forged digital pathways for the wider dissemination of scholarship. These developments, amid persistent critiques that the digital humanities lack true substance, give rise to new and urgent questions about the future of monograph publishing. In partnership with the Center for Faculty Development and Excellence, Emory Center for Digital Scholarship, and Scholarly Communications Office of Emory Libraries, DPH extends the Fox Centers mission into the conversation about evolving digital monographs and offers three pathways to their development. The first pathway offers a subsidy, directly to a university press, that supports the open access publication of a traditional monograph. The open access version replicates the content and form of its print edition and is typically disseminated as a PDF or an EPUB file. But as authors are increasingly eager to include digital enhancements, including audio and video clips, interactive maps, or data visualizations, alongside their written arguments, a second pathway offers support for an enhanced open access book, which integrates text with digital components. Here again, a print edition is typically available as well, and the essential form and structure of the monograph remains unchanged. The third pathway, toward an interactive open access monograph, raises fundamental questions of what humanities scholarship might look like in the digital environment. For authors committed to pushing the boundaries of the printed book, creating born-digital and interactive monographs not only allows them to showcase digital artifacts but also provides the opportunity to build an argument that doesnt shy away from complexity or ambiguity. Such works seek to embrace an enactment of digital rhetoric, perhaps inviting multiple conclusions and active participation from readers. But how far can such a work deviate in form from the traditional book and still be recognized as a substantial contribution to its field? How is it reviewed? Distributed? Preserved? How might digital scholarship and/or humanities centers collaborate with publishers to share resources and expertise? Answers to these questions are complicated and dependent on multiple contingencies. In our first two years of the DPH initiative, we find that the composition process itself, including research, for a digital publication must be relearned, or seen with fresh eyes. In some cases, DH methodologies alter the research and even the research question in innovative ways that demand new publication strategies. In others, the numerous options for assembling digital material into a compelling argument might overwhelm an author. Moreover, the printed monograph as a genre is not static, nor is it entirely consistent across disciplines. Digital variants of this traditional form, then, will also resist monolithic expression. We have found that one promising starting point is to guide authors through the essential attributes of a monograph using genre analysis. Other tactics include introducing authors to best practices in data managementand multimodal composition, and hosting workshops that guide project teams through visualization exercises or allow them to share works in progress with other researchers. Another critical aspect of our approach is the commitment to inviting collaboration with publishers early in a digital monographs development. While traditional publishers are increasingly willing to explore open access, including best practices for distribution and preservation, most have yet to test the waters of digital/multimodal composition beyond an enhanced e-book. Editorial and production staff at most publishing houses are not often trained in DH tools and methodologies, and so we offer expertise of our team at the Emory Center for Digital Scholarship to help offset that lack. On the other hand, DH practitioners of all stripes might benefit from the editorial eye and discipline that publishing professionals bring to the table, as well as their expertise in marketing and distribution to ensure that the work is discoverable and cited. My presentation will elaborate on these guiding principles behind the DPH initiative and offer case studies of digital monographs in development by Emory faculty. "
	},
	{
		"id": 251,
		"title": "Digital Synesthesia or the Point Of the Digital for the Humanities",
		"authors": [
			"Meister, Jan Christoph"
		],
		"body": " In his forward to the first edition of the Companion to Digital Humanities Roberto Busadeclares: Humanities computing is precisely the automation of every possible analysis of human expressionin the widest sense of the word, from music to the theater, from design and painting to phonetics, but whose nucleus remains the discourse of written texts. The revered pioneers claim was at the same time modest, and universal: universal in terms of attesting automation relevance across the entire spectrum of the humanities; modest in terms of reserving its application to analytical procedures. Although Humanities Computing has meanwhile morphed into DH, similar programmatic pragmatism still characterizes current text-book style introductions to the field. DHs internal methodological self-reflection however has long identified an exclusively pragmatic self-definition as reductionist. This is friendly fire though: nuances and methodological arguments within the DH are of little concern to hard core humanists, as Fishdemonstrates. He labels the digital humanities an anti-humanistic project, for the hope of the project is that a machine, unaided by anything but its immense computational powers, can decode texts produced by human beings. The substance of the argument is absurd; the fact that it continues to find an audience among traditional humanists warrants attention though. One cause of irritation is the focus of current DH research on methods such as stylometry, topic modeling, sentiment analysis, NER, SNA, visual pattern analysis, word2vec etc. All of these share an epistemological and, perhaps ontological premisewhich the traditional humanities cannot readily accommodate: in distant reading approaches human culture and symbolic practice are primarily conceptualized as a phenomenon of data patterns evolving over time, not as experiential and meaning-making artefacts. To many digital humanists the empirical focus of Humanities Computing and DH initially provided a welcome antidote to the plethora of changes-in-paradigm, grand visions and anti-realistic ideologies that have emerged since post-modernism became en vogue. Then, from the early 2000s onward, DH began to acknowledge that the mere identification of data patterns equals ἐμπειρίᾱ in the Platonian sense of: experience without knowledge of reasons and causes for the experienced phenomena. And so the empirical paradigm in DH was gradually counter balanced by the hermeneutic again; a process that is ongoing. But the outside perspective onto DH differs distinctly. From the traditionalists point of view DH is at best a Hilfswissenschaft, an ancillary science whose purpose is to support the old humanities disciplines–a problematic labelfor a field of practice which itself has finally begun to engage in discourse on the Critique of Digital Reason. Morettiattributes the difficulty of supplying a plausible raison dêtre to the vagueness of the label itself:  Digital humanities is simply a formula that has come to identify a large field. I use it only because everybody uses it and everybody will use it. But, frankly, I dont like it. I think it means nothing, whereas quantitative and computational mean something. But there is more at stake than terminology: it is the Computationality of Hermeneutics and a hermeneutic ethos which DH has to deliberate and communicate more prominently, as van Zundertpoints out. And it is precisely the topos of a fundamental lack of such ethos which skeptical and polemic interjections questioning the humanistic legitimacy and legacy of DH raise time and again. Take Fish: It is true, as digital humanists claim, that a corpus that has been digitized can then be searched for patterns the naked eye could never discern–frequency patterns, contiguity patterns, collocation patternsThe problem is that once such patterns have been uncovered, there is no legitimate route from them to the interpretation of texts. For Fish the only legitimate route from data to interpretation is the one whose point of departure is a concrete author subject, or as he puts it: Interpretation cant get started without the prior identification of an intentional agent. May this critic saunter down the yellow brick road of intentional fallacy; we will take the philosophical high way: The search for a plausible route, a genuinely humanistic method to connect data with meaning motivated among other Diltheys 1883 distinction between the objective explanation of external natural phenomena in the sciencesand the subjective, re-enacting comprehension of inner phenomena of the mind and soulin the humanities. Dilthey highlights reflexivity as the key characteristic of the workings of the human mind, and as that of humanistic methods for the study of its artefacts and practices. But Diltheyalso points out the impetus to interpret even our Erklären-motivated findings in terms of a Verstehen-motivated reflexivity: the same human will then turn back from nature to life, toward himself. This return of man to the experience by which nature exists for him in the first place, into life within which alone meaning, value and purpose appear, is the other great tendency which determines scientific work.The application of hermeneutics is thus not restricted to works of art and symbolic practices. Moreover, our 21 st century reality includes phenomena which we observe in theform of quantified, discrete data. Yet is DH willing to turn back to life and risk the return from digital data analysis, from the mathematical and statistical modelling of symbolic objects and practices, back into the domain of a self-reflexive practice of Verstehen? Individually: certainly; see e.g. Morettiwho demonstrates how digital operationalization can feed back into a reappraisal even of Hegelian aesthetic theory. Programmatically though: on what grounds? How could a humanistic, self-reflective legitimation of the digital humanities be argued? I propose a philosophical appraisal which situates DH provocatively and in the most unlikely of traditions: Romantic theory of science and aesthetics. For the point–that is: the core epistemological and philosophical added value–of the digital for the humanities stems from three characteristics that seem to resonate with Romantic ideals: The digital representational principle of analytical segmentation of an object into discrete points of observation can represent as captathree types of semiotic phenomena:symbols, i.e. phenomena which we process as having a meaning function,signs, i.e. phenomena which we process as having a referential function, andphenomena interpreted either as random effects, or as self-referential, intra-systemic data patterns. The only other semiotic system capable of this threefold function is: natural language. Unlike natural language the digital is however a lingua franca that comes without ontological, existential, conceptual, cultural or historical bias. Rather, because of its radical abstraction from context it provides a medium and conceptual space for the exploration and bottom-up modelling of dynamic structures, relations and effects within and across the realms of sensory modes, as well as across the cultural, the natural and the mental domains under investigation. This I term the conceptual affordance of digital synesthesia.. The digital is a particularly un-natural language when it comes to representing symbolic meaning in the Fregian sense of: phenomenological-existential relevance. It enforces abstraction from context–but more importantly, eventual explication of context once we begin to realize the deficiency of a digital model. Taking the detour toward meaning via formalization is the quintessential ethical principle of DH hermeneutics. However, in order to progress from the calculated alienation from and suspension of synthetic interpretation via abstraction and formalization to a new level of content-oriented epistemology DH will eventually need to engage in methodological self-reflection: the new mode of modeling and understanding meaning requires us to turn it onto itself. This scientific ideal and vision of a knowledge generation process based on a form as well as a content-oriented methodology, on a synesthetic epistemology and, at the same time, on an overarching self-reflective ethos was central to Friedrich Schlegels late 18 th / early 19 th Century philosophic and aesthetic theory. In this context the synesthetic epitomized the Romantic ideal of a universal, inter-modal and inter-operable poetic and scientific practice, an ideal taken to the extreme in Kleistsself- agrandissement as an individual capable of formula and metaphor. In Schlegels theory such risk of declaring a new intellectual absolutism had, however, already been anticipated and counter-balanced by the equally important postulate of parekbasis, that is: the commitment to constant critical self-reflection, encapsulated in Schlegels dictum Ironie ist Pflicht. I argue that it is precisely these two commitments and necessities whichcharacterize the methodological ethos of the digital humanities; something similar to the tendency toward an aesthetics of synesthetic, yet cognitively paradoxical inter-modality which Scarlettattests contemporary digital media art. DH does not require a poetics though: it is not concerned with producing symbolic artefacts, but with modeling, exploring, analyzing and interpreting them–and it does so by using the digital as a representationalsystem of unrivalled flexibility and applicability. But rather than mere breadth of scope its greatest affordance is that, unlike natural language, this is a system and language not already invested with meaning and value per se–a characteristic which, rather than rendering the digital hermeneutically dysfunctional, turns it into a universally relevant heuristic tool which in the end aids, rather than delimits hermeneutic reflection. And somewhere, over the rainbow, this prospect in combination with an appraisal of DH as an offspring of Romantic theory might catch our harshest critics unaware. For if the humanistic goal of DH is to arrive at meaning in its fullest phenomenological sense, albeit via a different route, then where exactly is the problem? Theres no place like home! "
	},
	{
		"id": 252,
		"title": "A New Journey Through Shared Ethnological Archives For Understanding Anthropology: The “Archives Des Ethnologues”, A Multifaceted Consortium",
		"authors": [
			"Melka, Fabrice",
			"Ginouvès, Véronique",
			"Giudicissi, Hélène"
		],
		"body": " Social anthropologists have produced numerous hitherto in the field that have been sometimes deposited in documentation centre of research facilities. The nine resource centres that make up the consortium Archives des ethnologues and their partners, house multi-media materials collected by French anthropologists who have worked in the field throughout the world and in multiple languages from the 1900s to the present day. Once archived, these notes, field notebooks or various papers, these photographs, films or sound recordings are digitized and some of them are posted online in accordance with ethical and legal guidelines. To combat the misleading way in which digital technology tends to standardize data, the Consortium Archives des ethnologues has chosen to diversify access to this data because the uniqueness of these archives reflects their scientific and heritage value, the wealth and diversity of the societies they attest to, the history of the sciences and the methodologies used in the course of time. To facilitate this access, we have to be able to account for the complexity of the past of these societies via a relational-ethics approach and by providing an overview of the different strata of research. The archive is not only a potentially reusable data for anyone who would like to revisit fieldwork, to compare it with others, or to validate evidence, but also a record of scientists real practices. Beyond the task of archiving and preservation, the aim of the consortium Archives des ethnologues is to help everyone grasp the diversity and complexity of field data, to keep in mind their production process, to facilitate their re-use in order to propose a new journey through the societies studied. We therefore need―and even more so than in the pre-digital age―to work together, to discuss to compare methods, to involve different professions as well as civil society. This poster aims to introduce the reader to the wealth of access to the archives of this discipline through the implementation of good practices in digital data processing. It will emphasize the collective development of tools the consortium uses for description and dissemination through which it published in November 2018 a guideline on ethical and legal issues for data dissemination in the humanities and social sciences. The focus will be on four digital manifestations of the archival collections and the technologies used: the geolocation of data, the creation of a frame of reference for tales and songs of oral tradition and more broadly speaking for ethnology as a discipline, annotations through an open-access sound player and the creation of a collaborative platform for transcribing field notebooks with Internet users. "
	},
	{
		"id": 253,
		"title": "Palatine Anthology. Complexity for a digital research project",
		"authors": [
			"Mellet, Margot Lise",
			"Vitali-Rosati, Marcello",
			"Bouchard, Elsa",
			"Monjour, Servanne",
			"Agostini-Marchese, Enrico",
			"Kearney, Beth"
		],
		"body": " Vitali Rosati, Marcello. 2018. On Editorialization: Structuring Space and Authority in the Digital Age. Institute of Network Cultures. Amsterdam. "
	},
	{
		"id": 254,
		"title": "A Framework to Quantify the Signs of Abandonment in Online Digital Humanities Projects",
		"authors": [
			"Meneses, Luis",
			"Martin, Jonathan",
			"Furuta, Richard",
			"Siemens, Ray"
		],
		"body": " Introduction As researchers in the digital humanities we have been successful in building online components for our work. However, we have failed in making it a priority to devise a plan to gracefully discard our online components once we no longer need them. Thus, many of the online projects in the digital humanities have an implied planned obsolesce —which means that they will degrade over time. Previous work presented in Digital Humanities 2017 and 2018 has explored the abandonment, and the average lifespan, of online projects in the digital humanities using metadata from HTTP response headersand contrasted how things have changed over the course of a year. We believe that managing and characterizing the degradation of online digital humanities projects is a complex problem that demands further analysis because the methods for identifying change in the Web do not fully apply; and the end of life for a digital humanities project may or may not be indicated by updates in its content and tools. In this sense abandonment is not necessarily a sufficient designation —as there are different nuances involved. We have seen many cases of successful projects in digital humanities that are shifting their focus from active development to data management. These are cases where a projects online presence has not received updates for some time but its online tools are stable and continue to be accessed by its users. In this case, the lack of updates and new content is not a signal of abandonment. These are examples of why the rules for traditional resources do not fully apply and new metrics are needed to identify issues concerning online projects in the digital humanities. In this abstract, we go one step further into exploring the collectively shared distinctive signs of abandonment to quantify the planned obsolesce of online digital humanities projects. For this purpose, we have created a framework that collectively quantifies their signs of abandonment. This study aims to answer three questions. First, can we systematically identify the signals of abandoned projects using computational methods? Second, can the degree of abandonment be quantified? And third, what features are more relevant than others when identifying instances of abandonment? Methodology A complete listing of research projects in the Digital Humanities does not exist. However, the Alliance of Digital Humanities Organizations publishes a Book of Abstracts after each Digital Humanities conference as a PDF. Each one of these volumes can be treated as a compendium of the research that is carried out in the field. To create a dataset, we downloaded the Books of Abstracts from 2006 to 2018. Then we proceeded to extract the text from these documents using Apache Tikaand parse the unique URLs for each Web resource using regular expressions. Then we periodically created a set of WARC filesfor each resource using Wget. The WARC files are systematically processed and analyzed using Pythonand Apache Sparkto create a hash that represents their contents —pinpointing changes over time— and to extract the analytics that we used in our statistical analysis. More specifically, our analysis has two parts that incorporate the retrieved HTTP response codes, number of redirects, a detailed examination of the contents and links returned by traversing the base node, external resources, HTTP headers and linked files. Figure 1 shows the workflow that we used in our framework to quantify the sings of abandonment. Figure 1: Workflow to quantify sings of abandonment in Online Digital Humanities Projects First, we carried out a preliminary classification of the websites into two groups depending on their correctness according to their HTTP response codes: validand decayed. If a Web resource reports more than one redirect, we placed it in the decayed category. This is a preliminary classification because a Web resource could return an HTTP response code implying correctness while showing erroneous content—justifying the second part of our analysis where we cluster the contents of each Web resource in the valid category. We perform the clustering using topic modeling and Term frequency–Inverse document frequency. The textual contents and the links associated with shared resources are the most obvious feature for clustering. Previous work has shown that shared resources are the first to disappear from the Web—which we interpret as premature indications of degradation. To detect these early signs, we generated topic and term frequency models to examine the similarity among the documents in a given project. We used Latent Dirichlet Allocationto model the content of the textand a simple Tf-Idf ranking function to measure and compare them. This ranking function is based on adding the Tf-Idf values for the documents linked to a Web resource, which were calculated using the terms from the topic modelling as a vocabulary. This combination metrics and techniques allow us to compare and assess the degree of change of online digital humanities projects over time. Conclusions In this this study we aim to computationally identify the indicators of the abandonment of digital humanities projects —a very specific domain— as well as quantify their degrees of neglect. It is important to highlight that not all projects are equal and thus require different levels of attention. Previous work in this area was based on the metadata from HTTP headers —emphasizing the need for a framework that utilizes robust metrics to identify the collectively shared indicators of degradation. We intend this study to be a step forward towards better preservation mechanisms and for adopting strategies for the planned obsolesce of digital humanities projects. "
	},
	{
		"id": 255,
		"title": "The Oldest Song Score in the Newest Notation: The Hurrian Hymn to Nikkal as Linked Data",
		"authors": [
			"Meroño Peñuela, Albert",
			"van Berchum, Marnix",
			"van den Hout, Bram"
		],
		"body": " Introduction The Web can constitute a natural medium for the publication and discovery of historical evidence, their related resources, and descriptive metadata. Increasingly, but steadily, more and more historical artifacts make their way from archaeological sites to musea, libraries, digital archives, and also the Web of data, besides their traditional proliferation to the wider public through books. The discovery and retrieval of historical digital objects and their descriptive metadata on the open data space of the Web are challenging tasks. For example, What is the oldest music score known? is a relevant question of interest to art historians, music historians, musicologists, and the wider public. Web search can lead users to at least a partial answer to this question; as it turns out, the oldest song known left to present in written form is a Sumerian Hymn written 3,400 years ago. Part of the Hurrian songs, the Hurrian Hymn to Nikkalis the oldest substantially complete work of notated music in the world, inscribed in cuneiform on clay tablets and excavated from the ancient city of Ugaritand dated 1400 BC. The work of a number of historians has allowed for these inscriptions to be transcribed in modern Western notation, ultimately leading to the possibility of being played in a modern lyre, which resembles ancient Sumerian instruments. The provenance trail of this unprecedented discovery of musical culture is complex, and this complexity manifests on the Web in the form of variety and heterogeneity. Necessary pieces of knowledge to reconstruct this breakthrough are scarce, hard to find, and, most importantly, spread through a number of heterogeneous and semantically incompatible information representation formats: HTML hypertext, PDF documents, scanned images, digital score transcriptions, MIDI files, MP3 files, etc. Therefore, the complexity of all human available Web knowledge on h.6 comes in the form of multimodality. In the broader context of digital data access and processing, we must therefore ask: how can these relevant, multimodal pieces of knowledge be queried together for further computation in a consistent and reproducible way? In this paper, and following established practice, we propose to use the Resource Description Framework, the Linked Data paradigm, and the SPARQL query languageto answer fundamental questions in music history about the Hurrian Hymn to Nikkal. In order to do so, we follow a three-step approach in which, first, we convert MIDI representations of the hymn into RDF, effectively transcribing the notation of the oldest known music written score into a modern, well understood, and machine processable knowledge representation language; second, we enrich this RDF representation with all provenance metadata facts we could collect from the Web about the hymn, its discovery, and its interpretation; and, third, we store data retrieving SPARQL queries to provide unique API links that reproduce the retrieval of relevant h.6 data without knowledge of these technologies. We use the results of this methodology to study the relevance and idiosyncrasy of the hymn when we compare its notation and metadata to a large dataset of modern MIDI music of 10 billion RDF facts. All relevant resources resulting from this approach are published online at https://github.com/midi-ld/h.6 Background The Hurrian songs are a collection of music inscribed in cuneiform on clay tablets. These tables were originally excavated in the 1950s from the ancient city of Ugarit, a headland in northern Syria, and date to approximately 1400 BC. The Hurrian hymn to Nikkalis encoded in one of these clay tablets, and it was first transcribed into modern Western notation in 1972. After this, a variety of alternative interpretations have been suggested. The proliferation of various plausible interpretations and theories on the correspondence of h.6s content and modern music notation has led to different modern recordings. Many of these are scarcely available, while some others are openly available on the Web. The use of the lyre is generally accepted as a proxy to recreate the timbre of original Sumerian instruments. Other transcriptions use the popular synthesizer language MIDI. Figure 1. Excerpt of the Hurrian hymn to Nikkal and its transcription into modern Western music notation. The Musical Instrument Digital Interfacestandard allows electronic musical devices to communicate by exchanging messages that can be interpreted as music notation. MIDI encodes so-called events into typically 3-byte messages that describe some event relevant for the production of musical sound. For example, the action of pressing the middle C key in the piano quickly can be expressed with the MIDI message <144, 60, 100>. The midi2rdf algorithmrepresents information originally encoded as MIDI in the open, standard and machine-processable knowledge representation language of the Web: the Resource Description Framework. RDF expresses knowledge as subject-predicate-object sentences. RDF triples use URIs identifiersto indicate these subjects, predicates and objects. For example, the fact that Tim Berners-Lee is a person can be expressed in RDF with the triple < https://www.w3.org/People/Berners-Lee/ > < http://www.w3.org/1999/02/22-rdf-syntax-ns#type > < http://xmlns.com/foaf/0.1/Person >. Therefore, the midi2rdf algorithm can be used to transform any MIDI file into a collection of such RDF triples. This has been done on a large collection of 500K MIDI files gathered from the Web, leading to the creation of the MIDI Linked Data Cloud, the largest dataset of machine-readable music notation, containing more than 10B RDF triples about MIDI songs and all their events and notes. More recent work proposes methods that leverage musicians performances, MIDI similarity algorithms, and entity recognition techniques to establish links between music notation contentand their descriptive Web metadata. Approach and Findings Here, we propose to follow a three-step method in order to represent historical symbolic music notation content and descriptive metadata of the MIDIs of the Hurrian Hymn to Nikkal as Linked Data through RDF, and incorporate those into the MIDI Linked Data Cloud: Symbolic music notation content. As a first step, we encode the MIDI files of the Hurrian Hymn to Nikkal in RDF, using the midi2rdf algorithm. This produces various graphs of RDF data encoding the different musical events of the digital notation. Descriptive metadata. In this step, we add additional contextual provenance and metadata triples to the RDF graphs produced in the previous step. These are harder to integrate in the historical case than in modern music, since generic approaches linking only large datasets is ineffective in this case. To address this, we investigate the resources and workflows around the creation of modern music notations of the h.6, and compile all these using newly minted URIs, shared vocabularies, and reusable modeling practices. Reproducible queries. Since the output of the previous steps is RDF data that can only be queried through theSPARQL query language, we compile a list of SPARQL queries encoding relevant competency questions over these new RDF graphs, interrogating them using combinations of musical knowledgeand contextual provenance. By doing so next to automatic API generation tools, we generate stable and reproducible links to execute these queries, retrieving predictable results without the need of knowledge in RDF or SPARQL. Our findings reveal both shortcomings and advantages to this approach. The first limitation is the manual, unprincipled, and scarcely understood nature of collecting Web-based relevant provenance and contextual resources, links and workflows over these music-historical objects. Secondly, this process is prone to be biased towards the selection of resources that are only available on the Web, ignoring those published elsewhere; this aims at the general problem of reachability of offline archives and repositories. These shortcomings are balanced by a number of advantages. First, the end result supports a thorough documentation process that generates a semantic RDF graph of historically relevant and connected Web resources, posing the added value of machine-readability over more traditional text-based documentations. Second, the result supports a more principled retrieval mechanism, based on mixing musicaland historicalknowledge under the same querying paradigm. Third, the availability of both MIDI and their history enables a more exploratory approach, in which besides providing query results, the serendipitous discovery of information is incentivated through similarity links. Ultimately, we hope to enable comparative studies through graph metrics pointing differences between historical and contemporary music. "
	},
	{
		"id": 256,
		"title": "From Fluency To Disfluency: Ranking Prosodic Features Of Poetry By Using Neural Networks",
		"authors": [
			"Meyer-Sickendiek, Burkhard",
			"Hussein, Hussein",
			"Baumann, Timo"
		],
		"body": " This work offers a method for detecting the degree of fluency and disfluency used by a poet when reading his/her poems. Such a determination offluency plays a particularly important role in the evaluation of poetry translations. Lawrence Venuti showed that the notion of fluency became a dominating principle by which to judge English translations: a translation reads fluently, when it gives the appearance that it is not translated [1, p. 4]. This was meant critically, because such translations often transform an elliptic and fragmentary style within a source poem into a tangible, concrete and fluent target language. Facing current machine-translation systems, there can be no doubt that Venutis critique is more on topic than ever: The fluency of the text in the target language became todays predominant translational ideal, due to so-called speech disfluency removal systems used in conversational speech translation [2]. To judge a good or bad translation thus means to estimate the degree of fluency within the original poem and its translation. Following Venutis critical approach, our paper will offer a new technique to estimate this degree offluency with regards to poetry. In a first step, we will offer a precise framework to use it for estimating a spectrum offluency by using two important theories for analyzing poetry: The grammetrical ranking and the rhythmic phrasing. The idea of grammetrical ranking was developed by Donald Wesling, whose neologism grammetrics is a hybridization of grammar and metrics, based on the key hypothesis that in poetry as a kind of versified language, the grammatical unitsand the metrical unitsinteract in a way for which Wesling finds scissoring an apt metaphor. The grammetrical raking assumes that meter and grammar can be scissored across each other [3, p. 67]. The second important approach to detectfluencies in poems is Richard Curetons theory on rhythmic phrasing. Cureton divided the poetic rhythm into three components: meter, grouping and prolongation [4, p. 125]. Meter is about the perception of beats in regular patterns, grouping refers to the linguistic units gathered around a single climax or peak of prominence, quite similar to Weslings ranking. Curetons new idea, basically, is that of prolongation which refers to the anticipation and overshooting of a goal, the experience of anticipation and arrival. Rhythmic prolongation is a matter of connected, goal-oriented motion, based on three levels: anticipation, arrival, and extension[4, p. 146]. For example: an extension occurs in the prosodic phrasing of an enjambment, where the line break is felt as a linear extension of the sentence before the end of the sentence is reached in the next line. Using this theoretical framework, we will establish a gradual one-dimensional continuum, whose two poles are denoted by the terms fluent and dis-fluent. We illustrate this prosodic spectrum by ranking nine different poetic styles within the free verse spectrum, starting with the most fluent one, the. The basic idea of the cadence is the breath-controlled line as an isochronous principle. Ezra Pound, who invented this idea of the cadence, was influenced by Chinese poetry, which lacks any enjambments. This explains the so-called line-sentence as the fundamental principle of the cadence. In difference to this class, more dis-fluent poems use weak enjambments separating the nominal phrase and the verbal phrase of a sentence. Such weak enjambments can be divided furthermore into those not emphasizing the enjambments, and those which do emphasize them. These two classes are also rather fluent ones, compared to those poems using strong enjambments. A strong enjambment separates articles or adjectives from their nouns or even splits a word across a line, like in Paul Celans poems. Poems using strong enjambments can also be divided into those not emphasizing the enjambments, and those emphasizing them. Moving forward towards to the more dis-fluent pole, the next pattern is the. A permutation is a conversion or exchange of words or parts of sentences or a progressive combination and rearrangement of linguistic-semantic elements of a poem, a principle that was very popular in German concrete poetry. The next pattern is the, the omission of one or more grammatically necessary phrases. This rhetorical figure can also affect the prosody of a poem, which has been observed for example in poems of Paul Celan. Even more radical kinds of poetic disfluency have been developed in modern sound poetry by dadaistic poets like Hugo Ball and Schwitters or concrete poets like Ernst Jandl. Within the genre of sound poetry, there are two main patterns: the, dividing the words into syllables; and the, the last and most disfluent pattern, which can be found for example in Ernst Jandls famous poem schtzngrmm. Using this spectrum, we can very accurately mark whether a translation is more fluent than the source text. Therefor we collected German poems available on the website of our partner. The philologist and literary scholar of the projectclassified 268 of a total of ∼ 2,400 German poems into the nine prosodic classes defined above. We also collected the corresponding audio recording of each poem as spoken by the original author, yielding a total of 52 hours of audio for all German poems. We perform forced-alignment of text and speech for the poems using the text-speech aligner published by [5] which uses a variation of the SailAlign algorithm [6] implemented via Sphinx-4 [7]. This process in spoken poetry is non-trivial. Therefore, the alignment data are corrected on the line levelas well as checked and corrected again by an expert. We present a model for the automatic classification of rhythmical patterns in the free verse poetry by using deep hierarchical attention networks. We do not use the processing on word level. Instead we used character-by-character encoding of lines in the poem and used character embeddings, sine we have a small amount of data. While processing on the word level might allow our model to build a better higher-level understanding of the poems meaning, this semantic information would likely not help in style differentiation. In addition, word representations would not capture the usage of whitespace, for example, in indentation to create justified paragraphs or other uses, nor special characters. We use a bidirectional recurrent neural networkcells) which encodes the sequence of characters into a multi-dimensional representation. As for the text, we use speech line-by-line via additional encoders. We extract Mel-frequency cepstral coefficientsfor every 10 milliseconds of the audio signal as well as fundamental frequency variationvectors, which are a continuous representation of the speakers pitch. We z-normalize all feature dimensions. We compute the mean and standard deviation of 10 consecutive frames for every feature. To satisfy the requirement of inspectability of the decision making process, we implement a notion of inner attention that is to learn how to combine the sequential states of each line encodingsto a representation that is best suited towards our training objective. We combine the line-by-line representations using a poem-level encoder which is fed to a decision layer and a final softmax to determine the poems class. Our model is implemented in dyNet and python. Since there are a broad variety and relatively a small number of poems. We implement the pre-training with additional data from German Text Archive [8]. We used the pre-trained models in the training procedure. We first leave out the poem-level encoding and directly pass each line representation to a line-by-line decision layer. Afterwards, we replace the line-by-line decision layer with the poem-level encoder and final decision layer and train towards the per-poem decisions based on the parameters estimated before. Thus, the final model is able to steer its attention mechanism towards the important lines and can learn to sacrifice the initially trained per-line optimization for the overall per-poem optimization. Each encoder is two layers deep and has a 20-dimensional state. We train a classifier to distinguish the nine classes of poetic styles with all featuresusing pre-processing and pre-training; given the little available data, we use 10-fold cross-validation. The best result, calculated by the average F-measure, for the classification of the nine rhythmical patterns is 0.62. This indicated that it is indeed possible to check Venutis critique of fluid translations automatically by distinguishing prosodic classes based on text, speech, and pauses using a deep neural model. "
	},
	{
		"id": 257,
		"title": "A Web-Based Tool for the Annotation of Scribe Data in Medieval Documents",
		"authors": [
			"Hanusch, Martin",
			"Birringer, Marc",
			"Milde, Jan-Torsten"
		],
		"body": " Einleitung Innerhalb des letzten Jahres wurde das System Signum zur interaktiven, webbasierten Annotation von mittelalterlichen Handschriften der Bibliotheka Fuldensis entwickelt. Die zentrale Zielsetzung ist die Erfassung relevanter Eigenschaften einzelner Buchstaben, bzw. von Buchstabenkomplexen innerhalb eines Dokuments. Die so erfassten Eigenschaften werden als gewichtete Feature-Vektoren betrachtet und sind der Input in einen Klassifikationsalgorithmus. Hier wird eine Zuordnung vom Feature-Vektor zu einem möglichen Schreiber berechnet, welche dann, so der theoretische Ansatz, dokumentübergreifend eine Schreiberidentifikation ermöglicht. Durch die schrittweise Weiterentwicklung der Webtechnologie ist es jetzt möglich, den Annotationeditor vollständig als leistungsfähige Webanwendung umzusetzten, und zwar ohne Qualitätsverlust in der Interaktion und Usability. Das System baut auf langjährigen Vorarbeiten auf,,,,). Gleichzeitig ist das System Signum zentraler Bestandteil der Abschlussarbeit von Author1und wurde im November 2018 aufgrund des besonderen Beitrages zur Umsetzung wissenschaftlicher Forschungsergebnisse mit einem landesweiten Förderpreis ausgezeichnet. Biblotheka Fuldensis Das Institut Bibliotheca Fuldensis widmet sich der Erforschung der alten und bedeutenden mittelalterlichen Bibliothek des Klosters Fulda, die im dreißigjährigen Krieg verschleppt, zerstreut und zu großen Teilen zerstört wurde. Seit Beginn der 80er Jahre des 20. Jahrhunderts wurde dafür eine umfangreiche Dokumentation zu den erhaltenen Handschriften und Handschriftenfragmenten angelegt, welchei nzwischen auch als Digitalisate vorliegen.Abbildung 1: Die Vorstudie erfasst relevante Beispieldaten und definiert ein erstes Klassifikationsmodell Vorstudie zur Schreiberidentifikation In einer konzeptionellen Vorstudie wurden zunächst die relevanten paläographischen Eigenschaften von besonders distinktiven Buchstaben identifiziert. Hierbei handelt es sich um ein kleinschrittige Elizitierung von Expertenwissen. Dieser Prozess ist sehr aufwändig, weil eine erheblicher Anteil des Wissens als Erfahrungswissen vorliegt und in vielfältiger Weise von weiteren Wissensebenen überlagert ist. Das Ergebnis der Vorstudie war ein komplexes Excel-Sheet mit dem es möglich war, Feature-Vektoren für 5 Schreiber zu definieren und diese bei 20 Testdokumenten mit einer nicht zufälligen Wahrscheinlichkeit zu identifizieren. Basierend auf den erfassten Daten der Testdokumente können Gesamtwahrscheinlichkeiten berechnet werden. Überschreiten diese einen vordefinierten Schwellwert, wird eine Korrelationzwischen Dokument und Schreiberhand postuliert. Allerdings sind die Schreiber in der Regel anonym. Eine eindeutige persönliche Identifikation ist also in den meisten Fällen nicht möglich. Konzeption des webbasierten Systems Ausgehend von der Annahme, dass weltweit nur eine geringe Anzahl von Paläographen, bzw. paläographisch interessierten Laien zur Verfügung stehen, wurde das zu entwickelnde System als webbasierte Anwendung konzipiert. Der Annotationsprozess soll stark geleitet werden. Der Anwender durchläuft einen immer gleichen strukturierten Ablauf: Anmeldeprozess Dokumentauswahl Geleiteter Auszeichnungsprozess Speicherung Auswertung der Annotation Benutzerbezogenes Feedback Das System muss also zwei wesentliche Funktionen erfüllen: Einfache Bedienbarkeit Gute Unterstützung des Annotierenden in Form von kontextsenesitiver Hilfestellung. Die Anwendung soll daher als community process verstanden werden, der leicht verständlich und einfach zu bedienen ist. Abbildung 2: Der Annotationsprozess ist vergleichsweise einfach: relevante Eigenschaften im Digitalisat werden durch eine Rechtecktauswahl markiert und die erkannten Eigenschaften im Dialogfeld ausgewählt. Dokumentauswahl Die Dokumentenauswahl erfolgt über eine Auswahlliste. Hier sind die bearbeitbaren Dokumente aufgeführt. Aktuell ist es notwendig, die Dokumente als Kopie auf dem Systemserver zu speichern. Als Format sind derzeit Bilddateien in allen vom Browser anzeigbaren Formaten zulässig. Technisch ist es problemlos möglich die Bilddaten auch von Fremdservern zu laden. Allerdings müssen in diesem Fall mögliche rechtliche Regelungen beachtet werden. Anmeldeprozess Der Anmeldeprozess soll niederschwellig, aber sicher sein. Die Anwender melden sich mit ihrer email Adresse an und wählen einen Benutzernamen und ein Passwort. Hilfesystem Eine wichtige Komponente der Anwendung ist das umgesetzte Hilfesystem. Um dem interessierten Laien Unterstützung zu bieten, wurden eine Reihe von erläuternden Hilfeseiten entwickelt. Die Erläuterungen basieren auf vergleichenden Beispielen und zeigen die wesentlichen zu identifizierenden Eigenschaften der unterschiedlichen Buchstaben. Zusätzlich wurde mehrsprachige Einführungsvideos produziert, die alle relevanten Teilschritte erläutern. Speicherung Alle Annotation werden in einer relationen Datenbank abgelegt. Hier liegen eine Reihe von Tabellen vor, die eine eindeutige Verknüpfung zwischen Anwender, Dokument, Annotation und Zeit erlauben. Damit ist sichergestellt, dass die Annotation versioniert rekonstruiert werden kann. Außerdem erlaubt das System eine einfache Anonymisierung der Daten. Schließlich können die Daten problemlos als XML-annotierter Datensatz exportiert werden und das System stellt eine REST Schnittstelle zur externen Nutzung der Daten zur Verfügung. Ergebnisse Das hier vorgestellte System Signum erlaubt die einfache web-basierte Annotation vonHandschriften. Das System wird getestet an den Handschriften der Bibliotheka Fuldensis. Grundlage der Implementierung ist eine einjährige Vorstudie, in der ein wahrscheinlichkeitsbasierter Klassifikationsalgorithmus zur Identifikation der Schreiberhand auf Basis von Buchstabeneigenschaften entwickelt wurde. Die so erhobenen Corpusdaten soll in der Folge dazu dienen, den Klassifikationsalgorithmus schrittweise zu verbessern. "
	},
	{
		"id": 258,
		"title": "The Diaries of John Quincy Adams Digital Project",
		"authors": [
			"Millikan, Neal"
		],
		"body": " John Quincy Adamswas one of Americas great statesmen. The oldest son of John and Abigail Adams of Quincy, Massachusetts, his public service career spanned six decades and included roles as diplomat, secretary of state, president, and congressman. For more than 68 years, JQA kept a diary of his public and private experiences. Begun in 1779, the diary grew to over 15,000 pages by the time the final entries were penned before his death in 1848. The resulting 51 diary volumes comprise the longest continuous record of any American of the time and provide an unparalleled resource for students, scholars, and all lovers of history. The value of JQAs diary lies beyond the study of one important individual. It can help inform broader historical discussions, such as gender, race, and scientific progress. This digital project is making the diary truly accessible for the first time by presenting a verified and searchable transcription of each dates entry. The project has been informed by the theories and practices of the documentary editing community. The purpose of documentary editing is not just to make historical primary source material available, but to facilitate engagement with the text by providing context and topical access through the editorial apparatus of annotations, an introduction, and an index. This project serves to address that overarching purpose but within a new approach. Instead of annotations and an index, it will incorporate an innovative context for usability via personal name and topical identifiers to facilitate deeper use by a broader audience. We are using a defined subset of the Text Encoding Initiativetagging language to encode our transcriptions, which are in XML files. Given the fact that JQAs diary spans 68 years, work on this project has been divided into five chronological periods: early, diplomatic, secretary of state, presidential, and congressional. The project is being undertaken by a combination of institutional staff, interns, and volunteers. This presentation will discuss the challenges and benefits of utilizing such a varied workforce on a digital humanities project. Enhanced access to the project will include keyword and personal name search ability, along with topical search features based on themes in middle and high school history curricula. The unique needs of this community were identified through consultation with educators in an effort to understand how to make the diaries more useful to this particular audience. From these discussions we learned that while online transcriptions address the accessibility issue for students who have difficulty reading handwritten documents, keyword searching alone cannot adequately reveal the full, rich content of these materials. Throughout his life, JQA interacted with thousands of individuals. A names list is regularly updated with an individuals name, birth and death dates, date first mentioned in the diary, brief biography, and a unique identifier. This list will allow our web developer to build a search tool to enable users to search for individuals, even when they have not been elucidated by JQA in the text. For example, if JQA mentions father and mother, the analysis provided by a staff member allows the names to be encoded with the identifiers adams-john and adams-abigail. To facilitate topical searches of the diary, project staff consulted with educators to identify approximately 100 themes related to American history. These form the basis of the subject analysis where a staff member assigns the relevant topical headings for each diary entry. For example, in a particular date entry, topical access could be added for Industrialization, Native Americans, Science and Technology, and Slave Trade—topics of particular interest to students and teachers and utilized in history courses—even though JQA may not have written any of those specific words in his diary. Once this search tool is created, we envision a browse list of subject headings that will enable more complex and informed discoverability on historical topics than a keyword search alone could provide. The project currently has two websites. The original site, produced in 2006, provides images of the diary manuscript pages that are date searchable. The new website, launched in 2017, provides transcriptions along with short and long versions headnotes for chronological periods of the diary. In the future, we will combine the content from these two websites to allow for display of the manuscript page images alongside the transcriptions. "
	},
	{
		"id": 259,
		"title": "The Sonnet Stretcher",
		"authors": [
			"Mimno, David",
			"Martin, Meredith",
			"Algee-Hewitt, Mark"
		],
		"body": " We often think about the placement of a particular word in its context in a poem. Constraints of syntax, meter, and rhyme interact with choices about emphasis. It is more difficult to compare an instance of a word to other instances of the same word across a collection. Which words appear early in poems and which late? Which words occur at the beginning of lines, and which at the end? This work presents a method for uncovering the average positions of words. We consider a collection of 10,000 16th to 20th century English-language sonnets from the Chadwyck Healey English Poetry collections, whose primary bibliographic source is the New Cambridge Bibliography of English Literature. Although limited to published poems, and skewing towards the canonical, the texts are accurate and offer a sizeable sample of poetry published during this period. We create a series of two-dimensional views, one for each distinct word. Imagine that each poem has been stretched to a fixed, square shape, with each line fully justified left-to-right. The first word of the first line sits at the top left of the square, and the last word of the last line sits at the bottom right corner. The following figure illustrates this process using Shakespeare Sonnet 91. Figure 1: For each word, we find the location of all instances of the word in all poems, after stretching all poems to a standard, idealized square. In the first panel, we highlight all seven instances of the word some. The second panel shows a reduced view, showing only a symbolic representation of those words. The third panel shows the final aggregate view, with all instances of the word some from all 10,000 poems, with a slight random jitter to show density. This corpus-level view surfaces patterns: some appears frequently at the beginning of lines and rather often about two thirds of the way through a line, but rarely at the end. The word occurs slightly more frequently in the second half of poems, but the primary pattern is left-to-right, not top-to-bottom. The layered visualization reveals surprisingly deep relationships between metrics and meaning. The pattern of some across the sonnet is both conceptual and functional: as a single-syllable word without a strong set of phonetic or graphic rhymes, the word is less valuable at the end of lines, while its conceptual flexibility allows it to be placed in either a stressed or unstressed position depending on its emphasis. As such, its utility in either establishing iambs or disrupting themis equally as likely to place it near the beginning of the line as its measured indeterminancy as either a pronoun or adjective. The Sonnet Stretcher provides a comprehensive, abstracted view, yet requires few theoretical commitments. Though scholars such as Natalie Houston have performed distant reading on a corpus of poems from the Victorian period, focusing in particular on Elizabeth Barrett Browning in her recent paper a distant reading of EBBs rhymes, her work focuses only on end-rhyme and not the way that certain words might appear more frequently in internal rhymes, at the beginning of poems, or throughout the poem. Poemage attempts to show all sonic elements of a single poem in a visual form, and presents a tremendous amount of information that many scholars disagree on, including assumptions about rhymes and metrical perfection. Programs modeling poetic meter are now quite common, from Ryan Heusers Prosodic to programs for Russian poetics, Medieval High German poetry, English and Italian, Bengali, Sanskrit. Figure 2: A sample of selected words illustrating typical patterns. Scholars have long wrestled with whether, how, and why algorithmic tools and visualizations might aid in the reading of poetry. After all, scansion itselfis a form of visualization. But visualization tools can only do so much with prosody; prosody is pronunciation and versification, and though there are rules to both pronunciation and versification there are also hundreds of variants and exceptions to those, not to mention disagreements. It is for this reason that the carefully rendered visualizations of these word positions in The Sonnet Stretcher are so evocative. The Sonnet Stretcher returns a crucial visual aspect of poetry frequently lost in the digitized reproduction of the text: the position of the words on the page. From Herberts Easter Wings to Plaths Bell Jar, the poems spatial arrangement – what words can appear where – remains a crucial axis of meaning. Not only do these visualizations appear as conglomerate poetic texts in their own right, creating a new poetic object for observation and critical commentary, but they both confirm and complicate several commonplaces about rhyme and metricality by the very fact of this accumulative effect. That is, scholars of poetry take for granted the ways that their reading practices, shaped by education and the memorization of the very rules we mention above, impact or bias their approaches to a poetic text. With this tool, scholars are evocatively challenged to think through these assumptions. The visualization reveals both presence and absence. We are not surprised to see that apart, start, and heart commonly appear as end-rhymes, with heart appearing more often throughout the line than the other two. But what of summer, which unlike winter, begins in the second and penultimate positions most often; summer seems to have overtaken spring as having both a beginning and an end. The evocative emptiness of farther begs to be read as a text on its own right, showing a constellation of positions with no adherence to one or the other, much like floating, which hovers across the cumulative sonnet image. Similarly, rich texturally imitates the words meaning in its stretched form. The interpretive possibilities for reading the visualizations are myriad, and this paper will seek to think through what we might learn from position outside of the realm of rhyme. "
	},
	{
		"id": 260,
		"title": "\"Open List\": How to Collect Primary Data on Soviet Terror",
		"authors": [
			"Mishina, Ekaterina"
		],
		"body": " Political terror in the USSR has remained a painful theme for Russian society. In academic and public history different waves of terror and controversial official statistics of victims are being widely discussed. Many documents that accompanied terror operations and even some investigative cases remain classified in archives, and almost 30 years after collapse of the USSR, we still know only 20% of names of political terror victims, i.e. about 3 million real names. These data are mentioned in different Books of Memory, created in almost every region of the former USSR and consist of short biographical cards on every victim known. All data of Books of Memory are collected in unified database made by International society Memorial. Its a SQL-based large dataset of victims, which updates every few years and support only Russian language. We initiated the creation of another database – Open list – to help Memorial collect names and eradicate mistakes from their data. Open list wiki-like open datasetis created on the basis of very heterogeneous and diverse dataset by International Memorial. That is the only unified dataset on political terror victims in Russia from 1917 to 1991 containing more than 3.1 million records and which has four pages on national languages: Russian, Ukrainian, Belorussian and Georgian. A data card of each wiki-page is unified for any of national subsections of the project. Some historical sources were not included in Memorial database, names and bios from these sources are mentioned only in Open list. The main advantage of Open list is that people can add new names and correct already existing information online. Open list updates daily. Users can edit pages via special form with user-friendly interface with 31 fields for personal data and description of arrests, or use wiki-markup to add fields mentioned above manually and upload files on pages. Editors are to verify all crowd-sourced data manually using documents and files that people provide when making their corrections. Most useful documents are digital copies from investigative cases or rehabilitation certificates. If a user cannot provide any file, page remains unapproved with special disclaimer on it. Crowdsourcing is a very important part of the project. There are several possible activities for our users: they can add new people to the list, parse data from biographies to fields in biographical form or add templates such as repressed relatives using inter-wiki to link pages of persons from a kinship family. The special algorithm automatically defines potential relatives according to similar surnames, patronymics and rehabilitation dates. We also have tools for our users to identify and unite duplicate pages. It usually requires much historical knowledge to identify description of two different arrests correctly, and the result of such research is almost always manually checked by editors. There are only few people who work with duplicate pages while we have about 100 thousand duplicates, so we need ideally automatic algorithms to merge them. These algorithms vary depending on the quantity of data on the particular page and mentioned type of repression. The simplest method is to compare a full name, a birth year and a birth place, but here additional parameters are required. If we need to find one person in two different historical sources, we add dates of arrest and conviction. In case of possible mistakes in full names and dates of birth we use full coincidence of biographical data in primary sources. Historians provide these algorithms and IT specialist realize them on Python. Advanced search of Open list contains 21 search fields; all text fields allow using logical operators. Users can gain a long list of personal pages by request. The search is not strict and sometimes it shows more pages than were requested. Data visualization is now possible only on certain types of information like dates of birth, or arrest, or conviction, so text fields should be normalized in close future. This is also one of the project goals to make analysis of these data easier and persistent. Occupation is one of the most difficult fields for normalization. We use classification made by historians on materials of All-Union 1937 and 1939 censuses and NKVD internal instructions for classification of occupations. HISCO is not in use on this step of work because it seems not suitable for linking occupations to Soviet social stratification in the 1930s, as this linking is a necessary step for analyzing social portrait of terror victims. Also such kind of work with HISCO has never been done on materials of early Soviet period. Now we are only preparing for normalizing data and will do most part of work automatically. As the database grows, we can use it for academic needs and deepen knowledge of political repression in the USSR through different ways. Open list provides an opportunity to make samples of data and construct a social portrait of terror victims. Pages with templates could be objects of network analysis as well as investigative cases published on some pages. Massive of biographies could be a source to study family history. We can also analyze geography of terror using field place of living, which in some sources contains concrete addresses. Open list itself conducts an archival work in cooperation with Russian state archive on compiling the united electronic Book of memory of Moscow and Moscow region. This work consist of two parts. Firstly, digitizing the materials of archival investigative cases is taking place. Secondly, the crowdsourcing takes the floor: our volunteers decrypts information from archival documents and create new pages in a list or contribute to existed. They use the instruction provided by professional historians based on principles of source-study of investigative cases. Editors verify all new records in electronic books. Thus, new historical source with names that had never been mentioned before is emerging online on Open list web site. This project helps not only to supplement the list of names, but also correct mistakes in Books of Memory. "
	},
	{
		"id": 261,
		"title": "The Begums of Bhopal: Digital Metadata Analysis In The Field of Representation",
		"authors": [
			"Mitchell, Olivia"
		],
		"body": " The use of imperial media in representing India and its people was an important aspect in the consolidation of colonial rule. This poster examines how the analysis of the representation of Indian individuals links to colonial consolidation of power using highly detailed encoded metadata in sources. I will be demonstrating how the development of an overlapping and layered approach to metadata in encoding can better represent and therefore aid research into media representations. The portrayal of the Indian nobility exemplifies a powerful, exotic, yet ultimately submissive form of Imperialism to the British, and the presentation of the dynasty of the female rulers of Bhopal is a prime example of this. My work charts the ways in which these female rulers were represented and misrepresented as a means of demonstrating Indian animosity and consolidating imperial power. As favoured rulers of a native state, the Begums are frequently discussed in newspapers or as the subject of photographs and while authoring letters and other personal papers. Shaharyar M. Khan, The Begums of Bhopal: A Dynasty of Women Rulers in Raj India,p. 102, p. 171 Combining colonial textual and visual sources allows for a comprehensive view of how these rulers were envisioned to live. Yet, the overall image of the Begums is highly contradictory; for example, in text, these women are typically portrayed as being veiled or in purdah while photographs show them to have almost masculine characteristics, embodying features of any great Indian ruler. Bourne and Shepherd, Sultan Shahjahan Begum, Begum of Bhopal: Prince of Wales Tour of India 1875-6, 1875-6, 27.3 x 23.3 cm, Albumen print Sydney Prior Hall, The Reception of the Begum of Bhopal, 1877, 16.0 x 24.3 cm, Pencil and watercolour The contradictory nature of representation has been a central theme in my research. How were the Indian nobility represented by the British? Were these contradictions central to the way in which Indian nobles were represented, or were they discrete mistakes, demonstrating inconsistency in the understanding of these figures by various British and Indian commentators? Answering these questions required me to uncover patterns in the depiction of these individuals, and their world, to develop a model of the perceived colonial Indian aristocrat. Moreover, it requires understanding how these patterns can transfer from textual to visual sources. To contrast physical and conceptual elements within a multi-modal dataset in a robust and reproducible manner requires detailed and nuanced hermeneutic encoding. mhbeals.com/the-platonic-ideal-of-a-newspaper-article This poster will, therefore, explore the use of digital metadata analysis in the field of representation. It will demonstrate the cross-referencing of digitalised newspaper material from the British Library, Trove, Papers Pastand Chronicling America, with biographies, autobiographies and personal letters held by the British Library and the India Office Collection and digitalised and physical photographs and drawings from throughout the Commonwealth in various combinations and at multiple resolutions. Current text encoding practises, TEI specifically and XML more broadly, do not allow for two overlapping elements of metadata, nor does annotation of visual material do so without damaging or destroying the integrity of the image. ids-pub.bsz-bw.de/frontdoor/index/index/docId/5878 Tufte, Edward, Beautiful Evidence, Graphics Press LLC, 2006 Although multiple layers of analysis within XML documents, namely multi-rooted trees or using XML linkages and then subsequent transformations to fully represent multiple hierarchies are possible, this is technically complex and requires both a definitive and transformed version of the document for recording and analysis. My system allows for straightforward annotation of a single text document with multiple overlapping segments in the same manner as it annotates full or segmented images. This unity of input and analysis is more appropriate to my specific case, comparing visual and textual representations of a common event or group. My poser will demonstrate a custom-designed database system that allows for consistent layering of metadata on textual and material historical objects, digital reproductions, and enhanced fragments – sections of textual or visual objects that have been transcribed, cropped or otherwise computationally transformed. Building upon concepts from the semantic web, the system allows the user to apply metadata and provenance details recursively between the original object and discrete fragments as well as an encoded controlled vocabulary on the content of those objects. This layering of metadata allows for a much deeper analysis of historical sources, as well as being able to maintain the integrity of the original object in the projects documentation. The database, written in Python 3.6 and stored in a plain text JSON file, functions as both an input and retrieval mechanism, allowing users to input data sources either manually or through templated input files. In addition to standardised MARC and Dublin Core provenance metadata, the database will encode both the aesthetic and artistic properties, and the historical characteristics. This will allow for consistent cross-referencing that may be difficult if not impossible to determine by human eye alone. Using metadata allows for a much more robust comparison between textual and visual sources, highlighting trends that may otherwise never be brought to light. Finally, although this level of metadata analysis requires a significant temporal commitment to data entry, the deep data created will inform not only the present research regarding representation but provide consistent training for unsupervised computation analysis in the future. "
	},
	{
		"id": 262,
		"title": "Applying Measures of Lexical Diversity to Classification of the Greek New Testament Editions",
		"authors": [
			"Miyake, Maki"
		],
		"body": " 1. Introduction A number of lexical diversity measures have been proposed and applied in stylometric studies. Covington et al.introduced three stylometric measures including moving-average type-token ratio, or MATTR. They applied the measures to classify ten English translations of the Gospel of Mark. The study focuses on decision tree models based on several measures of lexical diversity, aiming at classifying genres of authorship attribution and critical types in various editions of the Greek New Testament. We extract measures of lexical diversity that do not significantly correlate with tokens and investigate specific indices that highly contribute to the performance of discriminant models. After creating training and test subsets from ten editions, we apply two classification algorithms such as Classification and Regression Treeand Random Forest. We then figure out the classification accuracy with the token-independent measures. For all that the aim of the study is to classify genres of authorship attribution and critical types in various New Testament editions, we do not simply pursue higher accuracy of classification per se, especially in the edition types. We are rather focusing on the characteristics of misclassified texts and edition types. Before comparing contents among the editions, we try to identify the measures of lexical diversity contributing to classification according to purposes such as authorship and edition types in this case. 2. Methods 2.1. Data In this study, we focus on the top ten longest books over 4000 tokens in the New Testament: the four Gospels, Acts, Romans, the first and second Epistles to the Corinthians, Epistle to the Hebrew and Revelation. Table 1 shows the list of ten books with its abbreviated name, genres and authors. The authors names followed the general consensus in the biblical studies. We distinguish the authors between the Gospel of John and the Revelation and we do not specify the name of the author the Epistle to the Hebrew. Book Genre Author Stephanus Nestle-Aland Tokens Types Tokens Types Matthew Gospel Matthew the Evangelist 18769 4281 18348 4190 Mark Gospel Mark the Evangelist 11656 3128 11306 3005 Luke Gospel Luke the Evangelist 19949 4977 19490 4858 John Gospel John the Evangelist 15942 2878 15641 2809 Acts Acts Luke the Evangelist 18814 4927 18455 4846 Romans Epistle Paul 7220 2121 7111 2086 1 Corinthians Epistle Paul 6941 2084 6832 2055 2 Corinthians Epistle Paul 4499 1499 4478 1484 Hebrews Epistle Unknown 5016 1922 4955 1896 Revelation Revelation John of Patmos 9975 2301 9857 2218 Table 1: List of top 10 longest books in the New Testament The ten editions we selected can be divided into three types, such as the so-called Received Text, critical edition and Byzantine Textform. Table 2 shows the list of ten editions of the Greek New Testament that are used in the study. Most of the texts are obtained from the electric texts in two biblical software such as Bible Works 9.0.12 and Accordance 11.2.5. The last column of the list represents the electric version of the text. We apply 100 samples in totalto discriminant analyses. The samples of these editions are randomly distributed into training and test subsets. Edition Date Type Electric Text Vers. Stephanus1550 Textus Receptus 4.8 Tregelles 1857-1879 Critical Text 1.0 Tischendorf 8th ed. 1869-1872 Critical Text 2.7 Westcott-Hort 1881 Critical Text 2.7 Scrivener 1894 Textus Receptus - Von Soden 1902-1910 Critical Text 1.0 Robinson-Pierpont 2005 Byzantine Textform 2.8 Nestle-Aland 28th ed. 2012 Critical Text 2.0 BGNT 2014 Byzantine Textform - Tyndale House 2014 Critical Text - Table 2: List of the Greek New Testament editions 2.2. Measures of Lexical Diversity We use measures of lexical diversity as categorical variables for classification. First of all, we calculated 16 measures including basic indices such as types, tokens and punctuation using the koRpus packagein R version 3.5.1. Fig. 1 shows the Correlation Coefficients of each measure of lexical diversity with Tokens. We extract measures with no significant correlations with tokens at the 0.05 level of p-value. In this way, the following the eight measures are going to use the classification: Punctuation Ratio, Yules K, MATTR, Dugasts U, Maas, Somers, MTLD, and HD-D. Fig. 1: Correlation Coefficients with Tokens 2.3. Classification We apply two classification algorithms to classify authors and edition types in the Greek New Testament. One algorithm is Classification and Regression Tree, or CART and the other one is Random Forest, or RF. Discriminant analyses are performed in R 3.5.1 using the rpart packagefor the CART algorithm and the randomForest packagefor RF. For CART tree models, trees are split based on the Gini index and the values of variable importance are recalculated so that the sum total becomes 100. The minimum number of observations is set to 3. We prune trees based on the cost-complexity parameter and cross validated error results, if necessary. Meanwhile, the minimum size of terminal nodes in RF is varied to optimize the classification accuracy. The samples of the New Testament editions are randomly distributed into 50 training and 50 test subsets. Generating decision tree models from training samples, we apply the trees to test datasets and then examine the classification results. We also observe the discriminant measures of lexical diversity. The breakdowns of datasets are shown in Table 3 for authorship and in Table 4 for classification of edition types. The names of each authors referred to the third column in Table 1. The letter E of E_ stands for the Evangelist. E_Mark E_Matthew E_Luke E_John Paul Unknown John of Patmos Training 4 5 10 4 16 6 5 Test 6 5 10 6 14 4 5 Table 3: Breakdowns for authorship classification Byzantine Textform Textus Receptus Critical Text Training 9 11 30 Test 11 9 30 Table 4: Breakdowns for classification of edition types 3. Classification Results 3.1. Authorship First, we apply CART tree models to classify authors. Fig. 2 shows the variable importance in CART classification. Although MATTR indicates the highest score among eight measures, Dugasts U can be also regarded as important index for discrimination. There are many other relatively important measures such as Maas, Yules K and MTLD. Pruning trees was not needed in this model. Fig. 2: CARTs Variable Importance Fig. 3: RFs Variable Importance E_Mark E_Matthew E_Luke E_John Paul Unknown John of Patmos E_Mark 6 0 0 0 0 0 0 E_Matthew 0 5 0 0 0 0 0 E_Luke 0 0 10 0 0 0 0 E_John 0 0 0 6 0 0 0 Paul 0 0 0 0 12 2 0 Unknown 0 0 0 0 0 4 0 John of Patmos 0 0 0 0 0 0 5 Table 5: CART ClassificationAs shown in Table 5, two Pauline Epistles were misclassified as the author of Epistle to the Hebrews and the accuracy is 96.0%. Meanwhile, the RF algorithm classified with 100% accuracy. Fig. 2 plots Mean Decrease in Gini coefficient equivalent to variable importance in CART. Maas and Yules K highly are considered to contribute to the authorship classification. 3.2. Edition Types In the CART classification, we pruned the tree where the value of complexity parameter is 0.17. Fig. 4 shows the variable importance in CART. Punctuation Rate indicates exclusively the highest score and can be regarded as the most important index for discrimination. As shown in Table 6, all editions of Byzantine textform were misclassified into both Textus Receptus and Critical Text. The classification accuracy in CART is 60.0%, while that in RF is slightly higher: 62.0% shown in Table 7, where minimum size of terminal nodes is set to 10. As shown in Table 5, the constituent ratio of Mean Decrease in Gini coefficient is very similar to that in CART in Fig. 4: distinctive importance of punctuation rate. Fig. 4: CARTs Variable Importance Byzantine Textform Textus Receptus Critical Text Byzantine Textform 0 3 8 Textus Receptus 0 5 4 Critical Text 0 5 25 Table 6: CART ClassificationFig. 5: RFs Variable Importance Byzantine Textform Textus Receptus Critical Text Byzantine Textform 0 3 8 Textus Receptus 0 4 5 Critical Text 0 3 27 Table 7: RF Classification4. Discussion The token-independent measures of lexical diversity can be distinguished by Pearson correlation coefficients of each measure with tokens. Punctuation rate is exclusively crucial when classifying edition types, while the others are effective in the authorship classification. In both authorship and edition types, the classification accuracy in RF is higher than that in CART. For all that edition types were poorly discriminated against, that does not indicate the limitations of the techniques. We will rather focus on the misclassified texts, especially the editions of the Byzantine textform, to work out these peculiar characteristics. "
	},
	{
		"id": 263,
		"title": "Multimedia Markup Editor (M3): A Semi-automatic Annotation Software for Static Image-Text Media",
		"authors": [
			"Moisich, Oliver",
			"Hartel, Rita"
		],
		"body": " Introduction This poster introduces an editor software specifically designed for graphic narratives, including graphic novels and comics, but also other kinds of illustrated still-image media. Users are able to mark up these documents in XML via a Java-based GUI. The annotation language used in the system, which we call Graphic Novel Markup Language, is an extension of the TEI-based Comic Book Markup Language. Figure 1 presents an overview of the object types and relationships that have been implemented so far. Fig. 1: Overview of object types in GNML Features A number of automatic processes in the M3 editor facilitate a simple annotation process. The system localizes outlines of panels by using processes used in computer graphics, such as the marching squares algorithm. Annotators may select simple structures such as speech bubbles or captions with a single click on element backgrounds, thus prompting a flood-fill process that recognizes the entire structure. For more complex representations such as characters, the system interprets annotators selections with the help of livewire segmentationto identify high-contrast edges and, subsequently, contours of characters. In order to mitigate the error rate of manual transcription, the system has a built-in spell checker and autocomplete function for text and character names. One of the main goals of the annotation system is quantitative analysis, which aims to provide empirical evidence of narratological terminology, establish character networks, or provide access to comics text in the absence of automatic recognition. In the case of the former, a narratological annotation scheme that includes mark-up options for focalization and story worlds has been integrated into GNML. Theories of cognitive and transmedial narratology as well as empirical dataserve as a basis for this scheme. Figure 2 shows the annotation system GUI and a sample annotation. Fig. 2: GUI and sample annotation of Pepper & CarrotConclusion & Outlook The editor software facilitates the analysis of multimodal corpora with complex text-image interactions. Such evidence-based investigation may help revise existing theories of graphic narrative or falsify more qualitative scholarship. The software can also be used in classrooms for hands-on case studies and has been tested successfully in several university seminars. We are currently working on OCR integration, which presents particular challenges as graphic narratives often use handwritten or pseudo-handwritten fonts and existing OCR/ATR systems do not as yet provide satisfying results. We hope to integrate OCR either directly or as an optional step prior to manual annotation in future versions. The annotations systems latest version can be downloaded at: http://graphic- literature.upb.de/?page_id=3592 . An FAQ to help annotators is available at: http://graphic- literature.upb.de/?page_id=4123 . An open-source version will be made available by February 2020. "
	},
	{
		"id": 264,
		"title": "The Corpus of Historical Mapudungun:Digital Tools for New-World Language Change",
		"authors": [
			"Molineaux, Benjamin Joseph"
		],
		"body": " 1 Historical linguistics and the Americas Minority, non-European languages — such as indigenous American ones — are critically under- represented in the literature on language change. This not only narrows our view of the historical interaction of peoples and languages predating European expansion, but also limits our under- standing of language change as a whole. In the absence of the hundreds of years of philological study available for Old World languages, digital methods emerge as ideal means for systematically compiling and exploring the available data for historical languages in the New World. This said, carefully tagged, historically-oriented corpora for Native American languages are still few and far between, despite the general growth in Digital Humanities scholarship throughout the region. 2 Mapudungun, a prime candidate Mapudungun, the ancestral language of the Mapuche people of present-day Chile and Argentina, has been fairly well documented for over 400 years. While the types of available historical material for the language are fairly typical — including missionary, military and ethnographic works— the historical depth, as well as the number of texts available for Mapudungun is better-than-average for the region. Current Mapudungun varieties are mostly well described and remain in use, nevertheless, there is very little explicit work on their history. This availability of historical and contemporary data, coupled with a gap in historical research, makes Mapudungun an excellent candidate for a first corpus-based historical account of a language of the Americas. The building of such a resource, The Corpus of Historical Mapudungun, is the focus of an ongoing research project at the Angus McIntosh Centre for Historical Linguistics, Edinburgh, and the topic of this talk. Beyond the empirical and methodological advantages to working with historical data for Mapudungun, there are also theoretically interesting reasons to do so. Features such as nominal incorporation, verb serialisation, agglutination and polysynthesys raise interesting questions about the diachrony of units of sound and meaning, which cannot be probed by better-studied, yet typologically distinct languages. Overall, the rich word-internal morphology of Mapudungun challenges traditional domains for sound change. In particular, the fact that morphological transparency is paramount to the languages system, means that there is little indication of word-level reduction and neutralisation processes, which are key to the study of sound change in Indo-European languages. These facts of Mapudungun will be key both to our analysis of change in the language and to the practicalities of corpus-design. 3 Building the corpus The majority of texts included in the CHM are printed material dated between 1606 and 1930, making up some 400k words. Using both archival images and newly digitised materials from the Biblioteca Nacional de Chile and the Archivo Rodolfo Lenz in Santiago, Chile, the relevant texts have undergone optical character recognition. This was done via the Digital Humanities Dashboardand followed up by hand-checking. The result is a collection of digital text covering the first 324 years of written Mapudungun. While the texts are gathered from all major language areas, they inevitably remain imbalanced in nature — both temporally and spatially —, as tends to be the case for historical corpora. As the objective of the corpus is to provide a view into the synchrony and diachrony of lexical, morphological and phonological features, texts are being parsed at all three of these levels. The first stage of this process — lemmatisation — identifies the key root-elements, as well as the part-of-speechcategory for each word, providing a single identifiable label and reducing both morphological and spelling heterogeneity.Form Lemma Translation POS a. ⟨kude-kefuingu ⟩ kuden to play V b. ⟨kuthe-kalape ⟩ kuden to play V XML <w xml:lang=arn lemma=kuden pos=V corresp=play>kudekefuingu</w> <w xml:lang=arn lemma=kuden pos=V corresp=play >kuthekalape</w> The second stage is morphological parsing, which identifies individual morphemes beyond the root and labels them according to function. The result of both these processes is a TEI-standard XML text with the relevant tags embedded. A full 10% of the total word-types in the corpus texts is soon to be completed, after which an AI algorithm, developed at the University of Edinburghs NLP Group, will be trained to tag the remainder of the material both at the level of the lemma and the morpheme. Additional hand corrections will be necessary in order to complete the process.Form Morphemes a. ⟨kude-ke-fu-ingu ⟩ root- habit-broken.implicature-ind.3.dual b. ⟨kuthe-ka-la-pe ⟩ root- cont-neg-imp.3.sg XML <w><m baseForm=kude type=root corresp=play>kude</m><m baseForm=ke type=habit>ke</m><m baseForm=fu type=BI>fu</m><m baseForm=ingu type=ind.3.d>ingu</m></w> <w><m baseForm=kude type=root corresp=play>kude</m><m baseForm=ka type=cont>ke</m><m baseForm=la type=neg>la</m><m baseForm=pe type=imp.3.sg>pe</m></w> The final stage of the tagging will be grapho-phonological parsing, which entails providing sound values for each word, following a list of spelling-based rules for each text. The results should effectively reconstruct the phonic structure of each text, such that it can be compared with others from different periods and locations, helping to map phonological change from the bottom up. The front end of the corpus — soon to be available in beta form — will provide search optionsat all three levels of tagging, as well as allowing users to correlate these features across texts and with relevant non-linguistic metadata such as date, location, author, genre, etc. A simpler browser version will also be available for non-linguists, allowing for texts to be browsed and downloaded with parallel translations.Form Sound Lemma Translation Source Dialect a. ⟨vúta ⟩ [vɨta] fücha old/big Valdivia 1606 North b. ⟨fücha ⟩ [fɨʧa] fücha old/big Augusta 1916 Central/Coast XML <m><c ipa=v>v</c><c ipa=ɨ>ú</c><c ipa=t>t</c><c ipa=a>a</c></m> <m><c ipa=f >f</c><c ipa=ɨ>ü</c><c ipa=ʧ>ch</c><c ipa=a>a</c></m> 4 Applications In this talk I will give example applications of the CHM to language change, looking intothe spread of Quechua lexical borrowings andthe evolution of morpheme-boundary epenthesis. More generally, however, the CHM paves the way for the application of digital methods to the history of minority, non-standard languages, creating transferable tools, and foregrounding under-studied typological features. Such outcomes will broaden our understanding of language change overall, by allowing a detailed view of the interaction of the lexicon, morphological structure, and sound systems over time and space. Locally, the project will provide teachers, learners and advocates of Mapudungun with a repository of words in their historical usage and forms, which may be used to enhance word-building strategies, revitalise dialectal vocabulary and more generally improve transmission, both through written and spoken medium. "
	},
	{
		"id": 265,
		"title": "Gaming Genres: Using Crowd-Sourced Tags to Explore Family Resemblances in Steam Games.",
		"authors": [
			"Mol, Angus A. A."
		],
		"body": " Introduction The categorization of games into genres is one of the more complex issues in game studies. Some of the most widely recognized genres, such as Platformer, Beat em up, Shooter or Role Playing Games, have their roots in the renaissance of the medium in the late 80ies. However, the explosive growth of the last decades in the video game industry and the community surrounding it, is also mirrored by a wild growth of video game forms and styles. As a creative as well as billion-dollar industry, developers end up copying or emulating elements of each others works for artistic as well as financial reasons. As is the case in the production and consumption of other forms ofmedia, a video games classification is a topic of much debate among creators, critics, and consumers — see for example how and whyAssassins Creed: Odyssey is an RPG from the perspective of a Reddit thread, a critical appraisal, and the games developer Ubisoft. Perhaps the underlying reason for this is how games come to be defined through the experiences of its players. In short, a video game genre is an example of a personal, social, cultural, and technological construct that cannot be captured by strict boundaries, but arises from the complex interplay of interactive, multi-media elements in sets of games and their appraisal by the community at large. Language Games with Steam Games This in itself is not a new idea. The complex classification of games was already used as a discussion of language games and generalities in language by Wittgenstein in his Philosophical Investigations. In Statement 67, he examines the commonality ofgames through a comparison of the elements of individual examples and concludes: ‚[W]e see a complicated network of similarities overlapping and criss-crossing: sometimes overall similarities, sometimes similarities of detail. I can think of no better expression to characterize these similarities than „family resemblances.  This paper will provide the results of an ongoing project that puts Wittgensteins concept of game families into practice as a way to explore the complexity of genre in this medium. To this end a similarity network has been created with data drawn from the industrys leading digital distribution platform, Steam. Steam uses Steam Tags as a crowd-sourced recommender system. The system allows Steam 125 million users to tag games describing an element of a game they played. Examples of such tags include overall terms, such as Indie, RPG, or Action, but can also be relatively specific, such as Story Rich, Historic, or Choices Matter. It bears pointing out that, like any database created in and through public discourse, these tags should not be considered to be an objective description of the games contents. Furthermore, which tags can be used to apply to games is curated by Valve — after a brief time in which people could create any tags they wished, a trial predictable outcomes. These tags are a first step to a computational and network scientific-driven understanding of the idea of game families. Any classification system arising from this can then be cross-referenced versus existing ideas or potential other computational genre approaches. Using the public SteamSpy API, 342 different tags applied to 23985 games on Steamwere collected in a SQL database, including not only if they were applied to a game but also how many times. Tag Networks as Game Families This data provides the basis for a network science approach to genres in the form of a two-modenetwork. Such two-mode networks can be transformed to either game-to-game or tag-to-tag affiliation networks and have been used in a wide range of humanity contexts, including the study of stylistic diversity. Game-to-game networks would themselves be interesting for comparing overall similarities of games — and are used by Steam to suggest new games to consider for potential buyers based on their previous purchases— but will fall outside the scope of this paper. The tag-to-tag network can then be used to explore game families and genres using network community detection algorithms, such as Louvain Modularity. To finalize this short paper and illustrate ongoing work, two case-studies can be visually explored: one with all tags of games that are also tagged as historical and another highlighting Steams 100 best-reviewed games. While families of games can be detected in this tag-database, they may only make sense in combination with an advanced understanding of a video game corpus and community. This is relatively straightforward in the case of historical games, but is less useful if the subset of the corpus itself is fuzzily defined. For example, in the case of the best-reviewed games network, it does not necessarily present the ingredients for making a universally good game — unless one is interested in making Japanese romantic visual novels or other games with small but supportive communities. Preliminary results indicate that, as Wittgenstein predicted, game families as networks are better at capturing the multi-stranded nature of this media form than the more monolithic genre classification, especially when working with subsets of games. Next steps would be to check the robustness of these models by including similarity indices or votes as link weights. Still, the notion of game genre is unlikely to be abandoned soon, as anyone familiar with the internets collective fascination with language games will surely agree on. Figure 1: The tags in this network all occur in games that have also been tagged as historical. Width of links in this network indicates how often tags co-occur together. The color of nodes is based on a Louvain modularity measure and show what family of historical games they belong to. These can be broadly characterized as Strategy, Action-Adventure, and ShootersFigure 2: The tags in this network all occur in games that have the top 1% user reviews on Steam at the time of data collection. Width of links in this network indicates how often tags co-occur together. The color of nodes is based on a Louvain modularity measure and show what types of families . There is a large amount of variability in this family, which includese.g. puzzle games, retro platformers, cooperative survival gamesand Japanese visual novels. "
	},
	{
		"id": 266,
		"title": "C-SALT APIs - Connecting and Exposing Heterogeneous Language Resources",
		"authors": [
			"Mondaca, Francisco",
			"Rau, Felix",
			"Neuefeind, Claes",
			"Kiss, Börge",
			"Kölligan, Daniel",
			"Reinöhl, Uta",
			"Sahle, Patrick"
		],
		"body": " Introduction In this paper, we present a strategy for the integration of existing heterogeneous language resources such as texts and dictionaries by connecting these resources and making them available for internal projects and third-party applications throughAPIs. We describe our approach in the context of the C-SALT initiative, which gathers projects and resources hosted at the University of Cologne covering South Asian languages. To illustrate the potential use of our approach, we first introduce VedaWeb, a web-based platform that provides access to ancient Indian texts composed in Vedic Sanskrit, the oldest form of ancient Indo-Aryan. Then we describe the C-SALT APIs for dictionaries https://api.c-salt.uni-koeln.de . These APIs make several large Pāli and Sanskrit dictionaries available online. Building on that, we present the architecture behind these APIs, and finally we summarize by analyzing the potential role of APIs in Digital Humanitiesprojects. About VedaWeb The cornerstone of VedaWeb is a digital edition of the Rigveda, one of the oldest and most important texts of the Indo-European language family, which comprises approx. 160,000 words. VedaWeb can be accessed either via a web application https://vedaweb.uni-koeln.de/rigveda or directly via an API https://dh-cologne.github.io/vedaweb/#description-of-the-api . VedaWeb provides several layers of linguistic and philological information, alongside various editions of the text of the Rigveda. A search function with multiple linguistic parameters is available, which allows to execute queries across different levels of annotation by means of complex, combined search criteria. Besides the annotated version of the text, further layers include the display of translationsas well as commentaries to the Rigveda. Parallel to the morphological annotations, all of these additional information layers can be accessed via full-text search as well as a more structured search function. The possibility to combine these multiple layers is crucial for enabling novel perspectives on the data, e.g. by means of quantifying feature combinations or by identifying context-dependent phenomena such as different types of constructions. VedaWeb is meant to advance research in all areas of Vedic studies, for example in syntax, non-configurationality), morphology, ya-presents) or word formation). A Screenshot of the VedaWeb Application, with two layers selected. Rigveda data and the Dictionaries are proportioned via the C-SALT APIs An important feature of VedaWeb is the enrichment of the Rigveda text by linking each word with entries from the standard dictionary for the Rigveda by Hermann Grassmann. Instead of encapsulating the data in the application, our approach is to leave the resource in place and obtain the data via the C-SALT APIs for Sanskrit Dictionaries https://cceh.github.io/c-salt_sanskrit_api . C-SALT APIs for Dictionaries The C-SALT APIs for Dictionaries https://api.c-salt.uni-koeln.de have been developed to provide access to existing lexicographic resources in Pāli and Sanskrit without doubling work or hosting efforts. The dictionaries available via these APIs are also accessible through traditional monolithic web applications, like the Critical Pāli Dictionary Online http://cpd.uni-koeln.de , and the Cologne Digital Sanskrit Dictionaries http://www.sanskrit-lexicon.uni-koeln.de , which are a product of a major Sanskrit digitization project. C-SALT APIs Overview API Architecture The basis of the APIs and of the VedaWeb application are versions of the texts and dictionaries encoded in TEI Text Encoding Initiative: https://tei-c.org/ -XML Extensible Markup Language . We employ a TEI schema https://github.com/cceh/c-salt_dicts_schema developed initially for the three most complex Sanskrit dictionaries. By using one TEI schema, we not only achieve data persistence, but we also achieve a consistent structure for all dictionaries. While software such as frontend applications or APIs change over time, TEI offers the DH community the safest way to assure data persistence. For this reason, all the data accessed through APIs is ultimately based on TEI files. The different C-SALT projects use different technologies as middleware between TEI and endpoints and also different Web API technologies: RESTand GraphQL https://graphql.org/ . Independently of the technology employed, our APIs focus on performance and on providing well-documented access to curated linguistic data. Summary and Outlook Developing APIs means the separation of concerns. In the specific case of APIs: Well-curated data that should be efficiently accessed, through a clearly defined structure. For web applications this means : Focusing on a specific user target, employing, if required, multiple APIs. We have described the potential use of APIs for lexicographic resources. There are several advantages to making the data accessible through APIs instead of encapsulating the data within the application. Instead of forcibly homogenizing diverse data sets into a general data model, it is more efficient to provide a common interface for accessing them. This also opens up opportunities to employ the different resources in the context of other applications. The main goal in developing C-SALT is to keep all resources as modular as possible, so that they can be used and reused in different research scenarios. In the case of VedaWeb, this currently applies to the dictionaries involved, but we see the potential to transfer the concept onto the other information layers as well, in particular the Rigveda text and its translations. In general, we believe that an API based approach to digital resources and data in the Digital Humanities provides efficient access to data and encourages the reuse of available resources. It thus facilitates novel uses by other researchers while avoiding repetition of work and unnecessary redundancy of resource instances. Applications are transient, but the knowledge, represented by the data, may stay and be reused. "
	},
	{
		"id": 267,
		"title": "Digital Edition And Linguistic Database: A Fully Lemmatised And Searchable Model",
		"authors": [
			"Morcos, Hannah Jane"
		],
		"body": " This poster reports on a fruitful collaboration between specialists in the fields of medieval French literature, lexicography, and digital humanities. The outcome will be the first fully lemmatized digital edition of a medieval French text, with a search page directly linked to the Dictionnaire Étymologique de lAncien Français. As part of the ERC Advanced Grant project The Values of French Language and Literature in the European Middle Ages, we are editing the two most important manuscript copies of the earliest universal chronicle in French, the Histoire ancienne jusquà César. In addition to being the first edition of the whole text, it will provide an extensive corpusof lemmatized and searchable linguistic data. We hope it will complement existing medieval French databases, such as the impressive Base de Français Médiéval, which while largely based on edited texts is gradually increasing its content from manuscript sources. The two manuscripts we are editing were created in regions where French was not a native language. In order to analyse their philological as well as linguistic complexities, the edition design engages with the singularity of each witness. Following TEI guidelines, the XML transcription files are structured according to the presentation of the text on the manuscript page. Each manuscript is divided into multiple files saved on cloud storage, facilitating the distribution of labour. The semi-diplomatic transcription replicates scribal word division and medieval punctuation. The addition of a custom editorial shorthand renders the edited version, which has modern punctuation, diacritics, regularized word division, and minimal editorial corrections. Thanks to a collaboration with colleagues at the DEAF, we are lemmatizing the edited text using a custom-made digital tool called Lemming. A key word in contextindex generated from the edited text is uploaded to Lemming, which operates on the fly. Lemming enables the attribution of the same lemma to numerous keywords, the annotation of ambiguous cases, and multiple users to work at the same time. When users identify inaccuracies in the editorial work, corrections are made to the TEI source files and the content is then re-uploaded to Lemming. The deciphering of changed or new keywords/contexts posed a particular challenge. In response to this issue, a new interface was created where unverified imports can be reviewed individually or en bloc, and new lemmas attributed if necessary. This conflict resolution has had significant benefits for our collaborative editorial workflow, making it possible to work in parallel on editing and on lemmatizing, and enhancing the accuracy of both. Figure 1. Faceted search page Over the last six months, the primary development has been the integration of the output KWIC from Lemming into the editions search page. Using a faceted search, users can filter results by lemma, form, prose/verse, direct/indirect speech, manuscript, and narrative section. Most notably, it is possible to sort the results according to the context immediately following the keyword. This offers us an important opportunity for the study of syntax, in particular the syntax of Old French negation and the conditions for the expression of pronominal/nominal subjects in subordinate clauses. For example, we are able to explore occurrences of the conjunction que immediately followed by the adverb of negation ne: Quant Phutiphares oi ces paroles, sachiés que ne li pleurent mie. In such cases, sentential negation is stated in a way that is not possible in later French. Whereas middle and modern French would need the overt and compulsory expression of the subject between que and ne, old and very old French do not. This is just one of the exciting opportunities the lemmatized search results offer for the study of specific linguistic features, and in this case influences not only our understanding of the manuscripts that we are editing but, more importantly, of the history of the French language. The first iteration of the search page will be published on the public site in summer 2019. The data will therefore be available for other researchers working on linguistics, lexicography, discourse analysis and literary studies, before and after the end of the project. Figure 2. Editorial workflow Underlying these developments is our workflow, which features custom-designed scripts that automatically aggregate the TEI source files, convert the editorial shorthand, tokenize the words, and generate the KWIC index for Lemming as well as preview-able outputs on the projects staging site. The fully automated and frequent conversion of the fragmented TEI sources takes place every two hours, enabling the verification of the final output as the end-users will see it. Furthermore, a custom-made printable proof-reading pagefurther facilitates incremental publication and pre-emptive problem-solving. By the projects completion, the aggregated and converted TEI content will be available open access. Figure 3. Printable proof-reading page "
	},
	{
		"id": 268,
		"title": "Sacred Sound – Sacred Space: In Search Of Lost Sound",
		"authors": [
			"Morent, Stefan"
		],
		"body": " The Exploration Full Fund Sacred Soundinvestigates the interacting of architecture and furnishings of sacred spaces with sound and the relations between concepts of sacred spaces and religious experience as well as the shaping of liturgical forms. Such complex systems of relations are particularly demanding if sacred buildings and their acoustics dont exist anymore or at least not in their original form. New approaches of research are provided by recently refined methods of virtual reconstruction of historical acoustics based on reconstructed 3D-models of the architecture. This research project will explore the contextualization of liturgical singing in its original sound space. We expect that chant as a sacred sound was embedded in a complex system of relations between movement within sacred space during the liturgy and the acoustical characteristics of the space. As a result we are looking for new insights into the shaping parameters for transmission, notation and performance practice of liturgical music. Since the 1990s computer-aided methods are well established in acoustic research as well as the rendering of data into audible sound, which is called auralization. For sacred spaces however there is still lacking reliable in-depth data, although research into acoustics of churches Meyer, 2015; Girón et al., 2017) and the intelligibility of speech is well documented. Still lacking are investigations into the relations between acoustics of space and liturgical rituals performed within them. The Institut für Technische Akustikat RWTH Aachen provides technical means to set up a simulator with auralization, as complete simulation of room acoustics, auralization and 3D-audio reproduction. The ITA is one of the leading centers for auralization technology and will collaborate with the research project. A case study on the acoustical reconstruction of a 11th century church in Spain has been already carried out. The innovative character of the research project consists in the combination of musicological, liturgical and ritual studies with techniques of Digital Humanities. The project will start with the UNESCO World Heritage Cistercian monastery church of Maulbronn/Germany, one of the best-preserved churches of its kind. A 3D-laserscan of the church is performed in spring 2019 by the Hochschule Karlsruhe. In August 2019 we will record the office for St. Bernhard of Clairvauxwith ensemble Ordo Virtutum for medieval musicin the church in collaboration with the Südwest Rundfunk. In parallel the real acoustic will be measured. The 3D-model will be used as a basis to render the virtual acoustic of the church, taking into account its material and interior surfaces. Selected pieces of the music will then be recorded by the singers in an enechoic chamber at Aachen with a 32 microphones array, giving the singers the auralization of the sound of the church in real time. Both recordings will be compared in order to callibrate our results. The singers will also perform in the AixCave-space at Aachen, the biggest 5-side virtual acoustic immersion room of Europe. There it will be possible to test various positions of singing in the virtual model and its results on the acoustics. We hope to be able to describe the connection between Cistercian chant, theology and acoustics. There exist several papers on the acoustics of Cistercian churches but none of them used the amount of advanced techniques and music we intend to use. If the experiment is succesful we plan in 2020 a recording of the offices for St. Gall and St. Ottmar in a virtual reconstructed model of the former monastery church of St. Gall with its virtual acoustic. In the future we also intend to do recordings for Cluny and St. Peter and Paul of Hirsau, which was influenced by Cluny, but presents a different concept of architecture and acoustics at the beginning of the Hirsau reform. Virtual reconstructions of these churchesshall help to reveal the character of their acoustics. We hope to be able to describe the difference in sound concepts of a Benedictine church of the 9th century and a Cistercian church of the 12th century. Though this paper cant present results, it will describe what we want to do, which techniques will be used, what will be the challenges and questions and what impact on academic research and performance practice we expect. This short paper will present the current status of the project and its challenges and goals as a work-in-progress. "
	},
	{
		"id": 269,
		"title": "ADHO Geohumanities SIG Conversations Workshop",
		"authors": [
			"Murrieta-Flores, Patricia",
			"Page, Michael",
			"Brando, Carmen",
			"Vis, Benjamin"
		],
		"body": " The Geohumanities SIG at ADHO brings together a multiplicity of scholars and disciplines interested in the field of Spatial Humanities. Each year, the SIG meets with its members updating them with the news and activities we carry out, as well as invite them to get involved with the Geohumanities community. This year, aligning to the conference theme, the SIG wants to propose a pre-conference workshop that will bring together specialists and participants from diverse backgrounds and fields, looking to address the complexities behind Spatial Humanities theory and method. The panel includes a range of disciplines such as geography, literature, history and archaeology, and it will aim to initiate a conversation about the diversity of interdisciplinary research in the field. The session will start with an introduction by the SIG convenors about the group and the workshop, followed by commentaries and presentations by a panel of participants, finalising with a guided discussion focusing on the challenges that the field confronts today, including diversity in spatial conceptions and representations, technological changes, and visualisation. The workshop will be guided by the SIG convenors Patricia Murrieta-Flores, Carmen Brando, Benjamin Vis, and Michael Page and it includes the next scholars in the panel: Prof Ian Gregory, Lancaster University Prof Leif Isaksen, University of Exeter Dr Tiago Gil, Universidade Brasilia Dr Andrew Richardson, University of Northumbria Dr Jo Taylor, Manchester University Dr Pau de Soto, Universidade Nova de Lisboa Dr Enrique Cerillo-Cuenca, Consejo Superior de Investigaciones Cientificas, Spain Programme 9th of July 2019 Venue TBC 09:00-09:10 Welcome and introduction to ADHO Geohumanities SIG by Patricia Murrieta-Flores, Michael Page, and Carmen Brando 09:10-09:30 The impact of semi-automatic detection in archaeological knowledge: A case study and some reflections by Enrique Cerrillo Cuenca 09:30-09:50 The evolution of the historical transport networks of the Iberian Peninsula by Pau de Soto 09:50-10:10 Using Geographical Text Analysis to Understand the representation of poverty in UK newspapers by Ian Gregory and Laura Patterson 10:10-10:30 Supporting Diversity in the Spatial Humanities: the Pelagios Association by Leif Isaksen, Elton Barker, Rebecca Kahn, Valeria Vitale and Rainer Simon 10:30-10:50 Coffee Break 10:50-11:10 Narrative Maps in History by Tiago Gil 11:10-11:30 Creative Experiments into the Spatial Visualisation of Literary Texts by Andrew Richardson 11:30-11:50 Footprints in Spatial Narratives: Reading at the Limits of Digital Literary Mapping by Joanna Taylor and Christopher Donaldson 11:50-12:10 Coffee Break 12:10-13:00 Reflection: The complexities of the Spatial Humanities and its future by all the panel, guided by Michael Page, Patricia Murrieta-Flores, and Carmen Brando Speakers abstracts The impact of semi-automatic detection in archaeological knowledge. A study case and some reflections. Enrique Cerrillo Cuenca Researcher, Institute of History, Centre for Human and Social Sciences Spanish Council for Scientific Research The popularization of GIS represented for many archaeologists a powerful mechanism for accessing the most intricate aspects of past landscapes, in what can be considered a milestone in spatial and landscape Archaeology. Today, the novelties come from the field of remote sensing, where the development of semi-automatic detection of archaeological sites seems to be a groundbreaking approach. Through techniques from the field of computer vision and machine learning is possible to identify areas of archaeological interest among hundreds or even thousands of spatial datasets. This procedure has been named as semi-automatic detection, considering that the ultimate validation of detected areas is only possible through a careful revision by a specialist. A growing bibliography can show the interest of these approaches among archaeologists, but also their potential risks on the archaeological interpretation of landscape. In the present paper, we discuss the impact of these technique on the archaeological knowledge of the Late Prehistory Iberia. An artificial neural networkhas been trained to locate topographical features that match funerary mounds. The challenge is to understand if the current distribution of monuments -mainly build upon the data of the non-systematic archaeological activity from 20th century- can be balanced by an unsupervised detection. We will discuss: 1) the theoretical issues behind the design of the methodology, 2) the design and training of an ANN, 3) the preliminary and expected outcomes and 4) the risks of creating new biases in the data. We are also interested in the quality of these type of data and its possible impact on the creation of new knowledge, a problem that is common to other fields of Spatial Humanities. The evolution of the historical transport networks of the Iberian Peninsula. Pau de SotoThe configuration of a territory is a long-term evolution in which many factors influenced its shape and morphology. One of the most visible elements that help historians to analyse its configuration is the location of the urban settlements and all the transport infrastructuresthat connected them. In this paper, the Iberian Peninsula transport networks have been analysed in four different timeframes. By comparing the historical transport networks, it is possible to discover the continuities and differences between each period. Those changes let us relate each network with the political and economic situation of the Iberian Peninsula. This information is extremely useful to define the role of the political decisions in the design and organization of this territory and their impact in its economic situation. After almost 2 years of an intense digitization process, we have obtained high detailed historical transport networks from each time period studied. Methodologically, Networks Science analyses have been applied to the historical intermodal transport systems. The results of these mathematical processes let us visualize and understand the morphological configuration of the transport networks and determine distribution and mobility patterns and their connectivity degree, and the changes and continuities between periods. In this paper, the methodology of the project will also be presented, from the creation of a transport model of time and cost expenses using historical information to the application of weighted SNA to analyse the connectivity of each historical networks. Using Geographical Text Analysis to Understand the representation of poverty in UK newspapers Ian Gregory and Laura Paterson Analysing large textual corpora in ways that combine an understanding of the broad patterns within the text, while also preserving the detail and nuance is one of the major challenges facing the digital humanities. Geography represents one way of doing this: the challenge is to understand what is being said about particular places and how this varies from place to place. To do this we bring together methods from corpus linguistics, that allow us to summarise and explore large volumes of text, with geographical information science, that provides the tools to allow us to explore sources geographically. We term his approach Geographical Text Analysis. Quantitative geographies of poverty are well known, however the geographies of the perception of poverty are far less well understood. To explore these questions we make use of two large British newspaper corpora from the period 2010-15. The Guardian gives us the opinions of left-wing broadsheet newspaper, while the Daily Mail provides a contrast from a right-wing tabloid. Together these two newspapers provide us with 380 million words of text from the period. The questions that we explore are: a) what places within the UK do these newspapers associate with poverty; b) what aspects of poverty are they associating with these places; c) how does this vary from place to place, and; d) how the two newspapers differ in their coverage. To answer these we first use corpus linguistics-based approaches to identify a set of search-terms associated with poverty. Having done this, we then use an approach called concordance geoparsing to identify the place-names that occur near to the search terms. These place-names are allocated to a coordinate. Once the geoparsing has taken place, the overall patterns of poverty in the two newspapers can be mapped. From here, a range of GTA techniques, derived from combining approaches from corpus linguistics and GISc can be used to explore these patterns in more detail and to compare them with each other. We will presents the results of this analysis, while also exploring the wider implications of the advantages and limitations of being able to explore the geographies in large corpora in this way. Supporting Diversity in the Spatial Humanities: the Pelagios Association Leif Isaksen, University of Exeter Elton Barker, the Open University Rebecca Kahn, Humboldt Institute for the Internet and SocietyValeria Vitale, Institute of Classical Studies, School of Advanced Study, London Rainer Simon, AIT: Austrian Institute of Technology Since 2011 the Pelagios project has been dedicated to horizon-scanning, mapping and bridging the semantic annotation research landscape in the spatial humanities. It has done so by organising research events, developing software, formulating standards, writing documentation, and supporting new initiatives through an annual programme of small grants. Its community includes content providers, gazetteer maintainers and educators, in addition to more than 3000 registered users of its Recogito annotation platform. Its members vary enormously in the scale of their activities, their motivations and restrictions, their levels of technical literacy, and the domain of their interests across space, time and disciplinary agendas. But at the heart of this endeavour lies a paradox: how can initiatives like Pelagios whose funders expect them to offer unifying guidance and leadership also support diversity and difference? In attempting to classify and coordinate digital ecosystems do we risk constricting or even killing them? This presentation does notattempt provide a solution to this challenge but describes recent and current developments within the Pelagios initiative that are intended to help address it. Foremost among these is establishment of Pelagios as anvoluntary network association guided by a series of six chartered Activities. By clearly specifying the scope of its mission Pelagios is now able to devolve more control to independently managed projects that collectively realise its objectives. In return, these projects are supported in aligning their activities with those of others that can maximise their impact and benefit from their implementation. Three of the Associations Activities are dedicated to the theoretical development and concrete implementation of the production, registration and use of semantic annotations. Three more will support gazetteer production and alignment, education & training, and a small grants programme. Each of these activities is led by two people appointed bi-annually by their relevant stakeholders community, and collectively they will undertake to further a vision of linked open geodata founded on the principles of collaboration, mutual benefit and reducing barriers to entry for data providers and end users alike. The Association is thus open to any individual engaged within the field to help forge its future direction, and we openly invite expressions of interest to join it. With the Associations establishment in early 2019, DH2019 is an ideal opportunity to reflect on the early successes and challenges of this new organisational model as means for balancing diversity and synergy in the Spatial humanities landscape. Narrative Maps in History Tiago Gil University of Brasilia Maps have long been used to represent historical knowledge. Lately, this use has been widely diffused, with the publishing of several new works which emphasize cartography as a language for historians. The approaches are diverse and the research problems addressed to these methodologies are numerous. The purpose of this text is to highlight a specific type of cartographic production in history: the use of narrative maps. Under the expression narrative maps there are at least two possibilities of approach. The first concerns the mapping of narratives, that is, converting texts to maps, something close to Franco Morettis work called Atlas of the European Novel. The possibilities within this perspective are broad, allowing both the spatial representation of a specific work and that of a single variable in a set of works. In this case, the texts are read and, through various procedures, decomposed in the form of maps, from certain variables chosen by the one who analyzes. It is possible to map the course of a certain character in a given source, for instance. The second approach concerns a different purpose: the representation of the results of the historians researches in the form of maps oriented simultaneously in time and space. Although this visualization is commonly associated with economic data, it can also be used to describe and analyze other processes, such as population displacements, political movements, pilgrimages, conflicts, circulation in urban spaces, among others. The paper presents the first steps in this field to indicate the innumerable challenges of cartography applied to historical narrative. Some reference works on the subject have to be highlighted, especially the works of Jacques Bertin and Edward Tufte. Bertin was a geographer and the creator of the Semiology of graphics, through which he sought to create useful and efficient visualizations of data resulting from research. Tufte, a designer with a background in statistics, sought throughout his career to present models of visual representation that came out of what he called flatland, the flat information landscape. His goal had always been the creation of representations that combined multiple variables of analysis with clarity and beauty. There are at least six two-dimensional forms of cartographic representation of narratives: a) series of images, b) sequential arts, c) vector animations, d) small multiples, e) anamorphosis, f) traditional static maps, provided they have features that represent the narrative, usually through arrows, lines or other elements that indicate spatio-temporal dynamics. A conceptual apparatus taken from statistics may be important in understanding the visual processes of narrative mapping. Considering that cartography is a language, we can understand that narrative maps have their own dialect. There are several non-traditional elements to take into account and these elements are quite important: rhythm, speed, linearity, continuity and interpolation. In addition, traditional elements such as colors and shapes acquire a different meaning, a particular accent. These elements may allow for clearer communication in the production of maps, especially animated maps. Creative Experiments into the Spatial Visualisation of Literary Texts Dr Andrew Richardson. University of Northumbria, Newcastle. The field of digital humanities has seen a growth in the usage and range of digital tools and methods for the visualisation of literary texts. Increased availability of creative digital processes, tools and environmentshave opened up opportunities for the creative exploration of new methods, techniques and approaches for literary visualisation within geo-spatial contexts. It has also encouraged the development of new forms of engagement with literary material for academic and non-academic audiences. Using practical investigations as sample case studies, this paper will outline the visual possibilities of creative coding and computational methods for the development of novelgeolocated representations of literary texts. It will examine the opportunities afforded by computational processes for applying the spatial qualities of locational data to the visual representation of the words; outline methods, applications and outcomes applied to specific examples of literary texts; and provide a platform for discussion into current and future roles of creative digital methods and technologies in this field. Project examples will be used to highlight individual investigations into the use of locational data and creative technologies to produce new types of experiential textual encounters in both virtual and physical spatial environments. Applications of Augmented Realitytechnology for mapping and representation of the works of James Hogg in the physical landscape of the Scottish Borders will be used as a basis to discuss ideas around the representation of texts within real spaces and locations. The use of GIS and GPS data to generate geo-spatial visualisations of William Wordsworths The Prelude as a textual landscape of real and imaged pathways will highlight ideas around the use of spatial data as a ground for generating real and imagined literary landscapes. The presentation of digital and interactive visual outcomes from these practical projects will demonstrate some of the possibilities, opportunities and challenges of computational and game technology forlocating texts in digital and physical spaces; outline different types of experiential encounters with text as landscape and text in landscape; and raise questions around the possibilities of using physical spaces as a ground to help generate new fictional textual encounters and landscapes. The work contributes to a broader discussion around the use of computational methods to develop novel geo-spatial techniques and their possible application in wider contexts. The discussion of processes and outcomes in this presentation will be used as a basis to outline current approaches and discuss future possibilities for the application of creative digital technology towards new spatial visualisations and encounters of text in academic and public-facing contexts. Footprints in Spatial Narratives: Reading at the Limits of Digital Literary Mapping Joanna E. Taylor| joanna.taylor@manchester.ac.uk Christopher Donaldson | c.e.donaldson@lancaster.ac.uk and not simply by the fact that this shading of forest cannot show the fragrance of balsam, the gloom of cypresses is what I wish to prove. Eavan Boland, That the Science of Cartography is Limited The message at the heart of Eavan Bolands poem, That the Science of Cartography is Limited, is a straightforward one: that, while maps are good at representing a landscapes facts, they fail to capture the human stories – historical and personal – that imbue a place with meaning. More than this, a map reveals almost nothing about what its like to be in a place: to smell the foliage, to feel the ground underfoot, and to recognise the numberless interactions at both macro and micro scales that have made the place meaningful. Cartography, Bolands poem indicates, is good at representing where something is, but not at showing why it matters. These limitations are exaggerated by digital maps. Notwithstanding attempts to represent digitally the experience of standing in a location, digital maps – like their analogue precursors – cannot comprehend an embodied sense of place. This is largely a problem with the maps most fundamental characteristic: on its own, this kind of visual medium cannot capture the complex and multisensory feeling of being in a particular place at a particular time, and in a particular body. This chapter seeks to demonstrate how a literary spatial narrative might afford new ways of rectifying this limitation. It demonstrates how incorporating embodied data – including heart-rate monitoring and GPS tracks – alongside a literary text in a mapping environment might transform both how we read, and how we understand the role of embodiment in historical and contemporary place-making. To do so, it takes as a case study one particular text: Dorothy Wordsworths epistolary account of her pioneering ascent of Englands highest mountain, Scafell Pike, on October 7 1818. It reads this letter alongside data gathered from a recreation of this walk – precisely 200 years later – by a party of researchers, artists and mountaineers who followed in Wordsworths footsteps. In part, this was a recreation of an important moment in British Romantic literature and mountaineering history. But, as this chapter claims, the recreation was also an opportunity to reflect on the relationship between active reading and digital technologies, wherein the maps created by walking this route might transform the ways we read and respond to the texts the initial ascent inspired. The chapters ultimate claim is that bringing these two types of data – those generated by author and by reader – together can foreground a phenomenology of place that induces new ways of reading both text and map. "
	},
	{
		"id": 270,
		"title": "Charting the Material Development of Newspapers",
		"authors": [
			"Mäkelä, Eetu",
			"Tolonen, Mikko",
			"Kanner, Antti"
		],
		"body": " Introduction The present day transformation of newspapers from print to digital is not the first time they have evolved drastically. Instead, this change of format reminds of similar transformations when the newspaper first appeared as a distinct material genre. One influential definition separating a newspaper from a newsbook or pamphlet in its early days was that a newspaper was a sheet of two or four pages, made up in two or more columns. The Dutch had two-column news at the time, while civil war in Britain saw both the rebels and the crown printing their propaganda. It took, nevertheless, centuries before journalism became a profession of its own and newspapers took their particular shape in the mid-nineteenth century. In the context of digital humanities, newspapers have become an iconic example of big data research. While in localised researchthe material can be thought uniform, in the big data approaches it is striking how little attention is paid to what the data consists of. A telling example of waking up to this is the Oceanic Exchanges projectwhere M.H. Beals and Ryan Cordell quickly concluded that mapping metadata across its many datasets is to be one of its most important contributions. Framed against this background, the idea of this paper is to outline how we developed a tool to uncover and explore the varied materiality of newspapers. As part of the large-scale digitisation, the accessibility of historical newspapers has improved drastically, but at the same time much of the information about the size, shape and feel of the newspapers, that was so central to past readers in understanding what kind of documents they were perusing, has been hidden from view. Interestingly, the digitised versions of the newspapers also allow for large-scale study of their material dimensions – an opportunity that has so far been paid very little attention to. Our analysis of the data shows that the digitised newspapers are by no means uniform in their materiality, and further that changes and aberrations in the material aspects correlate with changes and aberrations in content. Thus, besides allowing to study the material development of newspapers in itself, the information is useful for both the discovery of anomalies as well as partitioning the data into more uniform subsets for content analysis. In short, our argument is that content and form interact, and one cannot be analysed without the other. In this, the paper continues on a path previously charted by for example Moreux, 2016, Tolonen et al. 2018 and Lahti et al. 2019, while providing an orthogonal axis to those expanding study from text to visual elements. In the following, we describe first in concrete terms how we extract and derive materiality-related information the ALTO XML format commonly used for newspaper data. Then, we evaluate the implications of charting the material development of newspapers. Extracting and deriving material aspects from ALTO XML ALTOfiles contain a description of the visual organisation of content on a page, at the core of which are the individual words and their page coordinates. At the same time, the words are also grouped into blocks, often corresponding to paragraphs or columns. The format also contains general layout information, such as the sizes of margins and main printed area. The usefulness of ALTO for analysing materiality crucially depends on the choice of the measurement unit in which all coordinate and size information is given. Unfortunately, many collections such as the Dutch Delpherand French Gallicaprovide their data using pixel coordinates. Luckily, the ALTO files of the National Library of Finlandhad a MeasurementUnit of mm10. Given this, we easily extract page size, printed area and character and words counts for each page. Directly given are also the fonts used. What proved problematic however was that the National Library of Finland had used altogether 22versions of scanning software, some of which did not differentiate between Fraktur and Antiqua. By using metadata to analyse which newspapers were scanned with which version, we determined that trustable font identification could only be had up to the year 1910. For each page, we also extract all text box coordinates. While these are primarily meant to locate text visually on the page in reader interfaces, they can be processed to yield layout information. First, we extract column counts using a lighter-weight process than the computer vision approach used in Buntinx et al., 2017b. We scan the page from top to bottom, for each Y coordinate counting the number of text boxes present there. This yields a distribution associating all column counts with the area they control on the page. ALTO text blocks overlaid on a newspaper page One problem hereis that the text boxes only cover areas with text. This would diminish the number of columns counted for coordinates laying between for example a heading and body text. To counteract this, we expand boxes to meet their neighbours. Another problem is that for tables, usually each column gets counted, sometimes yielding counts of over 40. For this, we have no automated correction. However, in the aggregate, such outliers quickly disappear, while in anomaly detection, they usually are interesting in themselves. Different layouts calculated for the Hufvudstadsbladet newspaper based on paper size and text box coverage. A2 and A1 sizes show two different layouts, necessitating further partitioning. We also keep the text boxes, associated with the number of characters each contains. From these we calculate information density distributions for each page, highlighting the role different parts have in holding headlines, advertisements or body text. This information can also be used to visualise how page layouts have changed in time. Finally, for each issue, we gather page count and date information, from which we derive publication frequency. For analysis, we combine all this with newspaper-level metadata, such as name, publisher, language and place of publication. Materiality explorer interface The materiality explorer After gathering the data, we developed an interface that allows it to be explored. Shown in Figure 3, a first view provides overview graphs that allow tracking material changes on long time scales, both for individual papers as well as for larger groups. Here, the materiality information is contrasted with a baseline measure of text per month. By counting the number of characters each newspaper produces in a month without regard to how they are divided between issues or pages, this measure shows how much content needs to be transmitted. As this quantity rises, newspapers must respond with material innovations, whether by increasing page count, page size or publication frequency, or by cramming more material into available space by decreasing font size, line breaks or margins. A second view allows grouping the data by a combination of material dimensions, thereby allowing exploration of archetypal materiality categories. Finally, two views allow the user to explore page and issue-level material anomalies: for example pages which have more text than usual or pages with abnormal layout, or issues with appendices or which appear on the same day as another issue. Common to all views are selectors allowing limiting the newspapers under comparison. Discussion For the Finnish newspapers, the data shows a general order in how they expanded: first, layout was changed to include more words per page; second, page size was increased; third, publication frequency was increased and only after that was the amount of pages increased. This last step often coincides with the introduction of rotary presses, which allowed newspapers to more easily be composed of more than four pages, and also allowed them to move back from large page sizes to more easily handled formats. Simultaneously, the data shows also high variability, where papers not only frequently printed supplements, but could switch back and forth between formats inside a single week, or cram text into a special issue through diminished line breaks. Similar shifts took place also with regard to fonts. Newspapers explored different Fraktur and Antiqua fonts to try out readability, but also because fonts were oftentimes used to signal that the contents was aimed for a particular audience. While there are plenty of exceptions to this, it seems that Fraktur was more often used when dealing with economy and religion, whereas Antiqua was reserved to politics, philosophy and the high arts. To test such hypotheses, we still need more robust statistical information. We also aim to compare fonts used with other factors, such as language frequency and size of newspapers.. Compared to earlier studies, our data driven approach gives us a great opportunity to evaluate the main findings of earlier historical studies of newspaper materiality. What we also aim to do with these patterns is to develop evidence-based archetypal categories of newspapers across history. We are then able to trace and compare these through time and place, but also use them to study the evolution of individual newspapers. These categories will also help us understand the newspapers as objects of intellectual activity, creating a theory of different historical maturity levels of newspapers. This in turn will help us chart the development of public discourse over time. Besides using the data to study the material development of newspapers as a genre in itself, we argue that content and form interact, and thus big data approaches to newspaper analyses also need to pay attention to material differences in order to accurately understand the subdivisions in large corpora.. For example, using the metadata we can create meaningful subsets of the data that are balanced by paper type for for example topic modelling or teaching automated transcription algorithms. Here, Finland makes an intriguing case for digital history because its public sphere is bilingual, with newspapers in both Swedish and Finnish. One interesting phenomena that arises from this are publishers publishing newspapers in both languages. For example, in Kotka there are both Finnish and Swedish newspapers by the same publisher with identical layouts and advertisements. Such could be used to create parallel corpora, interesting for the study of commonalities and differences between the different language public spheres, but also perhaps as material for machine translation. "
	},
	{
		"id": 271,
		"title": "DHmine: an Open Source Cloud-based Framework for DH Research",
		"authors": [
			"Mészáros, Tamás"
		],
		"body": " A couple of years ago our institutions started a DH project to process and study a large 18th century text corpora. During this project we faced many challenges like text digitalization and encoding, creating a large authors dictionary, processing critical annotations, storing available texts and data, providing search, retrieval and display functions, and finally performing various data analysis experiments. We applied and also developed many kinds of software to cope with these challenges. The result of this multi-year system development is the DHmine toolkit, a collection of software tools for DH projects. During its development we characterized the following requirements: it should support team cooperation, it should be easy to deploy and maintain in a shared environment, and it should be flexible and scalable for various problem complexities and data set sizes. In order to meet these requirements we decided to explore the capabilities of cloud computing that provides convenient, on-demand, network-based access to shared resources and tools. Cloud services can be deployed and reconfigured rapidly to meet the changing needs of their users, and they can be maintained with much less effort than traditional desktop applications. The DHmine toolkit is a set of cloud-based tools that provides many kinds of services for DH researchers. Its data storage solutions include a cloud-based file store based on the popular Nextcloud software, a noSQL database enginethat can hold unstructured data, a MySQL component to store relational data, and finally, and RDF tripestore based on RDF4J to store relational and factual knowledge. We extended these storage services with autonomous software toolsthat perform various tasks on demand, for example OCR, TEI encoding, document conversions and analysis, entity recognition and others. There are two statistical tools included in the system: RStudio that is a programmable environment for document and data analysis, and a stylometry tool called Shtylothat provides an easy-to-use Web interface to the popular stylo R package. All these componentsare integrated into a unified, cloud-based execution environment. There are two main interfaces to access the services of the DHmine toolkit. On the one hand there is a Web-based user interface that provides access to the individual toolsand it also includes a programmable environment to develop data import, retrieval and visualization tools that meet the needs of the individual research projects. On the other hand there is a programmers interfacefor external applications to access system functions like corpus creation and maintenance, accessing data in the relational database and in the knowledge-store. This makes easy to integrate the services of the cloud-based system into various desktop applications. In order to simplify the management of the DHmine toolkit a Docker-based virtual machine environment was developed. This deploys the components into their individual containers and it automatically reconfigures the external Web interface according to the actually running services. Adding or removing a service is as simple as running a command to enable or disable a container. The DHmine toolkit can be installed by cloning its git repository and running a simple text-based tool to select the required services. The DHmine system was extensively used to create the Digital Mikes-Dictionaryand Corpus, and critical annotations and related knowledge entries retrieved from DBpedia. Since the tools became available other research projects were also started developing their own solutions based on the toolkit. These include processing a large bilingual literary correspondence from the twentieth century, and analyzing of family networks from the 17th and 18th centuries. During our presentation we can illustrate how these tools work and it would be a great opportunity to help other researchers to take the first steps to install and tailor this open source software for their purposes. Acknowledgement The research has been supported by the European Union, co-financed by the European Social Fund. "
	},
	{
		"id": 272,
		"title": "A Collaborative System for Digital Research Environment via IIIF",
		"authors": [
			"Nagasaki, Kiyonori",
			"Muller, A. Charles",
			"Tomabechi, Toru",
			"Shimoda, Masahiro"
		],
		"body": " Introduction It is important for researchers not only of humanities but also digital humanitiesto observe primary resources or evidences to ensure the reliability of their research. Many cultural institutions such as research libraries and museums have provided their cultural properties in the form of digital images on their Web sites, but researchers have not efficiently taken good advantage of them until recently because the images have been kept behind their luxury Web interfaces. However, the situation has rapidly been improved due to the spread of the IIIF. [1] Due to the specifications of IIIF, researchers have been able to produce various solutions for their research with digital methods. In this situation, the authors will present an attempt of a collaborative environment leveraging IIIF-compliant digital images for the DH. The situation of digital resources Some institutions have a team of capable specialists which ensures that their digital collections will be available for researchers. However, as many institutions do not have such specialists, their digital collections often end up being released without sufficient metadata and descriptions. Moreover, in both cases, it isnt easy for researchers to find useful resources among the individual silo-like Web sites. while integrated search systems like Europeana and DPLA are offered. Researchers must often seek for a series of digital images of a work in digital collections on many Web sites with various interfaces. Hence the authors of this presentation have developed and managed an integrated collaborative system to gather IIIF-compliant digital resources for a restricted field to solve the situation. The systems: IIIF-BS and J-IIIF The system consists of open source software including CentOS7, Apache httpd, PostgreSQL, Apache Solr, and so on. As examples of the targeted fields, we adopted resources of Japanese studies and Buddhist studies which are included partially in many digital collections around the world such as Gallica and Bayerische Staatsbibliothek. The system offers a collaborative environment to aggregate IIIF-compliant digital resources so that users can search and leverage them through it. When an authorized user inputs a IIIF Manifest URI, the URI and its metadata are inserted into the databaseand the search software. After that, any user can search, browse, annotate, and crop the IIIF contents on the Web site with the included IIIF viewers. The system for Buddhist Studies called IIIF-BSincludes 6846 items from 24 institutions added by researchers of Buddhist studies. For Japanese Studies called J-IIIF includes 193 items added by participants of a series of IIIF workshops in Japan. As the idea of the J-IIIF has recently been updated to the other system, it will be used only for the workshops. The IIIF-BS has continuously been developed further in order to improve the digital research environment for Buddhist studies.IIIF-BS provides a function to add some specific metadata to the registered IIIF Manifest URIs so that the resources can be leveraged with a de-facto method in the field.The method means that readers can find a certain text in the scriptures by one or a pair of unique line numbers which are embedded in most of the major scriptures of Buddhism [2]. Then, digital images of fragmented manuscripts included in large digital collections can be identified and collated on this system. IIIF-BS also provides JSON-style data including the IIIF Manifest URI and collaboratively added metadata like start and end line numbers of the textual images by use of certain URLs.The current achievements of the system Currently, IIIF-BS has achieved two things:an integrated viewing of the digital textual images andoffering detailed metadata to a university library. The former is that of an integrated Buddhist text database, SAT2018. Here we show a table of the images of each scripture by use of the JSON data on-the-fly.The latter is that of Kyoto University Library which releases several digital collections including Buddhist scriptures with little or no metadata adopted from the detailed information embedded by IIIF-BS.Conclusion As this case shows, while IIIF is a general technology, specific function for a certain field is efficient and useful not only for researchers but also for cultural institutions as providers of research resources. The authors will seek for ways to make such functions more general through both aspects of the system and the IIIF specification itself. "
	},
	{
		"id": 273,
		"title": "The Birth of Boston: Reconstructing Boston’s Social History in 1648",
		"authors": [
			"Nebiolo, Molly E."
		],
		"body": " With the earliest printed map of Boston appearing only in 1722, visualizing the early history of the city has been a challenge for centuries. The collaborative project,  The Birth of Boston  is an online GIS map of seventeenth-century Bostons residents and major civic infrastructure. This project extends the work of Samuel Chester Clough, an early-twentieth-century cartographer who spent the last decades of his life producing an unpublished atlas of Bostons residents before 1800, and Annie Haven Thwing, an amateur historian who produced almost fifty thousand index cards with the demographic, social, and legal histories of early Bostonians. The creation of this project stemmed from the lack of resources on Bostons colonial history for undergraduate students to interact with during an early American history course last fall. Christopher Parsons, the creator of this project, began researching the presence of pre-Revolutionary Boston history at the Massachusetts Historical Society and came across the rich collections of Clough and Thwing. With Nebiolos experience in the graduate certificate program for DH at Northeastern, and from funding through the NULab for Texts, Maps, and Networks and the Boston Research Center, we were able to generate a preliminary site depicting Bostons social and spatial history in 1648. Yet The Birth of Boston is just in its early stages of digital work. Cloughs papers include maps of the entire city with numbered parcels and ownership information for the additional years of 1633, 1638, 1676, 1722, and 1798. Our hope is to digitize the 1633 and 1638 maps and use the online versions of the others to provide a digital time-lapse of Bostons early urban history, with the potential to extend the project into the 19 th century. By presenting the work at DH2019, we hope to discuss with attendees how to best standardize the data and presentation of the project as it grows over the next few years, and whether our creation of the tool is useful and engaging. The project provides space at the conference for debate around best practices for digital colonial map projects and pedagogy for seventeenth century data resources like this one. The poster is organized around the key questions, decisions, and objectives made for the creation of The Birth of Boston 1648 map. Some of these questions include: What exists on Bostons early colonial history? How can this history be best presented for an audience? What platforms should be used? How should we present that data? How can we incorporate the histories of other populations that were present in Boston, such as indigenous communities, slaves, and freed slaves? Information on the dataset produced from the citizen data in the Thwing collection are present on the poster. We will also provide visitors the opportunity to interact with the map with a laptop. The overall goals of The Birth of Boston poster presentation is to provide another example of how the field of early modern history can benefit from the methods of DH and how it can strengthen projects that focus on producing an educational, historical narrative for a public audience. "
	},
	{
		"id": 274,
		"title": "Sustaining the Musical Competitions Database: a TOSCA-based Approach to Application Preservation in the Digital Humanities",
		"authors": [
			"Neuefeind, Claes",
			"Schildkamp, Philip",
			"Mathiak, Brigitte",
			"Marčić, Aleksander",
			"Hentschel, Frank",
			"Harzenetter, Lukas",
			"Breitenbücher, Uwe",
			"Barzen, Johanna",
			"Leymann, Frank"
		],
		"body": " Introduction Within the Digital Humanities, research applications such as databases, digital editions, interactive visualizations, and virtual research environments play a central role in securing and presenting research results [16]. Often, such living systems [15] are the actual bearers of information content, thus representing the added value of the scientific output [16]. However, within the DH a great number of smaller, highly heterogeneous software solutions are produced, which all are subject to the problem of software aging [14]. Against this background, institutions like the Data Center for the Humanities at the University of Cologneface the challenge of preserving an unknown, potentially unlimited number of research software systems to assure their availability on a permanent basis. While there are well-established methods of preserving primary research data, e.g. in existing data repositories and archives, living systems are part of a constantly changing digital ecosystem and must regularly adapt to it, e. g. they needupdates. However, due to their steadily increasing number and their heterogeneity, permanent maintenance, support and provisioning of such living systems is a major technical, organizational, and therefore ultimately financial challenge. This contribution presents an approach to the preservation of web-based research applications in the DH, based on the Topology and Orchestration Specification for Cloud Applications[11, 12, 13]. TOSCA is an OASIS standard for modeling, provisioning, and managing cloud applications in a standardized and provider-independent way. The TOSCA standard aims at providing a superset of service modeling and orchestration features and can thus be seen as a meta-framework that includes vendor and domain specific solutions like e. g. Docker, OpenStack or VSphere. In the following, we focus on an exemplary use case to describe the main concepts of our approach. The Musical Competitions Database The DFG-funded project Musical Competitions between 1820 and 1870 is conducted by the Department of Musicology at the University of Cologne in cooperation with the Cologne Center for eHumanities. The aim of the project is to gather comprehensive information about music related competitions from 1820 to 1870 [6]. Data is extracted by musicologists from music-related journals and stored as JSON files in a document-oriented database. Access to the data is given through a web application written in React. Further, Elasticsearch is used to provide advanced options for querying/filtering and analysis of the data. At the time of writing, the database features information on approximately 1300 musical competitions, 1000 corporations and 3100 persons related to those competitions. The Musical Competitions Database contains and presents a unique data set relevant to the musicology community. To allow for reproducibility in the sense of good scientific practice, a sustainability strategy to keep this data accessible on a permanent basis must include the web application itself, because the separation and archiving of the primary data alone would inevitably lead to a loss of functionality. TOSCA and OpenTOSCA Technological basis of our approach is the OASIS standard TOSCA [11, 12, 13]. TOSCA allows for a portable description of IT systems to automate their provisioning and management. In TOSCA, a cloud application or service [9] is modeled as a Service Template. Inside a Service Template, the Topology Template describes the services topology as a directed multigraph, consisting of Node Templates and Relationship Templates that specify the edges between the nodes. Thus, this enables to describe arbitrary deployments in the form of declarative deployment models [5]. Underneath, TOSCA employs a type system defining common properties and attributes in Node Types and Relationship Types, respectively. To automatically deploy, provision and manage the modeled service, TOSCA defines an archive format called Cloud Service Archivewhich contains the Service Template, including all Node Types and Relationship Types, as well as all required software artifacts, scripts, and binaries needed for provisioning. Moreover, imperative management plans can be added to CSARs, which enables the implementation of arbitrary kinds of management functionality in an automatically executable manner. These plans can be implemented using standardized workflow languages such as BPEL or BPMN, or domain-specific modeling extensions such as BPMN4TOSCA [7]. Any TOSCA runtime environment can consume such a CSAR to automatically deploy and instantiate the enclosed application [2]. In a series of projects, the Institute for Architecture of Application Systemsat the University of Stuttgart has developed the OpenTOSCA ecosystem, an open source implementation for the TOSCA standard. OpenTOSCA includesthe graphical modeling tool Winery for the creation of TOSCA-based application models [8],the runtime environment OpenTOSCA container for automated provisioning and management of the modeled applications [1], andthe self-service portal Vinothek [4], which lists all applications installed in the OpenTOSCA container and serves as a graphical user interface. We believe that the TOSCA standard is generally suitable for assuring the digital sustainability of research results, as research applications, which are packaged in CSARs, can be executed years later by a TOSCA-compliant runtime environment [3]. A TOSCA Model for the Musical Competitions Database In the following, we describe an application model for the Musical Competitions Database to exemplify some of the basic concepts ofTOSCA. The application is composed of a CouchDB and Elasticsearch instance and accessed through a React frontend. The resulting TOSCA-compliant topology model is depicted in figure 1. Screenshot of OpenTOSCAs modeling tool Winery, showing the topology of the use case application. On the left side, the available components are listed. The topology consists of a React web application hosted on an Apache web server, which itself is hosted on a Docker container, where the container operation system can be passed as a Docker image identifier. Additionally, the React application connects to a CouchDB database and to Elasticsearch. To accommodate Elasticsearchs dependency on Java, an OpenJDK has to be available. Therefore, seven different node types, namely React, Apache, CouchDB, Elasticsearch, OpenJDK, DockerContainer and DockerEngine, as well as the HostedOn, the ConnectsTo and the DependsOn relationship types must be available. A TOSCA Service Template describing this application will contain those seven node templates and three relationship templates – where each template is an instance of the respective type definition. The resulting service template can then be packed in a CSAR which may be instantiated by any TOSCA runtime or to be archived in a repository. As the TOSCA standardthrives on being vendor-independent, the topology root depicted in figure 1, namely DockerEngine and DockerContainer, may be substituted for e.g. OpenStack or VSphere and their respective VM/container representations. Summary and Outlook The concepts described above emerged from SustainLife [10], a DFG-funded joint project of the DCH Cologne and the IAAS Stuttgart. The overall objective is to develop generic solutions for standards-based operation and maintenance of DH-applications and to implement them in a way that they find practical application, e.g. in humanities data centers like the DCH. As TOSCA depends on a generic type system enabling the reuse of recurring components, we work towards providing a set of typical system components. Examples for components, which were identified in further use cases [10] and modeled in TOSCA are Java runtime environments, the Spring framework and several types of databases like MySQL, mongoDB and eXist-db. In addition, reusable Service Templates reflecting typical software stacks are under development. For example, a common pattern for web applications is the so-called LAMP-stack, composed of a Linux operating system, an Apache web server, a MySQL/MariaDB database and a PHP/Perl/Python interpreter. These components can be reused are intended to simplify future application modelling, development and maintenance using TOSCA and the OpenTOSCA ecosystem. Beyond that, a number of further extensions of the OpenTOSCA ecosystem are in the scope of the SustainLife project. For example, applications that are archived in CSARs need to be deployable several years after their development. Therefore, approaches to freeze and defrost whole applications and their respective execution states are also part of our research. This includes the possibility to version TOSCA models, to reflect the fact that living systems are subject to constant changes. Another desideratum is to add the possibility to update a services components. If a component must be exchanged because of security issues or deprecation, the CSAR may no longer be deployable. We therefore work on additional management functionalities which provide standardized operating and maintenance solutions, e.g. applying updates or software patches. With our approach we expect to reduce maintenance costs significantly and will evaluate this expectation on the basis of selected use cases. Findings and best practices are prepared in a way that they can be transferred to partners and are communicated to the scientific community through workshops and publications. Thus, with this contribution, we want to trigger a discussion about the applicability of methods and technologies of professional cloud deployment and provisioning strategies to problems of long-term availability of research software in the DH-community. Acknowledgements This work is partially funded by the Deutsche Forschungsgemeinschaft. Project title: SustainLife – Erhalt lebender, digitaler Systeme für die Geisteswissenschaften. "
	},
	{
		"id": 275,
		"title": "Project Endings: Early Impressions From Our Recent Survey On Project Longevity In DH",
		"authors": [
			"Arneil, Stewart",
			"Holmes, Martin",
			"Newton, Greg"
		],
		"body": " Despite the thousands of digital projects launched during the past 20 years, experts warn of a new digital dark ageas our ability to produce digital information continues to outpace our capacity to preserve and access that knowledge for the long term, evenwhen using content management systems. Project Endings is a collaboration between the Humanities Faculty and the University Library which aims to provide practical solutions to issues attendant on ending a project and archiving the digital products of research, including not only data but also interactive applications and web-based publications. Project Endings endeavours to align the aims of faculty researchers producing projects and the archivists who will eventually be responsible for curating their work. The project divides digital projects into five primary components: data, products, processing, documentation, and release management. We aim at longevity primarily for data and products, but believe that this goal requires careful attention to processing, documentation and release management. We are developing preservation principles for all of these factors, using practice-based methods, diagnostic tools, and scholarly research as listed in the project bibliography at . The project conducted a survey on the LimeSurvey platform consisting of 30 questions to discover how project leaders dealt with the issues of long-term sustainability for each of the five primary components. We promoted the survey to Canadian and international professional communities and received 128 responses. 25 detailed interviews were run with a sample of the respondents to get more information on the issues raised by the survey results. Results of the survey show that concerns about longevity for digital humanities projects are not exaggerated. 57% of survey respondents did not consider an endpoint for their project, despite the fact that project management principles include declarations of goals, timelines, and milestones. In the light of this, perhaps it is not surprising that 54% did not have long-term preservation plans. These findings suggest that many researchers do not distinguish between products generated to exploit the features of the processing environment and products generated to survive after active work on the project ends or independent of development work in the project. Furthermore, only 32% considered benchmarks for assessing progress and 41% included precise timelines in their plans. In a group of projects that were for the most partless than 10 years old and 58% still in progress, 22% reported that project outputs stopped working due to software obsolescence. This is in a field of projects in which 74% started with born-digital data. If a failure occurs during the active life of the project it might be repairable, but repair is much less likely if the project has ended. The value of using a standardized data model is not universally recognized, with 14% of survey respondents not using one at all and 26% making up their own. Although a home-made data model is by definition not standardized, it may still be viable for a long time if well documented. 60% claimed to have a clearly documented data model, but 90% of those that had documentation considered it to be partial or inadequate, so it appears that a projects data model is well documented in only about 50% of cases. HTML is the most popular standard output for DH projects, despite the continued popularity of PDF, XML, and various binary media formats. Javascript is considered by manyto be a major technology in their project. HTML and Javascript are robust long-term, but if they are produced in a project only on-the-fly by a content management systemor database, then the longevity of the output is dependent on that of the CMS or database. 34% of the respondents used WordPress or Drupal, 31% used PHP/SQL databases, 38% used XML/XSLT/XQuery systems, and 41% used other software services and libraries. Some projects used more than one of these. Lack of ongoing funding was cited by 38% of respondents as the main obstacle to long term preservation. Perhaps more surprisingly, 33% of respondents rated either lack of expertise or bad technology choices as their main obstacle, which may explain the results reported above regarding software obsolescence. Early results from the interviews suggest that CMS and other software libraries and services are the likeliest sources of software failure over time. We hope that further analysis of the interviews will tell us whether a more expert assessment of software and output choices would have mitigated the issue of lack of ongoing funding. While a reassuringly high 42% of respondents reported that university services were responsible for long-term maintenance of the projects work, an alarming 45% reported that this responsibility fell to the Principal Investigator or nobody, demonstrating either significant vulnerability or great confidence. Our survey results suggest that there is a limited use of project managementand software lifespan principles in DH projects. Results further suggest that there is a need for an improved understanding by researchers of specific attributes of a project which are likely to facilitate long-term viability of the project data, outputs and documentation at minimal cost for those charged with preservation. Blurring the lines between data, processing, outputs and the management of those components over time can result in vulnerabilities for long term preservability which may not be apparent until it is too late. With all of this in mind Project Endings is working on a suite of recommendations that will provide guidance on project structure and management with long term viability as the goal. We are offering an online interactive questionnaire that assesses the long-term viability of each component in a project and provides recommendations for improving the prospects for long-term survival. Behind each question is the empirical evidence provided by survey/interview participants as well as the combined experience of the Project Endings team. The questionnaire is intended primarily to be a thought-provoking activity for project leaders and principal investigators. An early draft of the questionnaire is available at . "
	},
	{
		"id": 276,
		"title": "Digital Humanities Services in Academic Libraries: A Design Thinking Approach to Center Inclusive and Anti-Oppressive Approaches for DH Services",
		"authors": [
			"Nichols, Jennifer",
			"Wallace, Niamh"
		],
		"body": " Goal/Overview: This workshop aims to critically situate DH and academic librarianship within a design thinking framework; attendees will be led through equity-centered design thinking strategies that explore empathy building and critically work through the practice of defining problems; attendees will leave with practical strategies to systemically center inclusive and anti-oppressive within their own local contexts. Audience: Academic librarians and information professionals who want/are supporting digital humanities efforts in their own library/local context, but are lacking equitable and anti-oppressive strategies. This includes library school/iSchool faculty who train librarians. All levels of experience and technical expertise are encouraged to attend. No special software or hardware is required to attend. Intended length : 4 hoursIntroduction and overview - 30 mins Empathy and cultural norms - 30 mins Defining problems - 30 mins Break - 10 mins Topic and team selections -15 mins Ideation session - 60 mins Break - 10 mins Share-out - 40 mins Wrap-up and next steps - 15 minutes Presenters: Jennifer Nichols is an Assistant Librarian and Director of the Catalyst Studios at the University of Arizona Libraries in Tucson, Arizona. She led the growth and development of the iSpace, the first publicly accessible and interdisciplinary makerspace in the Albert B. Weaver Science-Engineering Library. Prior to this role, she was the Digital Scholarship Librarian, and coordinated training and digital scholarship support services for campus and the local community, including the successful annual UA Womens Hackathon and Research Bazaar. Her current research centers around designing for equity and inclusion in digital spaces, library services, and makerspaces. She has presented workshops and papers about makerspaces and digital scholarship practices at the Southwest Popular and American Culture Association, the American Library Association Annual Conference, the Association for College and Research Libraries Conference, the Digital Library Federation Forum, and the Coalition for Networked Information. Niamh Wallace is a social sciences research and instruction librarian at the University of Arizona Libraries. She has published and presented on affordable course content initiatives, the changing research needs of students and faculty, and library support for digital scholarship in the social sciences and humanities. The problems this workshop address include the following: Academic librarianship has predominantly been a white space or domain, and DH is no different. How then do we negotiate supporting equitable, safe, and supportive DH initiatives and academic librarianship when both domains are fraught with complex systems complicated by historically oppressive structures? How do we ensure that we do not pass on ideologically problematic approaches to knowledge acquisition and meaning making? Academic librarians roles are growing increasingly complex. In addition to providing traditional reference services and research consultations, academic librarians are tasked with supporting/building out highly complex, computational, DH projects. Librarians often enter these positions without formal training, and typically must obtain these skills on the job or during their personal time. Academic libraries have been increasingly adding digital humanities/digital scholarship support over the past decade, with a sharp increase in the last five years. The definition of Digital Scholarship Librarian varies widely, and there is yet to be a shared understanding of what the portfolio of a Digital Scholarship librarian should look like. Is it to train humanist scholars to be more technically proficient? Is it to build their projects for them? Is it to simply show them what is possible? Or merely host and preserve their projects? MLS curricula across the United States does not adequately reflect the changing landscape of academic libraries where DH support/services are increasingly located. It is a pipeline issue that further perpetuates and exacerbates the already existing burdens on librarians to develop more skills on the job. Responding to Hutner and Mahameds call for a New Deal in their book A New Deal for the Humanities, and specifically Nowviskies chapter Graduate Training for a Digital and Public Humanities , we propose what academic libraries can do, not just to augment the potential for the already-enrolled, participating graduate students in Humanities, but for the pipeline that feeds it. As she, and others argue, the pipeline problems are very much our problem too, and by concentrating on training and supporting undergraduates in cross-disciplinary ways, we not only build computational literacy for humanists, but simultaneously build greater potential for more future graduate students. This workshop will use tools created to cultivate anti-oppressive, equity-centered learning environments, including the tools and resources listed below: Equity-Centered Community Design Equity Design Collaborative Liberatory design toolkit Equity-Centered Design Thinking "
	},
	{
		"id": 277,
		"title": "3D Scholarly Digital Editions: Requirements And Challenges For New Publication Models",
		"authors": [
			"Nieves, Angel David",
			"Schreibman, Susan",
			"Papadopoulos, Costas",
			"Snyder, Lisa"
		],
		"body": " Digital and interactive forms of scholarship challenge established practices in the Arts & Humanities. Audiovisual content, graphic interfaces, and different types of visualizations employed in new forms of presentation and publicationdo not conform with existing concepts of scholarship, and established practices of evaluation. Bringing together 3D model makingand the work of digital scholarly editionsis in fact new, and not a series of scholarly theories and practices that have been previously deployed in the digital humanities. Although these digital vehicles powerfully disseminate and engage with scholarship, scholars who implement these new modalities are confronted by the same old, established mechanisms with which to gauge impact among ones scholarly peers, i.e. within traditional and long-established publications. Today, there exist few scholars who have combined expertise in both content and the use of 3D technologies. 3D digital scholarship may have been ignored, in part because scholars are unfamiliar with available technologies and often feel uncomfortable evaluating such. With little to no experience with 3D digital scholarship, peer-review among senior academics is especially difficult. Therefore, much highly original and innovative research and scholarship is ignored in the absence of evaluations and metrics for digital scholarship – for instance, when awarding tenured posts or when evaluating research outputs and impact. This panel discusses a particular form of interactive scholarship, 3D visualisation, that despite its long tradition in humanities research, is still faced with scepticism and hesitation, not only because of the constant technological shifts and exigencies and the fragile ecosystem within which projects are being developed, but also due to their non-conventional nature that does not adhere to established academic practices and metrics. The proposed panel, consisting of scholars working on 3D computer graphics and Digital Scholarly Editions, explores 3D Scholarly Editionsas a means to approach 3D modelling and reconstruction as scholarship. 3D Scholarly Editions areconstructions that include robust contextual information, metadata, and paradata either in the form of in-world textual or multimodal annotations and supplementary side sources. We would argue that the annotation needs to take advantage of the affordances of the medium to be truly effective when providing access to alternative versions of aconstruction and background information about the world being modelled, or when providing access to the creation process. This type draws from the theory and practice of Digital Scholarly Editingthat, in turn, draws from a long history of practice in editing texts for print. Paper I 3D as a Research Practice: Opportunities, Affordances, Challenges Literature and Art: Since the mid-80s, 3D modelling and visualisations have become a substantial research practice in digital history, archaeology, and heritage. These range from schematicconstructions of buildings to photorealistic renderings and spatiotemporal simulations of ancient structures. Standalone, offline 3D reconstructions have been used as analytical tools to investigate properties of past environments, providing metric data that can inform the interpretive process. For example, visibility and lighting analysis in 3D spaces has provided a new way of analysing space, as well as discussing and understanding social patterns and socio-symbolic meanings. Offline 3D modelling projects have a long and more consistent tradition as opposed to online virtual worlds, that were mainly developed during the years that Second Life and Open Simulator were at their peak. Both platforms democratised the process of creating virtual worlds as they did not require highly advanced 3D modelling skills, while enabling a novel way of experiencing history/heritage and working in an online environment for pedagogical and research purposes, unlike anything previously available. Both offline and online 3D visualisation projects have generated debate over their photorealistic and deceptive nature, as well as lengthy discussions about transparency and ambiguity. Proof-of-concept theoretical frameworks and implementations, such as the London Charter, Seville Principles, and the Epochs Interpretation Management, have been developed to demonstrate intellectual rigour in 3D models and explicate decision-making in the process of their creation. Changes in software, hardware, operating systems, and the Internet itself have also generated debate over the value of virtual 3D worlds as scholarship. Most of the early projects developed in Second Life and/or Unity 3D are not accessible anymore, thus making it difficult for new projects to learn from previous work, especially given that their experiential, spatiotemporal nature is only accessible via conventional, static, and two-dimensional images. The instability of the research environment is often a deterrent to researchers working with such methods, since even when virtual worlds can be ported from one framework to another, this involves significant costs, downsampling and even rebuilding models, and repurposing content that cannot be fully migrated. Online and interactive scholarship, even text-based, has always faced technological shifts and exigencies; however, even in such a fragile ecosystem, changes, successes, and failures enable alternative forms of research, inform the interpretive process, and assist knowledge production. This paper explores humanities research undertaken in 3D computer graphics discussing the affordances of the medium, as well as the opportunities and challenges that researchers face, paving the way for new and alternative forms of methodological and theoretical frameworks for 3D scholarship. Paper II A rationale for 3D Editions within the practice of textual scholarship Literature and Art: The nature, functionality, and theories informing Digital Scholarly Editionshave flourished over the past three decades, moving beyond how to represent print-based texts in digital forms into new types of knowledge production and dissemination informed by the affordances of the medium. In Derridean terms, these editions function more like an archive or knowledge site, a space for gathering together texts on a specific theme, individual, or topic, expanding the world of its creation into an archontic space for inscription and investigation. Moreover, these sites need not privilege the alphanumeric. As early as 1999, when the first digital archives were being published, McKenzie described the range of objects that could be open to the kinds of intensive bibliographical study that textual scholars had traditionally reserved for print and manuscript traditions, as verbal, visual, oral and numeric data. Conceiving three-dimensionalconstructions as Digital Scholarly Editions might seem anti-intuitive, at first glance. After all, the technologies, methodologies, and theories that have informed the creation of DSEs – everything from TEI/XML, documentary vs critical editions, and relational vs XML-aware databases to more recent discussions of how the texts created in DSEs can be repurposed, remodelled, and algorithmically analysed and visualized – share little with the technologies, methodologies, and theories that have informed scholarship expressed through 3Dconstructions. We argue in this panel, however, that 3Dconstructions, can, in the McKenziean sense, serve as the primary text of a digital scholarly edition. As argued in the previous paper, these include, broadly speaking, cultural heritage visualisations and simulations. What distinguishes 3Dconstructions as editions from the use of 3D technology for gaming, documentation of evidence, or sites for analysis, is that the 3Dconstruction is embedded in a knowledge site, where evidenceis gathered in the form of annotation and apparatus. As knowledge sites, they encompass within the same computational paradigm both the primary textand the evidence that informed the decisions in creating the text, thus providing the community which it serves a tool for prying problems apart and opening up a new space for the extension of learning. This framework provides the information structures and evidence that make up the edition so that the audience can understand the process behind the creation of the edition, and adjudicate its authenticity and reliability. Paper III Delineating &Constructing New Modes of Digital Scholarly Editing w/3D Modelling: A Look at Annotation, Ambiguity, and Transparency History: Over the past year, funded by a grant from both the National Historical Publications and Records Commissionand the Andrew W. Mellon Foundation, a team of sixteendigital humanists – including faculty, librarians, technologists and university publishers – has been working to bring together 3D modelling and digital editions in an effort to advance scholarly publishing practices. As part of this work, this team has reimagined how a scholarly digital edition might include 3D models in the form of annotation and apparatus. Four scholars, two working in the ancient world, and two others on sites/locations dating from the twentieth-century, have developed working prototypes for a more robust and sustainable digital platform that could include 3D models of cultural heritage objects, or entire virtual worlds of historical reconstructions within the framework of digital scholarly editions. However, bringing together these two areas of digital scholarship with their own unique histories and practices has required a re-examination of the methods deployed in digital scholarly editions including annotation, ambiguity, and transparency. Interestingly, scholars in textual editing and 3D re-construction share similar challenges in modelling the textual and material record. One of the more difficult areas of overlap in both fields is how to represent temporal and spatial concerns when the evidence is scant or imperfect and otherwise requires evaluating the evidentiary record. This paper will look at the ways in which the traditional apparatus in digital scholarly editing aligns with 3D modelling andconstruction to enhance methods in annotation, ambiguity, and transparency. Each of the four 3D scholarly editionscomprising parts of the year-long planning grant provides examples as to how these overlaps in knowledge production have been addressed and helps illustrate a process for embarking on this new area in digital humanities scholarship. The four-3DSEs include: 3D Saqqara Project; Contested Memories: The Battle of Mount Street BridgeProject; Apartheid HeritageProject; and Neolithic House Project. Annotation, as some have argued, might provide opportunities for enabling new forms of structured data permitting readers/users to arrive at their own interpretation of the text. Deep explanatory annotations could also provide ways of addressing how decisions are made by both the editor and the model maker in 3D scholarly editions. Annotations are a way of reconstructing the historical context of a work, and, in turn, a way to address the subjectivity of the editor in that reconstruction process. Scholars involved in 3Dconstructions can learn a great deal from the series of widely accepted practices deployed by digital edition editors, while also providing new ways of addressing some of the more contested aspects of scholarly interpretation, how to reconcile with the many facsimiles of the various witnesses in an edition, or how the reader/user might engage their own interpretation of the materials. Interestingly, three of the four 3DSEs are now coalescing around an agreed-upon framework for addressing the textthrough a dynamic interface design that bridges textual editing and 3D modelling practices. Interpretation remains the core activity in literary studies. Understanding the limitations of interpretation has also meant that most scholars will agree that literary texts are ambiguous or polyvalent, allowing for conflicting interpretations. Ambiguity, in 3Dconstructions is very similar to textual editing – particularly with regards to the subjective nature of gathering, selecting, and interpreting evidence – although there are less codified ways to express ambiguity. Finding practice-based, generally agreed upon methods to represent ambiguity remain unresolved as attempts to codify this work are often tailor-made to specific projects or constrained by the limitations of existing platforms. In an effort to provide a sustainable long-term solution for documenting the decision-making process, the editors of all four of the 3DSE in this project have arrived at a proof-of-concept that draws upon the core set of technologies and methods found in textual editing. Perhaps the most fruitful synergy in this year-long planning grant has been our attempt to make transparent how the work of the 3D modeler with respect to the textual editorinforms the other while developing a new paradigm for this sort of intertextual network edition. As Schreibman and Papadopoulos will argue in their papers, the 3DSE editor role requires a clear framework that brings together models, theories, and paradigms for the documentation and display of text while also acknowledging some of the difficulties in making all these processes visible to the reader/user. Critically important is outlining how might we provide a means for documenting the decision-making process that does not detract from the source materials, and also finds a balance between overall interpretation and individual decision making without overwhelming the reader/user. Bringing together the scholarly practices of 3Dconstructions and digital editions has meant an important re-thinking of how both areas can contribute to our understanding of scholarly editing practices, while also helping address the common concerns over sustainability, reader/user engagement, and, more broadly, scholarly publishing. As this paper suggests, annotation, ambiguity, and transparency provide the foundational elements for new knowledge practices that are richly layered and generative of new standards for our work in the fields of three-dimensional modeling, virtual reconstructions, and 3D computer graphic simulations. The theoretical and methodological practices we have outlined here also require a critical examination of the roles our institutions play in supporting this kind of work, both technically and programmatically, requiring as it does, a clear commitment to collaborative scholarship. In all four of our 3DSEs, these works are only made possible with the support of librarians, technologists, and modelers who understand the nature of collaborative work relying upon different areas of expertise. Paper IV Technological Requirements for 3D Publishing Information Technology: With the emergence of stable digital technologies capable of supporting ongoing knowledge production – specifically the publication of scholarly 3D material – its user base or academic audience will require a scholarly computer model also that also provides an interactive user experience, i.e.one that will permit the audience to test arguments and further immerse itself in the 3D experience. While acknowledging the importance of reality-based models produced from scans or photos of extant sites and artifacts, this paper focuses on stand-alone 3D environments that are the result of significant well-documented scholarship, that explore the requirements for their publication from the perspective of both scholars and presses, and discuss available technological options for academics seeking to publish 3D work. Whether created manually, procedurally, or through some combination of reality- and sourced-based modeling, these environments were developed with a research question or pedagogical objective at its core, and are also intended to demonstrate and acknowledge the interpretative decisions made during their construction. As acts of scholarship, these models are intended to be assessed and peer reviewed, and can arguably be considered knowledge production in and of themselves. Yet, integration of this type of work into the scholarly record remains a challenge. In a 2016 working group meeting, Elaine A. Sullivan from the University of California – Santa Cruz succinctly summarized the academics perspective: As a scholar, I want my work to be a part of the academic record. I want it to be published, I want it to be sustainable, I want it to be peer reviewed, I want it to be considered in the same conversations as other forms of academic output, I want it to be cited by colleagues in my discipline, and I want others to build off of my work for their own scholarship. Perhaps most importantly, I want it to count for promotion and tenure. To facilitate that depth of academic engagement, technology is needed that can facilitate the transformation of raw computer models into information-rich virtual environments that support annotation, bibliography, linkages to primary and secondary documentary evidence, citations, and secondary scholarship – all the while preserving and respecting the authors intellectual property. From the publishers perspective, technology is needed that will support the editing and peer-review process; will seamlessly integrate itself into existing publishing workflows; will deliver the 3D material to viewers/users in a manner that accommodates the intent of the author; will ensure access to the content for purposes of reproducibility, reuse, and purposeful deformation; and, perhaps most critically, will preserve this material as part of the academic record. Currently, there are limited options for publishing 3D content. The final section of the paper will consider the challenges of self-publishing or uploading material to an online asset warehouse or similar commercial option, review the few examples where presses are exploring mixed media and interactive user experiences, and describe ongoing efforts to building technology platforms for the publication of 3D material. "
	},
	{
		"id": 278,
		"title": "Unthinking Rubens and Rembrandt: Counterfactual Analysis and Digital Art History",
		"authors": [
			"Nijboer, Harm",
			"Brouwer, Judith",
			"Bok, Marten Jan"
		],
		"body": " Netherlandish art from the seventeenth century is closely associated with a few famous painters from that era: Rubens, Rembrandt, Van Dyck, Vermeer and a few others. This is not only true for the general public, but to a large degree for the field of art history as well. A great many books and articles have been written on Rembrandt and Rubens alone. Worldcat, for instance, lists 20 984 publications on Rembrandt and 11 589 publications on Rubens. The enormity of these figures becomes even more striking when we compare them to those of lesser known seventeenth century Netherlandish painters like Hercules Seghers, Cornelis de Vosand Daniel Vertangen. And when something is written about these lesser known painters it is often in relation or in contrast to Rembrandt or Rubens. To illustrate this: for his recent bookon all history painters from mid seventeenth century Amsterdam, Eric Jan Sluijter could think of no better title than Rembrandts Rivals. The omnipresence of Rembrandt and Rubens in our thinking about seventeenth century Netherlandish art has without doubt biased art historical documentation on this period. Data on these two famous painters are readily available, even in digital formats. In an effort to counter this bias towards the famous the authors of this paper have collected and structured biographical data on allpainters from seventeenth century Antwerp and Amsterdam in the ECARTICO prosopographical database. If available, we have also collected biographical data on their parents, spouses and often other direct relatives. Person in the data set have been extensively linked to external resources like VIAF, Getty ULAN, Wikidata and more. As a result ECARTICO provides avirtual model of the social networks that constituted the Antwerp and Amsterdam art worlds in the seventeenth century. In this paper we present the outcomes of a network analysis on the data derived from ECARTICO. The analysis shows that Rubens and Rembrandt – maybe as one might expect – were indeed key actors in the social networks of their artistic communities. We could be satisfied by this outcome because it once and for all shows that Rembrandt was not a lone genius. But we also have to raise the question whether this outcome is not affected by the fact that Rembrandt and Rubens are somewhat over-documented. Furthermore, we should ask the question to what extend the central positions of Rubens and Rembrandt shaped the structure of their artistic communities as a whole. To address the latter and related questions we have to deal with an important feature ofmodels that are not well understood by many historians, namely that models allow for experimentation. Network analysis packages allow users to delete nodes in the network and to rerun the analysis. For this paper we have done such an experiment and deleted Rubens and Rembrandt from their respective social networks. This experiment shows that despite the centrality of Rubens and Rembrandt in the art worlds of respectively Antwerp and Amsterdam, the social structure of these milieus was not entirely centered around them. Furthermore it will highlight the independent centrality of some painters that operated outside the sphere of influence of Rubens and Rembrandt. We are aware that such an experiment will meet skepticism by many historiansor – even worse – will be qualified as iffy history. However, we argue that such a counterfactual analysis is not intended to get an impression of a world without Rembrandt and Rubens. On the contrary, the whole experiment is intended to get a better understanding of the impact of Rubens and Rembrandt on their artistic communities. Furthermore, it might provide us some assistance in assessing potential bias in the underlying data. Last but not least there is an epistomological argument. Sunsteinrecently argued that all historical explanation implicitly involves counterfactual reasoning. Due to the ongoing digitization of sources and data,historians will increasingly have to deal with models of the past. It is a great opportunity to make our counterfactual reasoning explicit. "
	},
	{
		"id": 279,
		"title": "Log Analysis Method towards Understanding Detailed IIIF Image Usage",
		"authors": [
			"Nishioka, Chifumi",
			"Nagasaki, Kiyonori"
		],
		"body": " Introduction It is important for libraries and museums to analyze and understand how digital collections and their contents have been used for many reasons, e.g., accountability for stakeholders. Results of analysis can be used to improve digital collections. The usage analysis is conducted along with two steps:Selection of a measurement: A measurement that suits for the purpose of the usage analysis is chosen. Then, the measurement is calculated based on data such as server logs. So far, measurements such as the number of accesses to materialsand images have been widely employed.Visualization of the result: The result of the usage analysis is visualized to facilitate users to understand. Chartshave been used. In these years, a lot of libraries and museums have adopted IIIF, which promotes mutual use of images. IIIF defines a couple of APIs to enable interoperable use. In IIIF-compatible digital collections, images are fetched via IIIF Image API, whose syntax is defined as: {scheme}://{server}{/prefix}/{identifier}/{region}/{size}/{rotation}/{quality}.{format} Every time an image is zoomed and panned on an image viewer, the image is called via IIIF Image APIs with varying values of the region. Thus, it is possible to investigate the detailed image usage by examining which regions have been requested. In this paper, we show a method to analyze image usage and to visualize the analysis result. Specifically, we employ the number of accesses to each pixel as a measurement and visualize by heat maps. Since a pixel is the smallest unit of an image, we enable a fine-grained analysis different from previous studies. Method This section describes how to measure and visualize the detailed usage of images on IIIF-compatible digital collectionsand how to display the visualized result. Measurement and visualization The method is comprised of following two steps: Measure the number of accesses to each pixel: For each image, an H×W matrix, where all elements are 0, is generated. H and W are height and width of the image in pixel. Each element of a matrix corresponds to each pixel. The height and width of images are retrieved by info.json provided by IIIF Image API. Subsequently, requested images and regions are acquired by parsing logs of IIIF Image API. Based on requested regions, the number of accesses to each pixel is counted and recorded to the matrices. Generate heat maps: After counting the number of accesses to each pixel, the result is outputted as a heat map. The RGB value of each pixel is calculated considering the minimum and maximum values of the number of accesses to pixel in an image. In order to save the memory for the analysis, we count the number of accesses in the unit of N×N pixels, instead of counting the number of accesses to each pixel. In addition, we output heat maps in×pixels, instead of the same size with the target image, in order to save the storage. Display of heat maps Generated heat maps are displayed over corresponding target images, in order to enable usersto understand image usage. IIIF Presentation API enables overlay images by specifying two images on a page. Mirador, a popular viewer among IIIF community, implements a function of overlay display, as shown in Figure 1. One can manipulate visibility and opacity for each image in the left side panel. Figure 1. Overlay display of a heat map on its target image using Mirador. Photograph courtesy of the Main Library, Kyoto University - Konjaku monogatarishuu Example and Improvement This section illustrates examples of analysis results and improvements for the analysis method. Analysis considering probabilities to be accessed Figure 2 illustrates a typical heat map that represents image usage. The number of accesses close to the center is higher than others. This tendency has been observed in other images. The reason is that pixels close to the center have a higher probability to be accessed. In order to treat each pixel equitably, it is necessary to adjust the number of accesses according to the probability. Figure 2. Typical heat map. Photograph courtesy of the Main Library, Kyoto University – Yashiki-zufrom Nakai Collection We compute the probability to be accessed for a pixel that is a and b pixels from the midpoint of each side of the image as: Then, let cbe the number of accesses to a pixel that is a and b pixels from the midpoint of each side of the image. Then, the number of accesses can be adjusted as: is the probability to be accessed for the center of the image divided by the probability to be accessed for the point, which is a and b pixels from the midpoint of each side. We take logarithm in order to mitigate influence from the adjustment. As a result, the number of accesses for Figure 2 is adjusted as shown in Figure 3. Figure 3. Heat map where probabilities to be accessed are considered for Figure 2 Referrer of images As exemplified in Figure 4, we observe images, in which accesses are concentrated in specific regions. Looking into referrers of access logs, it turns out that these regions are referenced by IIIF Curation Platform. Since IIIF enables mutual use, regions and images have more opportunities to be referenced from other platforms. Referrers show motivation and background behind accesses. If the web site that the referrer indicates is completely disclosed, it is possible to present a link to the web site on a viewer as annotations. So that users can discover regions and images that are highly relevant. Figure 4. Example where specific regions get many accesses. Photograph courtesy of the Main Library, Kyoto University - The story of Benkei, a tragic warrior Applications This section lists possible applications of the result of the usage analysis. Collaborative research platform: The data model used in IIIF follows Web Annotation Data Model. Therefore, IIIF facilitates to share not only images but also information accompanying images. For this reason, IIIF-compatible collaborative research platforms have been developed. By presenting heat maps, researchers can understand which regions have not been examined by collaborators and work for these regions. Transcription Platform: In the decades, a lot of transcription projects have been launched. Transcribers zoom and pan images during generating transcriptions. If a platform is compatible with IIIF, it is possible to verify a pattern such as whether there is a difference in transcription performancebetween regions being zoomed and those being not zoomed. If there is a pattern, we can use it to facilitate verification process for transcriptions. Selection of thumbnails: In many cases, images on the first page of materials are used as thumbnails. However, the first image does not necessarily represent the material. We may select the most-viewed region of images in the material as a thumbnail, which can be revealed by the analysis method. Challenges Visualization of access logs is not a problem, if anonymization is conducted. However, anonymization does not make sense in some cases. For instance, for images that are only accessed by researchers in a specific field, colleagues can easily guess who accessed images and regions. Even if anonymization is complete, a series of activities of a researcher on images might reveal his/her viewpoint that would be a key issue of his/her academic outcome. In this case, his/her priority right of discovery may be infringed. Therefore, careful consideration will be necessary to exploit the analysis result as a service. In the future, we would like to consider some guidelines based on the above-mentioned challenges as well as existing policies such as privacy protection. "
	},
	{
		"id": 280,
		"title": "Semantic Deep Mapping in an Integrated Platform for Studying Historical AmsterdamThe Amsterdam Time Machine",
		"authors": [
			"Noordegraaf, Julia",
			"Vermaut, Thomas",
			"Raat, Mark",
			"Mol, Hans",
			"van Erp, Marieke",
			"Doreleijers, Kristel",
			"van der Sijs, Nicoline",
			"Zandhuis, Ivo",
			"Zijdeman, Richard",
			"Baptist, Vincent",
			"Rasterhoff, Claartje",
			"van Oort, Thunnis",
			"Vrielink, Charlotte",
			"Kisjes, Ivan",
			"Pierik, Bob",
			"van den Heuvel, Danielle",
			"Kaplan, Frederic"
		],
		"body": " The contributors to this panel are jointly participating in the development of the Amsterdam Time Machine - an integrated platform to present historical information about people, places, relations, events, and objects in its spatial and temporal context, focusing on the city of Amsterdam. The web of data on the history of Amsterdam is created by systematically linking existing datasets from social and humanities research with municipal and cultural heritage data, where possible in the form of Linked Open Data. The linked data can then be organized and presented in spatial representations, such as geographical and 3D visualizations. The result is a Google Earth for the past, which allows users to explore the city through space and time, at the level of neighbourhoods, streets and even individual houses. Recently, a first proof of concept was developed that connects linked data from the Amsterdam cultural heritage institutions and various scholarly research projects to a GIS infrastructure that provides the historical geographical and topological context for these linked datasets. The conceptual framework offers an instrument for research into urban space as a connecting factor for social and cultural processes. Charles Tilly described the city as a privileged site for study of the interaction between large social processes and routines of local life.We offer a research platform that operationalizes Tillys description by investigating the urban history of Amsterdam on a scale that varies from the micro level of a plot, person or place to the macro level of broader societal processes in the city as a whole - a microscope and telescope in one. Such a research environment offers an unprecedented opportunity to explore the relationship between physical and social space and how this connection was experienced over time. With space as a connecting factor, we provide a concrete illustration of the research potential of linking social and economic data to cultural data, allowing researchers to study specific historical and cultural phenomena against the background of broader societal developments. As such, it facilitates scalable digital humanities research, smoothly navigating historical data from the micro level of one location, anecdote or document to the macro level of patterns in large, linked datasets that expose broader social and cultural processes. This panel focuses on the use of the Amsterdam Time Machine as an infrastructure for scalable digital humanities research. The first contribution outlines the development of the plot-based geographical infrastructure, including the ways in which spatial semantics have been linked up with topological structures and their relation with geometries. The following contributions present use cases that focus on developments in language, social mobility and leisure activities, in particular going to the theatre and cinema. On the one hand, these use cases demonstrate the potential of the Amsterdam Time Machine for innovating disciplinary research in Linguistics, History and Media Studies. On the other hand, they show how the research infrastructure also supports interdisciplinary research, by making a connection between the social development of Amsterdams historical population groups, their language development and their leisure activities in local theatres and cinemas. The panel is concluded with a discussion, initiated by Frédéric Kaplan. 1. A Geographical Data-infrastructure for Historical Amsterdam Thomas Vermaut, Mark Raat and Hans Mol If the Amsterdam Time Machine wants to uphold its claim that it can process all kinds of historical data through space and time, it cannot do without a proper Geographical Information Systemconsisting of viewers, data entry environments and 3D visualizations. Such an infrastructure has to be based on the smallest geographical unit: the plot or parcel, which can be considered as the atom of space. In the last decade, much effort has been devoted to georeferencing, vectorizing and digitizing the Napoleonic cadastre of 1812-1832 for a series of Dutch provinces and cities, including Amsterdam. However, since most administrative sources for urban history are not listed per plot but per house number, it is necessary to connect the house numbers to the cadastral plots. For Amsterdam, this meant that the address system of the 1851 census had to be tuned to the parcel units of 1832 and to the lists of 1879/1880 with which the current Amsterdam street and house numbering system was introduced. All these parcel and house numbers are eventually connected to an unique georeferenced point location, in order to forestall historical parcel mutations trough time - such as the splitting, merge, demolition or construction of buildings. A provisional draft of this combined system was already presented in 2013 and could be brought in as the first building block for our infrastructure. It may seem that these tracks of data processing and homogenizing consecutive address systems would lead to a simple historical geometrical tool or coat rack to which many types of address systems can be hung. Dealing with house numbers, plots and persons in a digital way however, poses some serious problems as to addressing the spatial uncertainty and the fuzziness of temporal dynamics. Numbering schemes can be very complicated in their sequence, with all kinds of relationships between a person and a location, distinctions between buildings and residential units, and the possible differences between locations and geometries. In our research we have tried to solve these problems by using a semantic way of describing relations throughout space and time. This is based on a Linked Open Data ontology in which spatio-temporal entitiescan be related to other such units. Abstracting from the spatial implementation of the unit by describing it on a semantic level provides an opportunity to link multiple geometrical featuresto it. In our infrastructure, therefore, unique georeferenced ID-numbers can be semantically joined to the unit, regardless of spatio-temporal fuzziness that may exists towards other geometries. Moreover, the LOD approach enabled us to link geographical concepts for streets and neighbourhoods introduced at AdamLink. 2. A Reconstruction of 19th-Century Amsterdam Dialects and Sociolects Marieke van Erp, Kristel Doreleijers and Nicoline van der Sijs According to the nineteenth-century linguist Johan Winklera whopping 19 dialects were spoken in 19th-century Amsterdam, distributed over the various neighbourhoods. Alongside these dialects various sociolects were spoken throughout the city: those of the lower, middle and higher classes, and the slang language Bargoens. In our project, we are attempting to reconstruct the 19th-century Amsterdam dialect and social variation and by doing so we will test if the variation is indeed as high as Winkler claimed. For this purpose, we have collected over 8,000 phonological and lexical data points from sources dating from the end of the 18th century until the beginning of the 20th century. Our starting point were the lexical sources on Dutch Bargoens, collected by J.G.M. Moormann. To this we added primary and secondary sources about Amsterdam dialects in the 19th century, e.g., glossaries or historical descriptions of the city, the results of a survey on the pronunciation and words of the Amsterdam dialect, conducted in 1877, and finally recordings of dialect speakers born in the late 19th or early 20th century from the Meertens Nederlandse Dialectenbankand Nederlandse Liederenbank. The lexical data is converted to Linked Open Data using an adaptation of prior work on modelling diachronic data as RDFwhich follows open standards such as the Lemon lexicon model for ontologies. For our use case, we extend this model with fine grained geographical information from AdamLink, the Amsterdam georeference dataset. The data model is shown in Figure 1. Figure 1: Diachronic language model Besides modelling challenges, we also face several technical challenges in this conversion. One such challenge is for example the fact that dialectologists who have first identified the different dialects did not fully specify exactly where each dialect was spoken, but rather roughly identified areas as between streets X and Y including their cross-streets. Anchoring these areas geographically involves identifying the locations and lay of these streets in 1874, drawing the areas in a GIS system, and mapping the 1874 areas to coordinates. In all of these tasks, we benefit from prior GIS work. By connecting our language data to other data in the project, we can approach our research on the use and spread of data from different angles; for example, by investigating the connection to house prices and higher sociolects. 3. Amsterdam Elite in Geographical Context Ivo Zandhuis and Richard Zijdeman Recently, research has been done into the persistence of the elite over time, in studies such as Thomas Pikettys Capital in the 21st Centuryor Gregory Clarks The Son also Rises. This subject has also been studied in the Netherlands, in particular by Boudien de Vries, who conducted research into social mobility among the Amsterdam elite. Although innovative for the time, there are two major omissions in this study. It only looks at social and economic characteristics and does not take into account geographical or cultural context. In this contribution we suggest a method to fill this gap by combining existing datasets on elite, status and geography. In our research, we used the dataset by Boudien de Vries, containing a sample of persons from the Amsterdam electoral rolls of respectively 1854 and 1884. We converted it into Linked Data and standardized three factors: the neighbourhood, the address and the occupation. Thanks to this standardization we were able to link three datasets already available on the web. Via the neighbourhood we related our observations to the population size per neighbourhood, created in the CEDAR project. Secondly, via the addresses we related both a geographical location on the map as well as a street name. Finally, via the standardized occupationwe related a status value of the occupation. Standing on the shoulders of other data-giants we:visualised the distribution of the elitecompared the elite density of neighbourhoods in 1854 and 1884,studied the relation between status and elite density per neighbourhood. In addition, because of the same underlying geographical data we were able to relate our data to other datasets in this panel. By incorporating paintings and photographs from heritage collections of the City Archives, the Amsterdam Museum and the Rijksmuseum via the street name, we can paint a picture of the life of the elite, facilitating a presentation of our research for a broader audience. 4. Entertainment Culture in Amsterdam From the 19 th to the 20 th Century Vincent Baptist, Julia Noordegraaf, Claartje Rasterhoff, Thunnis van Oort and Charlotte Vrielink Recent investigations into the circulation, exhibition and reception of film have benefited from the increasing availability ofdata and the adjacent development of new, computational research methods, allowing scholars to place film within the broader socio-cultural and economic contexts in which it emerged. Nevertheless, the social composition of early cinema audiences has remained hard to grasp, since relevant sources were sparse and difficult to analyse in combination. Even less is known about the extent to which cinema audiences overlapped with audiences of earlier performing arts, such as theatre and music. The potential of geospatial research and digital mapping practices has allowed us to develop more thorough insights into the historical audiences that flocked to local places of entertainment, in this case in Amsterdam. The Amsterdam Time Machine project has opened up new, integrated research approaches to identify and understand the notoriously ephemeral audiences of urban amusements, by providing the opportunity to combine heterogeneous cultural datasetswith social, economic and demographic data to explore correlations between venue locations, their cultural offer and the composition of potential local audiences. Within the collaborative framework of the Amsterdam Time Machine project, georeferenced and vectorised historical maps of Amsterdam were made available, on which we projected data concerning cinema, theatre and nightlife locations. While the online Dutch database Cinema Context contains information on cinemas and screened film programmes in the early 20 th century, the Special Collections department of the Amsterdam University Library published an extensive overview of past theatre locations. By converting these data into Linked Open Data, the locations of entertainment venues were connected to lists of historical Amsterdam neighbourhoods and addresses made available within the overarching Amsterdam Time Machine project. In order to gain a more complete view of the entertainment culture and nightlife that existed in Amsterdam, we further mapped fine-grained historical data contained in address books and registers of night licenses, that were originally held in the Amsterdam City Archives. By additionally combining this with data on the socio-economic composition of city neighbourhoods, we traced the correlations between the cultural programming of venues, the locations and characteristics of bars and restaurants, and the socio-demographic profiles of the neighbourhoods in which they were located for selected sample years. The establishment of the entertainment areas that we identify, both in the historical citys centre and periphery, can be traced back through time. By investigating how sites of public entertainment were formed throughout the 19 th century and persisted in the early 20 th century, we further characterize these areas according to the diversity of culture, consumption and amusement that they offered. Additionally, this enables us to connect the emergence and distribution of entertainment sites to the socio-economic developments of the citys neighbourhoods from the 19 th to the 20 th century, and to the findings on class and status in Amsterdam that are uncovered in the other Amsterdam Time Machine use cases. 5. Uncovering Everyday Mobility and Street Use in Early Modern Amsterdam Through GIS and 3D Ivan Kisjes, Bob Pierik and Danielle van den Heuvel Throughout history and across cultures, women are seen as destined for the home instead of the street. At the same time, women ventured out regularly to shop, work, pray, and play. The project investigates the relationship between women and the urban environment in a time when historians believe restrictions on female movement greatly intensified, by comparing gendered street use in early modern Amsterdam and Tokyobetween 1600 and 1850. It employs an innovative approach to studying the gendering of urban space by digitally reconstructing street use. This paper discusses the data model and first outcomes of the research on Amsterdam. Information is gathered from a large variety of sources. To accommodate such disparate information, an event-based, tripartite data model is used: events, people and locations. This makes it possible to enter all relevant information from the sources and provides a flexible way to coherently connect such complex data in a way that makes quantitative analysis possible. All this information is put into a spatial database, making it possible to analyse movement of people and events through space and time. Interfaces specific to the source typewere developed for entering the data, and connected to the same database. Additional GIS techniquesprovides contextual data to which the events can be related. The data model facilitates a long-term intersectional analysis of gendered mobility and street use. We are able to show to what extent men and women stuck to areas of the same social class as those where they themselves resided. This provides important insights into how genderedmobility was dependent on social segregation. We can also reconstruct seasonality in movement, as well as how Amsterdams street life changed during the course of a day. Finally, 3D models of early modern Amsterdam allow for an investigation of the relationship between the urban form and activities in certain urban spaces. For instance, it helps determine the hiddenness of streets, to be able to investigate if particular events types occurred predominantly in unobserved spaces. A reconstruction of the early modern street network furthermore makes it possible to do spatial network queries like shortest path analysisand makes it possible to estimate how busy a particular street was at a certain time of day. The project uniquely records the events and people hitherto largely unmentioned in history because of their peripheral appearance in historic documents, and provides new insight into the historic urban environment in a quantitative way. The data gathered in the project is essential to the framework, as it consists of the daily goings-on in the streets of the historic city. "
	},
	{
		"id": 281,
		"title": "Analysis and Visualization of Narrative in Shanhaijing Using Linked Data",
		"authors": [
			"Wang, Qian",
			"Nurmikko-Fuller, Terhi",
			"Swift, Ben"
		],
		"body": " Introduction Existing Linked Dataliterature contains several examplesor on ancient Sumerian mythologies) where the fabula and syuzhet of Western folktales have been represented, and information regarding the stories themselves have been published in machine-readable formats such as RDF. However, similardatasets and analyses are largely non-existent for equivalent stories from Chinese mythology. This paper seeks to bridge that gap by creating, analyzing and publishing a case study example—the classic Shanhaijing. We recount the complexities of representing ancient Chinese literary narratives, captured in unstructured data, using tools developed from Western perspectives and for complete and largely homogeneous, highly-structured data. Shanhaijing is an ancient encyclopedia. Its origins can be traced back to the pre-Qin period of China, its development continuing through to the early Han Dynasty. It covers broad areas such as ancient mythology, geography, witchcraft, religion, medicine, and other aspects. Shanhaijing occupies a significant position in the literary and mythological corpora of the East, and is representative of a wider spectrum of Eastern mythologies. Over thousands of years, numerous Chinese novels, literary fictions and dramas have been derived from the book, such as Zhuangziand Strange Tales of Liaozhai. Mythologies from other Asian countries were influenced by it, e.g. Kaiki Choju Zukanand Hyakki Yagyō, both examples of Japanese folklore. In this paper, we report on the state of existing work combining LD methodologies and approaches with literary compositions, and summarize the narrative of the Shanhaijing. We outline our chosen methodology in Section IV. The custom-built user-interfaceis demonstrated in Section V, and we conclude the paper with a discussion of the complexities of the process in Section VI. Related work In the domain of literature, several publicly available datasets have been published as LD. For example, the Book-Sampo project, which provides information on fiction literature published in Finland going back to the 15th century, alongside rich descriptions of both content and context, or the Perseids Project, which provides a platform for creating, publishing, and sharing research data, in the form of textual transcriptions, annotations and analysis. Other essential work in this space include the Brothers Grimm project, an ontology for Greek mythology, and the Aarne-Thompsons Motif-Index project. Classic of mountains and seas, 山海经 The present version of Shanhaijing contains 18 chapters, approximately 31,000 words in total. It records ancient Chinese mythologies, where numerous monsters with fanciful descriptions are portrayed as possessing magical powers or as related to ancestorworship, such as the monster Lushu, which looks like a horse with a white head, a scarlet tail and tigers markings, and lives on Mount Niuyang. Whoever wears its fur will have a greater number of descendants. We focused exclusively on the capture of the data for the monsters in Shanhaijing. The reason for this is that the fascinating and detailed accounts of these creatures overwhelm the other aspects of the story; and these descriptions account for a notable proportion of instances of literary borrowings and inspirations in other cultures, increasing the likelihood of reuse and inter-linking with other ontologies. Methodology The first stage of the project focused exclusively on the cornucopia of monsters. Through a close reading in both English and classical Chinese, we extracted structured data from the unstructured narrative. In the second stage, we designed an ontological structure to model the domain. After considering several pre-existing ontology software libraries, we concluded that the suitability of these vocabularies and resources for the representation of Shanhaijing was limited. This necessitated the building of a new ontology, which captures data types and represents the relationships between them as .TTL file. The ontology represents the characteristics of the monsters and the complex relationships between them. It contains a taxonomy of body parts, the characteristics, and the habitats of all monsters. We used a combination of top-down and bottom-up ontologies and schemas to identify Class and Property hierarchies, captured through rdfs:subClassOf and rdfs:subPropertyOf and reused existing vocabularies such as DBPedia Ontology, BioTopOntology, Mahabarata Ontology, RDFS, and XML Schema. This approach enabled us to capture the specifics of the data, but also maximize the benefit from other well-developed and rich ontologies. All concepts related to monsters were collected, then split into terms. A term is considered as a Class if it has attributes pointing to other classes or literals, or it is a Superclass of other classes. Otherwise, it is defined as a property. For example, the term Monster is defined as a Class because it has attributes linking it to other Classes, such as Mountain, through the property livesIn. However, term Noise is not a Class because it is not the domain of any attribute. Hence, it is considered as a property hasSameNoiseAs with Monster as its domain, and a literal as its range. The class Monster is defined as a subclass of Character in FRBRoo, allowing the use of FRBRoo to represent the relations of works. Figure 1 demonstrates a graphic version of the Shanhaijing Ontology. Figure 1: The Shanhaijing Ontology Instance-level data was normalized by mapping it to the ontology, and the Silk-Link Discovery Frameworkwas used to automatically link appropriately matched resources to external datasetsusing owl:sameAs, owl:equivalentClass and owl:equivalentProperty. An interactive data explorer software toolwas built for visualizing, queryingand analyzing the data. The dataset, ontology, and source code are available via GitHub. User interface iSHJ was built as a domain specific application for Shanhaijing. This tool has Browse, Search and Visualization interaction modes. Users are provided with quick search functions to explore the data by clicking buttons and inputting keywords rather than writing SPARQL queries directly, although they can be when more flexible and variable searches are needed. The results are displayed both in plain text and charts for visualization. We also provide a graphical version of Shanhaijing, where mountains are placed to represent the locations described in the book, the monsters correspondingly placed on the mountains where they live. A video of the UI is available at YouTube. Figure 2: Results of Quick Search Example monster in Browse Section in iSHJ Figure 3: SPARQL Query Sample of Monsters Tail Number with Visualization Results in iSHJ Discussion Despite the work on the narrative of separate regions of some prominent Western myths, projects focused on Chinese literature within this interdisciplinary field are rare. Existing LD methods have been developed almost exclusively in the context of Western culture, and predominantly for highly structured data. When facing ancient Chinese mythologies, there are two main unsolved challenges: non-existent structured datasets and the unavailability of reusable ontologies. Before LD methods can be applied to the narrative of Chinese myths, a structured dataset capturing in-depth knowledge of Chinese mythologies must be constructed. However, the full potential of this pioneering project can only be tapped into once a greater number of external, disparate, but complementary datasets are published using the LD paradigm. That is to say, until other projects focusing on the analysis of Chinese literature engage in LD, there are limited opportunities for outward linkage. The protagonists of Eastern and Western mythologies are not entirely similar. For example, in many ancient Chinese mythologies, numerous gods and creatures are described as a monstrous combination of different animals, falling somewhere between, for example, the human-likegods and heroes of Greek myths and the anthropomorphized animals of Aesops tales. Although there are some complementary aspects – e.g. in the Aarne-Thompsons Motif-Index, the Nine-tailed Fox, is recorded as B15.7.7.1; other motifs are the four-eyed tigerand serpent with a jewel in its mouth– These ontologies neither contain the narrative of Chinese myths nor are they created for Chinese folktales. Ultimately, the existing overlaps are insufficient. Based on the differences in the narratives, most ontologies created for Western folktales are not completely suitable for the representation of ancient Chinese mythic classics and could not adequately demonstrate the characteristics of these gods and monsters in the Shanhaijing. Conclusion We used LD methods for textual analyses and visualization of a book of Chinese mythology, Shanhaijing. We created and published a structured dataset, relevant LD, and an interactive explorer to represent the monsters within the text. An extensive review of existing ontologies for literary motifs and mythological creatures revealed that there was insufficient overlap between them and the needs of the dataset, necessitating the development of a new ontology. Future work will see us expand this analysis to all the contents of Shanhaijing. New ontologies will be generated from the one in this paper, and structures will be redetermined and improved to adapt to other mythologies. Other ontologies could be reused or interlinked to, increasing the number of linked elements. We will also test the suitability of our ontology on other mythologies, ranging from Chinese mythologies appearing before and after Shanhaijing to other Asian mythologies such as Japanese tales. We will also apply our ontology to Western mythologies to assess the similarities and differences between Eastern and Western folk tales. "
	},
	{
		"id": 282,
		"title": "Comparing diagrams in Euclid’s Elements",
		"authors": [
			"Nury, Elisa"
		],
		"body": " Summary This poster aims to introduce a new research project on the potential of automated collation for non-textual data such as mathematical diagrams, focusing on the case of Euclids Elements. Netzargues that diagrams are crucial to Greek mathematics and necessary to reading the text, but he notes that this fact was little discussed in modern literature. In recent years, however, there has been a growing interest in including diagrams and their manuscript evidence in the preparation of scholarly editions. The use of diagrams in scholarly editing Saito, Sidoli, De Youngand Roughanhave all advocated for critically editing the diagrams which accompany ancient mathematical texts. Until now, the editorial practice was often confined to publishing diagrams which follow modern standards rather than manuscript evidence, as well as trying to benefit the reader. Modern editors print arbitrary parallelograms and triangles, whereas manuscripts have rectangles and isosceles triangles even when the text does not require it. The critical apparatus does not usually offer information regarding the state of diagrams in the manuscripts. Netzhas attempted to provide a kind of critical apparatus for diagrams in Archimedes work, but Roughanremarks that this approach is limited for printed edition due to the amount of space it consumes. The study of diagrams can broaden our knowledge of textual transmission by highlighting elements absent from the text: Raynaud, for instance, has shown how the tradition of diagrams can help create a stemma codicum of Ibn al-Haythams Epistle on the Shape of the Eclipse, an Arabic mathematical treatise from the eleventh century. The diagrams were analysed with regard to stated or unstated characters: unstated characters are aspects of a diagram not described in the text, and therefore much more difficult to correct. Raynaudconcluded that diagrams had a higher concentration of errors and were therefore particularly promising to study manuscript traditions, especially rich and complex traditions. As a result, automated collation of diagrams may be particularly useful for Euclids Elements. A broad survey of the Elements witnesses by Roughanhas recorded 477 manuscripts in Latin, Greek and Arabic. Preliminary to automated collation Automated textual collation is divided into a series of steps: the first is to split the text into smaller units, called tokens, which are then compared. How can diagrams be tokenized for the purpose of automated collation? Scholars have provided lists of a diagrams features: the features which usually define a technical drawing are its basic components: points, segments, curves, colored surfaces, and various labels. Sidoli and Lihave divided features between mathematicaland visual features. A potential implementation of automated collation could make use of CollateXs flexible input, which permits to encode a reading - here a diagram - as a set of features. Simplified examples of features could include labels:ABCD, labels:none, orientation:left or orientation:right. However, as soon as diagrams become more complex, the number of potential features increases: not only point, lines, and angles but also multiple surfaces would make automated collation more cumbersome. To attempt identifying relevant patterns of differences in diagrams and thus reduce the number of features to be compared, this project proposes to create a database of annotated diagrams with the tool Archetype https://archetype.ink/. Archetype is the framework underlying the Digipal project. Although Archetype was first developed to study palaeography, it has a rich annotation environment where a figure can be decomposed into a series of component with particular features. . Fifty-nine manuscripts freely available online and compiled by Roughanform the basis of the data sample. The scope of the text would be at first limited to the first two books, for which there is a higher number of manuscript coverage. The database currently holds over 600 isolated diagrams from book 1 of the Elements, including some manuscripts very important to the tradition, such as Vat.gr.190or Bodleian Library MS. DOrville 301. Future directions for this work could include identification of accidental versus substantial features in diagrams, or the relationship between text and diagrams. "
	},
	{
		"id": 283,
		"title": "The Application of HTR to Early-modern Museum Collections: a Case Study of Sir Hans Sloane's Miscellanies Catalogue",
		"authors": [
			"Humbel, Marco",
			"Nyhan, Julianne"
		],
		"body": " Research context Handwritten Text Recognitionis the ability of a computer to transform handwritten input represented in its spatial form of graphical marks into an equivalent symbolic representation as ASCII text.What is the state of the art of the application of HTR to early modern manuscripts? With what level of accuracy can HTR models automate their transcription? What is known about how HTR currently accommodates manuscript text that shows changing writing styles, hands and text in multiple languages? We will explore these questions with reference to the wider literature and a case study of the first HTR model to be created for the hand of Sir Hans Sloane. Optical Character Recognition on documents with perfectly machine-printed characters can reach an accuracy level of more than 99%. However OCR is often problematic for historical documents. It also cannot be used for handwritten documents, since the space between characters and words is inconsistent. Holistic segmentation-free off-line HTR technology works at a line level and can deal with cursive characters, slanted words and irregular calligraphy, but it must be trained for a specific handwriting. HTR is not accurate enough to replace human expertise, however it holds the potential to bolster the transcription process.  Enlightenment Architectures: Sir Hans Sloanes Catalogues of his Collections is a Leverhulme-funded collaboration between the British Museum and UCL. It studies 5 of the manuscript catalogues of Sloane, The catalogue of Miscellanies, two of his Natural History cataloguesand two of his library catalogues. and is encoding them in TEI to understand the information architectures they use. In 2017, selected catalogues were transcribed to a high level of accuracy by the company AEL Data Service in Chennai, India. We thus had high quality transcriptions of Sloanes manuscript materials, in addition to images of his catalogues, available for use in the training of an HTR model for Sloanes hand. The HTR model discussed here was trained using the software Transkribus. The aim of the e-Infrastructure project READis to make archival sources more accessible through technological development. The centrepiece of READ is the service platform and application Transkribus, which enables the automatic recognition and transcription of handwritten documents and the ability to search within them. Methodology To train an HTR model with Transkribus, one has to provide it with training data. This is known as ground truth or reference data. The segmentation of the document into its elements, in particular the baselines, and the actual transcription is crucial for creating an adequate HTR model. The ground truth data must consist of a representative sample of a collections documents and also respect the original appearance of the script, e.g. special characters, as closely as possible. With Transkribus, this serves the purpose of training the HTR model, and also the evaluation of its accuracy. Between 75 and 100 pagesof training data are necessary for an effective HTR model. A randomized selection of documents is recommended. We determined that the first sub-section of the Miscellanies cataloguewould give enough training and test data to evaluate the model because it contains important characteristics of the whole collection of catalogues, such as annotations and a complex layout. We wish to thank the members of the Enlightenment Architectures team for their assistance in making this selection and for their wider advice about this case study. For this research, five different HTR models were created to allow a comparison between their changing accuracy. This includes one pre-test model. Training started with 75 folios and was then increased to 100 and 125 folios. For the last model, in addition to the 125 folios of training data, a base model was added. Results The quality of an HTR transcription can be evaluated according to a Word Error Rateand Character Error Rate. Transkribus allows both measures. WER is […] the minimum number of words that need to be substituted, deleted or inserted to convert a sentence recognized by the system into the corresponding reference transcription, divided by the total number of words in the reference transcription […]. C ER is the minimum number of single characters which need to be corrected, divided by the total number of characters in the reference text. Transkribus also allows the evaluation of the general accuracy of a model with a learning curve visualisation and the accuracy of a model on the page level to be specified via the compute accuracy function. According to READ, a model with an accuracy rate of 90% can be regarded as an effective automated transcription. The evaluation showed that our current model of 20,803 words reached a CER of 12.73% without the base model. The transcription has not reached a level of accuracy that is sufficient for academic research without further human input. The model has problems transcribing names, abbreviations, double letters, punctuation, Latin text and the numbers in the margins correctly. Conclusion In the paper we will reflect on how our methodology and model might be refined in order to improve the CER, in line with the experiences of other projects. We will give particular attention to questions like  Where in particular does recognition fail?.  How much training data is necessary to create a model with an accuracy of at least 90%? and how might external resources like gazetteers and name authority lists be integrated into Transkribus and used in conjunction with the HTR model in order to increase the accuracy of the transcription of named entities? Our responses to questions like this are likely to be transferable to other projects who seek to build HTR models for the transcription of early-modern manuscript materials. Although our model reached a relative high level of accuracy, is it not good enough to be used for scholarly work. We will therefore also reflect on scenarios where the model could still be used, such as Authorship Attribution. "
	},
	{
		"id": 284,
		"title": "How To Detect Coup d’État 800 Years Later",
		"authors": [
			"Škvrňák, Jan",
			"Škvrňák, Michael",
			"Ochab, Jeremi K."
		],
		"body": " Introduction The thirteenth century in the Czech lands is undoubtedly the most interesting period for the nobility. After the prince period, the throne is surrounded by magnate families with precisely defined family relations, and the lower nobility rise in numbers. It is the last period when a staggering social rise is possible for a broader number of aristocrats and warriors. Over the century, the Estates are relatively precisely established. When the economic and social gap between the lower and higher nobility widens, the political development becomes dynamic. After an almost invariable group of families around the monarchs has been established, the impossibility of political upheaval led to the uprising of part of the nobility and the civil war in 1248-1249. Using social network analysiswe attempt to describe polarization within the nobility, identify who joined the uprising in the ranks of Přemysl Otakar II, and how it influenced their chances to be appointed to high-ranking positions within the kingdom. Whereas social network analysis has been qualitatively used by some medieval historians, current scholarship on the Czech civil warfocuses on individuals. It hypothesises about cliques around Václav I and Ottokar II based only on the holders of offices during their reigns. Our analysis relies on more detailed dataand more advanced method. Data The data concerning relations between Václav I and Ottokar II and the Bohemian nobility were collected manually from the charters released between year 1198 and 1283. In total, we collected data on approximately 2300 noblemen from 568 charters. Identification of individuals was at times ambiguous – for example, Jan, the son of George, and Jan of Brno appearing within a few years, may be one, two or three men – leading to arbitrary choices. A cross-check with other sources was not possible: a) there are only few charters common to Regesta Imperii or Monumenta Germaniae Historica and Czech sources; b) in the narrative sources, very brief Annales of 1198-1278 and longer The Stories of Wenceslaus I, there are only five person mentioned, two of them unknown to the charters. At this time, we cannot follow social relations in its own sense, we can only determine the agnatic kinship, the kinship between individual generations can only be thought of to discover names typical of another genus. The diagonal provides the number of noblemen included in a network for a given period. The off-diagonal tiles show how many noblemen appear in two given periods Methods The primary concern of this paper lies within the relations between noblemen. From the charters, we extracted weighted networks of noblemenand their co-occurrence in chartersin four periods 1240-47, 1248-49, 1250-1253, 1254-1257. The length of the first and last period was chosen so as to build networks of comparable sizes. We analysed only the largest connected components of such networks, in order to meaningfully define quantities such as shortest paths connecting any two people or their centrality indices. Changes in centrality of nodes in the network. Left: cluster of people thriving under Václav I, whose position declined under Ottokar II. Right: cluster of people who gained their position during rebellion; two example noblemen are indicated We used node strengthas a proxy for its centrality. Next, to each person appearing in the networks we attributed a vector of four values: their network strength in the four consecutive periods. These vectors were agglomeratively clustered with the use of so called chessboard distance into groups of noblemen whose centrality underwent similar changes due to the changes of reign. Analysis was performed in: Wolfram Mathematica 11.3; network visualisation in: Gephi 9.2. Results In Figure 1, we show that the networks of decision-makers in the consecutive periods overlap at most in one third, indicating considerable rotation of posts. Next, as shown in Figure 2, we automatically found two groups of people: benefitting or losing from the uprising. Using that information we extracted the names of noblemen hypothetically loyal to Václav I or opposing him. In Figure 3, we show one of the analysed networks, notably with future rebels in vicinity of Ottokar II, and some filial and brotherly kinships within the highest-ranking noblemen. Conclusions and outlook The results show that almost 800 years later we can still identify the people involved in coup détat and how it influenced their power in an important period of Czech history. We aim at extending this study by incorporating other information from the charters, e.g., their geographical location, the posts held by the noblemen or family membership. Methodologically, we plan to explore other centrality measures as well as community detection to corroborate the results with different techniques, and use bootstrap approach by generating ensembles of random networks with realistic properties to further assess statistical significance of the results. In terms of historical sociology, an interesting task would be to compare characteristics of the above networks with other known – contemporary or historical – social networks, and obtain a complementary insight into therelations in medieval societies. Network constructed from charters issued in 1240-1247. The size of nodes is proportional to their strength. The kingsand 10 noblemen with highest centrality are labelled. Red nodes correspond to the cluster that thrived during rebellion. Light green nodes correspond to the cluster of people thriving under Václav I. Insets A and B show Přemysl IIs and Václav Is subnetworks, respectively "
	},
	{
		"id": 285,
		"title": "A Model-to-model Approach for Developing Corpus Metadata. An “Odd” TEI Customization for Encoding Metadata",
		"authors": [
			"Odebrecht, Carolin"
		],
		"body": " Why do we need a metamodel for metadata? The Text Encoding Initiativeenvironment provides a generic document model. TEI customizations typically follow an explicit or implicit model for text representation. The TEI document model provides modules for encoding text via mark up as well as modules for the metadata referring to the TEI document and the encoded text. The specializedcustomization presented here follows an explicit Metamodel for Corpus Metadatarepresentation and expands the range of applications of the TEI metadata modules to non-TEI corpora. Metadata are defined as structured information that describes, explains, locates, or otherwise makes it easier to retrieve, use, or manage an information resource.. The application scenarios for metadata are diverse, which in turn generates requirements for metadata classifications. Corpus documentation is a typical example of the application of metadata. In order touse historical corpora, a researcher needs to be able to gain intellectual access to a heterogeneous set of properties of a corpus. Moreover, the reusability ofcorpora is an important indicator for their sustainability, following Simons and Birdwho stated that a [...] resource will be used if it still exists, if it is usable, and if a user finds it relevant. Historical corpora attest diverse methods and approaches towards corpus creation, exemplified by a variety of existing resources such as Referenzkorpus Althochdeutsch, Deutsches Textarchiv, and Kasseler Junktionskorpus. Thus, corpus documentation must cover a complex and heterogeneous field of corpus data. MCM provides a UML-based extensive and extensible metamodel for describing corpora, including their documents, annotations and preparation workflows, to address this complexity and enable intellectual access to corpus data. In this paper, I will discuss how MCM can serve as a content model for the TEI document model, which in turn is used for a realization of MCM. This way, information is passed from model to model. Following this approach, I will also show how flexibly the TEI framework can be used and how two types of models can interact with each other. The TEI document model The TEI Guidelines provide an extensive and easily adaptable framework for text encoding of various text types and genres, and for the encoding of metadata. Especially the digitization and encoding of historical documents are often achieved with the help of the TEI. Figure 1 illustrates the TEI document model, and particularly the relation of data and metadata. TEI document model with a basic structure containing teiHeader and body. The metadata in the teiHeader refer to the body part as well as to the external source and the TEI document itself. The body part refers as well to source. The source itself can be more complex than indicated here. A TEI-compliant document http://www.tei-c.org/release/doc/tei-p5-doc/en/html/ref-TEI.html Accessed 2018-11-25. contains a body, which contains the whole body of a single text http://www.tei-c.org/release/doc/tei-p5-doc/en/html/ref-body.html Accessed 2018-11-25. , and a teiHeader http://www.tei-c.org/release/doc/tei-p5-doc/en/html/HD.html Accessed 2018-11-25. for the metadata records and for the documentation of a TEI document. The metadata in the teiHeader refer to the TEI document itself, the body and the external source. The data in the body of the TEI document refer to the source as being the digital surrogate of the source. In this way, metadata and data are integrated in a single document, are defined by the TEI content model, and refer to the same source. The annotationof the document is typically not documented in the teiHeader. Instead, it is extracted from, and documented in, the TEI Guidelines, and modeled within the TEI environment. A model-to-model-based approach for developing corpus metadata The TEI Guidelines can be seen as a content standard, the TEI XML as a file exchange standard which is defined by the TEI modeling tool One Document Does it all. This is an advantage compared to other metadata schemeswhich do not have such a modeling environment. For a discussion of other approaches see OdebrechtBy modeling the TEI with the help of the TEI, the TEI is validated within TEI as well. This way, the data creation and documentation is exclusively dependent on the TEI environment. While using the TEI framework enables a high degree of interoperability across disciplines, other formats also mix TEI annotations with other annotations, or do not use the TEI at all. The complexity for corpus data documentation increases when dealing with digital surrogates of different encoding standards, which in turn are based on diverging text, data models, or formats. To handle this complexity, the MCM, by using the Unified Modeling Languagehttps://www.omg.org/spec/UML/ Accessed 2018-11-25. , identifies common features across diverse corpus data and defines which corpus metadata need to be included in a corpus documentation to enable intellectual access and corpus data reuse. This kind of documentation needs a stronger focus on information about corpus preparation, because the preparation steps for a historical corpus are not always included in a single environment such as the TEI. Adopting the TEI approach and focusing on the metadata framework, the MCM provides an additional conceptual layer. The model-to-model approach: The MCM provides extended metadata for corpus documentation which are handed over to the TEI modeling environment ODD. Each TEI customization refers to a central corpus documentation part: source, annotationand creation. The MCM serves as a blue print for metadata modeling and realization in TEI. It is a format-independent and extensive content model for the application of the TEI to corpus documentation. The realization of the model-to-model approach can explicitly add additional statements and information relationships which go beyond the original TEI model. From a TEI perspective, the document model with a focus on metadata is extended by including non-TEI corpora but looses the close relationship between the source and the TEI document. The customizations adapt the TEI for an originally unintended and novel use case which allows for the documentation of historical corpora of any format and any architecture. This is done via three customizations: One customization describes corpus-internal information such as the text representation which is close to the typical functionality of the TEI. The second customization contains explicit information about all kinds of annotations in the corpus. A third customization describes corpus-external information of the workflow. A concrete application scenario for historical corpora is as follows: TEI or non-TEI corpora which shall be archived in a repositorycan be documented comprehensively and deeply structured. The resulting TEI-XML metadata are then used for metadata search and display in this repository. The TEI customizations represent an innovative perspective on the application of the TEI. The TEI already provides a full metadata structure which is integrated in a document-centric model. With the help of another model – the MCM – the application of the teiHeader can be extended. The MCM can be defined as a concrete and extensive content model for metadata information. The application of the MCM benefits from the TEI environment and its interoperability, and can make use of the modeling tool ODD and its validation mechanism. This approach enables use cases for metadata that were previously separated from the TEI universe and proves that the adaptability and flexibility of the TEI allows reuse scenarios, in this case TEI XML for corpus metadata, which have not been initially intended. "
	},
	{
		"id": 286,
		"title": "Faceted Search for Discovering Software",
		"authors": [
			"Odijk, Jan"
		],
		"body": " Introduction Enabling the easy discovery of resources is an important service for Digital Humanities scholars. In CLARIN, the Virtual Language Observatoryserves this purpose, but it is currently mostly suited for the discovery of data. Discovering software is not so easy in the current VLO. In order to address this issue, we present a proposal for faceted search in metadata for software, which is based on a CLARIN Component Metadata Infrastructureprofile for the description of software that enables discovery of the software and formal documentation of aspects of the software. We have tested the profile and the faceted search based on this profile by making metadata for over 80 pieces of software, and by creating an implementation of the faceted search. It is available on the web. http://portal.clarin.nl/clariah-tools-fs We propose to add this faceted search to the VLO, and show how metadata curation software, combined with provided metadata curation files, can curate existing metadata descriptions for software using other profiles to make them suited for such faceted search. Metadata Profile ClarinSoftwareDescription The ClarinSoftwareDescriptionprofile http://catalog.clarin.eu/ds/ComponentRegistry/rest/registry/profiles/clarin.eu:cr1:p_1342181139640/xsd enables one to describe information about software in accordance with the CMDI metadata framework. The profile has been set up in such a way that it enablesthe description of properties that support discovery of the resource, andthe description of properties for documenting the resource, in as formal a manner as possible. Since the focus of this paper is on the faceted search for software, we only briefly describe some aspects of the profile. The profile consists of the CMDI components Generalinfo, SoftwareFunction, SoftwareImplementation, Access, ResourceDocumentation, SoftwareDevelopment, TechnicalInfo, Service and LRS. The SoftwareFunction component enables one to describe the function of the software in terms of the closed vocabulary elements tool category , tool tasks , research phase, research domains and, for the linguistics domain, relevant linguistic subdisciplines for which it was originally developed. The SoftwareImplementation component enables one to describe information for users on the implementation and installation of the software. The most important components are for the description of the interface , the input and the output of the software. The Service componentis intended for describing properties of web services. It is compatible with the CLARIN CMDI core model for Web Service description version 1.0.2. The LRS component is intended for the description of the properties of a particular task for the CLARIN Language Resource SwitchBoard). It is our viewpoint that specifications for an application for inclusion in the CLRS registry should be derivable from the metadata for this application. We devised a script to turn a CSD-compatible metadata record that contains an LRS component into the format required for the CLRS and tested it successfully with the Frog web service and application. Semantics Many of the profiles components, elements and their possible values have a semantic definition by a link to an entry in the CLARIN Concept Registry). For the ones that were lacking we created definitions and provided other relevant information required for inclusion into the CCR. These are currently being evaluated by the CCR coordinators for inclusion in the CCR. After our submission to the CCR, we made some new modifications to the profile, so there are new elements and values for which the semantics does not yet exist. Metadata Descriptions using the CSD profile We have described more than 80 software resources with the CSD profile. Describing these software resources resulted in various improvements of earlier versions of the profile. Most descriptions started from the information contained in an unformalized software overview. The information from this overview was semi-automatically converted to CMDI metadata in accordance with the CSD profile. The resulting descriptions were further extended and then submitted to the original developers and CLARIN Centres that host the resources for corrections and/or additions. Faceted Search A major purpose of metadata is to facilitate the discovery of resources. An important instrument for this purpose in CLARIN is the Virtual Language Observatory). The VLO offers faceted search for resources through their metadata, but its faceted search is fully tuned to the discovery of data . For this reason, we defined a new faceted search, specifically tuned to discovery of software . This faceted search offers search facets and display facets: Search Facets LifeCycleStatus, ResearchPhase, toolTask, researchDomain, linguisticsSubject, inputLanguage, applicationType, NationalProject, CLARINCentre, input modality, licence Display Facets name, title, version, inputMimetype, outputMimetype, outputLanguage, Country, Description, ResourceProxy, AccessContact, ProjectContact, CreatorContact, Documentation, Publication, sourcecodeURI, Project, MDSelfLink, OriginalLocation Furthermore, all metadata profiles for the description of software must be able to provide the values for the facets. That is the case to a large extent, though some metadata curation is needed in some cases and existing values must be mapped to a restricted vocabulary for use in the faceted search. Curation of existing metadata for software The basic idea is as follows: we create a new standardised metadata record for all software descriptions, in principle each time a record is harvested. This metadata record contains the components and elements that are required for the faceted search as defined above. The record is constructed from the original CMDI record for the resource, combined with the data for this resource contained in a curation file, by a script. The curation file can be used to add information that was lacking or only present in an unformalised way, and it can be used to map existing values to other values, e.g. to restrict them to a specific closed vocabulary. We report on our experiments with such a curation file for the WebLichtWebService profile, since it was most needed and most complex for this profile. Over 280 WebLicht Web Services can now be found with the faceted search. Concluding Remarks The faceted search is publicly available and in use by digital humanities researchers. We already received feedback from users for further improvements, which we hope to make in the course of 2019. We will also describe some problems we encountered, which we only briefly mention here:definition of closed vocabulariesseveral technical problems related to the CLARIN Component Registrylack of good CMDI metadata editors. Finally, we will identify some future work, in particular on deriving CLRS registry entries for CLAM-based applications and web services, https://proycon.github.io/clam/ for extracting metadata information from independent initiatives such as codemeta. https://codemeta.github.io/ References Daan Broeder, Menzo Windhouwer, Dieter Van Uytvanck, Twan Goosen, and Thorsten Trippel .. CMDI: A component metadata infrastructure. In Proceedings of the LREC workshop Describing LRs with Metadata: Towards Flexibility and Interoperability in the Documentation of LR. , pages 1–4, Istanbul, Turkey. European Language Resources Association. Davor Ostojic, Go Sugimoto, and Matej Ďurčo .. The curation module and statistical analysis on VLO metadata quality. In Selected papers from the CLARIN Annual Conference 2016, Aix-en-Provence, 2628 October 2016, number 136 in Linköping Electronic Conference Proceedings, pages 90–101. Linköping University Electronic Press, Linköpings Universitet. Ineke Schuurman, Menzo Windhouwer, Oddrun Ohren, and Daniel Zeman .. CLARIN Concept Registry: The New Semantic Registry. In Koenraad De Smedt, editor, Selected Papers from the CLARIN Annual Conference 2015, October 1416, 2015, Wroclaw, Poland, number 123 in Linköping Electronic Conference Proceedings, pages 62–70, Linköping, Sweden. CLARIN, Linköping University Electronic Press. http://www.ep.liu.se/ecp/article.asp?issue=123&article=004 . A. van den Bosch, G.J. Busser, W. Daelemans, and S. Canisius .. An efficient memory-based morphosyntactic tagger and parser for Dutch. In F. Van Eynde, P. Dirix, I. Schuurman, and V. Vandeghinste, editors, Selected Papers of the 17th Computational Linguistics in the Netherlands Meeting , pages 99–114. Leuven, Belgium. Dieter Van Uytvanck.. How can I find resources using CLARIN? Presentation held at the Using CLARIN for Digital Research tutorial workshop at the 2014 Digital Humanities Conference , Lausanne, Switzerland. https://www.clarin.eu/sites/default/files/CLARIN-dvu-dh2014_ VLO.pdf , July. Claus Zinn .. The CLARIN language resource switchboard. https://www.clarin.eu/ sites/default/files/08%20-%20ZINN-Lg-Sw-Board.pdf . Presentation at the CLARIN 2016 Annual Conference. "
	},
	{
		"id": 287,
		"title": "Culture Analytics Workshop : time series",
		"authors": [
			"Laudun, John",
			"Oiva, Mila",
			"Lapina-Kratasyuk, Ekaterina"
		],
		"body": " This one-day workshop offers an exploration of culture analytics, aimed at an audience of students and scholars interested in understanding the intersection of analytics and the humanities. It follows a three-year program organized by Institute for Pure and Applied Mathematicsat UCLA, gathering a numerous and lively community of people coming from humanities, media studies, computer science and mathematics. Culture Analytics is, by definition, a collaborative, translational data science that explores culture and cultural interaction as a multi-scale / multi-resolution phenomenon. The macroscopic view, that allows a researcher to move from the microscale of close reading, up through the mesoscales, and on to the macroscale of distant reading, is a hallmark of the discipline. Culture analytics as a field is focused on the productive intersection between humanities, mathematics, and data science. Researchers from the Humanities, the Social Sciences, the Mathematical Sciences, and the Data Sciences are now collaborating to identify, document, and integrate concepts, methods and tools that will provide an intellectually and ethically sound approach to the study of cultures across time and across space, leveraging the enormous gains made in the past decade in computation and machine readable cultural archives, from libraries and museum collections to the born digital cultural expressions of billions of people on the internet. This rapid proliferation of digital data has made the role of Culture Analytics all the more central particularly given the potential for significant benefit that lies in harnessing the domain expertise of researchers across these disciplines. Time series are a very important issue that can be addressed with culture analytics methodologies. People working with digitized archives, newspapers, huge dataset of images are interested to discover evolutions, patterns, trends over time. This kind of research questions requires specific computational approaches. This workshop after a general introduction about culture analyticswill present different study cases, followed by other short presentations. After the introductory part the program will be consecrated to a tutorial on time series in scale. For this workshop, the target audience within the digital humanities community are those scholars interested in utilizing more mathematical and algorithmic approaches in the analysis of cultural data. The expected number of participants is 25 persons. For the afternoon part of the workshop the participants will be sent instructions how to download the needed program and dataset to their laptops. The participants will bring their own laptops to the workshop. Schedule of the workshop What is Culture Analytics? - John LaudunEvent Flow: Moments of Innovation, Disruption, and Reflection in Public Discourse - Melvin Wevers& Kristoffer NielboTracing collaboration over time in the Leonardo journal - Clarisse Bardiot, Peter Broadwell, Maria dOrsogna, Mila Oiva, Pablo Suarez, Timothy Tangherlini, Melvin WeversMeasuring and Presenting time series of Astrophotography in Science, Public Discourse andMuseums Exhibitions - Ekaterina Lapina-KratasyukSpreading News Globally in the 19th century press - the Oceanic Exchanges - & Mila OivaLarge images dataset overtime : PixPlot new features - Peter LeonardAudio and video time series analysis - Peter Broadwell& Timothy TangherliniTutorial: Time Series in Texts: From the Micro to the Macro With Kristofer Nielbo, Taylor Arnold and Ross Deans Kristensen-McLachlan This hand-on tutorial offers participants a chance to explore how time series analysis can be used both to examine a single text and to examine a corpus. Examples include a short story, a novel, and a corpus of newspapers. Exercises include parsing texts in various ways and then deriving values through topics and sentiment and then understanding change over time through the Hurst exponent. Workshop participants will need to be familiar with Python or R, or at least interested in becoming so. Interactive possibilities, including application of data from participants, will be made possible through a Jupyter notebook—the default installation of Anaconda is acceptable. Organizing Committee members The organizing committee members were part of the core group of the Culture Analytics Long Program at IPAM. John Laudun is Doris H. Meriwether/BORSF Endowed Professor of English at the University of Louisiana. His research focuses on folk narrative, both as a textual production in and of itself as well as a networked phenomenon. His published work includes a book-length study of embedded creativity and articles on folklore in both traditional and digital environments. In addition to scholarly journals and anthologies, his work is also featured in archives, CDs, films, and television series, and he has received funding from the NSF, NEH, the Andrew Mellon Foundation, the MacArthur Foundation, and the U.S. Department of Education. His work on textual analytics with Python has been featured at CERN, Duke University, among others. Email: jlaudun@me.com Mila Oiva, postdoctoral researcher, University of Turku, Finland. Mila Oivais Cultural Historian and expert on Russian and Polish 20th century history and digital humanities. She works as a postdoctoral scholar at the Trans-Atlantic Platform/Digging into Data funded Oceanic Exchanges project at the University of Turku. She is currently co-editing a special issue Lab & Slack. Situated Research Practices in Digital Humanities to Digital Humanities Quarterly and an edited volume History in the Digital Era for the Helsinki University Press. Her research interests consist of transfer of knowledge and information flows and temporal change. Email: milaoiv@utu.fi Ekaterina Lapina-Kratasyuk, PhD, associate professor, National Research University Higher School of Economics. Ekaterina Lapina-Kratasyuk is a professor and researcher in Media & Cultural Studies with an especial focus on Culture Analytics in field of spatial studies. She is the co-editorof Tuning Language: Communication Management in Post-Soviet Space; co-editorof Interactive City: Urban Life in New Media Age, and author of numerous papers on digital humanities, new media, spatial media and transmedia. She is also a head of a MediaSpace research and educational program in the Space Museum in Moscow, Russia. Email : kratio@mail.ru Clarisse Bardiot, PhD, associate professor, Université Polytechnique Hauts-de-Franceand research fellow at the CNRS. Clarisse Bardiot is a researcher, a publisher and a curator, who has been working in the fields of digital performances history, digital humanities, documentation and preservation of time based media art works, and digital publishing. Since 2012, she has received various grants to develop Rekall and MemoRekall, an open source environment and a webapp to analyse, document and preserve time based media art. Workshop Facilitators Kristofer Nielbo is associate professor of humanities computing at University of Southern Denmark, where he runs an eScience unit for the humanities in the SDU eScience Center. KLN has specialized in applications of quantitative methods and computational tools in analysis, interpretation and storage of cultural data. He has participated in a range of collaborative and interdisciplinary research projects involving researchers from the humanities, social sciences, health science, and natural sciences. His research covers two areas of interest of which one is more recentand the otherhas followed him during his entire academic career. Both interests explore the cultural information space in new and innovative ways by combining cultural data and humanities theories with statistics, computer algorithms, and visualization. Taylor Arnold is assistant professor of statistics at the University of Richmond where he is the co-director of the Distant Viewing Lab. He studies massive cultural datasets in order to address new and existing research questions in the humanities and social sciences. He specializes in the application of statistical computing to large text and image corpora. The study of data containing linked text and images, such as newspapers with embedded figures or television shows with associated closed captions, is of particular interest. Dr. Arnolds work has been funded by the National Endowment for the Humanities, American Council of Learned Societies, Defense Advanced Research Projects Agency, and the Collegium de Lyons Institut détudes avancées. He has authored the books Humanities Data in Rand A Computational Approach to Statistical Learning. Ross Deans Kristensen-McLachlan is a Research Assistant based at Aarhus University. His academic background is in English Language and Linguistics but he now works more broadly in computational humanities and cultural analytics. He has a strong interest in English historical linguistics, as well as cognitive and computational approaches to lexical semantics and textual analysis. Most recently, he has been involved in the Digital Literacy project at Aarhus, where he provides digital support for a number of research projects working with a diverse range of texts – from 17th-century English drama and Stephen King novels, through to Facebook groups and contemporary Danish church sermons. "
	},
	{
		"id": 288,
		"title": "Semantic Corpus ExplorationIntroducing DBpedia Spotlight and WideNet for Digital Humanities",
		"authors": [
			"Olieman, Alex",
			"Beelen, Kaspar",
			"Kamps, Jaap"
		],
		"body": " This tutorial provides a hands-on experience with semantic annotations for selecting text sources from large corpora. While many humanist scholars are familiar with annotation during the analysis of their selected source texts, we invite participants to discover the utility of semantic annotations to identify documents that are relevant to their research question. Introduction Digitization efforts by libraries and archives have greatly expanded the potential of historical and diachronic corpora to be used as research objects. Whereas paper-based research favors a selection of sources that are known a priori to be relevant, the digitization of sources has opened up ways to find and identify source texts that are relevant beyond the usual suspects. Full-text search has been the primary means to retrieve and select a suitable set of sources, e.g. as a research corpus for close reading. Achieving a balanced and defensible selection of sources, however, can be highly challenging when delimiting sources with keyword-based queries. It, too often, requires scholars to handcraft elaborate queries which incorporate long synonym lists and enumerations of named entities. In this tutorial, we instruct participants in the use of semantic technology to bridge the gap between the conceptually-based information needs of scholars, and the term-based indices of traditional information retrieval systems. Since we address working with corpora of a scale that defies manual annotation, we will focus on automated semantic annotation, specifically onentity linking technology. Entity Linking with Spotlight We start the tutorial with a practical introduction to entity linking, using the open source DBpedia Spotlight software. The introduction follows several examples of entity linking in digital humanities projects, with an emphasis on search applications. Participants will be provided with a step-by-step explanation of the statistical model that forms the backbone of Spotlight and many other entity linking systems. The aim of this explanation is to provide participants with an intuition to distinguish between cases in which entity linking systems will reliably produce correct annotations, and more challenging cases which may lead to errors. To conclude this part, participants will engage with a sequence of exercises using Spotlight through a web interface. They will start off using Spotlight on text fragments that have been selected by the organizers, but may proceed to evaluate the software on documents that they have either brought to the tutorial, or can gather on the spot. Exploratory Semantic Search with WideNet The second part of the tutorial engages with exploratory semantic search from the angle of tool criticism. We explore WideNet, a tool based on entity-linking that facilitates the exploration of complex concepts in longitudinal data. Even though linking technologies such as Spotlight remain imperfect, they can be leveraged for useful research applications. WideNet has been created to assist researchers with investigating references to complex concepts in large data sets. Participants learn to apply semantically-enhanced search for the purpose of corpus buildingand investigate the underlying technology that enables them to perform such intricate and comprehensive queries. We discuss how entity-based search solves some of thelimitations of traditional keyword search, but we also attract attention to issues that arise with semantic search. Individual Use and Shared Infrastructure In the final part of the tutorial, we will invite participants to follow along with anJupyter notebook that implements a lightweight entity linking and document selection workflow. It processes text with DBpedia Spotlight, and stores the resulting annotations in an embedded database. The search target, a complex concept, is resolved into concrete entities by performing SPARQL queries against the DBpedia endpoint. We will take the time to inspect the intermediate output and data structures of these steps. At the end of the notebook, the annotations on the input text are filtered and faceted by the result of the SPARQL queries, leading to a set of potentially-relevant search results. For the remaining time, two options are provided to participants. They may choose to continue to work with the notebook, in which case we will assist to adapt the provided code to work on their own documents. This will be recommended for those who intend to apply the workflow from the notebook on locally stored documents in the future. The other participants will be encouraged to independently work with WideNet, to explore topics of their own interest in the pre-loaded corpora. The instructors will be available to answer any questions related to the usage of WideNet, and will provide information about the needed steps to load additional corpora into WideNet. Target audience : Scholars in the fields of Historical, Heritage, Memory, and Literary Studies who wish to explore larger text corpora beyond the traditional keyword search. The tutorial also targets stakeholders who are interested in understanding humanities research practices in digital environments, such as archivists, librarians, and infrastructure providers. Website : A tutorial website has been set up to provide materials to participants. This is a living document that will be updated in the run-up to the tutorial, and which can be used to publish the results of activity during the tutorial. Instructor Information Alex Olieman <olieman@uva.nl> Alex Olieman is a PhD candidate at the Institute for Logic, Language, and Computation at the University of Amsterdam. His research focuses on the application of information extraction, specifically entity linking, to improve the accessibility of information and data. Alex is interested in working with large open datasets, and has done much of his work with government archives. In addition to his research work, he holds a position as an R&D engineer for Qollap, a communication and knowledge sharing platform. He also is an avid open source software contributor, and is an active participant in the DBpedia community. Kaspar Beelen < KBeelen@turing.ac.uk > Kaspar Beelen is a research associate at the Alan Turing Institute. After completing his PhD at the University of Antwerp, he worked on the Digging into Linked Parliamentary Data Project at the University of Toronto and the University of Amsterdam. In Amsterdam, he held the position of assistant professor in Digital Humanities at the Media Studies department. He focuses on computational history, more specifically on the use of text-mining for political and cultural history. His main areas of interest include: gender and politics, the history of political representation and the evolution of affective discourse. Jaap Kamps <kamps@uva.nl> Dr. Ir. Jaap Kamps is an associate professor of information retrieval at the University of Amsterdam, PI of a stream of large research projects on information access funded by NWO and the European Union, vice-chair of the ACM SIG-IR, organizer of evaluation efforts at TREC and CLEF, and a prolific organizer of conferences and workshops. His research interests span all facets of information storage and retrieval – from user-centric to system-centric, and from basic research to applied research. A common element is the combination of textual information with additional structure, such as document structure, Web-link structure, and/or contextual information, such as meta-data, anchors, tags, clicks, or profiles. "
	},
	{
		"id": 289,
		"title": "Publishing (and Forgetting) the Small or Medium-sized Scholarly Edition or Cultural Heritage Collection as Linked Open Data: Using Zenodo and Github to Publish the Visionary Cross Project",
		"authors": [
			"O'Donnell, Daniel Paul",
			"Singh, Gurpreet",
			"Porter, Dot",
			"Rosselli Del Turco, Roberto",
			"Callieri, Marco",
			"Dellepiane, Matteo",
			"Scopigno, Roberto"
		],
		"body": " The Visionary Cross Project is medium-sized international digital scholarly edition and archive of texts and objects from Anglo-Saxon England. The objects it studies include two stone crosses, an altar cross/reliquary, and several poems from the Vercelli Book, one of the four main manuscripts of Old English verse. The objects range in size from approximately 5 m to little more than 30 cm. They date from the 8th through the 11th centuries. They include several of the best known and most studied artifacts from the period. The Visionary Cross Project is both a Digital Library and an edition. Its goals include providing a navigable library of digital replicas of these unique and important artifacts, and a scholarly mediation and assessment of their importance, key features, and relationships. This means that each object must be represented in an appropriate digital format: 3D models, 2D photography, and XML transcriptions. At the same time, the collection as a whole must be overseeable and navigable: users must be able to find their way from 3D object to XML transcription to 2D photograph and back, understanding the relationship of the parts to the whole. Finally, a design goal since the very beginning has been that the project must be easily extensible without negotiation. That is to say that other projects, researchers, and users must be able to discover, access, and reuse independently individual elements of our edition in their own projects without requiring either write-access to our infrastructure or additional permissions beyond the standard licences. In several cases, capturing our data required interventions with the environment that the owners of these objects are unlikely to allow again soon. In such circumstances, others need to be able to discover and access all our material and be able to reuse it for their own purposes. In this paper, we discuss the approach we are taking to the publication of this collection. In particular, we show how Zenodo and Github can be used together in an interrelated set of workflows to disseminate content as both aLinked Open Data Digital Library and aDigital Scholarly Edition. GitHub is a free commercial software development platform, recently purchased by Microsoft, that is widely used by developers to host and review code, manage projects, and build software; it provides a free integrated Jekyll-based web-server that can be used to serve out webpages built within the repository. Zenodo is the data repository of the European Community OpenAIRE project. It is hosted by CERN and provides ready and free access to and storage and tools more commonly used for Big Science projects. In addition to allowing uploads of data from scientific projects, Zenodo can also be used to archive snapshots of projects built in Github, providing long-term preservation and discovery aides such as DOIs to individual GitHub releases. Taken together, these services provide researchers with a complete, free, and archivally responsible way of storing and disseminating data and longer form analyses in formats that work well for both machines and human readers. We are aware that GitHub is a commercial repositorythat does not guarantee long-term preservation. As we demonstrate in the paper, however, the use of GitHub in conjunction with Zenodo does provide archival quality protection for the codebase, each snapshot of which is preserved as a Zenodo record. Zenodo guarantees preservation for the life of CERN + 20 years. Although we discuss these issues and our solution in the context of the Visionary Cross project, our paper is very much not a project report. The approach we present in this paper simply and easily addresses a number of long-standing issues surrounding the longevity, extensibility, and reusability of data and results in the Digital Humanities: It ensures the easy discovery and long-term survival of published data and results with no requirement for future maintenance; It conforms to archival standards and principles for longevity, machine-readability, and linking and is easily scaled; It is fully available for future extension, addition, excerption, reuse, repurposing, or reanalysis by others without negotiation; It ensures that data and contextual analysis are linked bi-directionally, meaning that users are always able to access the discrete data points from which a Humanities-focused analysis and commentary is built and understand each data point in the context of these larger synthetic research products. Just as importantly, the techniques we exemplify using objects and analyses from our collection are easily generalised and extended to any small or medium-sized scholarly edition or cultural heritage collection, especially if these objects and analysis are processed by hand or partial automation. These are projects that, in particular, may otherwise find it difficult to justify devoting resources to developing or implementing the workflows and infrastructure necessary to publish datasets or individual pieces of data in a linkable fashion, especially if synthetic, humanities-focussed research and results are the projects main interest. Although our focus in this paper is on small and medium-sized humanities and cultural heritage projects, the systems we are discussing were developed originally to support big science, which commonly works with data on a scale far beyond even the largest Digital Humanities or Cultural Heritage project. In our discussion, we indicate how our approach can be adjusted for use with fully or largely automated processing and publication systems typical of larger Humanities and Cultural Heritage projects. In conclusion, the main value of this work is that it shows how Linked Open Data standards and techniques more typically thought of in the context of large corpora and libraries can be used productively in small and medium scale projects, especially those in which more traditional, synthetic humanities commentary and research are understood to represent a major or even predominant part of the intellectual value of the edition or collection. At the same time, our paper also shows how dataand analysis can be published in an easily discoverable, archivally sound fashion that requires little to no subsequent maintenance while remaining open to non-negotiated reused and extension. And finally it demonstrates how data publication can complement rather than compete with traditional longer-form humanities research, ensuring that users can access bidirectionally the discrete data upon which an argument is built and the traditionally more synthetic humanities analysis and commentary that typically provides an essential context in small and medium sized collections and editions. "
	},
	{
		"id": 290,
		"title": "A Layered Digital Library for Cataloguing and Research: Practical Experiences with Medieval Manuscripts, from TEI to Linked Data",
		"authors": [
			"Page, Kevin",
			"Burrows, Toby",
			"Hankinson, Andrew",
			"Holford, Matthew",
			"Morrison, Andrew",
			"Lewis, David",
			"Velios, Athanasios"
		],
		"body": " In this paper we report our experiences developing and applying a set of digital infrastructure elements which, in combination, realise a layered digital libraryfor the investigation of manuscript provenance. We describe several related technical contributions: encoding of manuscript catalogue and local authority records as TEI; using Github for version control, issue tracking, and collaboration; automated production of catalogue user interfaces derived from the TEI; an XML processing workflow identifying, extracting, and processing TEI elements for reuse in research; mapping workflow output into a CIDOC-CRM RDF export; reconciliation of RDF entities with external authorities enabling the creation and use of Linked Data bridging multiple datasets. We contextualise the co-evolution of these components and exemplify their use in studies of the provenance of medieval manuscripts. We reflect on the flexibility and extensibility provided by our layered approach, and the independent benefits for catalogers and scholars. Catalogue implementation and Linked Data workflow The foundation layer of the approach described herein is the TEI encoding of manuscript metadata undertaken by the University of Oxford Bodleian Libraries. TEI has previously been used to encode text-based catalogues of manuscripts , and we briefly reference the particular problems and solutions posed for the Bodleian Libraries previously described elsewhere. The digital catalogue records are mostly derived from earlier printed catalogues, though in many cases they have been enhanced and updated for the digital catalogue. More than 9,200 Western medieval manuscripts are described. TEI XML was chosen for this detailed cataloguing of manuscripts because of its rich and flexible syntax: it can encode a complete retrospective conversion of existing catalogue description texts, adding structured markup of specific concepts and identifiers adapted to the various formats of historical catalogues, while allowing a variable degree of comprehensive or selective markup as required or desired. Catalogue records are implemented using a customisation of the TEI P5 manuscript description module , with minor variations for Western, Islamic and Oriental manuscripts. Significant effort has been invested in the creation of local authority files for works, people and places, also using TEI. These have been, in turn, manually reconciled with URIs of records in external authorities such as VIAF, Library of Congress, Bibliothèque nationale de France, Système Universitaire de Documentation, Gemeinsame Normdatei, and WikiData. TEI records are created and edited in the Oxygen editor, and stored in repositories using the Git version control system. GitHub provides for issue tracking and collaboration - requests for modifications or additional markup in support of researcher investigations, such as that described in this paper, can be added, trialled, reverted, or otherwise properly and consistently managed without negatively impacting on the traditional library functions of the catalogue. This TEI layer is, therefore, focussed purely on the creation and maintenance of the XML record files, with appropriate support tools and functionality, and which could easily be transferred to an alternative repository systems should the need arise. While the TEI records are freely and openly available via GitHub, the primary interface for library users is a Medieval Manuscripts collections website , where the full gamut of traditional searching, browsing and viewing functionalities are provided. The website is built using open source technologies including XSLT, XQuery, Solr and Blacklight . Since the website is generated atop a version-controlled check out of the TEI catalogue layer, it too can be developed independently of the other layers. A further benefit of this separation of concerns is the ability to create parallel specialised data layers targeted towards distinct areas of research, supplementing rather than supplanting the canonical TEI catalogue and core digital library functions. Here the flexibility and adaptability offered by TEI is excessive, perhaps counterproductive, for computational processing of the catalogue metadata. Answering specific research questions we instead desire reasoning over logically consistent relationships, such as those in the CIDOC Conceptual Reference Model The CIDOC-CRM , and an ability to cross-reference multiple corpora and authorities using Linked Data: we create a selective RDF layer derived from the catalogue metadata. While theoretically conversion could be include all available TEI elements and attributes, in practice undertaking the mapping is detailed and complex, and limiting scope according to the investigation engenders progress. RDF complements this approach through data structures well suited to future extensions retaining consistency. The first stage of processing simplifies the TEI records, extracting pertinent informationusing XQuery into a more rigid XML structure. This transforms records into a single file, conforming to desired metadata fields, normalising some data, referencing authority files, and building URIs. The second stage ingests our simplified XML into the 3M mapping toolfor transformation to a data model combining entities and relationships from CIDOC-CRM and FRBRoo An object-oriented version of FRBR harmonized with CIDOC-CRM . Entities are also reconciled with local and external authorities, and RDF is exported ready for querying against research questions. In creating these two alternate layers atop the TEI encoded catalogue, we serve several distinct but complementary motivations: a robust, maintainable and consistent record system for cataloguing; a visible and discoverable interface for browsing and searching the catalogue; and a malleable data structure for detailed scholarly investigation. These parallel the affordances offered by the layers encodings, deploying TEI and RDFby their strengths. In the remainder of the paper we focus on the last of these motivations. Application to manuscript provenance research Having described the infrastructural components and overall workflow, we demonstrate the use of this novel digital library for research into the provenance of medieval manuscripts: their origins and movements, and the collectors and owners involved in their history. As the result of changes in ownership over centuries, European manuscripts are spread across the world in diverse library, museum and gallery collections. Information relating their often-complicated histories is dispersed and fragmented across numerous sources, compelling historians and other researchers to make painstaking and time-consuming searches of printed and online catalogues. Digital tools which can assist in these searches, recording their outcomes, are of great benefit; cross-referencing and reconciliation across catalogues even more so. As such, our ultimate aim is to search across multiple distributed catalogues , Furthermore, the RDF described here for our manuscript catalogues has already formed part of Linked Data network combining records from the gardens, libraries and museums of the University of Oxford as part of the OXLOD project, including the Ashmolean, Museum of Natural History, and Pitt Rivers Museum. , using ontologies to resolve conceptual equivalencies and indirect relationships, overcoming differences in underlying catalogue structures, and so enabling unified searching and interrogation. Here, however, we constrain discussion to our completed implementation at the University of Oxford, noting it provides a template for equivalent layer creation over other catalogue systems, and that the queries below will be equally applicable to a cross-corpora search Indeed this reusability and extensibility is one of our primary motivations for using Linked Data. . There has been little previous work transforming TEI manuscript catalogues into RDF suitable for the combined data explorations described here. The Medieval Electronic Scholarly Alliancepublished samples of transformations from the Walters Art Museum into the Dublin Core based schema of the Advanced Research Consortium; while Compton and Schwartzoutline the general motivations and benefits of TEI to RDF conversion. For our modelling, we began by considering TEI markup for the manuscripts records themselves, which can be complex and hierarchical, often describing a manuscript divided into several parts each with its own history and containing works-within-works. Information about the provenance of a manuscript is sometimes encoded with a single XML element describing the entire history of the manuscript, and sometimes as multiple elements each recounting one event in that history. Dates might be encoded with date tags or attributes on the provenance element, and so on. Given this inherent complexity within the data, we identified a reduced set of frames of reference to practically scope our RDF conversion. Consulting with other manuscript scholars identified archetypal questions required of any data investigation. We include an illustrative selection of these queries here, which we reference to their realisation in TEI, simplified XML, 3M mapping, RDF. and SPARQL: How many manuscripts were produced in Northern Italy and/or Lombardy? How many manuscripts were produced in London in the 15th century? How many manuscripts did French collectors acquire from dissolved English monasteries? Our focus on manuscript provenance and associated research questions scoped our choice of elements and attributes to include in the simplified XML: selecting necessary entities, cross-referencing information from local authority files, and creating URIs required for the next stage of processing. The authority files themselves, being essentially flat lists, could be mapped in the 3M tool directly. Within 3M, we show examples of mapping from the TEI customisation to CIDOC-CRM and FRBRoo For example, from the catalogue work itemto F1 Work; msItem to F22 Self-contained Expression; and so on. , taking care to separate evidence derived directly from the text from that which embeds institutional knowledge. Finally, we provide examples of SPARQL resolving the research questions, above, paying attention to how data semantics can overcome complexities not immediately apparent in their natural language form. For example, breaking down a query to retrieve manuscripts from 1550-1600 produced in European countries entails reasoning over variable temporal constructs, mapping and resolution of external spatial definitions. Acknowledgements: We are grateful for the support and insights provided by our colleagues participating in the wider projects surrounding this work: Prof. Donna Kurtz and Gabriel Hanganu of the Oxford Linked Open Data project; and Mapping Manuscript Migrations project team members at the the University of Pennsylvania Libraries, the Institut de recherche et dhistoire des textes, and the Semantic Computing Group at Aalto University. "
	},
	{
		"id": 291,
		"title": "Using Linked Open Data to Navigate the Past: An Experiment in Teaching Archaeology",
		"authors": [
			"Palladino, Chiara",
			"Bergman, James",
			"Trammell, Caroline",
			"Mixon, Eleanor",
			"Fulford, Rebecca"
		],
		"body": " Introduction While there is increasing research about scholarly publication workflows through Linked Open Data in Archaeology, little has been done to take advantage of its potential for teaching. However, LOD is going to be the main method of publication and outreach in the discipline: therefore, it is becoming increasingly important for students and teachers to become familiar with its structure. Linked Open Data is a powerful tool for navigating through the complexity of the inherently multifaceted reality of archaeological sites, which results from the intersections of space, materiality, language, visual culture, history, text, and so on. However, LOD also poses the challenge of how to manage such complexity in a meaningful way. In this paper, we report on an experimental project developed during a Classical Archaeology course in 2018, during which we researched four different Graeco-Roman sites, with the goal of reconstructing the main aspects of their material history through exclusively LOD-based resources. The experiment We explored four sites: Ostia Antica, Herculaneum, Alexandria, and Eleusis. Because the designed ground of the exploration was geographical, we started by using resources that focused on location as the main interlinking structure. We started with Peripleo, the LOD-based search engine of Pelagios, a project whose main purpose is to connect together several partnered datasets, by using places as the common ground. Preliminary choosing the sites to explore enabled to establish a certain hierarchy in the information: first, we gathered the resulting links to gazetteers, to collect information about the site, including relevant geographical connections and attested chronologies. We then used Peripleos internal vocabulary to isolate specific periods of use of our sites, and classified the resulting links according to the database of provenance. Initially, we managed the richness of the results by focusing on specific typologies of information: so, we harvested links to Fasti Onlineto collect data about excavations in our sites; we looked for artifacts through a large variety of databases, including Ariadne, Arachne , WikiData , Vici, the Perseus, and Flickr ; we used these resources to find pictures and basic information, and as a starting point towards other datasets from museums and archives. Coinages and hoards were a prominent feature, which was explored through the resources of the American Numismatics Society, Nomisma , and the Münzkabinett Online Catalogue of Berlin. Textual sources were mainly collected through ToposText , which was privileged for its focus on locations. Inscriptions were found through the EAGLE Network and the University of Graz Online Portal . From each of these resources, we were further directed in our exploration according to the emerging patterns in our findings: for example, the information collected through Arachne was extremely useful to access bibliographical data on religious artifacts in Eleusis; we used Europeana to further research Herculaneum, and from there we accessed the Bodleian Library collection of the Herculaneum Papyri and the related information about their context of finding, the Villa of Papyri, on Pleiades. Researching Coinages of rulers in Alexandria provided a significant amount of information about the Roman emperors that controlled the city, through links to biographical databases. EAGLE enabled us to even access not LOD-based resources, such as the Epigraphic Database of Rome , to collect alternative transcriptions. Results We created object cards, specifying the essential information, the online identifier, and the further connections to other findings or contexts. Collectively, we were able to assemble approximately 50 cards, which, depending on the specificities of the site, included excavations, inscriptions, coinage, textual sources, monuments, mosaics, houses, papyri, sculptures, pottery, and even furniture remains. We then wrote reports about each site, focusing on the best attested chronological periodsor on prominent aspects of material history: in the case of Alexandria, for example, we propose an exploration of material evidence documenting cultural syncretism in the Graeco-Roman era. Pedagogical and scholarly outcomes Discoverability: through Linked Open Data, students are able to discover a variety of archaeological information with scientific reliability. Provided that the tools used are adequate to the purpose, this kind of exploration has a strong potential for non-experts, which is extended to the discovery of less known museums and archives around the world. Complexity: Linked Open Data enables to navigate through an immense set of records, discovering unexpected and continuously new types of findings. This contributes to create a rich picture of the complexity of an archaeological site, which is the result of several interconnected pieces of evidence. Contextualization: LOD makes it possible to explore contextualized information. Students are not only able to look for a specific artifact, but they can dig into the data that provide information on the context where it was found, the people and organizations related to it, the events connected to its chronology. Effective scholarly digital research: students can perform complex research across a variety of different resources, and learned how to navigate through the information of specialized datasets with very different structures, vocabularies and functions. Limitations There is still too much volatility in the adoption of shared vocabularies across LOD resources: this limits the range of searchable items, specifically types of archaeological findings. This is a well-known problem in the world of Archaeological Linked Open data, now further reinforced by user experience. While there has been much investment in LOD infrastructures, the actual availability of data and metadata is still questionable for some archaeological sites. Whereas sites like Ostia and Herculaneum display a considerable amount of semantically diversified and interlinked material, places like Alexandria are more of a challenge in researching accurate and rich information, for the scarce availability of digitized archaeological records. "
	},
	{
		"id": 292,
		"title": "Managing Ancient Textual Corpora through READ: Optimizing Text Input and Text Analysis, Multilingual Support, Recovery and Preservation",
		"authors": [
			"Pallan Gayol, Carlos",
			"Anderson, Deborah",
			"Vail, Gabrielle",
			"Hernandez, Christine",
			"Tamignaux, Céline",
			"Glass, Andrew",
			"White, Stephen",
			"Borghesi, Francesco",
			"Calvelli, Lorenzo",
			"McCrabb, Ian"
		],
		"body": " Moderated by Carlos Pallán Gayol University of Bonn, Germany and Deborah Anderson University of California at Berkeley General panel description: Working with historic text corpora poses specific difficulties for scholars and students: manuscripts and other text materials may be dispersed across different institutions, making it hard to compare materials; the materials may have restricted access, limiting the ability to view and/or publish text recorded on an artifact; and the writing may be difficult to reproduce digitally in a standardized font, impeding the ability to search the text digitally. In addition, proprietary software tools can make sharing texts and commenting on them nearly impossible. This panel will discuss the use of new tools that address the challenges of work on any historic text, drawing on examples from projects working on Mayan hieroglyphic and Latin texts. The tools are extensible and can be used for a wide variety of scripts and languages. This panel will include four papers, each of which brings a different perspective to the challenges of working with historic texts, and discusses how the Research Environment for Ancient Documentsplatform can be used to address those hurdles. A common thread linking all papers together is the evolution of the READ platform, originally created to support alphasyllabic languages such as Gandhari and Sanskrit, but later expanding to accommodate both fully alphabetic languagesand more recently, logosyllabic ones. Each panelist of the following papers will speak for twenty minutes, allowing ten minutes for questions and discussion at the end of the session. The first paper addresses ongoing work to integrate three significant image collections of Mayan hieroglyphic texts on the READ platform that come from two different institutions. The paper highlights the advantages of having a significant thematic overlap between collections that can be readily consulted, analyzed, and annotated by researchers in a single open-access repository. The second paper discusses the development of advanced input methods, enabling real-time typing and accurate rendering of Mayan texts by relying on two Unicode-compliant OpenType fonts and a novel virtual keyboard. These tools will make the Mayan corpus fully machine-readable, while also bypassing image restrictions and limited availability for certain research materials. The third paper will showcase the READ software system by describing the sophisticated database architecture at its core. The architecture bestows READ with increased flexibility to accommodate the varying needs and standards of different research communities working with various ancient languages and scripts. READ provides a range of tools to support palaeographical, phonological, morphological and lexicographical analyses. The fourth paper will discuss various case studies of current research on Latin epigraphic, manuscript, and printed text relying on READ. The authors will address some of the highly specialized tools and flexibility that this innovative platform offers for performing specific tasks and outcomes, such as identifying text forgeries, or more accurately, characterizing text genres through comparative and iterative pattern analyses. Recovering, Integrating and Preserving Textual Archives and Collections of Historical and Archaeological Value: The Case of the Maya Hieroglyphic Corpus Gabrielle Vail, The University of North Carolina at Chapel Hill Christine Hernandez, Tulane University Céline Tamignaux University of Bonn, Germany and Carlos Pallán Gayol University of Bonn, Germany Access by researchers to extant Maya hieroglyphic texts is complicated by the global dispersal of the primary documentation across institutional and personal archives, which may in turn be site- or country-based. Our goal is to reunite Maya texts in a virtual environment to develop a comprehensive repository in which materials from different collections can coexist and dialog within a single platform. Not only does this allow the development of a more historically informed archive to house the documentation history for each ancient text, but it makes it possible to seamlessly integrate data from Maya sites across international borders within a single corpus, while multiple types of documentation for specific objects can be readily compared. We have taken steps to building collaborations with institutions holding significant collections. Through these partnerships, and operating within the institutions policies, it is possible to bring collections together within an open-access platform designed to include the complete corpus of prehispanic Maya texts. Presently, our platform integrates four such collections: two from the National Institute of Anthropology and History, both focused on sites within Mexico, one from Tulane University in New Orleans, which has a broader focus across the Maya region, and the renowned Linda Schele Drawings collection. The first of the Mexican collections includes historical photographs from INAHs Technical Archive from the mid- to late-20th century, which often enhance analysis by showing monuments in a better state of preservation than today. Several monuments from this collection also show monuments that were later lost or show monuments still in situ that were later removed from their original contexts. The second INAH collection consists of systematic digital image acquisition made by the Agimaya-INAH project between 2006-2011 at several Maya archaeological sites, museums, and storage facilities in Mexico. The Tulane repository houses the collection of Merle Greene Robertson, which consists of primary documentation of carved texts from over 60 sites across the Maya area spanning the entire prehispanic period, beginning with the Late Preclassic. These include rubbings of carved monuments, drawings of carved and painted texts, and extensive photographic documentation. The best documented sites are Palenque, Yaxchilan, Bonampak, and Chichén Itzá. Lastly, the extensive Linda Schele Drawings collection is available through a partnership with the Los Angeles County Museum of Artand David Schele, and it encompasses over a thousand high quality drawings made by the late Mayanist Linda Schele at Maya sites such as Palenque, Copan, Yaxchilan and Chichen Itza. The Mayan-READ platform that we are currently developing is a powerful tool to manage and integrate robust collections of annotated image and metadata. To illustrate its workings, we will present examples from the sites of Palenque and Yaxchilan, for which considerable overlap of available documentation exists. Additionally, we illustrate examples of historic images showing portions of monuments that have since been lost. Once these collections are integrated on a single, comprehensive resource, scholars will be able to access them and generate their own analyses and digital editions of the hieroglyphic texts on several levels, and connect them with READs additional resources, such as period-specific glypharies, character lists, quadrats lists, site specific syllabaries, and glossaries cross-referencing terms attested in the hieroglyphic corpus across multiple Mayan languages. The Mayan-READ tool is also designed to pair documentation with entirely digital renderings of glyphic texts based on text input performed with encoded characters. Having these digital renderings online allows researchers to study and discuss them, even when access to high quality images may be restricted due to institutional policies. Moreover, monuments that are damaged, incomplete, or fragmented can be reconstituted by superimposing extant portions from different documentary sources over the digital renderings using an advanced multilayer viewer tool. Ultimately, the resources we discuss are geared towards creating an environment where these ancient documents can be recovered contextually to a significant degree, reintegrated within the larger Maya corpus, and preserved for future scholarship. Figure 1. Screen capture showing database integration of historical records from INAHs Technical Archive collections. Images © Instituto Nacional de Antropología e Historia, México Advanced Text Input, Rendering and Visualization of ancient Mayan texts: towards a fully digital repository of encoded texts Andrew Glass Microsoft and Carlos Pallán Gayol University of Bonn, Germany Recent breakthroughs in Open-Type font development and virtual keyboards made by our team will enable users for the first time to perform text input with encoded Mayan characters directly on a standard document-type or website. Two prototype fonts being developed realistically render the sign-repertoire found within the Mayan hieroglyphic codices and the expanded character set from the stone inscriptions from the Classic period. Since Mayan glyphic signs were not written in linear fashion, but arranged to conform into glyph-blocks, we addressed the challenge of rendering non-linear sign sequences. By thoroughly mapping the myriad possible arrangements that individual signs can take within these clusters, we are now able to describe them with only a small set of descriptors and joining controls. In our text-input method, we indicate not only the precise signs and variants involved, but also their specific cluster configurations. Thus, the user types sequences and presses a conversion key to change to the associated hieroglyph. Structuring is dynamic, based on internal font logic for the signs and joining controls and the prototype font uses a technique that will be fully Unicode conformant, once Mayan has been added to the scripts supported by this standard. While some of the patterns into which Mayan signs could be arranged closely resemble relatively simpler sign-clusters found in other ancient scripts, such as Sino-Japanese-Korean-Vietnamese characters, the Mayan scripts greater degree of visual complexity required us to expand on strategies recently developed for Egyptian hieroglyphs, by introducing additional joiner controls. This enables rendering of arbitrary glyph blocks including complex arrangements with as many as seven signs in a single block. Rendering Mayan texts authentically also requires replicating the ancient layouts by which blocks were arranged. In general, Classic texts were meant to be read from left to right and from top to bottom in paired columns. Accordingly, we are experimenting with a layout manager that allows users to display texts on a grid of n number of columns and rows using paired columns. This system will allow blocks to be arranged either in a purely vertical fashion, as in the Codices, or in a purely horizontal way. It should also support circular layoutsThe Mayan-READ tool that our team is currently developing addresses these text-input challenges from a hollistic perspective, fully realizing that contributing digital editions of Mayan texts to our repository require access to auxiliary tools, such as high-quality images of monuments and metadata from various collections, updated catalogs of Mayan characters, syllabaries specific to the region and period under study, lexical lists of attested terms in the script, and glossaries able to cross-reference several thousand cognate sets across extant Mayan languages. We will illustrate this integration at work, by showing how our tools enable users to annotate passages from the Dresden Codex and the site of Chichen Itza, and render them electronically in realistic, non-linear fashion with Unicode-compliant, encoded characters, and to create online publications and digital editions of texts. Our team is currently integrating these resources to provide users with a fully openly accessible platform able to integrate the combined outcome of multiple researchers and projects into a vast open-source, online Mayan text repository. These standard-conformant electronic texts will be fully machine-readable, thereby enhancing access, searchability, interchangeability and benefits for long-term preservation. These encoded Mayan texts are expected to also be of value in faithfully rendering historical documents without solely depending on images owned by historical collections, which may carry usage restrictions placed by various institutions. Figure 2. Our virtual keyboard for Mayan hieroglyphs, together with dedicated Open-Type fonts being created, allow for fast text-input and layouting of realistic digital renderings of Mayan glyph-blocks in complex arrangements or quadrats. Work by Andrew Glass and Carlos Pallan, NcodeX Project. Digital Text Analysis: Automating Text Workflows and Capturing the Standard of Practice Stephen White Ca Foscari University of Venice, Italy READis a web-based software platform that supports scholars in researching and presenting studies of ancient documents. It is designed especially for documents such as manuscripts, inscriptions, coins, etc. which have a preserved representation on a physical surface. The system supports workflows for text transcription, translation, and palaeographic, phonologic, morphological, and lexicographic analyses, as well as the ability to represent multiple interpretationsof a single document, which can be aligned in various output presentation formats. READ uses a linked data model that represent individual entities such as syllables, words and segmentsalong with the layers of interpretation identified by the researcher, such as physical lines of glyphs, grammatical structures and lemma with attested forms. At the core of the system, READ captures text transcription, the segment and the individual link between a transcription and a glyph. It uses defined ontologies to tag the wide range of research information gathered from the different disciplines using specialized tools. The system also manages the complexity of the link data using software engineering techniques like constraint systems, state machines and state tables. Originally designed to streamline research workflows for ancient Gandhāran textswritten in Kharoṣṭhī, where the writing system knowledge was encoded into entities, relationships, state tables and lookup tables, READ has been recently extended for other language types such as Mayanand Latin. This presentation discusses the challenges encountered while extending READ to support these writing systems and the different standards of practice used by the researchers that study them. Particular focus is given to text import, text critical markup, paleography, and presentation formats. READ Extension to Alphabetic Languages: Case Studies for Latin Francesco Borghesi, University of Sydney, Australia Lorenzo Calvelli, Ca Foscari University of Venice, Italy Ian McCrabb, University of Sydney, Australia and Stephen White Ca Foscari University of Venice, Italy This paper reports on progress of a research collaboration involving the University of Sydney, Ca Foscari University of Venice, Brown Universityand Prakaś Foundation, to undertake the extension of the READplatform from its original support for alphasyllabary languagesto support for the alphabetic language group. The extension of the underlying READ model and enhancement of READ editors to support presentation of alphabetic languages provides a comprehensive platform for Latin epigraphic, manuscript and printed text research, and allows for the transcription, research, understanding, analysis and publishing of scholarly editions and studies. In the first section, the authors will outline the formation of the collaboration emphasizing the advantages of a multi-institutional strategy and alignment with an organization with specialist technical and project management expertise in spite of the challenges that it poses in terms of rigidity of the academic structure and funding difficulties. They will show some preliminary results of case studies on texts ranging from ancient Roman epigraphy to fifteenth century incunabula, and outline the relationship of these research outputs to each of the teams research projects. The project has exercised and extended READ paleographic features to explore the identification of inscribed forgeries. The underlying READ design model of atomization of data to its smallest indivisible components and the linking and sequencing of these entities supports a READ module with the capacity to tag individual characters with palaeographic properties and generate palaeographic reports. This granular annotation of individual characters may also support the identification of epigraphic forgeries through faceted comparison of individual characters. The project has extended READs generic annotation and sequencing features and exercised a strata management workflow, to structure the implementation of multiple orthogonal analysis strataon an edition substrate. An analysis stratum can be any thematic complex of annotations, sequences and semantic links identified with an existing edition substrate. Uniquely labelling the constituents supports collaborating researchers in managing the ownershipand visibilityof an analysis stratum themselves. This stratified implementation was predicated on the delivery of READ capability through the READ Workbench portal. Hosted at the University of Sydney, Workbench delivers READ configured for language/script for individual projects, as software as a service. READ Workbench supported the management of research consultants and research specialists in the development of edition substrates and their collaborations to register their own analysis strata in support of diverse research objectives. "
	},
	{
		"id": 293,
		"title": "Digital Database of WWI Victims from Slovenia (ZV1): Project Cooperation Between the Digital Humanities and Cultural Heritage",
		"authors": [
			"Pančur, Andrej",
			"Blaj Hribar, Neja",
			"Ojsteršek, Mihael",
			"Šorn, Mojca"
		],
		"body": " Various events, which were related to marking the 100th anniversary of WWI, have in recent years further increased the interest in historical resources and research on WWI. Some successful projects took place on the international level, for example, the Europeana Collections 1914–1918, 1914–1918-online and CENDARI, while even more were carried out at the national level. Most of these projects aim at securing access to the digitized cultural WWI heritage, which is kept by GLAM institutions. Some institutions also published databases of thesoldiers from the WW1.The poster will present a digital database of WWI victims from Slovenia, which was created in cooperation between various research and cultural institutions. As many as 10 museums, two archive facilities and two research institutions participated in the consortium. The project was coordinated by the Institute of Contemporary History, which has a well-developed Digital Humanities Research Group, and acts as a national coordinating institution for DARIAH in Slovenia. The project of collecting data on the WWI victims from Sloveniais a good example of successful institutional integration of the local GLAM institutions, research organizations and the digital research infrastructure. Since the project was created upon the initiative of the GLAM Institutions, it provides a good example of successfully overcoming the challenges that the initiators of the Cultural Heritage Data Reuse Charter are facing.The work of the consortium partners was based on the following principles: Separate funding: The project was financed from the existing activities of individual partners. Use of the existing databases: Partners contributed data from their existing databases. Separate data collection: Partners collected data primarily for the purpose of activities carried out in the parent institutions. Open access: The collected information is made available by the partners under the terms of the Creative Commons licenses. Uniform data protection: Partners stored the collected data in the verified repository. Single database: Collected data from all partners were unified and imported into the MySQL relational database. We opted for this type of operation for the following reasons: For the purpose of collecting data on all WWI victims, it was impossible to obtain adequate project financing. Although a lot of volunteer work was carried out as part of the project, the intensity of the project work was determined by the partners alone, according to their wishes and expectations. The consortium has brought together researchers from diverse research institutions, who had varying levels of technical knowledge and experience. This way, the project partners selected the appropriate tools and software by themselves. Most of them used spreadsheet programs, primarily the Microsoft Excel. The designed project had the following shortcomings: Missing data: Since the project partners mostly collected data for a specific region and the consortium was not joined by all relevant institutions from Slovenia, in some regions the data was not collected at all. Non-uniform data: Since the project partners collected data for various reasons, the collected data was not always interoperable. Duplicates: Due to the fact that data was collected by different partners, whereby each did it for their own purpose, it is possible that multiple records were collected for the same person within the common database. The Digital Humanities Research Group eliminated these deficiencies in the following ways: Missing data: Following the model of the German and Austrian genealogical projects, the group used the digitized Casualty Listsfor the entire Austria-Hungary. On the ANNO website, we have scraped OCRs of all available casualty lists in the TXT format. When converting from the TXT format to TEI XML, we used the semi-automatic annotation: regular expression, XPath, XSL Stylesheets. We used the TEI Module for Names, Dates, People, and Places. Non-uniform data: The ability to import and export data in the CSV format and subsequent cleaning of messy data: OpenRefine, R. Duplicates: The core record linkage and deduplication tool is already available in the ZV1 database. For more advanced deduplication of people records, we use the RecordLinkage R-package and the Fine-grained Record Integration and Linkage Tool. An example of an XML TEI markup for WWI victims The editorial project team established a unified data model on the basis of which a MySQL relational database was built. There are currently 26.205 people in the database. The digital base of the WWI victims was designed by using traditional online technologies: HTML5, CSS, PHP7 and JavaScript. We use the ElasticSearch engine to perform the base search. For a tabular display of search results, we use the DataTables, which enables further filtering of data by selected columns. ZV1 database Future Plans: Short-term: To convert as many source databases from project partners into TEI as possible and publish them as digital editions. Long-term: Within the project, GeoNames was used to mark historic places. At the same time, we are planning to classify events and military units. Following the example of successful Linked Open Dataprojects that used WWI data, our long-term goal is to publish information about the WWI victims from Slovenia on the semantic web. "
	},
	{
		"id": 294,
		"title": "The CORLI Consortium: CORpus, Languages and Interaction",
		"authors": [
			"Parisse, Christophe",
			"Etienne, Carole",
			"Poudat, Céline"
		],
		"body": " What is CORLI? CORLIis a consortium of Huma-Numdedicated to the sharing of methodological approaches, tools and software, best practices and training within the community of linguists building and investigating corpora. Organising such a community is particularly complex because people have different theoretical and methodological approaches, practices and needs. The development of digital humanities and the widespread access to data have introduced significant changes in linguistics, both on the methodological and theoretical levels. Tools and methods are developing very quickly and in many places. CORLI is a response to these changes and a means to offer actual solutions to the use, share and reuse of linguistic data. CORLI was created in 2016, following 5 years of experience as previous linguistic consortiums. Its target is written, oral and multimodal language. CORLI is a self-organised network stemming from the linguistic community. It gathers researchers and engineers from as many as possible French linguistic laboratories. The steering committee represents more than 22 different laboratories and CORLI involves more than 180 participants representing various research fields. The goals of CORLI CORLI has a bottom-up approach. It is the practices that researchers use that are promoted to be used by the other groups. If there is a competition between different views, CORLI try to help people to discuss this, but doesnt decide for others what is good or not. The goals of CORLI are to promote visibility, reuse, easy access, good practices in corpus linguistics, as well as to help developing corpus, tools and formats. CORLI help researchers in performing complex tasks they would not have the means to achieve without this support. Researchers can ask help to CORLI, especially in domains where they feel underpowered or not sufficiently knowledgeable. Then the goal of CORLI is to find a way to answer this or to find information about this - or organise people to work about this in the community. CORLI has general and specific actions according to what is needed by the community. General actions mainly concern financial help of people having corpora at their disposal that were not published by lack of financial resource, technical knowledge or adequate support from the infrastructure. CORLI helps projects to finalise research or organise data so that new corpus are available. Gathering and choosing data is up to the researchers. General actions also concern recommendations about the evaluation of resources according to relevant criteria, the availability of data, legal information, and formation to the use of software for data creation, promoting international standards, editing both data and annotations, and search tools. Moreover, CORLI contributes to CLARIN and DARIAH international infrastructures. Specific actions are on the other hand carried out within three workgroups which have been created in that respect:the first one focuses on interoperability, standards and corpus exploration;the second group concentrates on complex corporawhereas the third group works on multilingual corpora. The three groups work on three scientific challenges of corpus research which are basically related to different levels of complexity: oral and written corpus communities have developed separately, adopting different standards and tools. One of the goals of CORLI is to reunite the two communities, developing common tools and standards, as well as interoperable software; how to standardise and analyse complex corpora? Another goal of CORLI is to provide researchers the means and the infrastructure to develop adjusted standards and to share methods and options. Realisations made by CORLI in the last few years were: about 50K € each year to help finalise projects participation in norm and recommendation for coding corpus and metadata creation of software to help conversion of format and metadata edition creation of white papers and recommendation for juridical problems workshops and meetings to discuss actual technics or difficulties information to the community about CLARIN continuing education for tools and practices Usually, meetings and recommendations are in French because they target French speaking people. Some projects had an international public, so in this case, English was used. One of the goal for 2019 is to create a bilingualwebsite so as to extend our work with other communities and other countries. "
	},
	{
		"id": 295,
		"title": "Some GIS-Based Analysis of the Complete Taiwan Poems",
		"authors": [
			"Peng, Yi-Fan",
			"Liu, Chao-Lin"
		],
		"body": " Introduction The collection of Complete Taiwan Poemsincludes traditional Chinese poems that were authored between 1661 CE and 1945 CE in Taiwan. This time period covers the Kingdom of Tungning in Taiwan, the Qing Dynasty in Taiwan, the Japanese occupation periodand the post-war periodWe combine natural language processing and spatial information technology to analyze the literary trajectory and geographical distribution of poets and their poems. Assisted by digital tools such as geographic information systemswhich can manage and present spatial data, we can describe the connection and evolution between poets, poems and space. Spatialization of literary works In this study, we extract spatial and temporal information about the poets and their poems, and present the information via a GIS interface. Figure 1 shows the main steps of our work. Figure 1. Main steps of our work Identification of place names At this moment, we focus on poems that consist of five or seven-character lines because they are the majority in CTP. Table 1 shows the statistics for the poems in CTP. In order to spatialize the literary materials, we extract the words that carry spatial attributes by marking words as place names at the step of part-of-speechtagging. Before that, we have to segment Chinese strings into words. Chinese word segmentation is a very difficult task for traditional poems, and the segmentation results have a considerable impact on the subsequent analysis. In addition to employing the CKIP segmenter, we also rely on some heuristics for Chinese segmentation that are applicable to traditional Chinese poems. Table 1. Statistics for the poems in CTP Table 2. Statistics about the Nc words. The numbers in parentheses show the amount of the unique Nc words Figure 2. The distributions of lengths of Nc words Relying on a heuristic, we may segment a line of five characters in two ways: 212 and 221. For example, we can divide 奉命籌軍國 into 奉命-籌-軍國or 奉命-籌軍-國. Similarly, we may segment a line with seven characters into two ways: 2212 or 2221, e.g., producing 昔日-東寧-今-豫章or 昔日-東寧-今豫-章from 昔日東寧今豫章. Applying these heuristics with the CKIP segmenter, we segment the CTP poems. The CKIP segmenter also does POS tagging, and Nc is of the code for place names. We show some statistics about the place names in Table 2. Filtering and geocoding After the POS step, we filter the list of Nc words for unique place names. Figure 2 shows the distribution of lengths of these unique words. After filtering the place names, we attempt to geocode the unique place names. The process of geocoding is shown in Figure 3. We do geocoding in two steps. First, we use the API of Chinese Civilization in Time and Spaceprovided by the Academia Sinica. This is a service created by the Center of GIS, RCHSS, Academia Sinica. The place names are geocoded by specialists based on many historical documents. Available from: http://ccts.sinica.edu.tw/api [cited 2019 April 29] CCTS reports reliable spatial information for places in Taiwan and China. If we cannot find a location for a name via CCTS, we continue geocoding via the Google Map API. Google. Developer Guide | Geocoding API [cited 2019 April 29]. Available from: https://developers.google.com/maps/documentation/geocoding/intro. We ignore names that cannot be geocoded by both services. Figure 3. Flowchart of the place name geocoding Recall that the results of the word segmentation are not completely correct, so there may be some errors with geocoding. For instance, some place names are geocoded to other countries. To avoid these problems at this moment, we use a spatial processing method to eliminate the points outside Taiwan and China. Application examples We can present and analyze the data with GIS, and offer three applications to demonstrate the spatial visualization of literary data. Temporal and spatial distribution of poets birthplaces After analyzing the personal data of 844 poets, we can show the poets birthplaces on maps. They lived in different dynasties. With these two variables, we generated four maps for different time periods, shown in Figure 4. Figure 4. Temporal and spatial distributions of poets birthplaces If scholars have more ideas about the distribution of data, the data can be further analyzed through spatial methods. Figure 5 shows a further example. It analyzes the distribution of poets in parts of Taiwan. It shows that the changing distribution densities of poets in Tainan, Taipei, and Changhuain these time periods. This may provide hints for studies of the socioeconomic developments and cultural activities of those years. Figure 5. Kernel density estimation maps of poets in Taiwan Temporal and spatial distribution of place names in poems When literary experts can confirm the creation times of poems, we can analyze the places that were mentioned in poems of different time periods. Figure 6 shows some possible outcomes. There are 12 pointsin Figure 6, 1674 points in Figure 6, 3210 points in Figure 6, and 221 points in Figure 6, respectively. Figure 6. Temporal and spatial distributions of place names in CTP Splitting the 200 years of the Qing administration in Taiwan into four sub-periods, Figure 7 helps us see that some place names are more popular in different periods. Place names of the eastern part of Taiwan were mentioned less in the early Qing Dynasty. Scholars can discover more phenomena based on the spatial-temporal results, and interested scholars can certainly investigate the implications of the changing trends. Figure 7. Distributions of place names in Taiwan in CTP poems in different Qing periods A poets trail We analyze the contents of the poems and personal data of Lian Heng, Lian Hengwas a very famous and influential person in the history of Taiwan. and show the results in Figure 8. When he was young, most of the places he mentioned were in Taiwan and China. During his middle age, he mentioned many places in Taiwan and Japan. In his older period, he mainly focused on Taiwan, occasionally mentioning the place names of China and Japan. Figure 8. The trail of Lian Hengs place names in different periods of life We can find that poets may have some preferences for the place names in their works in different periods. Through the spatiotemporal analysis of the poets trails, we may pursue deeper issues about the observations. Wang et al. have done similar analysis for Yu Yonghe.Conclusion We analyzed the poets and poems in the Complete Taiwan Poems via GIS viewpoints. By combining the techniques of natural language processing and the methods of spatiotemporal analysis, we present information about poets and poems geographically. We demonstrate our ideas with three examples: a poets trail, the distributions of poets birthplaces, and the distributions of place names in the poems. We hope that these cross-disciplinary explorative results may inspire fruitful ideas for literary research. Acknowledgements The research was supported in part by contracts MOST-104-2221-E-004-005-MY3 and MOST-107-2221-E-004-009-MY3 of the Ministry of Science and Technology of Taiwan and in part by projects 107H121-06, 107H121-08, 108-H121-06, and 108H121-08 of the National Chengchi University. We are grateful to the National Museum of Taiwan Literature for their sharing the Complete Taiwan Poems and their comments on our work. "
	},
	{
		"id": 296,
		"title": "DH Text Submission GuidelinesA Digital Enquiry On The Italian Reception Of The English Novel In The Periodical Press Of The Long Eighteenth Century",
		"authors": [
			"Penso, Andrea"
		],
		"body": " This poster aims at showing the first results of the ongoing collaborative research project The reception of the English novel in the Italian literary press between 1700 and 1830: a transcultural enquiry into the early shaping of the modern Italian literary and cultural identity. The project aims at investigating the reception of English novels in the Italian literary press during the Long Eighteenth Century. The analysis focuses on an existing corpus of data relative to the publication, dissemination, translations, critical reviews, and editorial advertisements of English novels in Italian literary newspapers and journals of the time. The main purpose of the project is to uncover how the English novels were introduced to the Italian readership through literary journalism with the application of digital methodologies of investigation. One of the project goals is in fact to create a methodological paradigm that may be extended to the study of the reception of English novels in the literary journalism of other nations. The present paper therefore has three primary objects: a) To show to the public the first research output: an open access, bilingual, and annotated digital repository, which consists of a Drupal-based software for corpora, and represents an immediate way to develop the research. The first step of the project has been the cataloguing, analysis, and digitization of the corpus of reviews. This preliminarily created digital database allows the subsequent computational, textual and critical surveys. b) To show how the TEI has been applied to the analysis of the corpus. The text encoding of the reviews, conducted following the TEI standards, makes possible to point out the elements that are original and innovative with respect to the foreign reviews of the time which the Italian press copied from, often adapting the contents. In fact, the encoding of the reviews allows also to understand and visualise their genealogical dimension. These sources have already been identified, and the comparison between the encoded versions will allow to understand the extent of the influence French and English journalism had on the Italian press, and to outline the specific Italian input. The TEI will therefore make possible to focus on the re-interpretations of Italian reviewers who drew on the Italian literary tradition but challenged its subjects, genres and linguistic structures. Ultimately, the TEI will also be applied to integrate the stylometric analysis and the gathering of Geospatial information: the short presentation will allow me to show a case study and to explain how the mark-up process proves to be a fundamental part of the methodology for the analysis of the corpus. c) To illustrate the two main lines of approach that will be applied in order to digitally explore the corpus. The first consists of a stylistic and linguistic analysis of the reviews, which will be pursued equalizing and comparing stylistic and lexical constellations belonging to different discursive practices from a number of periodicals and journalist. Digital stylometry, word frequency and statistical analyses tools such as R, MiniTab and Intelligent Archive will be used during this phase. The study of the readers response to the contents, spread by the novels via the reviews, is deeply connected to the stylistic analysis of the reviews. In fact, the outlining of the reviews stylistic features is crucial to understanding in which ways the contents were revealed to the public, and how the audience was influenced in the perception of the moral values and the social messages of the novels. The second line of approach will concern the spatial analysis of the data, which will be mapped thanks to GIS digital tools integrated with Geo-criticism. The spatial analysis allows the visualization of popular reading trends in 18th and early 19th century Italy. "
	},
	{
		"id": 297,
		"title": "The Open Citations Movement",
		"authors": [
			"Peroni, Silvio"
		],
		"body": " Purpose A bibliographic citation is a conceptual directional link from a citing entity to a cited entity, for the purpose of acknowledging or ascribing credit for the contribution made by the authorof the cited entity, as stated by Newton in his motto – If I have seen further it is by standing on the shoulders of Giants. The citing and cited entities may be scholarly publications, online documents, blog posts, datasets, or any other authored entity capable of giving or receiving citations. While the act of citing by the author may be the work of a moment, the citation itself, once the citing work is published, becomes an enduring component of the academic ecosystem. This abstract introduces the uptakes and the benefits of releasing a huge set of open citation data as public domain material. In particular, we introduce the main events that have characterised this movement towards opening citations by discussing what has happened with the establishment of the Initiative for Open Citations. We accompany the discussion with examples and projects that were born as a consequence of this movement, including some contributions from the Digital Humanities domain. Definition As stated by Peroni and Shotton, a bibliographic citation is an open citation when the data needed to define the citation are FAIRand, in particular, are: Structured – expressed in one or more machine-readable formats, such as XML, JSON, RDF; Separate – available without the need to access the source bibliographic entityin which the citation is defined; Open – freely accessible and reusable without restrictions, for example by publication under the CC0 1.0 Universal waiver/license or generally released as public domain material; Identifiable – the citing and cited entities described by an open citation must be clearly identified by using a specific persistent identifier schemeor a URL; Available – there must exist a mechanism to resolve the identifiers of the citing and cited entity to obtain the basic metadata of both the entities, i.e. sufficient information to create or retrieve textual bibliographic references for each of the entities. Findings The first project that introduced the open availability of open bibliographic and citation data by means of Semantic Webtechnologies was the OpenCitations Corpus, in 2010, which was one of the outputs of the Open Citations Project funded by Jisc. However, the availability of open citation data recently changed drastically with the establishment of the Initiative for Open Citations, in April 2017. The Initiative was born with the idea of promoting the release of open citation data and has explicitly asked the main scholarly publishers, who deposited their citations on Crossref, to release them in the public domain. As a result, now we have several millions of citation data openly available on the Web, a list of important stakeholders – such as libraries, consortiums, projects, organizations, companies, and, in particular, founders– supporting the movement, and several international eventsorganised for promoting the open availability of citation data. Several projects and datasets have been released so far so as to leverage the open citation data available online. As a result, there is a growing list of publishers that release their citation data in Crossref, and these citation data also come from important journals in the Digital Humanities field such as Digital Scholarship in the Humanities published by the Oxford University Press. Research implications Several citation indexes, such as Clarivate Analyticss Web of Science and Elseviers Scopus, make available citation data in a structured form. However, their access is possible only by paying expensive fees of several hundreds of dollars. In addition, the license associated with these data is quite restrictive and does not allow one to reuse them for any purpose. The current coverage of the open citation data available is still far from being competitive with the aforementioned well-known proprietary citation indexes. However, open citation data already allows researchers – particularly those ones working in institutions that cannot pay exaggerate fees to access the aforementioned commercial indexes – to work on citation data and perform interesting and important discoveries. Practical implications Collecting all the citation data from the whole scientific knowledge in just one centralised repository is practically unfeasible. The only long-term solution is to set up a federation of decentralised citation databases that can co-operate with each other by means of Web technologies, as suggested in– see, for instance, the RDF datasets made available by Wikidata and OpenCitations. Social implications Citations have been one of the core parts of the scholarly system since the beginning. However, they are not currently seen only as an acknowledgement medium, but rather have recently been used according to additional functions. For instance, citation networks can be characterisedtopologically by defining the connected graph between citing and cited entities during time,sociologically such as for identifying odd conducts in or elitist access paths to scientific research,according to quantitative rationales by creating citation-based metrics for evaluating the impact of an idea or a person, anddefining a sort of economic value, i.e. the currency with which a researcher addresses his/her own academic sustenance. Having open citation data available is fair, since it enables anyone, from scholars to citizen scientists, to study and follow the evolution of science during time according to all the aforementioned perspectives. Value Open citation data makes a positive disruption in the world of scholarly communication since they change entirely how we face to science, its evolution, and all the related context, such as research assessment evaluations, science of science, bibliometrics, and future scientific discoveries in all the domains – including the Digital Humanities domain. "
	},
	{
		"id": 298,
		"title": "African Languages And Digital Humanities: Challenges And Solutions",
		"authors": [
			"Petrollino, Sara",
			"Nyst, Victoria"
		],
		"body": " The field of African language technologyhas seen a rapid development in recent years, and several digital humanities projects and hubs have been established across the continent. Language documentation projects have focused on several endangered and minority languages, producing large digital corpora and data sets for African under-described languages. What is the place of African languages in the African digital landscape and what is the state-of-the-art of African digital scholarship? What are the challenges and the solutions for a DH approach in the field of African languages and linguistics? What are the good practices for building African-based repositories, language infrastructures and other digital capabilities? What is a sustainable model for the engagement of wider audiences and for digital capacity building in Africa? How can we address ethical issues and the tension between the trend towards open access and the need to protect privacy and property rights of community speakers and researchers? The panel brings together scholars from different backgroundsto answer these and other related questions and to share the experiences of scholars who are directly involved in DH research in Africa and in the management of African-based digital archives, repositories and infrastructures. In this way we hope to have a first birds eye-view of DH research for African languages, which will allow a critical discussion of the nature and future of the field. Papers: Tunde Ope-Davies, Digital Humanities, University of Lagos Reframing community building and civic engagement in L2 public sphere: A study of new media multilingualism in Nigeria democracy. One striking effect of the ongoing digital revolution is the evolving reconfiguration of the public sphere in most socio-political jurisdictions. From Europe to the Americas, and from Asia to Africa, social media is revolutionizing communications and social networking activities, redefining the mechanics of our daily interactions in private and public spaces. In the last one decade and more, reforms in most Telecommunication sectors and increase in internet penetration have positively impacted communication practices in Africa. In some of these democratic contexts, the phenomenal growth in computer-mediated communication has made the practice of democracy more participatory; creating a more virile public sphere. Citizens online activities have expanded due to rapid growth in internet penetration and the proliferation of social media platforms now accessible through various handheld devices, laptops, and more recently affordable smartphones. This present study addresses gaps in online multilingual political discourse studies in Africa, using Nigeria as a case study. First, it examines the use of social media within the Nigerian socio-political context. Second, it discusses the extent to which social media platforms have provided tools and possibilities for participatory and inclusive democratic process. Pivotally, the study focuses on how the emergence of new media multilingual mechanism reshapes online political conversation and social engagement in Nigeria. It studies how the use of English and some local languagesin online posts help to facilitate community building, social mobilization, social networking, and foster civic engagement. Also, it discusses how these new technologies absorb/adopt some offline linguistic behaviour to inspire online social networking and participation in public conversation on current and topical political issues. The data was drawn from the repository of the Corpus of Nigeria New media Discourse in English, an ongoing Digital Humanities project at the University of Lagos. In eliciting the data deposited in the corpus, we utilized web-based corpus tools and applications to harvest relevant posts and chats from the websites, Twitter handles and Facebook pages of key political parties and political actors in Nigeria between July 2014 and October 2018. Additional data was elicited from the online portals of three major national newspapers in Nigeria. Relying on theoretical insights from Computer Mediated Discourse Analysisand Speech Accommodation Theory, the study considers language choices mediated by new technologies as strong motivation for innovative discourse strategies being deployed in this context. Among others, some of our findings suggest that new media platforms now accommodate and instantiate some offline socio-linguistic behaviour found in second language English-speaking contexts in Africa. For instance, some features of Speech Accommodation strategiescommon in African socio-cultural contexts, and online socio-pragmatic discourse cues, now constitute a key component of communication strategies adopted in building online community and promoting civic engagement within the public sphere. Emmanuel Ngué UmThe Asynchronous relationship between Digital Archives and the discipline of Linguistics in Africa The dominant trends in present day Linguistics in Africa are both theoretical and applied. Theoretical research is geared towards either eliciting structural features of envisaged linguistic systems or the testing of grammatical theories which require little or no data for their operationalization. Applied linguistics is mostly concerned with language teaching and resorts to naturalistic language data only for the sake of illustration. Running contrary to this trend, Digital scholarship and language archiving operate on infrastructures that provide a integrated, data-conducive environment for the assembly, organization, processing and publication of research. As it is, therefore, current practices in Linguistics in Africa do not seem to be in vital need of Digital Archives. This presentation does not focus as such on the structural discrepancy between language archives and the discipline of linguistics in Africa. Instead I will attempt to interrogate the relevance of a century of academic endeavorwith regard to the complexity of its object, namely Africas extensive multilingualism. One crucial question which needs to be asked, at this juncture, is to know the ultimate end of current linguistic research. What do linguists in Africa aim at when they set out to analyse aspects of the linguistic reality? What is the significance of the information that each piecemeal research produces? Are linguists in Africa working together towards achieving common goals or, has the discipline of linguistics become a routine in the academia which serves as justification for the continuity of an institutional business? Answers to the above questions may neither be obvious nor straightforward. However, most stake-holders will admit that our discipline, in Africa, does not pursue coherent objectives from one university to another. It could have been hoped that the spur of language documentation into Africas linguistics over the past fifteen years and the lobby thereto of funding bodies such as DOBES and ELDP could have regenerated the linguistics scholarship in the continent. The increasing amount of data which has been harnessed from documentary projects and which are accessible via well-established and archives such as LATand ELAR do not seem to have substantially impacted the scientific agenda of the discipline of linguistics in Africa so far. I will content in this presentation that, embracing digital archives and digital scholarship in the discipline of linguistics in Africa is tantamount to shifting from the current ad hoc, individualistic research paradigm, to communal scholarship. This entails definition of common research objectives and methodologies, from data collection and organization to data dissemination. This also entails mutualization of the scientific information and cross-verification of structural analyses from one named language data set to another, which would aim at pursuing a general understanding of the underlying experiences which justify the fact that, under apparent linguistic variation, there seems to be an abstract cultural reality which no individual grammar of a named language could help to uncover. Moses E. Ekpenyong Department of Computer Science, University of Uyo, Nigeria Intelligent Humanities: Towards High Performance Applications for African Languages Todays society is witnessing the production of extreme large data setsthat has challenged traditional processing and storage methods. This sudden state of data explosion has indeed introduced major changes into existing data management processes, demanding a robust and sustainable solution, to efficiently process, analyze, store, share, and disseminate data into the future. From experience, the processing mechanism can be structured following the standard data mining pipeline, and the degree of difficulty scales with the complexity and volume of data. Hence, massive digital objects in the humanitiesrequire suitable methods to guarantee useful extractions, for meaningful interpretations. In this contribution, we examine the role computer algorithms play in mining, shaping and representing data in the humanities. We propose an intelligent framework that maintains consistent applications that go beyond traditional methodologies, to reveal inherent patterns, trends, associations, and especially relates to human behavior and interaction – the Big Data experience. Linking the past and the future constitute our Digital Humanities efforts in the University of Uyo, Nigeria – a product of an interdisciplinary cooperation between the Computer Science and Linguistics and Nigerian Languages departments. The first stage of the proposed framework is predicated on cross-domain metadata, and metadata mapping – which pose the problem of connecting existing metadata to embedded links. The key challenge however, is the metadata mapping – as heterogeneity of inputs complicates one-to-one mapping, and harmonization of the metadata and ontologies appear intractable. As such, experience and best practices are mandatory when transforming and consolidating formats into internal knowledge representation, for clustering and reasoning. Learning from existing data constitutes the second phase of our framework that drives the intelligence, required to enhance the classification process – for accurate prediction and visualization of the data sets. This at the end maximizes the use of the digital resources. Further, dissemination of the resources can be achieved through a Web Management Interface, after proper ethical and copyright procedure has been followed. An implementation of the proposed framework to salvage a critically endangered language, Medefaidrin is demonstrated in this paper. A multilingual application that embeds intelligent techniques for the analysis and visualization of prosodic features of selected West African languages: Medefaidrin, Ibibio, Igbo, Yoruba, and Hausa, is developed. The speech corpus adopted is the Ibadan 400 words – a list of basiclexical items of any language, translated and recorded in the various languages. The developed application is Web-based and enables useful pattern discovery that reveals the dynamic nature of these languages, as well as aid the pronunciation and simulation of sentences – a necessary Computer Aided Learningtool. Juan Steyn, South African Centre for Digital Language Resources & Digital humanities Association of Southern Africa Building Sustainable Digital Infrastructures The South African Centre for Digital Language Resourcesis a new research infrastructureset up by the Department of Science and Technologyforming part of the new South African Research Infrastructure RoadmapThe centre, which is currently still in its incubation phase runs two main programmes. A digitisation programme and a Digital Humanitiesprogramme. The digitisation programme focusses on the creation text, audio and multimodal datasets as well as the development of NLP tools and software for the 11 official languages of South Africa. The DH programme focusses on enabling and promoting the use of digital data and new methodological approaches within the broad Humanities and Social Sciences. The establishment of the SARIR programme, which aims to provide a high-level strategic and systemic intervention, provides South African researchers with a unique opportunity to develop and foster new research fields and possibilities. This paper will share some of the challenges experienced during the current incubation period as well lessons learned by specifically reflecting on: What to keep in mind when building a new language related RI. Providing access to resources and tools through a RI is not enough. Why catalysing and sustaining contextualised capacity development is very important. Where to find collaborators and training partners. The value of engaging and working with the international Software, Data and Library Carpentry community. Successes in promoting DH approaches in African language teaching, learning and research contexts for faculty and students. Stories on re-defining DH from a Southern African perspective. Setting up a RI is only part of the process. The end goal is to ensure that the wider research community is able to not only access resources and tools but enabled to critically apply new skills to address the challenges of tomorrow. Felix Ameka, Leiden University Ethical issues in digital data collection and exploration All forms of research in the language sciences critically depend on data collection from language users. Increasingly and universally data collection methods and processing are digital. Moreover, there is the growing realisation that to understand language practices we cannot ignore the visual mode of language. This can only be captured through digital video. Furthermore, researchers are increasingly under pressure to make their data openly accessible. However, such digital data carries representative of data providers. How can we ethically go about these? In this talk, I will raise the ethical challenges involved in collecting, processing, curating, storing and exploring different forms of data. How can we carry out these activities paying attention to the principles of justice, and of beneficence/maleficence, i.e. practices should not be harmful to research participants "
	},
	{
		"id": 299,
		"title": "Identifying Similarities in Text Analysis: Hierarchical Clustering (Linkage) versus Network Clustering (Community Detection)",
		"authors": [
			"Jeremi, Ochab",
			"Joanna, Byszuk",
			"Pielström, Steffen",
			"Maciej, Eder"
		],
		"body": " The aim of this paper is to introduce to stylometry the methods allowing for evaluation of classification results obtained withhierarchical clustering methods, with the distinction of performance of individual linkage methods, andnetwork clustering, with the comparison of community detection techniques. We compare three recognized evaluation measures: AMI, ARI and NMI using 6 model datasets of known clustering, of which three constitute binary problems and three – corpora with a large numberof expected internal groups, which were designed for authorship attribution. Our results showsuperiority of Ward linkage method as compared to 5 other,greater performance and stability of Cosine Delta for both hierarchical and network clustering,Louvain as the most reliable method of community detection, andusefulness of AMI method for hierarchical clustering, which we propose for general use making our scripts available. Introduction For visualizing the stylometric structure of a text corpus, many studies and popular tools like stylorely on explanatory methods and intuitively-interpretable visualizations, usually belonging to either tree charts as produced by hierarchical clustering, or network representations. The intuition behind it is that the graphs will group texts of high stylistic similarity close together, thereby allowing the spectator to visually identify multiple levels of groupings instead of simply singling out one nearest neighbour for each individual text. There were even some methodological studies on stylometric distance measures based on evaluating the clusterings they produce. Despite the popularity dendrograms and networks have in stylometry, there is to this date no systematic investigation on how these methods behave when applied to problems in computational stylistics – research usually focuses on method selection from general pool of classification approaches. In clustering, both fine-grained clustersand larger groups containing several textsare generated by the method itself – a very reproducible process – but particular clustering method used is a matter of choice. Most cases rely on the Ward linkage methodwhich is the default in stylo, but its advantages and disadvantages for the research field have not been investigated systematically. E.g., Hoovers choice of Ward linkage is based on a concise comparison of Ward, complete, and average linkages, but the methodological design of the comparison has not been outlined in the paper. The case of network analysis is even more problematic, as low and/or high level groups are often/usually only identified visually. The best approach to operationalize how people perceive group formation in networks is to use community detection. This study aims to constitute an initial step to the systematic investigation of these issues, which we do by analyzing the quality of grouping achieved by different clustering and community detection algorithms in comparison to known properties of different test corpora. Corpora We conducted our study on 6 text corpora, each assembled with an inherent internal structure with groups expected to be stylistically distinct. We used both corpora with a large numberof internal groups, and corpora that can be separated into two major stylistic groups. As examples with a large number of low-level groups, we used three reference corpora for authorship attribution, each composed of 75 novels by 25 authors. These corpora, from English, French and German literature were previously used in various studies on text distance measuresand are freely available. As examples for problems featuring two major stylistic groups, we tooka corpus of 17th century French drama published by Christof Schöch, where we removed all texts not clearly labeled comedies or tragedies;a corpus containing Latin verse and prose from the so-called Golden Age;a corpus of Latin historiography containing texts from the very same Golden Ageon the one hand, and the Silver Age on the other. The Latin materials were assembled from the Latin Library. Methods: Clustering quality measures There are many similarity indices for clustering evaluation. In this pilot study we focus on three best understood: ARI, AMIand NMI. The first two are adjusted for baseline value in case of random clusteringbut not fully for selection bias, while the third one is not adjusted. Standardized Mutual Informationmight offer a further improvement in future studies. Methods: Community detection methods in networks Community detection methods allow for automatized search for natural division of network into smaller clusters, that is communities. They can be applied to bare distance tables, or on pruned or otherwise sparse networks. We use A not so small collection of clustering methods facilitating consensus clustering, which includes such algorithms as OSLOM, Infomap, label propagation method, and modularity optimization by simulated annealingand by Louvain method. Methods: Experiments We first conducted a large series of stylometric tests that were later used for evaluation. For each dataset we conducted myriads of tests for a total of 160 basic experimental scenarios, being a combination of the following changing factors of:number of MFWs,distance measureandlinkage method. We then evaluated quality of the clusterings in the results, considering the number of clusters ranging from 2 to the number of texts in the corpus using a script implementing ARI, AMI and NMI indices. E.g.: for any of the 75-book corpora, we would obtain 160 scenarios and test 74 clustering options for each of them, obtaining a total of 11,840 clustering scenarios up for evaluation. We performed similar experiments utilising networks: in one, we used all the mentioned community detection methods together with consensus networks; in the second experiment, we used distance tables instead. Let us note that modularity optimization is well defined in the case of directed and weighted networksand indeed it is the only one that produces non-trivial clustering of distance tables, yielding weights in the range. Owing to that we could use consensus clustering over distance tables that were generated by a range of the selected most frequent words. Results: hierarchical clustering In all of our tests, Wards linkage outperformed other algorithms, both for Classic and Cosine Deltas. Notably, this particular linkage method was designed for large-scale tests of more than 100 samples: the speed, not the optimal clustering was a priority. Cosine Delta proved superior of the two distances – it systematically scored higher and gave maximal agreement with ground truth at the same number of clusters, even for a small MFW number. Fig. 1 Clustering quality for the French corpus, CosineDelta distance measure and Ward linkage using different MFW ranges. An example of clustering assessment, here for the French corpus, is shown in Fig. 1. Vertical dashed line marks the expected number of clusters. Each curve represents values of one quality measure for a given number of MFWs, and dots mark their maxima. AMI maximum falls around the expected and intuitive number of authorial clusters. NMI is heavily biased towards a larger number of smaller clusters, in line with its characteristics as described in the method section, proving that it should not be recommended as a quality measure for this type of problems. The conclusions for all considered corpora are qualitatively similar. Fig. 2 Clustering quality for different linkage methods – Ward performing best, and centroid worst – at 1000 MFWs. Layout as in Fig. 1. Results: networks The results of Louvain method on distance tables indicate that this scenario should be avoided: neither Classic nor Cosine Delta could reach AMI=0.2 on any corpus, which is worse than most of the hierarchical clustering methods. However, a more sophisticated approach involving undirected consensus networksoffers much better results, see Fig. 3–4: the community detection methods rarely score below AMI=0.4, and the best one scores above AMI=0.6 which contends for the first place with Ward linkage clustering. While hierarchical clustering typically provides a number of clusters higher than expected, community detection tends to produce a smaller number of larger clusters. This might be caused by the less detailedinformation contained in the consensus networks. One should also take into account that hierarchical clustering did not take advantage of the consensus scheme and hence is computationally less demanding. Fig. 3 Layout analogous to Fig. 1–2. Results of community detection on consensus networks based on Classic Delta. Each community detection method provided one clustering for the consensus network. All methods predict smaller number of clusters than expected. Louvain method typically scores highest. Fig. 4 Layout as in Fig. 3. Results of community detection on consensus networks based on Cosine Delta. Conclusion Our study provided empirical proof for the choice of Ward linkage method in clustering applications on literary text, and further strengthened the argument for the use of Cosine Delta as a method more resistant to changes in the number of used MFWs and factors such as language or size of the corpus. The best community detection methods, which again make use of Cosine Delta, can contend with hierarchical clustering, however, they clearly require previous data filtering, e.g., by means of constructing consensus networks. Interestingly, they offer a complementary more coarse-grained view. Importantly, we also propose introduction of clustering evaluation step into the analysis, in particular AMI method which worked best for all model cases with expected divisions. We will make the evaluation script available in our github repository, encouraging other scholars to use this or similar techniques. Acknowledgements JB was partially funded for the research by Polands National Science Centre, SP contributed to this research as part of a Short Term Scientific Mission financed by the EU COST Action Distant Reading, ME was partially funded by the National Science Centre. "
	},
	{
		"id": 300,
		"title": "Topic Modeling with Interactive Visualizations in a GUI Tool",
		"authors": [
			"Simmler, Severin",
			"Thorsten, Vitt",
			"Pielström, Steffen"
		],
		"body": " One of the algorithms that have been established in digital literary studies during the recent years is LDAtopic modeling. This method allows researchers to analyze the distributions of semantic groups in a corpus of texts, which can be useful for both the exploration of contents and automated text classification tasks. With regard to the increasing interest on topic modeling in the DH community, the infrastructure project DARIAH-DE has started in 2017 to develop the TopicsExplorer. Based on the python libraries DARIAHTopicsand LDA by Allan Riddell, the TopicsExplorer is a locally run standalone software for topic modeling that allows researchers to generate and explore topic models on their own computers, with their own text files, all within the comfort of a single GUItool that supports the entire process from preprocessing to the visualization of results. Though lacking the performance and flexibility of popular command line solutions, such as MALLET, or programing libraries, like Gensim, the advantage of the TopicsExplorer is its simplicity and usability. It can be used productively - within its limitations - by researchers without any programing skills. It does not even require users to use the command line. It thereby covers a significant gap: curious researchers lacking the technical skills required to use conventional tools are now able to try topic modeling and learn about the potentials and limitations of the method. On the one hand, this enlarges the spectrum of researchers able to participate in an informed discourse about DH-related research that relies in topic modeling as a method. On the other hand, researchers who think about using topic modeling in their own investigations can either directly use the TopicsExplorer for simpler problem, or at least learn beforehand enough about the method to make an informed decision before investing the effort to acquire the technical skills necessary to work with more advanced tools. The first version of the TopicsExplorer has been presented to various groups of researchers and students in a number of workshops in 2017 and 2018. Experiences and user feedback collected in these workshops have shaped the development work in the past year, and the changes implemented go far beyond simple debugging and securing sustainable functionality on as many platforms as possible. The tool began as a so-called GUI Demonstrator for the DARIAHTopics Python library that required installation of a Python environment, ran as a local server and was displayed in a browser. With version 1.0, it became a fully functional standalone software that can be downloaded and run on common Windows, MacOS and Linux systems without further preparation. It features interactive visualizations and csv export of results. A number of smaller enhancements proposed by test users, like a progress bar and abort button, have incrementally improved the usability of the 1.x versions. With the recently published version 2.0 architecture and interface have undergone a major redesign that addresses the more complicated feature demands derived from the feedback from our test users. On the technical level, the former solution for interactive visualization that was based on Pythons Bokeh library has been replaced by a Javascript-based solution. This allows for more flexibility in the implementation of additional interactivity features. To improve user experience especially for users who want to explore a corpus visually through a topic model, a new visualization concept based on the ideas of Chaney and Bleiwas implemented in this version. The concept allows, for example, to display a document from the corpus with all its related information from the topic model, and at the same time to list other documents with similar or related content. Topic modeling already is a research method often encountered in the digital humanities, though one exclusively used and critically discussed by researchers with advanced technical skills. It is our hope that the TopicsExplorer, with all the ongoing improvements, will help to move the method out of that particular niche. Figure 1: Version 1.x progress bar. Figure 2: Overview for a single document in version 2.0. "
	},
	{
		"id": 301,
		"title": "Stylometry for Noisy Medieval Data: Evaluating Paul Meyer's Hagiographic Hypothesis",
		"authors": [
			"Pinche, Ariane",
			"Camps, Jean-Baptiste",
			"Clérice, Thibault"
		],
		"body": " The history of the composition of French saints Lives collections in prose is still a mystery. At the beginning of the XIII th century, legendaries were already constituted and intermediary steps are missing to understand how the Lives have been gathered. One of the existing hypotheses is that small collections of saints Lives might have circulated as small independent units about one saint or a series of saints, sometimes traceable to a single author or translator, under the material form of libelli. The first milestone in the study of the composition of legendaries was laid by Paul Meyer. His studies led him to discover that some of these legendaries were derived from successive compilations. Using their macrostructure, Meyer dispatched them into families. He also had the intuition that the families were constituted of smaller components, groups or rather sequences of texts. He proposed the existence of primitive series based on authorship and the repetitive grouping of selected lives, such as the hagiographic collection of Wauchier de Denains Seint Confessor of the family C or the consecutive and recurrent series in the family B and C of the following three lives: Sixte, Laurent and Hippolyte. However, because most of the French saints lives are anonymous and because the collections were rearranged by multiple editors over time, it is extremely difficult to find what could have been the primitive series and Meyer couldnt go further. This serial composition of the Lives of Saints is a datum also noted by other specialists of Latin hagiography such as Perrotand Philippart, who even points out that these hagiographic series must be studied in their entirety in the same way as a literary work. But it is still very rare today that hagiographic text editing concerns a complete authors legendary, mostly because of a lacking certainty about these groupings. From there, with an exploratory stylometric analysis, we first want to find if Meyers hypotheses are wrong, can be nuanced or completed. In a second time, we would like to discover if the observed proximity between saints lives can reveal series from the same author. We based our study on a legendary from family C, namely the manuscript fr. 412 of the Bibliothèque nationale de France. The task is complex, because textual variants and the absence of a standardised spelling can affect the stylometric analysis. While the lemmatization of the texts could have nullified the problem of spelling variation, in our case, the preparation of the corpus would have been extremely time-consuming. We decided to work from witnesses written by a single hand in the same manuscript in order to limit biases that could have been induced by spelling variations linked to the scribes and not to the author. The analysis of the complete legendary is made possible by the use of the OCR software Kraken, trained on about 8890 transcribed lines of ground truth and tested on 897 lines, which results in up to 95.2% success in Character Recognition Score. Such results allow us to hope that the stylometric analysis is not corrupted by the margin of error. We work from the text thus obtained. Because most of the texts of the legendary are anonymous, we follow an unsupervised approach to the analysis of the texts, using agglomerative hierarchical clustering with Wards criterion, guided by its ability to form coherent clusters. The texts are, on average, quite short, a known difficulty for stylometry, with a median value of 16,863 characters, but with extreme values of 1,364 and 85,378. Texts that are too short create a problem of reliability, as the observed frequencies may not accurately represent the actual probability of a given variables appearance. To limit this issue, we removed texts below 5,000 characters. Because of the errors regarding segmentation in the OCR, we extracted character n-grams, ignoring spaces. We experimented with different lengths, but, following existing benchmarks, we retained the 4000 most frequent 3-grams. The metric and choices of normalisation are also an important parameter, one to which much attention has been devoted. Following the benchmark by Evert et al. 2017, we chose to use Manhattan distance with z‑transformationand vector-length Euclidean normalisation. The results are partially presented in figures 1 & 2. Dendogram of agglomerative hierarchical clustering using Manhattan distance, z-transformation and vector length normalisation over 4000 most frequent 3-grams Dendogram of agglomerative hierarchical clustering using Kohonen SOM coordinates over 4000 most frequent 3-grams Because, at the same time, the corpus is homogeneous, the texts can be quite short, and the data is noisy, separating them in stable clusters can prove quite hard. We tried to improve the quality of the signal by applying, first, a Kohonen self-organising map, and then using the coordinates of the points in the SOM for hierarchical clustering. In addition, the specificity of composition of the legendary C by successive additionsallows us to ensure a quick control of the likelihood of some proposed groupings. The presence of the hagiographic collection of Seint Confessors of Wauchier de Denain where the author identifies itself twicealso serves as an indicator of validity. The study has already shown interesting connections between the legendary of Wauchier de Denain and the Vie de Saint Lambert de Liège and some collections have been revealed. Two of them are quite certain, one of the first five texts of C, all about saint apostles and hypothetically from A, and another one of six virgin saints lives, all from B including the Lives of saint Agathe, Lucie, Agnès, Felicité, Christine and Cécile. To conclude, our analysis attempts to evaluate the best parameters for our study and to overcome certain difficulties inherent to our corpus. Two major obstacles have to be overcome: the lack of spelling standardisation and the lack of homogeneity in the separation of words. At the end of this prospective study, we hope to be able to reveal new hagiographic series prior to the composition of legendaries that were transmitted to us and that could have escaped us so far. "
	},
	{
		"id": 302,
		"title": "Diving Into The Complexities Of The Tech Blog Sphere",
		"authors": [
			"Pohlmann, Jens",
			"Barbaresi, Adrien"
		],
		"body": " Following the assumption that the tech blog sphere represents an avant-garde of technologically and socially interested experts, we describe an experimental setting to observe its input on the public discussion of matters situated at the intersection of technology and society. Our interdisciplinary approach consists in joining forces on a common base of texts and tools. This cooperation stems from work on the impact of digital media on democratic processes and institutionsand corpus and computational linguistics. The major aims of the effort described here are twofold:compiling a text basefrom a curated list of blogs dedicated to technological topics for lexicographical and linguistic research, as well asconducting exemplary studies using the compiled corpus, focusing on specific research questions regarding public discourse in Germany and the United States on questions of internet policy. 1. A Tech Blog Corpus for Linguistic Research and Discourse Analysis The purpose of focused web corpora in linguistic research is to complement existing collections, as they allow for better coverage of specific written text types and genres, user-generated content, as well as latest language evolutions. However, it is quite rare to find ready-made resources, especially for a topically centered approach. Blogs are of particular interest for our research, since they are intricately intertwined in what has been called the blogosphere, as the active cross-linking helps create a strong sense of community. Specific tech blogs first evolved aside from and in opposition to traditional mass media settings and amateur blogs have been shown to have the capacity to open up public space for the debate of socially relevant issues. Technological questions are indeed not restricted to the world of specialists, precisely since their implications often turn into political and ethical realities that affect society as a whole. However, the small, local communities of the beginnings have mostly been relegated by commercially driven websites targeting passive readers, which certainly has an impact on the content of discussions. These tech blog outlets may thereby have taken on the status of influential information providers, agenda setters, and gatekeepers. We need both data and scientific instruments to shed light on this subfield of the digital public sphere. Our first use case focuses on German, for which historical and contemporary corpora have been built as part of an aggregated lexical information platform, the Digital Dictionary of the German Language. For the project presented here, we construct a specialized web corpuswhich will then be compared to existing generalist resources such as newspapers and websites. Following the research on blogs/weblogs, we define blogs according to their form, consisting of dated entries often managed by a broadly available publishing tool or web space. The discovery of relevant portions of the web is performed semi-automatically by pre-selecting hundreds of sources. Second, important metadata such as the publication date and main text content are extracted automatically based on a series of heuristics. The resulting text base resides in a subset of web pages which could be found, downloaded and processed; documents with non-existent or missed date or entry content are discarded during processing and are not part of the corpus. 2. Application and Case Studies The corpus is usedto search for definitory elements related to newly created words or word senses, which involves an automated extraction of content and manual screening, andto study discussions on lawmaking, which involves finding ones way through convoluted and heterogenous documents, a task for which philologists can be assisted by large specialized text corpora and databases as well as distant reading processes such as topic modeling and outlier detection. One exemplary topic in the examination of this corpus focuses on the public discussion of the German Network Enforcement Act or NetzDG. This controversial anti-hate speech law, which forces social media platforms to take down flagged content that is manifestly unlawful within 24 hours of receiving the complaint, has been discussed and criticized in Germany, but has been very much condemned in the United States. The criticism put forward focuses on the abetting of overblocking that may lead to forms of censorship, on the outsourcing of juridical decisions to private companies, and on setting examples for authoritarian regimes copycat laws. The discourse about NetzDG is an extremely relevant case study for the analysis of the ways in which the societal implications of technology are currently discussed and negotiated in the public sphere, especially with regards to the threats that the misuse of social media platforms poses to political decision-making processes in Western democracies. The discourse about NetzDG particularly points to the diverging cultures regarding freedom of expression in Germany and the United States and it illustrates the extent to which the historical roots of these differences inform the current transatlantic debate about the restriction of content online and the regulation of social media platforms. The debate in itself includes a highly technical vocabulary and the need to transfer knowledge from a small community of experts to the general public. Finally, we also tackle questions related to the changing nature of the Web, as a web corpus almost instantly turns into a web archive, which calls for the long-term examination of its content through philological concepts and technical tools which are crafted expressly. 3. Further Developments This specially compiled blog corpus will shed light on processes of societal negotiation located at the crossroads of technology, public policy, society and the Internet, most notably by providing access to discourses in the tech blog sphere and by allowing for comparisons with other text types and sources. Aside from investigations regarding free speech issues and the NetzDG, potential inquiries include topics such as privacy laws, upload filters, AI, or copyright legislation in the digital age, as well as the analysis of the communication strategies employed by the respective stakeholders and the linguistically distinct characteristics observed. We plan to make a series of resources available in order to support the cause of web sources and the modernization of research methodology. The tools we work with or develop are released under open-source licenses and our sources will be published as curated lists of websites. In addition, the texts will be included into the DWDS web platform so that they can be queried by the wider public. If and to the extent applicable, we will release corpus data to apply and test further methods of automated text analysis. "
	},
	{
		"id": 303,
		"title": "Paving the Way to Linked Open Data: Evaluating the Path to LOD for the Census of Antique Works of Art and Architecture Known in the Renaissance",
		"authors": [
			"Pohl, Oliver",
			"Notroff, Andrea"
		],
		"body": " The Census of Antique Artworks and Architecture Known in the Renaissanceidentifies and collects antique monuments and related Renaissance documents in a database, such as works of architecture, statues, frescoes, sarcophagi, paintings, drawings, sketches, manuscripts and more. Established in 1983, data has continually been added to the database. Since then, the fundamentals of the underlying relational data model of the Census did not have to be changed. Its main focus is to help researchers in art history expand their understanding about the relation between works of art produced in the Antiquity and their reception and perception in the Renaissance. Although the data model is robust, the research environment using the Census database does not meet current user expectations like a modern and responsive user interface and search capabilities that are easy to understand. Moreover, the site does not make use of best practices established in the Digital Humanities community, such as providing a RESTful API or making use of Linked Open Datatechnologies. Another issue the Census project is currently facing is the fact that the website runs on a proprietary digital asset management systemwhich handles data entry, retrieval front- and back-end. The support for easydb 4 will be running out shortly. In order to address the issues of a) openness, b) usability and c) maintainability, the we are currently currently evaluating how to port its data and research supporting functionalities in the coming two years into an open source-based system with LOD capabilities that also provides a modern user experience. In the beginning of the evaluation process, the Census project looked at solutions of other projects in the domain history that seem to fit the requirements mentioned above. While researching and speaking to other members of the Digital Humanities and art history community, we identified the following software solutions as possible contenders for the future of the Census project: Software Description Developer / Maintainer conedaKor Open source web application for storing arbitrary entity types and interconnect them. conedaOmeka-S Open source web publishing platform and content management system for cultural heritage collections with LOD in mind Roy Rosenzweig Center for History and New Media; George Mason UniversityresearchSpaceopen source Semantic Web environment for research and collaboration The British Museum; MetaphactsWissKI Drupal extension for annotating arbitrary data using LOD Data in a CMS-based research environment Germanisches Nationalmuseumarches Open source data management system forheritage data Getty Conservation Instituteeasydb 5 Closed source successor of the current Census system with open source extensions ProgrammfabrikWe established a catalog of criteria to test these system against, taking inspirations from Jackson et al.and Knodel and Naabwhile also taking advice from other members of the Digital Humanities community. This list includes criteria and questions such as: How easy is it to re-use the current data model of the Census in the new system? Is the new system easy to understand and handle for users and developers? Does the system have built-in LOD capabilities? Can the new system be installed and deployed easily? Can you extend the new systems front-end and back-end components without breaking upgradeability? Is the new system available as open source software and is theresupport available? How big is the user community for the new software? While Omeka-S, researchSpace, WissKI and arches are built with Semantic Web technologies in mind, conedaKOR just focuses on employing a non-RDF-based graph model. When comparing the systems regarding usability and maintainability, Omeka-S offered the best documentation, modern user interface with CMS functionalities as well a modular approach for extending its source code. researchSpace impressed us with its software architectural design by only relying on a triple store and possibilities to visualize any data using React components. However, it turns out researchSpace is very hard to deploy and complicated to maintain on a source level. While testing all these systems, we noticed there would not be an easy plug-and-play solution to re-use the Census database. These system either require a specific yet generic data model and/or Semantic Web ontology. Thus, we would have to re-design the current structure of Census relational database and thereby risk losing important data and relations without even having re-implemented basic functions such as searching and data entry. Instead of settling on a holistic system that covers database interaction, front-end, back-end and Linked Open Data, we had to rethink our approach to a new software architecture for the Census: We intend to establish a modular software architecture revolving around a RESTful LOD-API. Having a well documented API, e.g. in form of an OpenAPI specification , allows us to build front-end components using that API endpoint for presentation and research, while also developing a back-end system that handles data base interactions and preparing the data for the API at the same time, In other words, having an API centric software architecture makes it programming language agnostic, making it easier to swap, extend and update front- and back-end components as well as the database if the need should arise, as long as the API still functions as specified. The Census project recently turned 35 years and aims to continue doing its research in the future. We conclude to not adapt the next one size fits all solution for the Census database, and instead focus on establishing a modular approach to remain flexible for future technologies and best practices. "
	},
	{
		"id": 304,
		"title": "MicrogenresA computational model of disciplinarity and the novel",
		"authors": [
			"Porter, J.D.",
			"Algee-Hewitt, Mark",
			"Fredner, Erik",
			"Bronstein, Michaela",
			"Manshel, Alexander",
			"Nomura, Nichole",
			"Droge, Abigail"
		],
		"body": " Introduction From Bakhtins heteroglossia to Genettes transextuality, theories of the novel describe its form as a mixture of diverse elements or styles. While many of these mixtures are contiguous with the text as a whole, in this project, we are interested in overt stylistic changes within the text of a novel that mimic the discourse of non-novel genres like psychology, philosophy, or natural science—what we call here shifts in microgenre. For example, in Edward Bellamys 1888 novel, Looking Backward, the first-person narration of the time-travel story transforms key moments into third-person voices that explain to the narratorhow Bostons future society operates. In Stokers Dracula, the same kind of shift takes place through the overt inclusion of multiple media forms, such as newspaper articles and excerpts from the Demeters log. More subtle is the sermon embedded in the third chapter of Joyces Portrait of the Artist, delivered in the voice of Father Arnell. In each of these cases, there is a strong shift in the style of the text towards the discourse of other disciplines. In this project, we use computational methods to identify the stylistic differences between disciplinary writing that enables this phenomenon and to reveal when and how these microgeneric shifts take place within the framework of a novel. Corpus and Methods While the phenomenon that we describe above is present throughout the history of the novel, and in many different novelistic traditions, we restrict our inquiry in this paper to the period between 1880 and 1914. Not only does this enable our investigation to control for the historical change in both disciplinary discourse and fiction, but the late nineteenth century is also a critical period in the history of modern disciplinarity. In both Americaand in the United Kingdom, this period saw a massive growth in university education and, with it, in the number of disciplines concretized through their inclusion in the academy. For example, the subjects offered at Mason Science Collegein 1880 included only Math, Physics, Chemistry and Biology. By 1898, this had grown to 21 subjects, including Literature, Physiology, Philosophy and Engineering. By concentrating our research on this period, we are able not only to find evidence of disciplinary styles within the novels written at the time, but also to recover the stylistic differences that accompanied this growth and division of disciplinary writing. To explore both areas, we hand-assembled a corpus of 97 texts written between 1880 and 1914. This corpus included 10 texts each from a range of disciplines, including Anthropology, Economics, Philosophy, Psychology, History, Politics and Natural Science. These texts were chosen by the project participants to be representative of the range of possible discursive styles of each discipline and to be of a length similar to the novels of the period. The corpus also includes 27 novels, which both served as an example of the narrative discursive style for our model, and also allowed for further classification of the various disciplinary styles as they are embedded in these texts. The theory of microgeneric shifts that we propose here is similar to the multi-authorial problem addressed in stylometric work by rolling delta. Like rolling delta, our concern is also with the linear analysis of sequential phenomenon; however rather than intra-textual changes in authorship, our project seeks to identify stylistic shifts within a narrative written by a single author, but which reveal the presence of extra-disciplinary discourses. For this reason, our approach builds on the work of stylometry, but departs in two important ways. First, because we are interested in shifts in discourse rather than content, we have opted to take a non-lexical approach. Rather than a feature set built from a subset of token frequencies, we use grammatical features in our model, including the frequency of the Penn Treebank POS tags, the average sentence length and the average number of clauses per sentence. In a lexical approach, a model built on the most frequent words, as in the calculation of delta, is too tied to authorial signals, while a model built on distinctive words would be too indicative of the subject matter, rather than the style, of the individual disciplines. Secondly rather than a cluster-based distance approach, we adopt a classification model that builds upon research into literary genres using computational methods. By training a model on 200-sentence subsections of our corpus of disciplinary texts and then classifying similarly sized passages of contemporaneous novels, we can use the posterior probabilities of the classification results to identify the mixture of disciplinary writing in each part of each novel. Analysis The project seeks to answer two related questions. First, are there differences in the discourse of disciplines that are stylistic rather than semantic? And, second, if these differences in style can be identified, is it possible to detect evidence of these styles embedded within novelistic prose? To answer the first question, we created a classification model using a discriminate function analysis. Our choice of a method spoke to both our hypothesis that the classification of disciplinary style would be found in a linear combination of grammatical features, as well our subsequent use of the posterior probabilities calculated by the modelas proxies for the disciplinary mixtures in each of the passages that we classified. The results of our initial classificationreveal that our model was able to classify the texts from each discipline at a far greater percentage than chance. Even the most ambiguous discipline, economics, was classified correctly more than 50% of the time, far greater than chance. The misclassifications in each genre speak both to the complexities of the model and to the same microgeneric phenomena that occur in novels. In a book on natural science, for example, the straightforward discourse of history includes aspects of natural science in its attention to the details of historical events. Figure 1: Discriminate Function Analysis of corpus using grammatical features For the second question, we used the model we had developed to classify individual parts of novels. For example, in the resulting graph of Sir Arthur Conan Doyles A Study in Scarlet, each bar represents a 200-sentence slice of the text, and the mixture of colors indicate the posterior probabilities of each disciplinary discourse in each slice as assigned by the model. Although the majority of slices are, as one might expect, dominated by novelistic discourse, the eighth slice is divided between anthropology and natural science. In fact, that portion of the novel is given over to a description of the geography of the southwest United States, followed by details about the appearance and customs of Mormon migrants—in other words, something like natural science and anthropology. This is an intuitive confirmation that disciplines have discursive styles, and that novels can deploy them in strategic ways. Examples like this suggest not just the technical accuracy but also the literary critical use of microgenres, especially for more densely interdisciplinary works, like Bellamys Looking Backward. Figure 2: Classification of 200 sentence slices of A Study in Scarlet Figure 3: Classification of 200 sentence slices of Looking Backward The results of our project reveal that the disciplinary discourses of the late nineteenth century did indeed have unique formal stylistic markers that extend beyond their subject matters or word choices. Moreover, by classifying parts of novels using a model built on these features we can trace how different disciplinary styles were embedded as microgenres within the narratives of nineteenth century prose writing. "
	},
	{
		"id": 305,
		"title": "ADHO Libraries and Digital Humanities SIG Pre-conference: Libraries as Research Partners in Digital Humanities",
		"authors": [
			"Wilms, Lotte",
			"Potvin, Sarah",
			"Galina, Isabel",
			"Gutiérrez, Silvia",
			"Degkwitz, Andreas",
			"Claeyssens, Steven",
			"Lefferts, Marian",
			"Hosoien, Hege"
		],
		"body": " Libraries and other cultural heritage organisationsand their staff have a central role in digital humanities, as collection builders, project and service managers, and experts on how to use their collections to the fullest by digital means. Given the importance many of these organisations attach to their role as supporters of research, a case can be made for the opportunity and need for libraries and their staff to position and assert themselves as partners in research. We will address the issues raised when talking about libraries as digital humanities research partners. This pre-conference will present opportunities and examples to showcase library-based digital humanities work that makes the case for the partner model. This will be approached from different angles; discussing why this model is mutually beneficial to both library and partner organisations, what policies and infrastructure are needed for the organisation to be able to become a sustainable partner, which steps are needed for staff to obtain relevant skills, and finally how to choose who to partner with and then build those relationships. Two keynote speakers will set the theme of the event. The rest of the programme will be filled through a call for participation for short papers, lightning talks, panels, and workshops. This full-day pre-conference will be held at the National Library of the Netherlands in the Hague. Transportation will be provided from Utrecht. Please see our website for up-to-date information, including a CFP: https://adholibdh.github.io/dh2019-preconference/ Programme committee Hege Hosoien, National Library of Norway Sarah Potvin, Texas A&M University Isabel Galina, Universidad Nacional Autónoma de México Silvia Gutiérrez, El Colegio de México Andreas Degkwitz, Humboldt University of Berlin Tamara Butigan, National Library of Serbia Dawn Childress, University of California, Los Angeles Steven Claeyssens, KB National Library of the Netherlands Marian Lefferts, Consortium of European Research Libraries Lotte Wilms, KB National Library of the Netherlands Local organizers Lotte Wilms, KB National Library of the Netherlands Martine Klaassen, KB National Library of the Netherlands Steven Claeyssens, KB National Library of the Netherlands Marian Lefferts, Consortium of European Research Libraries Additional SIG conveners Glen Worthey, Stanford University Angela Courtney, Indiana University Zoe Borovsky, University of California, Los Angeles External links Preconference website "
	},
	{
		"id": 306,
		"title": "Documentation of Digital Heritage Information Resources: Expanding Access for Research and Education",
		"authors": [
			"Povroznik, Nadezhda Georgievna"
		],
		"body": " This paper discusses the latest approaches to developing information systems for digital cultural heritage on a global scale, including the creation of catalogs and infrastructure for resource documentation. Digital cultural heritage resources are diverse in content, origin, purpose, scale, technology and user audience. They may take the form of digital collections of institutions in the GLAM sector, such as virtual museums [11], electronic archives and libraries [10], or national infrastructures and international aggregators, systems and services for visualization and analysis of sources, and for representing virtual reconstructions. Documentation systems are essential to facilitate advanced digital humanities research and to provide greater user access. Whereas object-oriented documentation may be used effectively to describe individual digital cultural heritage objects[5], information resources for digital cultural heritage require additional description of their creation process and the technologies used, details of content, and other features. The V-Must project[12], contains a set of virtual museums and a catalog including 54 information resources of predominantly European museums containing digital collections of exhibits. V-Must has a meta-description system for organizing resources, which includes 8 description fields, including for content, duration, audience engagement, interaction technology, level of immersion, format, scope and sustainability. There are also lists of information resources organized thematically [13], by useor by type of information resource [3]. However, such lists are not catalogs in the full sense, as they do not allow sophisticated search or sorting functions. It is important to note that national infrastructures for digital cultural heritage [for example, 7, 9] and international aggregators [2, 4] integrate digital collections of GLAM institutions at the object level, providing cross-collection search capabilities. However, they are not primarily intended for documenting the information resourcesof these institutions. Effective systems for documenting such resources require a different approach because their purpose is to organize data by categorizing a very wide range of types of information resources. In the Center for Digital Humanities at Perm State University, Russia, a documentation system has been developed – a project known as Historically-oriented information systems or the DigitalHistory.ru platform [6]. Initially, this information system was designed to solve a specific scientific problem related to the study of historical-oriented information systems [8]. The platform was then upgraded and the database of the system was expanded to include a much wider range of characteristics related to describing information resources for digital cultural heritage. The current system is designed to document digital historical and cultural heritage information resources of various types, for wider use in research and education. The resource meta-description structure includes 39 fields that represent 3 groups of data: 1) Data on the creators of the information resource; 2) General information about the information resource; 3) Content description metadata. The platform currently contains 1397 documented information resources, and it continues to be expanded, pre-moderated by a specialist. In the structure of the documentation system, there are several ways to present created catalog data: 1) by country; and 2) by the full list of all resources. The extended search form allows the user to select one or more desired characteristics for a list of results. The documentation system is freely available online and is widely used as a reference and information retrieval system. In addition, the system is actively used in the training of Bachelor and Masters students in courses related to Digital Humanities. The Center for Digital Humanities has developed a Masters degree program Digital Sociocultural and Art Practices, which includes the course Virtual Museums, Archives and Libraries, which includes practical tasks related to the search and analysis of information resources for digital cultural heritage. In summary, documentation systems play an important role in the modern world of information infrastructure for digital cultural heritage. The diversity of information resources requires further study and classification, which is also necessary for more detailed documentation and cataloging of these resources. The method and solutions proposed expand possibilities for finding thematically similar information resources, and provide a global model to make such resources more accessible for research and education. "
	},
	{
		"id": 307,
		"title": "Oceanic Exchanges: Transnational Textual Migration And Viral Culture",
		"authors": [
			"Priewe, Marc",
			"Viola, Lorella",
			"Verheul, Jaap",
			"Keck, Jana",
			"Knabben, Moritz",
			"Priani Siasó, Ernesto",
			"Salmi, Hannu",
			"Oiva, Mila"
		],
		"body": " Chair: Marc Priewe The dramatic expansion of newspapers during the nineteenth century created a global culture of abundant, rapidly circulating information. For scholars of nineteenth-century periodicals and intellectual history, the digitization of newspaper archives and the ever-growing array of tools for accessing and assessing them provide a fruitful platform of new evidence to re-evaluate how readers around the world perceived each other and to obtain fresh insights on the global networks through which news and concepts traveled. Through the identification of patterns and trends, text mining is particularly suited for this task. In this panel, we want to focus on the array of methods within text miningthat are used in current research to identify and model patterns of information flow as well as to trace the migration of concepts across different communities over space and time. The panel brings together the efforts of scholars from different disciplines within the humanitiesand computer scientists to explore and share how text mining methods and tools are currently used to answer research questions such as: Which stories spread between nations and how quickly? How did the migration of texts facilitate the circulation of knowledge, ideas and concepts and how were these ideas transformed once they migrated from one side of the Atlantic to the other? How did geopolitical realitiesinfluence the directionality of these transnational exchanges? How did reporting within ethnic communities differ from reporting in surrounding host countries? The panel will begin with a five-minute introduction to the Oceanic Exchanges project and then present five ten-minute test cases exploring sets of data in a variety of languagesand employing different text mining methods. The panels main objective is not to prove one method right and another wrong but to explore the dominant strategies and methodological approaches that unveil the thematic and textual complexities between historical newspapers archives. Throughout the five papers of this panel, we offer multi-layered text mining approaches that, encompassing several methods, varying sets of data, and different discourse scenarios in a range of languages, are structurally coherent, methodologically solid, and comparatively rich. The panel will appeal to historians, linguists, media and communication scholars, literary scholars, and computer scientists interested in textual and conceptual changes, continuities and replacements, but also to those focusing on representation of actors and events in public discourses. Oceanic Exchanges between Italy and the United States: How Italian Americans became WhitePresenter: Lorella Viola, Jaap Verheul Between 1880 and 1930, it is estimated that more than 22 million people from all over the world migrated to the United States, 4 million of whom were Italian. As immigrant communities grew, the immigrant press boomed accordingly. As far as the Italian language is concerned, between 1880 and 1920, there were between 150 and 264 Italian language newspapers published in the United States. Diasporic newspapers became an instrument for community building and helped immigrants cope with life in the New World, including easing their transition into American society as well as serving as powerful tools of language retention and national identity preservation. At the same time, acting as advocates for the rights of the respective immigrant communities, they performed social control by drawing attention to what was acceptable within the Italian immigrant community but also within the dominant norms and values of the American society. This paper explores the role of printed media in constructing the concept of Italian identity, the so called italianità, between the end of the nineteenth century and the beginning of the twentieth century in Italy and the United States. The overarching aim is to investigate the dynamics of knowledge transfer between the two Italian communities: Italians of the newly formed Italy and Italian diasporic groups in the United States. Using a mixed-methods approach that pairs Digital Humanities technologies such as text mining and semantic modelling with the discourse-historical approach pioneered by Ruth Wodak, this study compares Italian newspapers published in Italy from 1867 to 1900 with Italian language newspapers published in the United States from 1880 to 1920. The results will show that the concept of being Italian, after originating in Italy as a synonym for national identity, travelled to the United States, where it was reshaped by the Italian language press into a means for fighting against marginality and vindicating whiteness and social inclusion. In this way, in Italy italianità embedded the ideals of the Risorgimento and conveyed connotations of patriotism and nationalism. In the United States, on the contrary, Italian ethnic newspapers became powerful tools through which the Italian immigrant community could negotiate social integration in the host country. Italianità became a way to uplift the Italian race which was simultaneously distancing itself from African-Americans. Tracing the Traffic of Cholera in Nineteenth-Century Newspaper RepositoriesPresenter: Jana Keck, Moritz Knabben In this presentation we introduce a new corpus exploration tool which offers access to datasets of historical newspapers from North America, Europe, and Australasia, including at least six languages. With this tool, we are able to search for news articles containing specific keywords. However, since our corpus covers over one hundred million articles, the set of documents containing even specific keywords might be extensive. We address this problem by using visualizations to summarize the results and to find ways of deriving meaning from a vast body of texts. Our project is framed around a historical case study examining the traffic of cholera, which was one of the deadliest and most feared diseases. The nineteenth-century press played a crucial role in helping to construct the public image of cholera and other diseases. Contemporary medical scientists and practitioners, especially in the United States, wrote mostly negatively about the role of newspapers in insufficiently informing the public about possible causes, transmission roots and routes, and treatment methods. In this paper we will show how cholera was covered by the press in different languages and in different countries. To this end, we add functionality to a quantitative search method by applying further visualization techniques. This allows both a distant reading of a large newspaper corpus, while retaining some of its contextual meaning by filtering and displaying the set of found articles. Questions we ask include, but are not limited to: Can we identify a narrative of disease propagation circulating in the press that was absent, or even suppressed and disagreed with, in the official conduits of medicine? Did the increasing discrediting of miasma theory and of cholera itself cause a similar decline in the news coverage? We highlight the potential of this search tool to show how newspaper articles de-centered medical knowledge from being located only within the medical field to illustrate instead that the acceptance of these medical developments was not only a scientific, but moreover a social and cultural effort. By placing the non-scientific newspaper articles into conversation with the social and medical developments, a more complete picture of the modernization and professionalization of medicine andjournalism is possible. The Origins of Fake News: Lajos Kossuth, Political Celebrity, and Dis/Information in the Nineteenth-Century PressPresenter: Mila Oiva In December of 1851, the Hungarian revolutionary Lajos Kossuth landed in New York City at the outset of a publicity campaign to secure American support for Hungarian independence. His trip along the eastern seaboard, ultimately arriving in Washington, DC, was exhaustively covered by not only the American press but by an increasingly connected network of international newspapers. In many ways, that network established Kossuth as an international celebrity, stirring the crowds that flocked to his speeches, inspiring far-flung editors to adapt Kossuths messages, and inviting Hungarian political operatives and others to discredit Kossuth and manipulate the news network itself. This paper argues that Kossuths journey—and the international news coverage and censorship it inspired—showcases the mid-century operations of a networked mass media system. Connected by domestic telegraph wires, railways, steamships, exchange networks, and extensive reprinting practices, the international newspaper network by the mid-nineteenth century was increasingly functioning as a system. The Kossuth case shows how its properties as a system were exploited for various ends, including not only Kossuths political aspirations but a complex set of goals that shift with language and location. The manipulation of the international news network shows the stirrings of dissemination, disinformation, and censorship that now shape political discourse via globally-connected social media platforms. Our sources span the United States, Britain, Germany, Austria, Finland, and Russia, comprising thousands of articles published in multiple languages following Kossuths arrival in New York City in December 1851. We identify news reprints manually and with the help of computational linguistic tools, including the use of rare word tokens and Named Entity Recognition. This selected corpus is then used to create visualizations of news reprints and their dissemination. The patterns and disparities of those reprints, in turn, allow us to evaluate the accuracy of these reports, to identify fake news, and to explore how and why it spread. Much like contemporary social media, international media in the nineteenth century allowed for disinformation practices based on the political goals of national stakeholders. Then as now, these practices exploit the paradoxes of any international media system: its seeming connectedness and yet its enduring distances. As Lajos Kossuth carried his message personally across the Atlantic, stories about his trip circulated and spun behind him. The international news network at once consolidated Kossuth as a global celebrity and fractured his message for various regional political ends. Remember the Maine: Newspapers in the Spanish-American War Ernesto Priani Saisó, Jamie Parker, Marc Priewe, Isabel Galina Russell, Miriam Peña Pentel, Rocio Castellanos, Laura Martinez Dominguez, Laura Lopez, Ximena Gutierrez-Vasques, Adan Lerma. Presenter: Ernesto Priani Saisó On February 15, 1898, an explosion destroyed the US-American battleship Maine stationed at Havanas port and the following morning the news hit the front pages of many cities in the United States, Mexico, Spain, the United Kingdom, Germany, Austria, and Finland. The explosion happened amidst already existing diplomatic tensions that would result in the Spanish-American War, which in turn marked the emergence of the United States as a new imperial actor in international politics. The objective of this paper is to show how the information about the explosion of the Maine was disseminated throughout selected European countries, the United States, and Mexico by analyzing news reuse in the press using several digital tools as well as text mining and Natural Language Processingtechniques. In addition, we employ Automatic Author Profiling and Vector Space models to characterize the news semantic content applied to a corpus constituted from several National Newspaper Libraries. This allows us to compare historical news coverage of events in ways that were hitherto impossible. In Mexico, for instance, the news diffusion occurred through newspapers from the Spanish, French and American communities living in the country. Internationally, the news dispatch system depended on telegraph cables connecting the US and Europe and an alternative one between Spain, Cuba, Mexico, and Central America, inaugurated two years before the outbreak of war in April 1898. Our analysis shows further that the location source of the news was initially Havana but then moves to Washington, New York, and Key West in the United States. Quickly the American voice was the dominant one on the event; meanwhile, there was a debate in virtually all national presses about the explosion, whether it was an accident or provoked and what the political implications might be of either cause of the explosion. News Flows in 1904: The Media Coverage of Nikolay Bobrikovs AssassinationPresenter: Hannu Salmi General Governor of the Grand Duchy of Finland Nikolai Bobrikov was assassinated in Helsinki in 1904. While the first shot was heard in Helsinki at noon on 16 June, the news was published in Mexico City, thanks to the eight-hour time difference backwards, the next morning, on 17 June. The shooting of Bobrikov offers an illuminating case for the study of European and global news circulation at the beginning of the twentieth century. The efficient international telegraph network and press agencies forwarded information that was estimated to be of interest for the news market quite quickly. The system of news delivery emphasized big centers, but the Bobrikov case shows that news from the remotest corners of Europe could also get a wide circulation. The rhizomatic communication network had its centers, but it was not centralized. This presentation studies the tempo and the ways in which news about Bobrikov was disseminated in the network of newspapers. It concentrates particularly on the viral expansion of the news during the first week after the assassination. What kinds of temporal rhythms did it have? What narrative elements did it include? What kinds of spatial ramifications did the news have? What is the diagram of news flow behind the spread of this particular case? The presentation also focuses on the thematic differences in the discourse on Bobrikovs murder in different cultural settings. The presentation utilizes digitized newspaper collections from Finnish, French, German, Dutch, US, Australian, Austrian, Swedish, and Danish archives. In addition, we have used microfilm and physical newspaper collections of Russian newspapers at the Finnish National Library. The study takes advantage of the transnational news dataset and follows the most frequent paths of news flow, and visualizes them using tools like Palladio. "
	},
	{
		"id": 308,
		"title": "Visualizing Poetic Meter in South Asian Languages",
		"authors": [
			"Pue, A. Sean",
			"Atta, Ahmad",
			"Ranjan, Rajiv"
		],
		"body": " The explication of poetic meter in the modern languages of South Asia is a source of consternation even for experienced poets. Even if poets can competently employ established meters in their work, they and their readers or listeners often cannot describe poetic form using established classificatory schema. Traditional prosodic systems, which are inherited from classical languages, do not align well with the phonological features of modern South Asian languages. Modern scholars have developed alternatives that address these deficiencies. We augment that work by presenting an interactive web-based software package under development to visualize poetic meter using directed graphs that accommodate multiple languages and scripts in order to make poetic knowledge accessible to readers, scholars, and poets. Background In South Asia, there are two competing theories of prosody, one derived from Arabicand one from Sanskrit. The languages of Urdu and Hindi, which share a common grammar but have differing vocabularies, draw upon both systems of meter. The Arabic system traces its origins to the revelation of the eight century Arab prosodist al-Khalil Ibn Ahmad of Basra. Its basic unit is orthography, specifically the written Arabic letter. Though quite precise for classical Arabic, when used for South Asian languages the system breaks down, because there are so many possible combinations of metrical units, i.e. it is combinatorially explosive. The system used for Sanskrit is also defined by writing, based on abugida, or long and short units. Modern languages typically drop short vowel endings, however. Both systems have a preference for complex and precise classification. That propensity towards complexity, combined with the disalignment of the systems with modern languages, have posed challenges for modern literary critics, poets, and readers, alike. Modern prosodists of Urdu and Hindi poetry, often themselves proponents of elegant systems, have attempted to make these prosodies more accessible by referring to patterns of long and short metrical units. These are often represented using macrons, breves, or other symbols familiar to readers of English metrical texts. Modern prosodists are also challenged by the difference between the durational basis for poetic prosody in South Asian languages and the basis of meter in English and other European languages on stress. Durational meter is based on metrical unit length. This difference also poses a problem for graphical exploration of poems, which are mostly designed for English and confine their prosodic domains to stress. Our visualization software addresses both of these challenges. Graphical visualization of poetic data Our visualization software consists of a Python and Javascript module in a free and opensource software package that turns poetic texts into poetic data. It provides long and short units, as well as labeled rules for particular contexts. The current implementation supports Hindi and Urdu meters, along with Hindi, Urdu, and Englishtranslations of its interface. The graph layout is performed by the Graphviz library. Representing poetic meter as a directed graph By representing poetic meter as a particular walk through a directed graph, this model offers a significant advancement over previous metrical representations. We assume for our graphical model start and end nodes, short and long metrical unit nodes, and edges between them. We use start and end nodes with distinct shapes and colors. We suggest a circle as the shape for a short metrical unit, and a rectangle as that of a long. For uncounted metrical units at the end of lines or before caesuras, we suggest a dashed circle. Units that compose metrical feet are grouped into labeled clusters. This system resolves the issues of metrical flexibility and complexity that, in traditional prosody, led to excessive categorization, while visualizing the patterns of durational sound that produce meaning for poets and their listeners. Discussion This software package works across the multiple scripts of South Asian reading and listening publics. It advances earlier methods of visualizing meter by affording new sorts of interaction, particularly in web-based environments. For scholars, directed graphs allow an elegant means to visualize metrical complexity. All of the possible meters of a particular poet can be compared to those of another. The flow through a network also opens new sorts of metrics for comparative analysis. Further, as we will demonstrate, a walk through a directed graph can be colored in time with a particular audio or visual recording that has been marked up for phoneme timings as well as metrical units, allowing new sorts of insights that are not easily available in text alone. For listeners who have various levels of knowledge about meter, interactive versions of directed graphs can have instructive qualities. Listeners can learn the rules of prosodic systems by clicking on nodes that represent poetic data. For poets, this method offers a visual means for composing verse. While based in Urdu and Hindi, the methodologies described can be easily adapted and applied to a large number of South Asian and other languages to provide renewed access to poetry, conceived as data, in the digital age. Graphical representation of a sample Urdu/Hindi metrical pattern. Rectangles represent long units, circles short units, and dashed circles uncounted short units. Clusters represent metrical feet, here in the Perso-Arabic system. Graphical sample of an Urdu verse in the Hindi language interface. Text is rendered in the Devanagari script. Clusters represent metrical feet, here labeled using the traditional Perso-Arabic nomenclature. Word separation is indicated using an interpunct. Graphical representation of a sample Urdu verse in the Urdu language interface. Text is rendered in the Perso-Arabic script using Perso-Arabic diacritical markings to show short vowels, which are normally unmarked. Clusters represent metrical feet, here labeled using traditional Perso-Arabic nomenclature. Graphical representation of a sample Urdu verse in the English language Interface. Text is rendered in scholarly transliteration. Clusters represent metrical feet in traditional Perso-Arabic prosody. Acknowledgments This research was supported by an Andrew W. Mellon New Directions Fellowshipand by matching funds from the College of Arts and Letters, Michigan State University. "
	},
	{
		"id": 309,
		"title": "Improving the understanding and preservation of European Silk Heritage. Producing accessible and reusable Cultural Heritage data with the SILKNOW ontology in CIDOC-CRM",
		"authors": [
			"Puren, Marie",
			"Vernus, Pierre"
		],
		"body": " Silk played an important role in European history, mostly along the Western Silk Roads network of production and market centres. Silk, however, has become a seriously endangered heritage. Although many European specialized museums are devoted to its preservation, they usually lack size and resources to establish networks or connections with other collections. The H2020 SILKNOW projectaims to produce an intelligent computational system in order to improve our understanding of European silk heritage. The SILKNOW platform will form a coherently integrated system to give access to a wide variety of data describing silk-related objects to researchers, museums curators or general public with a single interface. This computational system is modelized and trained thanks to these datasets, mapped according to the SILKNOW ontology. In this paper, we will present how we have defined this data model, and how we have specified the entities to be represented by the ontology and the existing relationships between these entities. The design and implementation of the SILKNOW ontology representing the model is based on CIDOC-CRM. SILKNOW has crawled datasets from websites or online databases of Cultural Heritage Institutionspreserving silk-related artefacts - such as the Musée des Tissus de Lyon, the Victoria and Albert Museum or the Museos estatales del MEC. The SILKNOW crawler is made in Node.js and its source code is available at: . We have then analyzed the structure of the records on silk-related items from these different institutions. In order to give access to these various datasets via a unique point of entry, it is necessary to harmonize them by designing and implementing a unique and complete data model, well adapted to Cultural Heritage data describing textile-related artefacts and more precisely silk-related artefacts.This data model is based on the CIDOC Conceptual Reference Modelwhich provides definitions and a formal structure for describing the underlying semantic of the structure of documentation on Cultural Heritage. The CRM is an object-oriented model independent from any technical implementation framework. It defines a limited set of objects with which it is possible to describe complex realities. More precisely, the CRM is a core ontology – that is to say a formal representation of knowledge – with more specialist extensions. It is an empirical ontology, elaborated from the analysis of the data produced by the cultural heritage experts. Moreover, the CRM data model is flexible and extensible. In other words, given that it is based on a double hierarchy of classes and properties, if needed, it is possible to add new subclasses and sub-properties, in order to express more specific relationships and properties, without modifying the basic structure of the model. There is yet no CRM extension for dealing with the production of textile artefacts; something similar to FRBRoo, for the creation, production and expression process in literature and the performing arts. Therefore, some of the free-text fields, especially the complementary fields on the technical and material information, are still in need of a more thorough reflection. The more complex modeling of the semantics included in data about the creative and productive process of silk textiles requires elaborating new classes and properties. SILKNOW takes these digital silk textiles data to analyze and and processed them with advanced text analytics. A text analytic module is currently designed and developed, in order to analyse the text content from the data collection. The ontology is used to structure the analysed information and map this information to populate the SILKNOW ontology. The text semantic meaning is based on English and will be translated from/to the other languagesin order to be processed. Thus, when text content is analysed, many different natural language processing techniques are applied to splitting sentences, tokenization, and entity extraction. The result of these methods is used to enrich the SILKNOW ontology by employing matching algorithms to determine the correct corresponding semantic concept of a named entity. The SILKNOW ontology will thus allow the project to elaborate new CRM classes and properties well adapted to describe silk textiles data. These results will be freely available, and will help to describe more precisely silk-related objects, and improve the way we analyze and understand these Cultural Heritage items. They can also be used as a basis to elaborate new CRM classes and properties for textiles data and not only silk textiles data. Moreover using the SILKNOW ontology will also allow to a wide variety of users to have access to an endangered heritage, and encourage new research on this heritage. "
	},
	{
		"id": 310,
		"title": "A European-Hindustani Dictionary? Reflections on Methods",
		"authors": [
			"Pytlowany, Anna"
		],
		"body": " This presentation is the first report on the project Hindi Lexicography and the Cosmopolitan Cultural Encounter between Europe and India around 1700 from Uppsala University. The primary goal of the project is to produce an online dictionaryon the basis of the unpublished Thesaurus Linguae Indianæ by François-Marie de Tours. The shortcomings of the Uppsala project will guide the design of an extended cross-linked online dictionary of early modern Hindustani based on little known wordlists and vocabularies compiled by European merchants and missionaries in the 17th c. India. The novelty of the approach resides in combining multilingual sources describing a foreign language to create a pan-European perspective, which may offer new comparative insights for the historical linguistics of target languages. If successful, this approach can be applied to other early modern vocabularies containing unique descriptions of non-European languages. Preparing a digital edition of de Tours Thesaurus faced many challenges from its inception. The tool chosen for this project was Fieldworks Language Explorercreated by SIL. This decision was motivated by the ease of access to infrastructure and know-how, as the same software is used across many linguistic projects at UU. FLEx is a tool developed for field linguists allowing them to create a semantically categorized glossary that can later be elaborated into a dictionary. However, using it for a multilingual historical text had many disadvantages that had to be mitigated, with a different level of success. The original text consists of four columns across two pages: Latin, French, and Hindustani in both Devanagari script and its Romanisation. The nature of FLEx required a decision which language to prioritise for headwords. This was not a straightforward choice, as it had lexicographic implications going further. The most challenging part was the transcription of the quite unusual form of Devanagari script. The only readily available option–normalising it to modern Hindi–however practical, meant information loss and reduced historical linguistic value of the resulting dictionary. Similarly, the complicated diacritics invented by the author to render the foreign sounds were arbitrarily simplified. Since FLEx proved to be not particularly well suited for a historical dictionary, and especially, going forward with extending the project by including other early modern Hindustani wordlists and vocabularies, more sustainable and scalable solutions are required. The main innovation of the proposed project lies in the ambition to create an integrated historical dictionary of Hindustani, in which multilingual unpublished sources are edited and linked. The majority of existing historical lexicographic projects mimetically retro-digitise printed dictionaries, not taking full advantage of the possibilities the new digital environment offers. By contrast, the information from the Hindustani manuscripts will be linked in all possible ways: between corresponding headwords in the respective works, but possibly also by external links to existing 19th-century dictionaries of Hindustani online. The user will, therefore, be able to see all the meanings of a Hindustani word from a pan-European perspective. Not only this extended project will offer new functionalities, but it will also deal with methodological issues the Uppsala project had to put away. Nevertheless, the challenges are still manifold. This project tackles a few issues at the same time: 1) Includes data from unpublished manuscripts 2) Multilingual sources3) Three different scripts4) Special characters 5) Various arrangements6) Cross-linking of entries between vocabularies The primary task will be to prepare the transcription of the works. All languages will require normalisation of spelling variants next to the added modern form. The biggest challenge of these early modern texts is that the words in the target language were often written down from hearing, using the orthography of the writers native language and employing many diacritic innovations. And so, a Dutch author would write down the sound /u/ as <oe>, use <g> for /x/, and <oo> for /o:/. At the same time, A French author would probably mark the sound /u/ as <ou>, use <g> for /ʒ/ or /ɡ/; an English author would use <oo> for /u/, but a Dutch person would read it as long /o:/. Understandably, it can be quite confusing for non-native speakers. To further complicate the matter, many values change over time. To deal with this issue, a common solution needs to be found for entities, which then will be converted to Unicode, thus creating a set of special characters for the project. In the second step, the normalised entries will be analysed by phoneticians of individual languages and translated into IPA, revealing the metalanguage-independent form of a Hindustani word. This will offer a solid ground for comparing and contrasting the Hindustani sounds as recorded by French, Dutch and Portuguese speakers. Since Hindustani is the focus of the project, the main task will be the linking of the Hindustani glosses from all the dictionaries in one integrated database. To utilise the thematic arrangement of two works, establishing the ontology for the whole project will allow adding an additional layer of enrichment in the database with semantic categorisation. This will enable researchers to categorise, select and study certain types of vocabulary – a development not available in FLEx. If successful, this approach can be applied to other early modern vocabularies, which were often created by both trained and amateur philologists in different source languages, and which today are unique and valuable descriptions of Asian languages. "
	},
	{
		"id": 311,
		"title": "How to Sustain an International Digital Infrastructure for the Arts and Humanities",
		"authors": [
			"Raciti, Marco",
			"Jorge, Maria do Rosário",
			"Fernandes, João",
			"Moranville, Yoann",
			"Gabay, Simon"
		],
		"body": " Europe has a long and rich tradition as a centre of research and teaching in the arts and humanities. However, the huge digital transformation that affects the arts and humanities research landscape all over the world requires that we set up sustainable research infrastructures, new and refined techniques, state-of-the-art methods and an expanded skills base. Responding to these challenges, the Digital Research Infrastructure for Arts and Humanitieswas launched as a pan-European network and research infrastructure. After expansion and consolidation, which involved DARIAHs inclusion in the ESFRI roadmap, DARIAH became a European Research Infrastructure Consortiumin 2014. The Horizon 2020 funded project DESIRsets out to strengthen the sustainability of DARIAH and help establish it as a reliable long-term partner within our communities. Sustaining existing digital expertise, tools, resources in Europe in the context of DESIR involves a goal-oriented set of measures in order to first, maintain, expand and develop DARIAH in its capacities as an organisation and technical research infrastructure; secondly, to engage its members further, as well as measure and increase their trust in DARIAH; thirdly, to expand the network in order to integrate new regions and communities. The DESIR consortium is composed of core DARIAH members, representatives from potential new DARIAH members and external technical experts. The sustainability of a research infrastructure is the capacity to remain operative, effective and competitive over its expected lifetime. In DESIR, this definition is translated into an evolving 6-dimensional process, divided into the following challenges: Dissemination Growth Technology Robustness Trust Education With our poster, we would like to show how the project helps sustaining DARIAH. Within DESIR, dissemination is the ability to communicate DARIAHs strategy and benefits effectively within the DARIAH community and in new areas, spreading out to new communities. Through the international workshops https://dbe.hypotheses.org/ held at Stanford University and at the Library of Congress, DARIAH has been introduced to many non-European DH scholars. These events were an important first step to foster international cooperation between US and European colleagues as well as a catalyst for ongoing collaborations in the future. A third workshop took place in Canberra at the Australian Research Data Commons in March 2019. DARIAH has currently 17 members from all over Europe. Nevertheless, efforts should be made to include as many countries as possible to bring in and scale, to a European level, even more state-of-the-art DH activities. Six candidates ready for building strong national consortia have been identified, enabling a substantial expansion of DARIAHs country coverage. Additionally, thematic workshops are organised in each country as well as tailored training measures. DESIR widens the research infrastructure in core areas which are vital for DARIAHs sustainability but are not yet covered by the existing set-up. As DARIAH expands across Europe, continuously enhancing and further developing the ERIC exceeds DARIAHs internal technological capacities. Two notable results were achieved so far: firstly, the publication of a technical reference https://github.com/eurise-network/technical-reference as a result of a workshop organised in October 2017 with CESSDA and CLARIN. Its a collection of basic guidelines and references for development and maintenance of infrastructure services within DARIAH and beyond, addressing an ongoing issue for research infrastructures, namely software sustainability. Secondly, the organisation of a Code Sprint https://desircodesprint.sciencesconf.org/ , focusing on bibliographical and citation metadata, which helped shaping DARIAHs profile in four technology areas. Another Code sprint is expected to take place in Summer 2019. Another output is the implementation of a centralized helpdesk. This helpdesk is hosted by CLARIN-D and the solution of integration within the existing DARIAH website was the creation of a WordPress plugin. This plugin is used to connect our website with the OTRS https://community.otrs.com/ server and allows the creation of issues easily by users unfamiliar with OTRS. Sustaining a research infrastructure involves also two important aspects: trust and education. For DARIAH, it is crucial to increase trust and confidence from its users. In DESIR we develop recommendations and strategies accordingly, targeting new cross-disciplinary communities, based on the results of a survey and interviews addressed to the scientific community, with different levels of approach - national, institutional and individual. In addition, education is a key area and the project contributes to the ongoing discussions about the role and modalities of training and education in the development, consolidation and sustainability of digital research infrastructures. We believe that investing time and efforts into training and educating users is a way of securing the social sustainability of a research infrastructure. "
	},
	{
		"id": 312,
		"title": "Advanced Manuscript Analysis Portal (AMAP): An Interactive Visual Language Environment for Manuscript Studies",
		"authors": [
			"Rajan, Vinodh",
			"Stiehl, H. Siegfried"
		],
		"body": " Introduction Application of state-of-the-art digital methods in the fields of Digital Paleography and Manuscript Studies has long been a challenging task, even with the proliferation of techniques within the field of Document Image Analysis. Several reasons can be attributed to this. From a methodological perspective, many of these techniques are black boxes, whose results cannot be completely scrutinized and understood. From a development perspective, there is a distinct lack of accessibility to the various published and open-source services and methods. Furthermore, many require technically complicated configurations to execute them. Projects like DIVAattempt to overcome the latter by making them available as web services. However, one still requires programming experience to use the methods and create computational solutions for scholarly research questions. Motivation Recently, Visual Language-based applications like AppInventorand Blocklyhave gained a lot of attention. By using an intuitive visual syntax, instead of textual syntax, they let non-programmers to create computational solutions without the overhead of learning a traditional programming language. However, such Visual Languageenvironments do not exist for Manuscript Studies. Although there are tools like VMR CREand DigiPalthat are interesting in their own setting, as yet, they do not provide a programming-like environment to implement DIA methods. DIVA comes very close to this idea, but mainly focuses on providing a seamless backend for other programs to take advantage of. However, very recently, Würsch et al.have come up with a new workflow design tool DIVA-DIP that involves users in the design and execution of workflows. In this context, we introduce the Advanced Manuscript Analysis Portal. The aim of AMAP is twofold. On the one hand, it offers a largely self-usable toolbox that humanists can use to build solutions themselves. On the other hand, it facilitates communication between experts from Computer Scientists and Humanists. Allowing users to jointly develop solutions minimizes the black box problem as they better understand their final system of interacting DIA modules. AMAP also strives to support reproducibility and data provenance. It further intends to encourage exploration of various relevant tools and algorithms. AMAP Design and Implementation AMAP has been designed to be platform independent and, thereby, allows the utilization of mobile devices. The architecture is entirely based on web-based technologies to facilitate this. It was ideally designed to be used on large-touch based devices and encourages touch-based interaction and collaboration). Figure 1: AMAP WorkspaceAMAP in its current version consists of a central workspace that serves as the main canvas for interaction. Imagesare the central focus of this design space. They can be imported individually or in multiples. If the latter, they are imported as a virtual pile that can be stacked and unstacked as necessary. All other kinds of operations and methods that act upon images are visualized as virtual objects that can be attached or detached to a specific image or image pile. The images can also be subsetted and the derived subsets can be further processed independently. Any subsetting will maintain a visible connection to the main image that it is derived from and, hence, preserve provenance. Various DIA methods are available as objects called action chips, which get attached to the right-side of an image. They can either return a completely processed image or just image segmentations with specific Regions of Interest. If an action chip modifies the images itself, the source image is directly changed to reflect the new modification. This is to maintain the focus on the image itself. In case of action chips for image segmentation, the ROIs are shown as selection boxes that can be used to create image subsets. The parameters of DIA methods are visualized as knobs. Thus, abstracting the type and the range of the parameter space. Experimenting by various parameters is now simply a matter of selecting the value from a given range as provided by the knob. Filters that will not directly affect the image but only change the visual appearance temporarily are made available as plugs. These are attached to the bottom part of an image and can affect image characteristics such as transparency and brightness. The chips can also be connected to each other to form an experiment processing chain or a workflow. Any changes in parameters of the methods in the chain are instantly propagated to the next elements. The intermediate results for the source images are kept for viewing and inspection. If two chips are compatible in terms of chainability, they intuitively click together, whereas, incompatible objects either do not click or repel each other. In this way, users with little or no DIA experience are guided towards configuring mutually compatible chips. It is also possible to create loops for large-scale experimenting and, hence, find the optimal parameter range for a given problem setting. All DIA operations performed in the workspace are logged in terms of timestamps, I/O and the various processesthat the source image has gone through. Logging is essential to perform well-documented scientific experiments and saves effort by avoiding experiment replication and improving on previous experiments. AMAP also provides various virtual tools that can be used for paleographic purposes. For instance, tools such as scales and protractors can be used to measure individual features of characters or images. The workspace is live and gives immediate feedback by being always reflective of the current status of overall processing in a chain. The entire workspace can be saved, allowing work continuity. It also enables sharing experiments among the community and will foster greater transparency and reproducibility. By providing an exploratory environment, scholars will have better access to the latest DIA techniques and will be empowered to create computational solutions themselves. It also provides them a hands-on experience to communicate with external developers. AMAP currently supports image processing, image segmentation, basic keyword spotting, keypoints visualization, OCR and writer identification. Conclusion In our short paper, we have briefly introduced Advanced Manuscript Analysis Portalfor programming with open source DIA methods. We initially outlined the need and motivations for developing AMAP and its potential applications, and finally, elaborated the design and implementation of AMAP. As of now, AMAP is a proof-of-concept tailored to joint experimentation/workflow design for use cases from pilot projects in SFB 950. A first larger-scale use case analysis along with user studies from a variety of domains is in the making to solicit user feedback and improve the interface design. Acknowledgement We gratefully acknowledge support by Sonderforschungsbereich950 - Manuscript Cultures in Asia, Africa and Europe -through Deutsche Forschungsgemeinschaft. "
	},
	{
		"id": 313,
		"title": "The Problem of Hobbes and the Bible: A Textometric Approach",
		"authors": [
			"Rebasti, Francesca",
			"Heiden, Serge Louis"
		],
		"body": " Context During the elaboration of his political theory, the materialist philosopher Thomas Hobbesdeveloped a complex relation with the Bible. Although he credited himself with the scientific foundation of politics, in The Elements of Law, De Cive, and the English and Latin Leviathan, Hobbes made an increasing use of Scriptural evidence in support of his arguments. While fight ing Scholastic philosop h y on its own ground, not only did he la y the foundations of modern B iblical criticism, but he also provide d a marked ly heterodox interpretation of a growing number of Scriptural passages. And y et , t he hundreds of Biblical quotations s prinkled in the political writings, Hobbess simultaneous use of multiple Scriptural sources and his inconsistent handling of the source-texts make it hard to account for the nature and development of Hobbess treatment of the Bible. That is why, w ith the notable exception of the pioneering study by Harold W. Jones, even modern scholarship has rarely dared to venture in a methodical examination of Hobbess B iblical knowledge and exegesis. O bjectives With this poster, we aim to showcase the benefits of a textometric approach to what Jones defined  the problem of Hobbes and the Bible . W e draw on pilot evidence from the ongoing Digital Theological Hobbes project, where textometric functionalities are put in the service of the study of Hobbess Biblical discussions thanks to a TXM-based corpus of diplomatic transcriptions of the English political works. Based on the preliminary results obtained from the exploitation of the prototype corpus, we argue that the unique combination of automatic and semi-automatic tools of the TXM so f tware platform < >will enable to achieve the following milestones: 1) a n index of Hobbess explicit Scriptural quotations in the political works; 2 ) a quantitative and qualitative representation of both the evolution of Hobbess exegetical discussions and their growing impact on his political theor y; 3 ) the systematic contextualization of Hobbess Biblical references in his politic al science ; 4 ) the intertextual reconstruction of the Scriptural sources of a relevant sample of quotations . First r esults Our first results are the following: 1) the TXM-based prototype corpus of diplomatic transcriptions of Hobbess English political works , built by linguistically annotat ing, refining and importing the XML-TEI P5 - encodedEEBO-TCP transcriptionsof Human Nature , De Corpore Politico , Philosophicall Rudiments concerning Government and Society and Leviathaninto the open-source and TEI-oriented TXM software platform . - TCP files are annotated with standard spellings, parts of speech and lemmata by MorhAdorner < >; - th e corpus is i nstrumented by qualitativeand quantitativeanalysis tools; - the URS annotation service allows to manually annotate word sequenc es according to a Unit-Relation-Schema model. 2) a demo of the automatic and semi-automatic tools of the TXM-based prototype corpus , in order to: - systematic ally map and index Hobbess Scriptural references in vacuo, i.e. out of their contexts and with no consideration of the textual bases on which rest the quotations, see Figs. 1 and 2 ; - ground the contextual study of Hobbess Biblical citations and the intertextual analysis of their sources by means of the URS manual annotation service , see Fig. 3 . Conclusion B y shedding new light on Hobbess knowledge, use and handling of the Scriptures over time, we will take a fundamental step forward in accomplishing Joness research agenda. However, we will also make a methodological contribution to many historical humanities. T he textometric analysis of Hobbess exegetical practices will actually allow the development of a computational approach to the study of Biblical reception that could be applied to many modern textual corpora. Fig. 1 KWIC concordance of Hobbes s Biblical referencesin the prototype corpus and full-text view of the highlighted concordance line in the synoptic edition of De Cive Fig. 2 Hierarchical index of Hobbess Biblical referencesand their progression graph in the Leviathan sub-corpus. By clicking on the 58th occurrence of the graph, its KWIC concordance opens below it F i g. 3 URS manual annotation of the quotation  Numb . 1.4. taken from The Elements of Law as: a reference devoid of citation, a simple quotation, and a reference to the Old Testament. The context property allows the extraction of the title of the chapter section analysed : Parallel of the Twelve Princes of Israel, and the Twelve Apostles "
	},
	{
		"id": 314,
		"title": "Digital Humanities for the Study of Social Reading",
		"authors": [
			"Rebora, Simone",
			"Lauer, Gerhard",
			"Herrmann, J. Berenike",
			"Kuijpers, Moniek",
			"Kraxenberger, Maria",
			"Pianzola, Federico",
			"Boot, Peter",
			"Lendvai, Piroska",
			"Messerli, Thomas",
			"Salgaro, Massimo"
		],
		"body": " Introduction Over the last decades, with technological advancements such as growing digitalization and the development of social media platforms, the act of reading has transformed into an interactive experience, where the Internet plays a key role. Social reading platforms like Goodreads and Wattpad are online environments where millions of people from all over the world come to share their love for the written word. Members come together to discuss what they have read and what they judge as good or bad literature, they recommend books to one another, and even try their hand at writing fiction. While a growing number of studies have been dedicated to this phenomenon, so far only a few have adopted computational methodsand none has combined these methods with empirical approaches to the study of literature and its effects. As the online environment is very different from the literary field as we know it, showing new types of complex interactions, we need to explore social reading and writing from both social and content perspectives. Social questions that should be investigated include interaction among users, questions of power, the effects on literacy and on reading behaviors, the changing system of social values. Content questions include questions about style, about the distribution and originality of comments, about the affective, reflective or social nature of content. With this panel, we will showcase the potential of studying social reading through the combination of multiple and interrelated approaches: from purely statistical, data-driven, and stylometric analyses, through qualitative and quantitative surveys of key users and a theory-driven qualitative taxonomy of reading valuation, towards a combination of the empirical and the computational, supported by a sound theoretical/methodological awareness. The substantial variety of case studies in four languageswill reflect the diversity of social reading, which can and should be studied from multiple points of view as well as with an array of methodological tools. Visualizing Wattpad Federico Pianzola Simone Rebora With this abstract, we focus on Wattpad , where social reading takes the form of a discussion in the margin, i.e. where texts are commented upon by millions of users, paragraph by paragraph. A central goal of our study is to understand how the corpus of the most appreciated texts on Wattpad is different from the canonical corpus of Western literature. For a preliminary analysis, we focused on the categories of Classics and Teen Fiction. Analysis I. For a first understanding of readers engagement, we examined the progression of the number of comments over the 20 most-commented books for each genre. Figures 1 and 2 show that TF is more stable, whereas for many Classics the majority of comments is on the first chapters. Also the total numbers are very different: 2,569,405 comments for the first TF title and just 42,013 for Pride and Prejudice. Figure 1. Progression of the number of comments over the 20 most-commented Classics books Figure 2. Progression of the number of comments over the 20 most-commented TF books Analysis II. For a deeper understanding of reader response, we adopted sentiment analysis and compared the emotional arcsof both paragraphs and comments. The analysis was performed using the Syuzhet package on 6 books per genre: all details and limitations of the approach are described in. Figures 3 and 4 show a better attunement between paragraphs and comments in TF: the highest Pearsons correlationwas reached by TF title #3. Figure 3. Emotional arcs for paragraphs and comments of six Classics books Figure 4. Emotional arcs for paragraphs and comments of six TF books Analysis III. To explore the relations between users while reading the books, we adopted network analysisbased on a very simple rule: the more two users reply to each others comments, the stronger is their connection. The visualizations were realized through the Gephi platform. Figures 5 and 6 show the networks of the most commented Classics and TF titles. To make the samples comparable, the networks were reduced to the 1,000 strongest connections. As evident, Classics readers tend to group in a single cluster, while TF readers group in multiple sub-clusters. This phenomenon seems to suggest that reading classics enhances the formation of a more homogeneous community. Figure 5. Network graph of the 1,000 strongest connections between Pride and Prejudice readers Figure 6. Network graph of the 1,000 strongest connections between The Bad Boys Girl readers Analysis IV. To visualize Wattpads user distribution geographically, we analyzed a sample of 300,000 user profiles. Out of these, only 34.3% provide locations, many of which are fictitious. We converted these locations into standardized state names with the help of the Google Maps Place Autocomplete API. Notwithstanding some errors, the analysis provided confirmation to the supposed relevance of states like India and Philippines, together with the USA. Figure 7. Geographic map of Wattpad users All these analyses are to be considered as preliminary to a more extensive and detailed study, but they efficiently showcase the high potential of investigating the social reading phenomenon through visualizations. Wattpad data urge us to rethink concepts like world literature and its dynamics. Of course, we cannot generalize these findings for books circulating through traditional publishing channels, but we can gain interesting insights for a more in-depth critical reflection on publishing and reading in the 21st century. Sources of authority in online book reviews Peter Boot It is a commonplace to state that with the advent of Amazon, book blogs and readers communities the role of traditional authorities in the literary field has become less important. Rather than following the lead of professional critics, teachers or professors, readers are said to take advice from fellow readers. In my contribution to this panel, I will look at the evidence that a corpus of online Dutch book reviews can provide for answering the question which persons or institutions are considered authoritative by online reviewers. In a pilot investigation, we have looked at references to possible authorities from a number of domains: traditional critics, newspapers, prizes, television programs, the book trade, authors, teachers, websites and private contacts. Reviews were downloaded from some prominent weblogs, some dedicatedreview sites, the online magazine 8weekly and, for contrast, the most prominent Dutch newspaper NRC. We used collections of search terms and regular expressions to search the downloaded reviews using AntConc. Irrelevant hits were removed manually. A subset of the remaining results was annotated and assigned a type, roleand agreement. In all 1518 references to some form of authority were annotated. Because of many practical limitations and ad-hoc decisions, the procedure wont give us any firm numbers but it does give a clear indication of sources of authority that are recognised in the online domain. Figure 1. Main sources of authority in online reviews The main findings are summarized in Figure 1. The main sources of authority in the online reviews are companiesand literary prizes. Authorsare also important. Online criticsare not unimportant, but print critics are hardly mentioned. Personal contactsare more important than print critics. Frequently, people refer to a medium rather than to a critic by name, but here too, online media are more often mentioned than print media. Teachers are almost invisible. Much can be said about this. For one thing, we counted all references equal but some will weigh heavier than others. There are also important differences between the online platforms. The collection we used did not include reviews from booksellers sites. For now, though, the most important conclusion is that no, traditional critics are not highly valued on online platforms; however, the beneficiaries are commercial companies rather than fellow users of online platforms. Classifying the styleof criticism. A computational analysis of Italian book reviews Massimo Salgaro Simone Rebora In this abstract, we use a corpus of book reviewsto answer the following question: how do professional critics, journalists and passionate readers differ in the writing of reviews and what features can be used to identify them? The corpus is divided into three subsets: reviews published on social reading platforms, in paper magazines, and in scientific journals. All sub-corpora have an approximate size of 650,000 tokens. First, we adopted stylometry to classify the texts. As demonstrated by, the first element to influence the quality of a stylometric classification is text length. Preliminary tests—ran with the Stylo R packageon 5,000-word-long text chunks—showed how Cosine Delta distance, based on just 50 MFW, was able to almost perfectly separate the three subgroups. Considered the high variance of text length in our corpus, we artificially generated a series of sub-corpora composed by text chunks of the same lengthand we evaluated clustering quality through the adjusted Rand index in the PyDelta Python library. Figure 2 confirms how Cosine Delta distance with 2,000 MFW is the best performing classifier, but also 200 MFWreach a similar—and, in some cases, even better—efficiency. As for text length, clustering quality is quite poor below 1,000 words, while a plateau is reached at about 3,000 words. Figure 1. Network graph of the corpus. In green: aNobii; in violet: Sole 24 Ore; in red: Between, Osservatorio, and OBLIO. Figure 2. Clustering quality per slice length and MFW usedTo improve the results for shorter chunks, we developed a framework for a machine learning classifier, by operationalizing a series of traditional definitions of literary criticism. An extensive lexicon of literary criticismwas translated into Italian; selections of terms related to mental imagery and emotional aesthetic response were extracted from questionnaires and tools in empirical aesthetics, translated into Italian, and expanded through the fastText Italian word-embedding model. These resources—together with selected features in the LIWC Italian dictionary—were used to measure emotional and cognitive involvement with the reviewed text. The measurements were combined with the results of the stylometric analysisand used to train an SVM classifier. For a corpus composed by 500-word-long chunks, the sole stylometric analysis reached an attribution accuracy of 90.1%, while the SVM classifier scored 93.2%. A slight but promising improvement, if we consider the simplicity of the framework—that can and should be refined further. With this paper, we hope to have cast the groundwork for a research that might fruitfully combine computational methods and literary theory to study the style of criticism of professional and non-professional readers. Shared Reading. Digital Reading and Writing of Literature Gerhard Lauer Maria Kraxenberger In the digital age, the practice and habits of reading change fundamentally. Not few speak of the end of the book and ofreading. However, and contrary to this popular statement, reading and writing literature has never been practiced as intensely as within digital societies– a development that involves in particular young, adolescent readers. In preliminary studies, we categorized various reading and writing platforms and conducted a first study on Harry Potter-fanfiction, using a personality questionnaire. Results indicate a dominance of romance as well as of fantasy genresand that authors of fanfiction are usually between 14 and 20 years old. They show both introverted and extroverted personality traits and come from diverse educational backgrounds. Generally, they can be considered rather empathetic. In a similar vein, other reading research suggests that dealing with literary texts increases performance in Theory of Mind, and thus might the support the development of prosocial skills such as empathy. Despite justified criticism on over-generalizations, research on the emerging changes towards digital, joint reading of texts with presumably much lower literary demands and its potential implications is missing. This is the more surprising, since first explorative studies in schools indicate that digital formats can be used very well for education through writing and readingand not only shed light on, but also foster a variety of effects. Nevertheless, it is still unclear how and with what consequences digitization affects the reading behavior of adolescents, including complex processes such aslanguage acquisition, establishments of social peer-groups and their interactions, as well as their understanding and appreciation of literature and the written word. Our project focuses on the changing social processes that accompany the reading and writing of literature in the digital age. In doing so, we focus on the young readers and their distinct reading and writing socialization on digital readingplatforms such as Wattpad. Based on a social interactionist-reading model and the theoretical concept of scaffoldingthe project integrates descriptive, qualitative and quantitative methods in a multi-methodological approach. We present a questionnaire that combines established scales from social psychology with items used in reading research to explore online and offline reading habits. Using new methods of field research enables a better understanding of how intensively young readers think and feel about reading and writing literature and what kind of values are used to talk about literature in social media. To arrive at a more comprehensive picture of reading in the digital age, we provide a general theoretical framework that helps to integrate the thus acquired data. Wheres your attention? An empirical assessment of Web 2.0 users literary values J. Berenike Herrmann Thomas Messerli Traditionally, reading fiction has been seen as a tool for honing crucial sense-making capacities, enabling an integrated sensual and intellectual personal development. Web 2.0 users, on the other hand, are understood not so much as interested in the challenges and pleasures of literature-as-art, but in the easy gratification of popular genres, driven by an economy of attention. Users book reviews are held to show an affirmative bias, documenting the lack of a deep and discerning reading engagement. Mirroring a stated trend towards a digestible documentary and authentic literature, lay reviewers are held to operate with a heavy content bias and to neglect formal criteria. However, many of these verdicts lack comprehensive empirical support, leaving questions such as the following ones open: What do web 2.0 readers actually do when judging literary value? What are the grounds for value judgements? Do readers apply aesthetic and ethical premises? Do they prioritize content over formal criteria? In our paper, we will scrutinize a corpus of lay literature reviews published on the German reading platform lovelybooks.de for the premises underlying users valuation statements. Using sentiment analysis and rule-based techniques as a bootstrap methodology for semi-automatic identification of valuation statements in our corpus, our exploratory case study analyzes the lovelybooks.de categories classics and novels Combining quantitative and qualitative methods, we examine how users evaluate literary texts in terms of quality criteria such as subjectively perceived effects, but also more formal ones, including authorial style, literary character motivation and plot construction. In addition to semi-automatic coding and running KWICs of recurrent phrases and keywords, we analyze a number of reviews as integral cases. How do users develop theirassessments? How do assessments appear in terms of tone and habitus? How do users prioritize the different categories for their evaluation? The qualitative coding is aimed at fine-tuning a coding schema that adapts the axiological model by Heydebrand & Winkofor web 2.0 lay reviews. Our first results do indeed point to users heightened subjectivity and a tendency to plain statements, reporting subjectively perceived effectsand marking taste. Yet, we found considerable attention to formal aspects such as plot constructionand character motivation, as well as criticaljudgements. What is more, our results indicate that many users do apply a plain and subjectively toned language, but handle a reflected taxonomy of value criteria to support their taste judgements, expressed by statements such as Noch schwerer wog bei der Sternevergabe … [The awarding of stars was even more influenced by…], Ein Punkt, der mir sehr wichtig ist [A point that is very important to me]. We thus suggest that instead of propelling a decline of intellectual scrutiny and differentiation, online reading platforms offer a potential for personal insight and development. Our study is one of the first few data-driven approaches to scrutinize the literary values underlying peoples engagement with literary texts in a participatory culture. Empirical Goodreads Simone Rebora Moniek Kuijpers Piroska Lendvai Our research aims to complement computational linguistics with methods used in empirical literary studies to investigate the experience of getting lost in a book. Using software from the field of natural language processing, we match the 18 statements from the Story World Absorption Scale– a questionnaire used to identify absorbing reading experiences – to reader reviews posted on the online platform Goodreads . The SWAS taps into four different aspects of absorbed reading, namely sustained concentration, vivid imagery of the story world, feelings of empathy or sympathy for the characters of a storyand the sensation of having made a deictic shift from real world to story world. The reader reviews on Goodreads more often than not include descriptions of peoples experiences with certain books and evaluations of their reading experiences, and therefore it could be argued that they fall somewhere between accounts of reader response and literary criticism. They certainly constitute a rare treasure trove of qualitative, user-generated data on reading and reading evaluation. The aims of our project are twofold: 1) validating the SWAS, and 2) enabling comparative analyses of absorption across different books, genres, and reader groups. We have performed a manual analysis on 180 Goodreads reviews of three contemporary blockbuster novels, confirming that, in many cases, SWAS statements and particular sentences in Goodreads reviews overlap substantially. For example, one reviewer writes: Im so absorbed in the world Martin produced out of his wits; while another reviewer expresses her identification with the main character: I went through all the emotional ups and downs right along with her. A total of 130 matching sentences were identified. Figure 1. SWAS/Goodreads matches. A1-A5: Attention, T1-T5: Transportation, EE1-EE5: Emotional Engagement, MS1-MS3: Mental Imagery In order to extend the analysis to the entire Goodreads corpus, which collects about 80 million book reviews, we combine two technologies: textual entailment detection software, i.e., EOPand text reuse detection software, i.e., TRACER. Preliminary experimentsshow that both tools need adaptation and training for this specific task, as our best out of the box recall score is 0.28, while training on the manually-annotated reviews increases recall to 0.49. Based on the 130 identified sentences plus the 18 SWAS statements, we defined a provisional absorption lexicon and expanded it through a word-embedding modelbased on 2.5 million reviews on Goodreads. Figure 2 shows a synthetic representation of this lexicon. Figure 2. Absorption lexicon. The lexicon was used to identify, through a standard bag-of-words approach, the reviews that showed the highest levels of absorption. The results, while significant in themselves, are used as the starting point for extensive semi-automatic annotation work – with four annotators working in parallel for a total of 18 months, starting in December 2018. Our goal will be that of producing a ground truth corpus as training data for machine learning algorithms, towards a fully-automated matching of Goodreads reviews with the different aspects of absorbed reading. Figure 3. Network graph based on the absorption scores for the four SWAS categories. Figure 4. Zoom of Fig. 3. "
	},
	{
		"id": 315,
		"title": "Using Visualization to Understand the Complex Spatiality of Mappae Mundi",
		"authors": [
			"Reckziegel, Martin",
			"Wrisley, David Joseph",
			"Hixson, Taylor Wright",
			"Jänicke, Stefan"
		],
		"body": " Introduction A common practice in spatial humanities is georeferencing historical maps to generate rasters for use in Geographic Information Systems. This method is typically carried out with different levels of precision and with different goals in mind; for example, spatial humanities practitioners create base maps from historical maps, they extract vector data or they compare spatial representations over time. If we consider both the historical map and the base map as imperfect representations--rather than reflections--of reality, the georeferencing process creates a relation which needs to be critically analyzed. This paper aims to expand the modes of visualization available to scholars of cartography interested in exploratory spatial analysis of medieval maps in a way that the complex spatiality of the historical map is not overtaken by that of normative base maps. In the case of historical maps whose coordinates and scales are roughly comparable to modern maps, valuable historical information can be extracted from the georeferencing process. Furthermore, algorithmic analysis, such as differential distortion, can help identify geometric inaccuracies of so-called old maps. Visualizing such divergence allows one to analyze historical cartographic technique. In this paper we turn instead to very old maps--examples of so-called complex medieval maps that blend conventional T-O structure with pseudo-geographic detail. Historical cartographic studies have emphasized the encyclopedic quality of such maps, their textual sources and their blend of geography and sacred, cosmological detail. Rather than a modern coordinate system, we find in some of them notions of spatial orientation suggested by the abundance of detail and named entities on the surface of the map. Instead of consistently stretching the map, georeferencing can lead to problems of occultation, that is, to hidden folds in the distorted map. Another way of expressing this is that the distances or orientations of places or features on one map representation may not be consistent with another map. Furthermore, a map predating modern coordinate systems may contain non-geographic information. Stretching such a pre-modern map to a contemporary map projection, therefore, might seem counterintuitive. We argue that visualization of purposeful distortion can allow us to understand better the pre-modern organizational structure of maps. In our research we have found that this practice allows us to situate such pre-modern maps on a spectrum between the more topological and the more symbolic. Map Projections and Georeferencing The method of drawing a map of the almost spherical earth on a planar surface is called a map projection, a process only possible with distortion of some important geographic properties. One such projection known as the Mercator, distorts areal scaling inconsistently, so that the areal scaling factor increases as one moves north. In the process of georeferencing, control points on the historic map are manually referenced on the modern map. For each control point in the coordinate system of a scanned historic map image, there is a control point in the coordinate system of the modern map image, which represents the same entity. Several algorithms exist to use this information to project the whole historic image onto the modern coordinate system. In this paper we use the Thin Plate Splinemethod, which maps the control points accurately and interpolates the locations in between, such that the bending energy of an imaginary surface is minimized. Existing Visualization Techniques for Showing Distortion The computation and visualization of modern map projection distortion of a globe-to-map setting has been extensively studied. A common approach is to show ellipses on the map, indicating the distortion of infinitesimal circles on the globe. This method is named after French mathematician Tissotand has been extended to visualize flexion and skewness simultaneously. Further visualization techniques for the globe-to-map scenario have been elaborated, including the use of color maps and contour lines. Methods to compute and visualize distortion in a map-to-map scenario also exist. The latter study presents a methodology to visualize areal and angular distortion using differential analysis for every point of the modern map as projected onto the historic map or vice versa, given an arbitrary, differentiable projection function. Visualizing Areal Distortion in Mappae Mundi It goes without saying that because medieval maps do not follow the projections used in modern cartography, by georeferencing them as described in section 2 we do not expect a seamless, intuitive result. Instead, when projecting mappae mundi onto modern maps or vice versa, topological inconsistencies are likely to occur. As shown by the control points in Figure 1a, an imaginary place A might be left to a place B on the historic map, while their order is reversed on the modern map. Further, the places C and D remain their order. To resolve this, the TPS method creates a folded projection of the historic map on the modern map as shown in Figure 1b. Topological inconsistent projection. In order to show the pre-modern organizational structure of historical maps, possibly demonstrated by those regions of topological inconsistency, we visualize how the historic map will be scaled and where it will be folded by the TPS method. We base our calculations on an established areal scaling factor formulashown in Eqin Figure 2 which can be simplified to Eq. This factor can also be derived by calculating the area A of the parallelogram the partial derivatives span using the shoelace formula, shown in Eq. The term Â will be negative only if the points of the parallelogram are found in a clockwise direction; in which case the TPS projection is flipped. As such, this property helps us to identify folded regions. The resulting visualization for the historic map of the example outlined above is shown in Figure 1c. Our visualization design can be read like a traditional contour map showing a mountain area: the higher the mountain, the larger the scaling factor of the image. Each contour line in the visualization represents a region where the scaling is identical. The wider such contour line is drawn, the smaller the gradient is, i.e., the smaller the change of scaling is at that location. To remain independent from units, we normalize that factor between the minimal and maximal occuring scales. Black contour lines represent mean scaling. The more saturated the lines are, the larger the magnification or shrinkage is with respect to the mean. The color blue indicates a factor smaller, respectively the color red a factor greater than the mean. Turquoise indicates shrinkage, and yellow, magnification in a folded region, that is, where the orientation of features of the historic map is flipped. A white line represents the crease of the fold. In addition to the contour lines, we identify the regions which will overlap in the projected image and draw those darker, resulting in shadows around folded areas. The result of applying the projection on the visualization itself can be seen in Figure 1d. Areal scaling factor calculation. Discussion At this stage of our research, we have used three medieval mappae mundi: the Ebstorf map made in Northern Germany around 1235, the Cotton Anglo-Saxon map created at Canterbury between 1025 and 1050 and the Hanss Rüst woodcut map made in Augsburg around 1480. The abovementioned distortion techniques for medieval maps provide us with important insight into the particular blend of pseudo-geographic and non-geographic features found in each document. Results of this visualization are shown in Figure 3. The Ebstorf map is the most detailed of our three examples. The key distortion, located around the map center, is due to the seal with Christ indicating Jerusalem, a symbolic, rather than geographic, placement. Other distortion is visible around Sicily, Crete, Cologne and cities of the Alexander legends. In the Cotton Anglo-Saxon map, no folds were produced given the chosen control points. On the other hand, we observed significant scalar expansion relative to the mean in the regions of Barbary and North Africa as well as the Western Mediterranean and the Black seas. Shrinkage of scale was observed particularly in two directions: along the axis Jerusalem-Babilonia-India as well as over Scotland and the Hebrides. The Hanss Rüst map exhibited the least geographic realism of all three. Visualization results. From top to bottom: The historic map image, the historic map including our visualization and the historic map projected onto a mercator map. The folds created by map distortion, visualized in our design, provide strong visual stimuli for understanding the continuum of pseudo-geographic features of old maps, that is, for the critical evaluation of the spatiality of such maps on their own terms. Whereas for the TPS method used in this paper we chose control points based on written geographic information, other choices for control points could lead to contradictory results. In future work, we intend not only to expand the set of maps that we can annotate, but also expand and develop our visualization methods. Finally, we intend to alter our mode of annotation, by choosing, as in classic modes of georeferencing, features on the map, such as peninsulas, mountains or rivers, instead of text or in combination with text to assess what different results we might obtain. "
	},
	{
		"id": 316,
		"title": "The Past, Present and Future of Digital Scholarship with Newspaper Collections",
		"authors": [
			"Ridge, Mia",
			"Colavizza, Giovanni",
			"Brake, Laurel",
			"Ehrmann, Maud",
			"Moreux, Jean-Philippe",
			"Prescott, Andrew"
		],
		"body": " The Past, Present and Future of Digital Scholarship with Newspaper Collections Panel Overview Mia Ridge, Giovanni ColavizzaHistorical newspapers are of interest to many humanities scholars, valued as sources of information and language closely tied to a particular time, social context and place. Following library and commercial microfilming and, more recently, digitisation projects, newspapers have been an accessible and valued source for researchers. The ability to use keyword searches through more data than ever before via digitised newspapers has transformed the work of researchers. Digitised historic newspapers are also of interest to many researchers who seek large bodies of relatively easily computationally-transcribed text on which they can try new methods and tools. Intensive digitisation over the past two decades has seen smaller-scale or repository-focused projects flourish in the Anglophone and European world. However, just as earlier scholarship was potentially over-reliant on The Times of London and other metropolitan dailies, this has been replicated and reinforced by digitisation projects. In the last years, several large consortia projects proposing to apply data science and computational methods to historical newspapers at scale have emerged, including NewsEye, impresso, Oceanic Exchanges and Living with Machines. This panel has been convened by some consortia members to cast a critical view on past and ongoing digital scholarship with newspapers collections, and to inform its future. Digitisation can involve both complexities and simplifications. Knowledge about the imperfections of digitisation, cataloguing, corpus construction, text transcription and mining is rarely shared outside cultural institutions or projects. How can these imperfections and absences be made visible to users of digital repositories? Furthermore, how does the over-representation of some aspects of society through the successive winnowing and remediation of potential sources - from creation to collection, microfilming, preservation, licensing and digitisation - affect scholarship based on digitised newspapers. How can computational methods address some of these issues? The panel proposes the following format: short papers will be delivered by existing projects working on large collections of historical newspapers, presenting their vision and results to date. Each project is at different stages of development and will discuss their choice to work with newspapers, and reflect on what have they learnt to date on practical, methodological and user-focused aspects of this digital humanities work. The panel is additionally an opportunity to consider important questions of interoperability and legacy beyond the life of the project. Two further papers will follow, given by scholars with significant experience using these collections for research, in order to provide the panel with critical reflections. The floor will then open for debate and discussion. This panel is a unique opportunity to bring senior scholars with a long perspective on the uses of newspapers in scholarship together with projects at formative stages. More broadly, convening this panel is an opportunity for the DH2019 community to ask their own questions of newspaper-based projects, and for researchers to map methodological similarities between projects. Our hope is that this panel will foster a community of practice around the topic and encourage discussions of the methodological and pedagogical implications of digital scholarship with newspapers. Short Paper: Living with Machines Paper authors: Giovanni Colavizza 1 , Mia Ridge 2 with Ruth Ahnert 3 , Claire Austin 2 , David Beavan 1 , Kaspar Beelens 1 , Mariona Coll Ardanuy 1 , Adam Farquhar 2 , Emma Griffin 4 , James Hetherington 1 , Jon Lawrence 5 , Katie McDonough 1 , Barbara McGillivray 6 , André Piza 1 , Daniel van Strien 2 , Giorgia Tolfo 2 , Alan Wilson 1 , Daniel Wilson 1 . 1 The Alan Turing Institute; 2 British Library; 3 Queen Mary University of London; 4 University of East Anglia; 5 University of Exeter; 6 The Alan Turing Institute / University of Cambridge Living with Machines is a five-year interdisciplinary research project, whose ambition is to blend data science with historical enquiry to study the human impact of the industrial revolution. Set to be one of the biggest and most ambitious digital humanities research initiatives ever to launch in the UK, Living with Machines is developing a large-scale infrastructure to perform data analyses on a variety of historical sources, and in so doing provide vital insights into the debates and discussions taking place in response to todays digital industrial revolution. Seeking to make the most of a self-described radical collaboration, the project will iteratively develop research questions as computational linguists, historians, library curators and data scientists work on a shared corpus of digitised newspapers, books and biographical data. For example, in the process of answering historical research questions, the project could take advantage of access to expertise in computational linguistics to overcome issues with choosing unambiguous and temporally stable keywords for analysis, previously reported by others. A key methodological objective of the project is to translate history research questions into data models, in order to inspect and integrate them into historical narratives. In order to enable this process, a digital infrastructure is being collaboratively designed and developed, whose purpose is to marshal and interlink a variety of historical datasets, including newspapers, and allow for historians and data scientists to engage with them. In this paper we will present our vision for Living with Machines, focusing on how we plan to approach it, and the ways in which digital infrastructure enables this multidisciplinary exchange. We will also showcase preliminary results from the different research laboratories, and detail the historical sources we plan to use within the project. Short Paper: impresso - Media Monitoring of the Past Paper authors: Maud Ehrmann ☨ , Matteo Romanello ☨ , Frédéric Kaplan ☨ , Marten Düring * , Estelle Bunout * , Daniele Guido * , Paul Schroeder * , Thijs van Beek * , Andreas Fickers * , Simon Clematide § , Phillip Ströbel § , Martin Volk § ☨ École polytechnique fédérale de Lausanne, DHLAB; * University of Luxembourg, C2DH; § University of Zurich, Institute of Computational Linguistics Historical newspapers are mirrors of past societies. Published over centuries on a regular basis, they record wars and minor events, report on international, national and local matters, and document the day-to-day life; in a word, they keep track of the great and small history. They reflect the political, moral, and economic environments in which they were produced and they hold dense, continuous, and multi-level information which can help us understand how contemporaries experienced their present. This makes them invaluable primary sources for historians. How can newspapers help understanding the past? How to explore them? Long held on library and archive shelving, newspapers are undergoing mass digitization. Millions of facsimiles, along with their machine-readable content acquired via optical character recognition, are becoming accessible via a variety of online portals. If this represents a major step forward in terms of preservation of and access to documents, much remains to be done in order to provide extensive and sophisticated access to the content of these digital resources. In this regard, we still face many challenges. First, not all historical newspapers are digitized, and heterogeneous schemes of availability and accessibility lead to an opaque landscape of historical media silos. Next, the quality of OCR outputs often makes subsequent automatic text processing difficult and unreliable. This content accessibility issue closely relates to the more fundamental -- and promising -- challenge of content exploitation and exploration: how to make sense of the vast amount of available unstructured text? To achieve this, we need to semantically enrich the contents of historical newspapers, i.e. to extract, process, and link the information they contain. Another challenge relates to data visualization and exploration which need to accompany enhanced text analysis capacities and comply with historical research imperatives. Finally, these challenges can only be met through the close interplay between computer sciences and history, an essential factor for enabling new and methodologically reflected digital history scholarship. The interdisciplinary impresso aims to develop a methodologically-reflected technological framework to enable new ways of engaging with multilingual digital content of historical newspapers and new approaches to address historical questions. More precisely, the project applies text mining techniques to transform noisy and unstructured textual content into semantically indexed, structured, and linked data; develops innovative visualization interfaces to enable the seamless exploration of complex and vast amounts of historical data; identifies needs on the side of historians which may also translate into new text mining applications and new ways to study history; and synergistically reflects on the usage of digital tools in historical sciences from a practical, methodological, and epistemological point of view. We will try to answer the question how to build a historical media monitoring tool suite?, by introducing the main objectives of impresso and present what was achieved as well as lessons learned so far. In particular, we will focus on how we manage, process and represent data, and how we operationalize interdisciplinary collaboration via interface co-design in order to welcome the scholarly use of the material. Impresso is supported by the Swiss National Science Foundation under grant CR- SII5_173719. Short Paper: Construire avec les usagers la numérisation des collections de périodiques Paper author: Jean-Philippe Moreux, Bibliothèque nationale de France Les collections de périodiques anciens numérisés ont désormais un âge respectable, et le paysage des usages quelles auront contribué à dessiner a donc considérablement changé. Différentes politiques se sont succédé, depuis la numérisation dune sélection restreinte de titres de presse quotidienne versée dans un portail documentaire, jusquaux programmes de numérisation de masse sinscrivant dans un objectif de préservation. Les usages soutenus par ces politiques auront évolué de manière concomitante, de la recherche dinformation aux pratiques plus récentes de fouille de données à finalité scientifique. Une tension constante traverse cette période, celle de la documentation de ces politiques et de sa communication aux usagers. Si en 2007, il était aisé dexpliciter le choix de numériser Le Figaro, dix ans plus tard, ce titre est une ressource parmi 3 300 autres, toutes puisées dans le réservoir des 250 000 notices de périodiques conservées à la BnF. Cette trajectoire menant à une abondance de la ressource, participe cependant à alimenter le questionnement légitime de toutes les catégories dusagers : quel est le contenu de la boîte noire que je suis en train dinterroger ? Et pour les utilisateurs relevant du champ des humanités numériques, ce questionnement est dautant plus impératif quils doivent prendre en compte les enjeux de description, de représentativité, de biais, etc. de leur corpus de travail. Répondre à cette question, du point de vue des gestionnaires des bibliothèques numériques, nest pas trivial, car elle agrège plusieurs dimensions et appelle de ce fait des actions multiples. Au niveau macroscopique, il sagirait tout dabord de décrire des choix documentaireset de les situer dans un paysage plus vaste. Les catalogues, les portails agrégateurs, participent de cet effort et aident les utilisateurs à découvrir et localiser les ressources à disposition. Mais ils ne dévoilent que rarement les politiques documentaires et techniques à lœuvre. Des initiatives spécifiques au public des humanités numériques sont également nécessaires, par exemple le recensement ou la création de corpus de référence préparés pour des usages de fouille de données. A une échelle inférieure, celle du périodique numérisé lui-même, lutilisateur est le plus souvent confronté à la sécheresse dune notice bibliographique, alors que les questions quil se pose sont nombreuses : contexte et historique de publication, exhaustivité, pratiques de numérisation, qualité du texte transcrit par lOCR. On peut imaginer que certains de ces défis devront être relevés par linstauration dune dynamique douverture des données aux usagers et de coconstruction avec les usagers. Ainsi, enjamber les frontières, quelles soient nationales ou techniques, se fera via des protocoles et API interopérables, autorisant la création de corpus ad hoc à des fins de recherche. Co-construire une politique de numérisation impliquera de donner aux utilisateurs la liberté démettre des suggestions de numérisation et aux équipes de recherche un rôle de partenaire actif des choix de numérisation. Enfin, enrichir la description des ressources numérisées, jusquau niveau élémentaire du numéro ou du fascicule, pourrait aussi sappuyer sur lintelligence collective des usagers. Overview Paper: Digital Editions of Serials and media historians: an overview Paper author: Laurel Brake, Birkbeck, University of London The second reflective paper is from a 19th century media historian whose experience as a Principal Investigator includes working collaboratively to design and produce a curated online edition of runs of six 19th century serials across the century. Ncse, the Nineteenth-Century Serials Edition, includes a Chartist newspaper, a satirical illustrated paper, a professional journal, an early feminist monthly, a weekly news magazine, and an early theology journal. A decade old, ncse recently required upgrading, and this paper reflects this timely encounter with sustainability and how projects grow in the light of changing contexts. Nsce defines its remit as serials, including newspapers and magazines, but I wish to problematise their relationship from the perspective of nineteenth century studies – are these forms of media best treated as parts of the same cultural industry, sharing contributors, editors, and illustrators as they did; and if so, why are newspapers so often presented separately from journals and magazines? It is arguable that the press is the product of a single cultural industry in the nineteenth century, and is better studied as a whole. This retrospective will next inform some constructive criticism of ongoing projects. Language barriers are an issue for searching and accessing records from sites, such as Europeana, that feature items in multiple languages. The problem of multiple languages is enhanced for media historians who need to search or browse in long runs, involving thousands of pages. More work is needed on the cross-national nature of the press, facilitating comparative work, and asking teams to consider the integration of translation software for those welcome digital media projects like Oceanic Exchanges and NewsEye that address the problem of titles in multiple languages by drawing on global titles in their corpora. Building on an interest in the press itself, ongoing projects could also make it possible to interrogate corpora of data for characteristics of the media itself. For example, how did titles and publishing patterns change by year, by country, after 1850? How did issue prices or internal structure change over time for different categories of the press? How do the names and pseudonyms of correspondents and authors overlap with other titles? Furthermore, can these projects enable the creation of a single digital hub that allows researchers to see what has been digitised and what remains undigitised. It would also be important for software for analysis of these corpora to include effective visualisations that users can themselves deploy to circulate their findings meaningfully. These will help users understand algorithmic input to and results from analysis, often reported statistically in graphs that are not always readable to humanist scholars. Both Impresso and Living with Machines seem conscious of users, and the desirability of including analysis and reflection in their projects. In general, consultations between projects and potential readers should happen much earlier in the conceptual and project building cycles, at a point where their input can be formative to project design and aims. Finally, sustainability should be foregrounded while at the moment it appears as a secondary concern. Overview Paper: Towards a Critical Framework for Digital Newspaper Scholarship Paper author: Andrew Prescott, University of Glasgow I feel privileged to have witnessed the use of newspapers for historical research in Britain evolve from grappling with microfilm reels through to the wealth of resources currently available online. The way in which digital projects have opened up newspapers as accessible historical sources for a wide variety of topics is remarkable. While many scholarsuse fairly traditional research methodologies in exploring online historical newspapers, the way younger scholars are using innovative digital methodologies to explore such topics as social networking, history of ideas, and epidemiology is very exciting. As one of the most extensive international pools of historical data created by the first wave of the digital revolution, newspapers are already demonstrating the transformative potential of digital scholarship. My own use of newspapers in historical research has been focussed on investigations of associational culture in eighteenth- and nineteenth-century Britain, and the availability of digital newspapers has been fundamental to this research. However, I have become increasingly aware of a number of issues which we need to address. I suggest that these questions might form the basis for a framework of critical newspaper studies. Some of these themes are: What is a newspaper? Many existing newspaper digitisation projects were set up in response to immediate curatorial pressures, such as the British Librarys need to vacate its premises at Colindale. The scope of these newspaper collections reflect long-standing pragmatic curatorial decisions and these require further exploration. Why should some periodical publications be included in digital newspaper collections and others not? What are the implications of the way in which collections have been divided for the structure and character of our digital corpora? National boundaries. While the web is international, the way in which digitisation of newspapers has proceeded has reinforced national boundaries. National libraries have prioritised digitising newspapers from their own country. The British Library has focussed on its British holdings, but has paid little attention to digitising foreign holdings, even though some of these are unavailable in their home countries. Trans-national use of newspapers appear to be limited. Fragmentation. Digitisation has proceeded on an institutional basis, so that runs of early newspapers are split across different packages. The split of the British Librarys newspaper holdings between two packages, one not generally available to university researchers, is unfortunate. By contrast, microfilm coverage of newspapers was more systematically planned. What are the effects of this fragmentation of digital coverage? What are We Dealing With? We have little information about how digitised collections such as the Burney Newspaper collection were formed. There are evident gaps in the digitisation coverage of particular collections and the implications of this are not clear. Above all, we are unclear as to how different collections relate to each other and what collections remain undigitised. As scholars increasingly use newspapers for innovative forms of quantitative analysis, a strong critical understanding of the nature of the newspaper archive will be essential. "
	},
	{
		"id": 317,
		"title": "Born-Digital Archives A Digital Forensic Perspective on the Historicity of Born-digital Primary Records",
		"authors": [
			"Ries, Thorsten"
		],
		"body": " The proposed paper will scope the complexity of born-digital archives from a digital forensic, historical and philological perspective. Personal digital archives, institutional repositories, web archives, email archives and social media archives createdigital primary records that the historical humanities struggle to fully recognize as documents in their own right. The historicity of the forensic materiality and structure of the born-digital record is a concept still to be methodologically and theoretically understood in the humanities and in archival science. Digital forensic features of primary records shape digital evidence by virtue of historically specific undocumented or unintended software and operating system behaviour, hard­- or soft­wa­re bugs, physical damage, system crashes, malware or deliberate manipulation that leave born-digital traces or historically specific lacunae in the primary record that call for a historical understanding of distributed digital forensic materiality. Selected examples from forensic investigations into Friedrich Kittlers, Michael Speiers, Marcel Beyers, Hanif Kureishis, Craig Taylorsand Glyn Moodyspersonal digital archives, born-digital records in the Mass Observation Archiveand digital art and web archives – which are all part of the ongoing research project based on Ries 2010, 2017, 2018 - will serve as examples of irreducible forensic complexity of born-digital archives that needs to be preserved in appropriate forensic formats, not only to ensure authenticity and chain of custody, but also to preserve historically specific computing artefacts and traces. These include recoverable drafts of writing projects, file and file system structure artefacts, error correction reports, recoverable temporary data, i/o-driver data and metdata, operating system traces, revealing fragmentation and lacunae that trace the history of digital events, based on historically specific forensic features and mechanisms. Born-digital forensic methods and tools themselves are subject to digital history, with their bugs and limitations that have to be methodologically and historically reflected. The purpose of this paper is to argue that forensic materiality and analysis is methodologically relevant for critical appraisal and understanding of production processes of born-digital sources in the humanities as a whole, including history, social history, political and culture studies. "
	},
	{
		"id": 318,
		"title": "Publishing Digital History: Integrating Methods, Sources, and Argument",
		"authors": [
			"Rivard, Courtney J."
		],
		"body": " In 2012, Tom Scheinfeld asked, Wheres the Beef? Does Digital Humanities Have to Answer Questions? In other words, what humanities arguments does digital humanities make?, he asked. Responses abound but one major issue is that the way DH makes arguments isnt always readily perceived by traditional humanities disciplines. In an effort to address this issue, the Center for History and New Media, a national leader in digital history in the United States, organized a workshop in 2017 with leading historians in digital history. The result was a white paper, Digital History and Argument. The paper noted that the responsibility for integrating digital history with argumentation thus rests both with the digital historians who make implicit or explicit historical arguments and with the rest of the profession who must learn to recognize them. In this poster, we directly respond to the important call made in this white paper by outlining our framework for a new digital book, Voice of a Nation: Mapping Documentary Expression in New America. The poster will make explicit the scholarly intervention of the project and then explain how the books arguments are being conveyed through digital forms, specifically organized around layers and thick mapping building off of the spatial turn in digital history. Digital humanities and digital history, we argue in our poster, allows us to make a unique scholarly argument only through a digital form. Intervention: This digital manuscript recovers the significant history of the Southern Life History Projectby applying computational methods to analyze the collection. The SLHP, part of the United States New Deals Federal Writers Project, was dedicated to capturing the stories of everyday Americans, especially those who had been previously marginalized from historical accounts, including African Americans, women, and the working class though a newly created genre they called life history. The SLHP together with other regional units of the FWP employed over 6,000 writers to produce nearly 10,000 interviews nationwide, constituting what one archivist called the largest body of first person narratives ever collected in this country. The life histories are written narratives in which writers gave their interpretation of interviewees lives, leading Jerrold Hirschto call them conversational narratives as they often evidence more about the writer than interviewee. Because this project was part of the Federal Writers Project during the New Deal, the life histories offer valuable insights into how notions of race, gender, class and national belonging were conceptualized during this pivotal time in American history. By pairing close and distant readings of this archive, our digital-based argument presents an entangled story about how a new form of documentary evidence called a life history helped to reshape notions of what it meant to be American during a time of political, social and economic unrest. Organization: Our digital book builds off of the spatial turn in digital history led by scholars such as Stanfords Richard White and over two decades of critical cartography. Scholars are turning to visualizations such as maps to convey scholarly knowledge and arguments. In the same way that audiences have learned how to read text to interpret an argument, audiences also have the tools to interpret visualizations. This project uses visualizations such as interactive mapping as a form of argumentation and then puts them in conversation with textual argumentation. Specifically, we employ a thick mapping approach as outlined by Todd Presner, David Shepard and Yoh Kawano, who define this approach as the process of collecting, aggregating and visualizing ever more layers of geographic or place-specific data. The resulting interactive thick maps serve as visual arguments that are complemented by expository text organized in what we call layers. Like a chapter, each layer provides an argument about the cultural and political work of the Federal Writers Project. Rather than restricted to a linear narrative, audiences can pick a layer to explore moving through as they see fit. The layered approach offers audiences flexibility to navigate the project through non-linear argumentation. Moreover, it differs from print narratives in incorporating the sources themselves as a central element, which can allow for more analysis and engagementas well as permit different relationships between argument and evidence CHNM. The project then expands the notion of thick mapping in the context of the spatial turn to rethink the organization of scholarship. The layers become a thick map that demonstrate how the Federal Writers Project and the resulting life histories were imbricated in the development of social documentary as a genre and the renegotiation of Southern identity during the New Deal era. "
	},
	{
		"id": 319,
		"title": "Performing Historical Place: Leveraging Theatre Historiography to Generate Presence in Virtual Reality Design for Restorative Justice",
		"authors": [
			"Roberts-Smith, Jennifer"
		],
		"body": " This long paper reports the development of 1) a new theoretical approach, with accompanying methodology, to the representation of place in historical digital humanities projects arising out of 2) a new digital resource that will be used in fall 2019 in secondary schools in Nova Scotia, Canada to teach restorative justice practices as a means of addressing the impacts of systemic historical racisms that are still active in participating school communities. There is a robust tradition of virtual reconstructions of historical places in the digital humanities, with a strong thread of applications in theatre and performance history. Digital theatre history projects, like cognate projects in other domains, have tended to approach the virtual reconstruction of historical place as a process of synthesizing and visualizing surviving documentary and archaeological evidence of material places as accurately as possible. Normally, we have better evidence of the venue in which a past performance took place than we do of the performance that took place there, so DH researchers working in this area have consistently been careful to make explicit the relative stability of the architectural models their projects have produced, in comparison to their much more hypothetical virtual reconstructions of performance. While this is a rigorous and productive approach to historical performance research in the digital humanities, its emphasis on documentary and material evidence is limiting in at least two ways: first, it implicitly privileges place over event, which can result in an exaggerated emphasis on the semiotic impact of the venue for a past performance or of the location for a past historical event; and second, it cannot accommodate some of the most important threads of longstanding theatre-historiographical theory, namely those arising out of emphases on discourse, memory, and repertoireas repositories of performance history. These limitations have been especially problematic in the Digital Oral Histories for Reconciliation project, which is charged with the responsibility of creating a virtual reality experience, based on oral histories, to teach 17-year-old high school students about the harms suffered by former residents of the Nova Scotia Home for Coloured Children. The DOHR VR experience, titled The Home , is the centrepiece of a two-week curricular unit that fulfils the educational mandate of the Restorative Inquiry currently underway in the province of Nova Scotia, Canada. The Inquiry, building on the work of the Victims of Institutional Childhood Exploitation Society, an association of former residents of the NSHCC, has collected the oral histories of more than three hundred former residents, which bring to light harmful events and experiences that were systematically excluded from the public documentary record. As a result, although the oral histories that we are rendering in the VR environment of the The Home are set in the historical NSHCC building, it is essential that our virtual reconstruction of the fabric of the building not be represented as more stable than accounts of the events that took place there: after decades of protest against a false and harmful documentary record, the voices of the projects first person storytellers must not be overshadowed by it. In response to this challenge, the DOHR projects virtual reality development teamhas drawn on discourse-, memory-, and repertoire-based theatre-historiographical theory to attempt to render the historical NSHCC as a place that is performed bythe speech actsof oral historians, rather than occupied by them. The concrete implications of this approach include some unusual engagements with the conventions of VR, especially with regard to character representation; configuration of the VR participants role; and affectivityover the widely-celebrated VR simulation-induced empathy ). In combination, these practices arguea definition of past place that supersedes reconstruction and construes it instead as a complex form of presence- a sense of being there that in our VR experience is generated by a participants growing ability to reflect on their own perspective in relation tothe perspectives of the storytellers whose histories they are witnessing. The goal of this sense of relationalityis to equip Nova Scotian young people with tools to help them identify - and begin to mobilize their communities to work together to address - the systemic causes of racism that are still actively causing harm in Nova Scotia and elsewhere in Canada. The Home will be tested systematically with youth during our in-school pilot study in fall 2019. Team-internal assessment has been an integral part of the iterative, participatory design processundertaken by our community-based research partnership, which includes former residents of the NSHCC, the NSHCC Restorative Inquiry, VOICES, Nova Scotian education administrators, teachers, and school resource workers, historians, and legal experts, as well as theatre artists and games studies scholars. This paper will be illustrated by screen capture video of excerpts from the Beta build of The Home ; the VR experience will also be available. "
	},
	{
		"id": 320,
		"title": "Digital Ecosystem For The French Archaeological Community",
		"authors": [
			"Rodier, Xavier",
			"Marlet, Olivier"
		],
		"body": " Created in 2012, the Mémoires des Archéologues et des Sites ArchéologiquesConsortium has been certified by the Very Large Research Infrastructure Huma-Num. MASA was born from the experience acquired by and within several Maisons des Sciences de lHomme in the field of processing the documentation produced by archaeologists. MASAs partners have pooled their skills to meet the needs of the archaeological community. The issues identified are multiple and involve several levels of complexity intertwined. The first level of complexity is related to the nature of the discipline itself: by digging, the archaeologist irreparably destroys his own object of study, even with a rigorous protocol for recording data during their production, the experiment is not reproducible. This places a particular responsibility on the archaeologist and gives his records the status of primary data. The return to this data is often necessary for comparative purposes and reinterpretation. The second issue is related to the nature of the documentation and its supports, which are very diversified and sometimes very fragileand whose digitization should facilitate consultation and storage. The excavation archives are made up of these various recordings and the artifacts collected, which represent a considerable mass of material elements. As the two are inseparable, information systems must ensure that the link between them is maintained. The third challenge stems from the habit of archaeologists to work with heterogeneous databases, often designed without a methodological and technical choice protocol. Ex-post work is therefore necessary to make these databases standardised and interoperable, to use common repositories that make it possible to consider linking these data on the web of data, to document them with new metadata where appropriate, to enrich them and to facilitate access while ensuring their sustainability. To meet these objectives, the MASA consortium proposes to the archaeological community a process of data manipulation from acquisition to publication according to a systemic approach that respects FAIR principles. The MASA ecosystem is composed of bricks for archiving and sharing archaeological data sets. Once processed, documented and standardized, the archaeological data sets are put online according to the standards in force. Standardised repositories are used for spatial, temporaland descriptiveinformation. The OpenArchaeo platform ensures their interoperability in a MASA triplestore and allows their interrogation via a simplified HMI that translates requests into SPARQL according to a generic model for mapping archaeological data with the CIDOC CRM ontology. The data are accessible from the publications that mobilize them. The LogicistWriter logicist writing tool offers the matching of inferences with the CIDOC CRMinf extension on reasoning. Each step of the process is documented by a good practice guide in the OpenGuide platform developed for this purpose. The whole process will be illustrated by the example of the excavation in Rigny, which follows all these recommendations from the field recording put online to the logicist publication of the results. With this digital ecosystem, the MASA consortium relies on the data culture of archaeologists and their long experience in computerization to bring the community to respect the FAIR principles and to open these corpus in the Linked Open Data. "
	},
	{
		"id": 321,
		"title": "Enlightenment Legacies: Sequence Alignment and Text-Reuse at Scale",
		"authors": [
			"Roe, Glenn H",
			"Gladstone, Clovis",
			"Olsen, Mark",
			"Morrissey, Robert"
		],
		"body": " The recent appearance of Steven Pinkers Enlightenment Now is a topical reminder of the enduring importance of 18th-century legacies in contemporary thought, culture, and politics. Considered pernicious or positive, French intellectuals began assessing the legacy of les Lumières almost from the outset of the events of 1789 and continued this polemic throughout the 19th century. While the study of pro- and anti-Enlightenment or Revolution writers active during the 19th century is certainly of great value, our work in this project aims to examine the complexities of Enlightenment legacies using new distant reading approaches. To this end, and in conjunction with the Observatoire de la Vie Littéraireat Sorbonne Université, we have developed a new generation of sequence alignment software that detects reused passages in very large corpora; we use this software to compare several important collections of 18th-century texts to the Très Grande Bibliothèquecorpus of 19th-century printed materials made available by the Bibliothèque Nationale de France. Following an overview of the datasets and software developed for this effort, we will sketch some preliminary results arising from this project and conclude with an outline of further work we will carry out based on this database. We used three different text collections for this project. The 18th-century sample is drawn from the holdings of the ARTFL Project and includes the Encyclopédie of Diderot and dAlembert as well as 1,367 17th-18th century texts from the ARTFL Frantext database. See http://encyclopedie.uchicago.edu/ and http://artfl-project.uchicago.edu/content/artfl-frantext/. Both are well-curated collections and provide solid samples of Enlightenment discourse. For the 19th century, we were able to employ selections of the TGB collection released by the BNF in conjunction with OBVIL. The collection consists of 128,441 documents by more than 58,000 authors almost all of which have metadata drawn from the BNF catalogue. The vast majority of the collection was published during the 19th century, though this includes a significant number of reprints of older texts. The collection contains a broad selection of themes and subjects with 35,710 documents listed as Littérature, 28,885 as Histoire de la France and 23,776 Droit. As expected, the quality of the raw data – based on uncorrected Optical Character Recognition – varies widely depending on a range of factors, including age, preservation status and print quality. In order to identify those texts that were originally published before 1800, we used a series of heuristics based on the metadata provided by the BNF to eliminate duplicates and near-duplicate texts. This left 112,907 documents in our working TGB sample. While the ARTFL Project has developed text alignment packages in the past, this system is less-suited for very large-scale comparisons, e.g., those in 100,000+ document range. Detecting identical or similar passages requires a one-to-one document comparison of every text in the dataset. Given the scale of the TGB dataset, we developed the TextPAIR system to address limitations of the previous model using new technologies. Installed as a Python package, it includes a text preprocessing component written in Python, a sequence aligner written in Go to maximize speed and scalability, and a single-page web application written with the VueJS framework to guarantee maximum interactivity when text alignments are deployed in the browser. The package is available as open-source code on Github, with accompanying documentation meant to assist other research groups in installing and running their own text-reuse experiments. See https://github.com/ARTFL-Project/text-pair. The sequence alignment of the pre 19th-century sample of Frantext and the Encyclopédie against the 112,000 documents of the TGB produced a large number of resulting passage pairs, our basic unit of analysis. Figure One shows a typical alignment pair, in this case a passage from the famous Discours Préliminare reused with some indication of the source in Peignots 1801 Dictionnaire raisonné de bibliologie. It is important to note that TextPAIR can detect similar passages with considerable variations which can arise from textual insertions, deletions or modifications along with data capture errors, differences in spellings and word order changes. Figure 1 uses the show differences feature to highlight the variations between the passage pair. Figure 1 Each record of the result database stores metadata for each document of the pair from the TEI headers, byte locations and offsets in the corresponding text data files, the passages in question, the size of the alignments, and whether or not the alignment is considered banal or uninteresting. The databases are loaded into a PostgreSQL relational database with a dedicated interface to allow users to query the document pairs, get summary results and navigate to the original documents at will. Figure 2 shows the query form of the Encyclopédie to TGB alignment database, which supports metadata queries to allow the user to focus on specific questions, in this case a search for all aligned passages from articles written by Rousseau. Figure 2. Searching for similar passages from articles by Rousseau in the Encyclopédie The query returns 611 passages, as shown in Figure 3, where the first reused passage in this query is his article Accolade, which is found almost verbatim in a dictionary of music from 1825. The query interface makes extensive use of facets, allowing the user to consult frequencies broken down by different criteria. Looking at the reuses of Rousseaus contributions to the Encyclopédie, it is interesting to note that while most of Rousseaus entries in the Encyclopédie were about music, it is his political philosophy article ECONOMIE that is most reused in the 19th century. Figure 3 The interface also supports the generation of time series graphs of the results. Figure 4 shows that reuses of the article ECONOMIE was fairly consistent through the 19th century. Figure 4 The Baron dHolbach presents another interesting case. As one of the philosophes with the most notorious reputation as a free-thinking materialist he contributed some of the most controversial articles to the Encyclopédie, such as Représentants and Prêtres. As shown in Figure 5, it was however his work on chemistry, mineralogy, and German history that is most reused in the 19th century. Instead of his scandalous article on Prêtres being cited, as one would expect, you find resonances of the rather orthodox article EVÊQUE which outlines the historical background of elector Bishops under the Holy Roman Empire. In fact, not one reuse of dHolbachs controversial material was found in the TGB, which sheds new light on our vision of dHolbach as not simply an atheist propagandist, but as a man of science whose articles in various domains continued to be cited and used well into the 19th century. This is an image of dHolbach that rarely, if ever, occurs in modern intellectual and literary histories. Figure 5 The 19th-century reuses of passages from the 17 texts by Rousseau found in ARTFL Frantext, show a similar combination of expected and unexpected avenues of influence. It is not particularly surprising to find the nearly 1,500 instances of passages from his Contrat social in works dealing with political theory even if they are used in a negative fashion. As shown in Figure 6, the most frequent reuse is in Pierre Landes attack on the philosophe in his Principes du droit politique, mis en opposition avec ceux de J.-J. Rousseau sur le contrat social followed by numerous expositions of Rousseaus political thought. Figure 6 By contrast, as shown in Figure 7, the over 10,000 reuses of Rousseaus work more generally seem to focus on his reputation as a prose stylist, with the most frequent reuses found in various dictionaries and grammatical works. It is important to note the various vectors through which particular texts or authors can exert influence, even if it is indirect. Figure 7 We believe that we can begin to use these techniques and these sorts of large-scale databases to refashion literary history, to give a more expansive vision of literary culture by identifying various forms of intertextual activity, from reuse to referencing, in a broadened set of 18th-century corpora and to eventually make use of various visualisation tools to navigate the output. While our interpretive work on this set of reuses is still in its initial phases, we have already been able to identify significant findings that challenge our understanding of the impact of the Lumières in the 19th century. Our full paper will expand on our observations above and begin the systematic exposition of the various complexities of identifying text reuse at such an unprecedented scale. We are aware, however, that these larger questions are well beyond the ability of any small group of researchers to explore, and thus invite interested parties to consult the alignment databases themselves. See http://artfl-project.uchicago.edu/legacy_eighteenth. "
	},
	{
		"id": 322,
		"title": "Designing Multilingual Digital Pedagogy Initiatives: The Programming Historian for English, Spanish, and French speaking DH Communities",
		"authors": [
			"Rojas Castro, Antonio",
			"Sichani, Anna-Maria",
			"Papastamkou, Sofia"
		],
		"body": " The Programming Historian is an initiative launched in Canada and the United States in 2012 that aims to publish peer-reviewed tutorials for digital humanists. The project stands nowadays as an exceptional open multilingual educational resourcethanks to its open infrastructure and international profile, bringing together practitioners from Canada and the United States to Chile, Colombia, Spain, France, Mexico and the United Kingdom. With this poster we would like to discuss the design of a multilingual strategy for The Programming Historian , to reflect on some theoretical concepts — contact zone, lingua franca vs. vernacular, and writing for a global audience — that are guiding our current transition from a monolingual project to a multilingual one available in English, Spanish and French. On the one hand, after translating 41 lessons from English, in April 2019, The Programming Historian en español has recently published two original lessons while two more are currently under review. On the other hand, also in April 2019, the Programming Historian en français was launched and has already published one translation from English into French. The current situation of the Programming Historian is the result of a commitment to openness and diversity. In 2016 the editorial board identified barriers of genderand it took steps to promote a more inclusive community in terms of gender and LGTBQ representation; but the field of DH is not diverse enough in geographic and linguistic termsand the Programming Historian wasnt an exception in this regard. Although all lessons were written in English and could potentially reach a global audience, resources, datasets and references clearly expressed research questions led in Anglo-American institutions and authors often assumed an audience that share the same world view. In other words, cultural and language barriers remained a problem. In 2017, three new editors from Colombia, Mexico and Spain joined the editorial team to begin an edition in Spanish and, one year later, in 2018, three French-speaking members joined the Editorial Board to work on a francophone edition. These two new editions confirm the status of the Programming Historian as a transnational project. Covering a broad linguistic area, the Spanish and the French-speaking editions aim to provide both original lessons and translations addressed to their specific linguistic community. Thus, the translation process can be seen as a localization activity where adaptation and domestication of Anglophone language, resources and cultural references may be needed to meet our Hispanic and Francophone audiences expectations. For this reason, The Programming Historian can be seen nowadays as a contact zone, that is, social spaces where cultures meet, clash and grapple with each other, often in contexts of highly asymmetrical relations of power, but also where people — editors, translators, reviewers, users — can exchange successfully ideas around computing methods and digital tools. "
	},
	{
		"id": 323,
		"title": "Open Islamicate Texts Initiative: a Machine-Readable Corpus of Texts Produced the Premodern Islamicate World",
		"authors": [
			"Romanov, Maxim",
			"Seydi, Masoumeh",
			"Savant, Sarah Bowen",
			"Miller, Matthew Thomas"
		],
		"body": " The written heritage of the Islamicate Introduced by the University of Chicago historian Marshall Hodgson, the term Islamicate refers to anything Islamic and non-Islamic, religious and non-religious that was produced in the vast geographical region we refer to as the Islamic world. cultures that stretch from modern Bengal to Spain is as vast as it is understudied and underrepresented in the digital humanities. The sheer volume and diversity of the surviving works produced in Arabic and Persian in the premodern period makes this body of texts ideal for computational analysis. While a great number of texts has been digitized over past two decades, OpenITI is the first corpus of Islamicate texts that is open, machine readable, and aims at being comprehensive. OpenITI strives to provide the essential textual infrastructure in Persian and Arabic for new forms of macro textual analysis and digital scholarship. The corpus is already actively used in several ERC projects. Currently, OpenITI includes about 4,300 unique book titlesby over 1,800 authors, which amounts to approximately 750 million words. Most texts have been collected from open-access online collections of high-quality digital reproductions of premodern texts. Such as http://shamela.ws/, http://shiaonlinelibrary.com/, https://ganjoor.net/, etc.. Figure 1. Chronological distribution of authors and books in OpenITI. OpenITI is organized in compliance with Canonical Text Servicesguidelines as implemented in the CapiTainS Suite, with two exceptions made for practical purposes. First, we implemented human-readable URNsas this allows for easier subsetting of the corpus and makes it easier to engage non-DH specialists in Islamicate studies in collaboration. Second, we are postponing conversion to TEI XML, since it poses a number of challenges for right-to-left languages, connected scripts, and extensive texts. Figure 2. CTS URN Structure: al-Ḏahabīs Taʾrīḫ al-islām. Figure 3. CapiTainS -Compliant Folder Structure: Versions of al-Ġazālīs Iḥyāʾ ʿulūm al-dīn incorporated into the repository 0525AH within OpenITI. CTS offers a powerful mechanism for building expandable and interoperable corpora. The power of CTS lies in the URN, which provides the permanent canonical references to texts and are used by CTS to identify or retrieve passages of text. The tree-like structure of CTS URNs together with modular folder organizationallow one to easily expand the corpus in a decentralized manner as well as to accommodate as many texts and their versions in any number of languages as might be necessary. Texts have been automatically converted into our custom format— OpenITI mARkdown. Facilitating conversion of raw texts into machine-actionable formats, this flavor of markdown: 1) simplifies work with multivolume texts that make up the core of the corpus; and 2) helps to avoid problems that one faces when paired symbols, LTR and RTL languages, and connected scripts occur in the same document, when even a simple editing task becomes overly complicated. Additionally, mARkdown will facilitate conversion into TEI XML, the de facto standard for publishing digital editions. The detailed description of OpenITI mARkdown can be found at https://maximromanov.github.io/mARkdown/. Figure 3. OpenITI mARkdown: A sample texttagged morphologically and semantically. Available on GitHub, OpenITI includes four different types of repositories: Raw texts repositories, where texts collected from online libraries are stored in their original formats. The total count exceeds 50,000 files. Texts here must be manually reviewed and moved into main working repositories. Main working repositories, where texts in mARkdown are grouped into 25 year periods, based on authors death dates. This makes repositories more manageable, allowing scholars to focus on relevant texts. Manual editing, reviewing, and tagging of texts happens here. Instantiationsinclude all texts of the corpus adapted for use with specific software. For example, in ` i.stylo`, texts are renamed and reformatted as required by the ` stylo` package for R. Releaseswill have citable time-stamped versions of the corpus. In order to achieve comprehensiveness, we currently are working on the improvement of Arabic-script OCRand the development of a digital text production pipeline. Called CorpusBuilder, this user-friendly, web-based, open-source application will significantly lower the technological barriers to entry and help us involve colleagues, librarians and citizen scientists from around the world in our collaborative project of corpus creation. "
	},
	{
		"id": 324,
		"title": "Orbis-in-a-Box (OIB): Modeling Historical Geographical Networks",
		"authors": [
			"Romanov, Maxim",
			"Seydi, Masoumeh",
			"Baillie, James",
			"Grossner, Karl",
			"Simon, Rainer",
			"Vargha, María"
		],
		"body": " In 2012, researchers at Stanforddeveloped ORBISwhich offered a complex model of connectivity by reconstructing the duration and financial cost of travel in antiquity. Revealing the true shape of the Roman world, ORBIS provided a unique perspective on premodern history and became an object of envy for scholars working in other historical contexts. Since ORBIS was not designed to be easily adaptable to other contexts, a DH-team at the University of Vienna organized a hackathon, where participants worked on a tool which historians with minimal DH skills could easily install and run, and, by supplying their own data, could explore their own historical networks in ways similar to ORBIS. We used the al-Ṯurayyā Project, https://althurayya.github.io/, as the sandbox, since it 1) re-uses a significant amount of code written in D3 for ORBIS 2.0; and 2) is modular enough to facilitate experimental development. Whereas the al-Ṯurayyā Project used a modified version of the Dijkstra pathfinding algorithm, we chose to reduce the algorithmic complexity for OIB to the necessary minimum, as not all potential modifications can be foreseen; historians will have full control over their networks through the modification of node/edge properties. With this approach, our application generates a network from supplied data, then continuously reconfigures it for specific queries by applying modifiers to edge weightsand switching on/off specific nodes/edges; the visualization is then generated from the latest state of the network. Figure 1. Modeling routes from Baghdadto Damascus. Figure 1 provides an example. [ Left] al-Ṯurayyā shows two routes: RED-L is the shortest route generated with the vanilla Dijkstra algorithm; GREEN-L is the optimal route generated with a modified Dijkstra algorithm, searching for the next shortest route with a higher number of settlements along the way. [ Right] OIB Sandbox shows two routes generated with the vanilla Dijkstra algorithm, but from differently configured networks: the RED-R uses the initial network; the GREEN-R uses a reconfigured network: here, settlement type is applied as a modifier, making route sections that lead to larger settlements shorter and therefore preferable for the Dijkstra algorithm. While GREEN-Loffers a better alternative to RED-L—the suicidal option through the Syrian desert— GREEN-Rnot only avoids the desert, but also runs through all major cities in the region, a route usually found in medieval Arabic chronicles. OIB is being developed as a modular application, whose functionality can be extended without any disruption. The OIB requires users to supply CSV data files for edges and nodes, and to modify a YAML settings file. For example, the EDGES file should look as shown in Figure 2, where RouteID, Start, End, Length, and Coordinates are required fields. In the network, Length is used by the Dijkstra algorithm to find the shortest route; additional fields—here Terrain and Safety—provide modifiers to Length values for network reconfiguration. Figure 2. EDGES file example. Additional fields are coded categorically and converted into numerical values via a config file, through which one adjusts model parameters. Numeric values are used as multipliers for Length values. For example, the weight of the route section ID3becomes 63,580, if both Terrain and Safety modifiers are applied; that of the route section ID2becomes 25,925, which makes ID2 shorter than ID3, and therefore preferable within the vanilla Dijkstra algorithm. In a similar manner, nodes and edges of specific types can be excluded from the network. Figure 3. YAML Settings File Although this approach puts lots of weight on historians to produce appropriate data, it gives them the utmost freedom in modeling their research questions as well as makes OIB suitable for most use cases without any additional modifications of the tool itself. Much of the OIB Sandbox already works, yet an interface for dynamic network modification needs to be further developed, which we hope will happen in the near future. "
	},
	{
		"id": 325,
		"title": "Entangled Histories of Early Modern Ordinances. Segmentation of Text and Machine-Learned Metadating.",
		"authors": [
			"Romein, Christel Annemieke",
			"Veldhoen, Sara Floor"
		],
		"body": " Libraries and archives throughout Europe host books with ordinances, or individual ordinancesfrom the 15th to the 18th century. These texts contain indications of how governments of burgeoning states dealt with unexpected threats to safety, security, and order through home-invented measures, borrowed rules, or adjustments of what was established elsewhere. The hundreds of texts within these books are frequently consulted by researchers of various disciplinesto unravel rules for controlling complex societies. Having the possibility of a longitudinal search, based upon contents rather than the index or title, as well as having an overview based upon several states has so far been impossible due to the impenetrable amount of scanned texts. This project will disclose the entangled histories of neighbouring states, due to synchronic and diachronic comparisons – allowing a wider search and implementation in other projects Europe-wide. This project-in-process will focus on the Dutch Early Modern Ordinances, but the techniques will have a widespread positive effect. This project consists of three steps: 1. Improving the OCR-techniques of digitised sources. We plan to use the Handwritten Text Recognition suite Transkribus to reprocess the files which currently have poor quality OCR. By treating the printed works – which were printed with hand-carved letters – as very consistent handwriting we hope to obtain a higher quality of recognition. 2. Segmentation of the texts, going from sentence-recognitionto article-segmentation of larger texts; this requires that the computer is trained to recognise the beginning and end of texts, either as a chapter or as an individual text within a compilation of texts. Initially, this can be based upon image/layout but could evolve into integration with HTR, depending on its feasibility. 3. We want to create machine-generated metadata by training a computer to recognise the conditions that categories were based upon. This will be based on an already developed genre classifierthat can be fitted to automated content analysis. After supervised training, the computer can then suggest, apply and supplement categories to other texts based on the idea of topic modelling. This is a pilot that will prove the applicability of the tool to other languages as well. Due to its significance for such a broadly studied range of sources, we hope to make the output available as RDF. We will use NLP such as NER technologies to identify dates, titles, persons; and implement the output in the Dutch national infrastructure. With the data generated in this project, visualisations of the development of laws across the Netherlands – and possibly Europe - would become possible. Outcomes of this project: Improved performance of the current OCR-recognition oftexts by incorporating HTR-techniques on printed texts.The possibility to automatically recognise segments in text layout: beginning or end, columns, titles, dates, summaries and the body of the text. This data will be used for the RDF-compliant tool. Expansion of automatic content analysis based upon segments, rather than on lines or sentences, with a machine-learned algorithm, and applying this with standardised machine-learned metadata to early modern normative texts. This will allow researchers to search through approximately 15.000 ordinances and resolutions from various provinces in the Low Countries, accessible through uniform search terms. An RDF-compliant tool to enable application in other ongoing and future projects. Integration of the enhanced datasets in the CLARIAH/CLARIN ecosystem. "
	},
	{
		"id": 326,
		"title": "Interactive Reading of the Silver Age: teaching and research promoted by the National Library of Spain",
		"authors": [
			"Reina-Navarro, Alicia",
			"Romero-López, Dolores"
		],
		"body": " PROJECT ELITE-LOEP The National Library of Spainand the research group on Spanish Silver Age Literatureat the Complutense University of Madrid present La Edad de Plata interactiva, a collaborative project to enrich the Librarys digital collections and their use in teaching and research, exploring the resources that new digital technologies offer to the edition of texts in the field of cultural dissemination. The results of this research are inserted within the framework of eLITE-CMs Project. THEORETICAL FRAMEWORK This Project started with the question of whether todays philology, supported by new digital technologies, could propose rereadings of certain authors and works that, for reasons which are not often strictly literary, have fallen into oblivion or are not currently occupying the space that would correspond to them within literary historiography if the quality of their writings is taking into account. After the official canon of the Silver Age there is, indeed, another dimension of Spanish literature that must be discovered or rediscovered. In the context of this debate, lines of research have emerged precisely around the concept of rereading to propose innovative interpretations of the literary past. Among them, we want to highlight those referring to the critical review wich has been developed by Digital Humanities. The application of new technologies to humanistic analysis has undoubtedly revolutionized the way in which we read, interpret and interact with literary works. Over the last years, the concept of rereading has served to designate a process of critical textual revision that could eventually influence the formation of a new extended canon. Nevertheless, nowadays it serves to provide todays readers and researchers a brand new form of access to authors and texts which were no longer read and now have been recovered. Complutenses research group La Otra Edad de Platahas witnessed all these methodological changes that are taking place in the humanities sphere as a result of the technological revolution. Furthermore, the aforementioned group intends to participate in the digital rereading of the Silver Age through the project eLITE-CM, with the aim of constructing textualities that can satisfy the digital natives. In addition, it allows to recover the voices of forgotten authors of this literary period. With this project we want to demonstrate our commitment to Research, Development and Innovation, including the Information and Communication Technologies for Development as one of the most significant challenges of our working group. OBJECTIVE In response to these new reading habits that the Digital Revolution has brought to the current panorama, the LOEP group of research has had as objective within the project eLITE-CM the development of three collections of forgotten texts of the Silver Age, rescued through digitization and enriched by computer programs, with the intention of exploring new resources that hypertext offers to philologycal studies in the Digital Age. To achieve our goals, it has been taken into account that dematerialization of cultural heritage linked to digitalization leads, necessarily, to new representations of cultural objects, which now overcome their physical limitations to become universally accessible through the Internet. This transformation, within the scope of text editing, implies a reconsideration of the book as a knowledge disseminatorthat now assumes characteristics such as transmediality or interactivity, which should be reinterpreted from an hermeneutic and phenomenological point of view, as part of a new perspective on the Reception theory linked to the field of Digital Humanities. Indeed, in terms of cultural dissemination, the enrichment of texts with images, sounds, concept maps, geolocators, hyperlinks, thematic transversality with other arts, etc. modifies the traditional concept of personal reading, which now becomes interactive. If Bajtinconsidered that the literary work already constitutes a polyphony of voices, now the text enters into dialogue with the voices of the world that speak to us from the network. With this regard, it has also been an objective of the project to expose and evaluate the research results of our interactive collections. Therefore, this new dialogue established between the readers and the literary work has been analyzed as well. For this, data has been collected, not only from the users of the National Library of Spains website, but also from a wide number of students that have used our editions within a context of meaningful teaching in the classroom. As a result of this practical experience, some theoretical reflections will be offered. The main one is that the benefits that digital reading brings to the user on a cognitive level follow the five principles of what can be called a smart reading: Simplicity, Motivation, Accessibility, Recycling and Transference to the global community. In this sense the smart reading entails an innovative way of learning within a close-knit community that reaches high levels of thinking and emocional engagement to access knowledge, according to Edward D. Hess and Katherine Ludwig. OUR COLLECTIONS Taking this into consideration,  La Edad de Plata interactiva has carried out three collections of interactive books from the Silver Age linked to different topics, which will allow readers to access knowledge about this period through points of view not so widely considered by canonical historiography: 1) The Modern Woman in the Silver Age Literature; 2) Childrens Literature in the Silver Age; 3) Madrid in the Silver Age Literature. The  Modern Woman in the Silver Age collection offers interactive editions of short stories, novels and essays written by modern Spanish women authors that, in most cases, were no longer readand has been rescued and critically annotated for a general public thanks to technological resources. This collection is complemented with two monographs related to the topic of modern women, which allows users to access relevant information for the interpretation of the literary works and the historical period itself, carried out by specialists. The  Childrens Literature collection presents two stories that has been updated for todays readers –most of them digital natives– in an interactive way. Both tales were originally published in Spain anonymously in 1923 by the publisher Calleja: Plague of dragons and Spoiled Summer. As a result of this research, our project discovered that the real author of these stories was the writer Edith Nesbit, one of the best English-language authors considered a background of current fantasy literature. In this collection we have implemented an audiobook system so that users can listen to the stories in a dramatized way. Readers will also be able to consult two magazines of additional information in which the historical context of the works have been studied and the evolution of dragons myth in the fantastic literature has been traced. In  Madrid in the Silver Age Literature collection, we have selected several works from this period in which the city of Madrid presents itself as a space of modernity. For this purpose we propound a new approach to the works of the novelist and journalist Andrés Carranque de Ríos, who represented through his writings the complexities of a world in transformation towards modernity. This collection is complemented with a geolocator through which readers can access to several maps where the itineraries of the characters that appear in the texts have been traced, so that the different corners of Madrid during this historical period can be explored in a virtual way. INSTITUTIONS AND FINANCING This work has been funded by the Biblioreca Nacional de España and by the project itself, within the Program of Research and Development Activities among research groups of the Comunidad de Madrid in Social Sciences and Humanities, co-financed at 50% with the European Social Fund. The result of the research has resulted in an interactive kiosk of the Silver Age of Spanish Literature hosted on the website of the Biblioteca Nacional de España. LINKS OF INTEREST BNE interactive collections http://www.bne.es/es/Colecciones/LibrosInteractivos/index.html Silver Age Literature kiosk: http://cloud.madgazine.com/46f185c3185976675/?quiosco=46f185c3185976675&t=1542576418 "
	},
	{
		"id": 327,
		"title": "DSE Visualisation with EVT: Simplicity is Complex",
		"authors": [
			"Rosselli Del Turco, Roberto",
			"Martignano, Chiara",
			"Di Pietro, Chiara",
			"Cacioli, Giulia",
			"Del Grosso, Angelo Mario",
			"Zenzaro, Simone"
		],
		"body": " Introduction Edition Visualization Technologyis an open source tool to produce digital scholarly editions on the basis of TEI XML-encoded documents. Born to serve the goals of a single project, the Digital Vercelli Book, it has been developed in such a way as to become a general purpose tool. Several DSE projects are using it to publish digital editions, in fact many researchers have found in EVT the perfect tool for their needs: it is easy to configure and deploy, it is fully customizable, it includes several useful research tools out of the box. The current development versionis based on the AngularJS programming framework, which it is especially suitable for the development of complex client-side Web applications since it supports the Model View Controller design pattern, a very popular architectural pattern that separates the data of the application domain, its visualisation, and the user interaction with the data and the view. EVT 1 At the present moment EVT is available as a usable beta version, and a fully functional release is due in 2019. The forthcoming release will include all the features present in the original version, i.e. support for diplomatic editions, text-image linking, a sophisticated image viewer, a powerful text engine, and more. At the same time, the support for critical editions - which was the starting point for EVT 2- will be further improved thanks to the feedback received from many interested scholars. EVT 2 first beta version Current and future developments In 2005, describing the DSE as a dynamic device, P. Robinson implicitly alluded at the complexity of building such a software tool. In fact, at the end of the first development cycle we discovered that, in spite of our search for flexibility, the base frameworkwouldnt allow further extension. The current development version, on the other hand, while much more flexible from a technical point of view, presents challenges on multiple levels: Design. EVT must be able to combine standardized representation and publication with the peculiarities of each edition: this is a very complex task considering the great variability of TEI XML, in fact the same encoding may have different interpretations and the same interpretation may have different encodings. Significant variation of the data to be processed requires great flexibility by the processing software. Implementation. The original AngularJS framework has been replaced by a vastly improved new version, simply known as Angular, which is incompatible with the former. Upgrading to Angular would solve some efficiency issues present in the current AngularJS-based version, and make available all the recent improvements. Furthermore, EVT has always been based on a client-only architecture, which is great to keep technical complexity at a minimum, making it very easy to publish and maintain a web-based digital edition. This is also a limiting factor, however, when it comes to advanced features such as user annotation of text and images, search engine, and heavy-load textual processing. Adding client-server functionality would solve this dilemma, but this is a complex task which requires appropriate planning, support and funding in order to be carried out. User Interface. This is probably the area that needs most attention. Handling of heterogeneous materials. Text and images are directly visible in the UI, while other materials, such as introductory text, lists of witnesses, critical apparatus entries, named entities, links between text and image are displayed on request. Their presence, however, must be reported in the UI and must be easily accessible. The text can be consulted in the traditional way, i.e. sequentially, or different parts of the text and different materials can be consulted in parallel. Each possible combination has its own display mode, designed specifically to enrich that type of interaction with the text. Designing graphic solutions suitable for the digital context but in line with the conventions of printed editions. The simultaneous presence of text in both diplomatic and critical editions requires both careful planning of navigation and special care in identifying those graphic solutions that allow us to represent these conventions without overburdening the UI. Complexity in finding the right compromise between web design principles and traditional layout. The UI should be innovative, but not so far from the conventions of traditional editions as to require a learning effort that is too high. Conclusion In this poster we will present and discuss the complexity of developing a sophisticated tool such as EVT, explaining the design principles and the methodological aspects at its base. "
	},
	{
		"id": 328,
		"title": "Conceptual Vocabularies and Changing Meanings of “Foreign” in Dutch Foreign News (1815-1914)",
		"authors": [
			"Ros, Ruben"
		],
		"body": " The nineteenth century saw a transformation of the world in the form of increasing communication, technological advancements and international political action. One of the prime vehicles through which nineteenth century publics registered these changes was foreign news. Newspaper articles not only described, but also defined what was considered global, international and foreign. This poster shows how the concept of the foreigne) changed over the course of the long nineteenth century. It does so by a computational analysis of foreign news in Dutch newspapers between 1815 and 1914. From all newspapers present in Delpherthis research uses a sample of twenty one newspapers, selected based on size, place of publication, religious-political affiliation and timespan. The sample includes prominent newspapers such as Algemeen Handelsblad and the Nieuwe Rotterdamsche Courant. The newspaper articles were downloaded through the Library API and subsequently tokenized and ngramized. Newspapers are a suitable source for long-term conceptual history. However, as this research shows, the character of newspapers changes significantly over the course of the nineteenth century. Therefore, I pay particular attention to the changes in the nineteenth century newspaper landscape such as commercialization, politicization and technological advances in printing and reporting. With regard to foreign news, it is especially important to keep track of changes in newspaper layout, as this has a methodological as well as an historical effect. This research aims to unite the flourishing body of literature on spatial history and the recent interest in spatial concepts. It does so by following three keywordse) and their linguistic context through time. This is done by means of bigram frequency measures such as productivity and creativity. Also, I cluster the bigrams into semantically similar clusters using the popular word2vec method. In light of the limited number of newspapers in the period 1815-1830, the models are trained on ten-year periods of time. I also aligned the models diachronically. Besides the study of these words and their context I also shed a light on the broader semantic fields in which the concept of the foreign resided. Concepts such as binnenland, natieand internationaalwere highly important in shaping the concept of buitenland, and shed a light on the socio-political context in which the concept operated. Conceptual interrelations are analyzed by measuring the overlap between bigram vocabularies, as well as by using the diachronically aligned vector space models. By interpreting concepts as lexical structures, cosine distances between words can be used to construct networks. Clusters in these networks, as well as the overall connectedness of the networks can then be used to measure conceptual relations. The third and last step in the analysis is the study of word associations. Over the course of the nineteenth century, the singularized buitenland was not only conceptually constructed, but also invested with particular meanings and associations. Based on historical and contemporary reflections on foreignness, I focus on four associations: scale, distance, stability, and temporality. I analyze how the concept of buitenland became associated with largeness, closeness/farness, instability and the future. Given that they are expressed as words or conditions of words I use diachronically aligned vector space models to detect words semantically related to the associations. The resulting vocabularies are then connected to n-gram frequency measures and collocations to diachronically analyze the form and quantity of these associations in connection to the concept of buitenland. This research shows how during the nineteenth century the concept of buitenland singularized. Foreign space was no longer spoken of in the plural, but in the singular. In the late nineteenth century mental geographies, buitenland maintained a crucial place. Moreover, in the process of singularization several semantic properties became attached to the concept. The foreign was increasingly associated with largeness, instability and an uncertain future. The conceptual history of buitenland hereby contributes to our understanding of todays idea of globalization, an idea that is also permeated by associations of uncertainty and unstoppable change. Foreign news is thus not only a silent witness, but also an historically shaping factor of these developments. "
	},
	{
		"id": 329,
		"title": "Disentangling a Trinity: A Digital Approach to Modernity, Civilization and Europe in Dutch Newspapers (1840-1990)",
		"authors": [
			"Ros, Ruben",
			"van Eijnatten, Joris"
		],
		"body": " Introduction This paper fleshes out the relations between the conceptual trinity of modernity, civilization and Europeusing digital history techniques. The idea of Europe as it emerged during the early modern period and developed over the nineteenth and twentieth centuries is often said to coincide with both civilization and modernity. In the latest contribution to this topic, Murray-Miller argues that the intertwining of these concepts is so extensive that, historically, one has typically served as a metonym for the other. In this research we elaborate on this conceptual entanglement and evaluate the semantic boundaries that are said to define the MCE trinity. Based on a computational analysis of four Dutch newspapers spanning the period 1840-1990 we conclude that, in contrast to what the literature claims, semantic relations among the MCE elements are hardly visible and when they do, they are far from a trinity. Histories of concepts such as civilization and modernity are often based empirically on a selection of semantically dense works written by a limited number of intellectuals. While we do not claim that newspapers fully represent the historical Zeitgeist we do argue that they reflect a broader swath of periodically iterated public opinion, and thus help us understand the development of concepts in a broader segment of society. This research shows how the streetlight effect in intellectual history can lead to what in effect is a eurocentric fallacy in the study of modernity and civilization. By investigating conceptual interrelations we add to the emergent field of digital conceptual history. Earlier historians of concepts already recognized the value of tracing patterns in word use to understand conceptual change. Digital datasets and computational methods now enable more advanced enquiries into frequencies and distributions that reveal important aspects of conceptual change. Corpora We restrict ourselves to four Dutch-language newspapers issued between 1800 and 1990: Algemeen Handelsblad, Leeuwarder Courant, Nieuwe Rotterdamsche Courant, and De Telegraaf. These newspapers are fairly representative of the Dutch newspaper landscape and cover a large part of the nineteenth and twentieth centuries. In this period, the form and content of the newspapers in questions changed significantly: the size and regularity increased, political affiliations became more explicit and commercialization contributed to a focus on local, regional and national matters. Although we find these changes not to affect our concepts, we are aware that our selection consists of newspapers distributed nationally. Comparisons with regional newspapers, or newspapers more strongly attached to political ideologies would be fruitful for further research. Methodology Considering the variable and imprecise meaning and usage of Europe, we approach the trinity from the perspective of modernity and civilization. We first aggregate bigrams that contain modern * , beschaafd *and beschaving *and subsequently analyze bigram-frequency, -productivityand -creativityto identify change and stability in the word usage. We complement these methods with PMI-collocations. Subsequently, we look into our concepts meanings and interrelationships by employing word embeddings. Since the introduction of this method, much work has been undertaken to employ word embeddings in the study of diachronic word evolution). We analyze the concepts in five periods, a periodization based on the trends observed in the bigram frequencies and the availability of newspaper data. Because the corpus size increases considerably over time we use random sampling to obtain equally sized input data for our vector space models. After aligning the models, we extract the most similar terms that appear in all the models in a given period and rank them based on the average cosine distance. Entanglement is investigated using bigrams that can be seen as combinations of two concepts, collocations and a network-based approach to word embeddings. We extract the thirty words most similar those thirty words most similar words to the adjectives modern, civilized and european across five periods. Using the open-source visualization software Gephi we visualized the results, using the resulting 13,500 words as nodes in a network, and the relations between them as edges. The entanglement thus becomes visible through the degree of connectedness of the nodes, as well as the overall network density. Findings We show how during the nineteenth century the concept of modernity experienced an interpretative shift from modernity as the present to modernity as a stage in history. Bigrams show that the adjective modern was also increasingly combined with abstract phenomena such as state, time and freedom. Similarly, the concept of civilization shifted from being associated with enlightenment and prosperity to a geographically locatable counterpart of barbarism. Elements of purity and superiority followed this localization of civilization in the decades surrounding 1900, only to be combined with notions of global values, science and the history of humankind in the second half of the twentieth century. While these conceptual developments hint at a tightly woven idea of a modern European civilization, this idea is hardly corroborated by our data. If we assume that to be modern meant to be European, and to be European meant to be civilized, we would to find overlap in semantically similar words. Semantically similar words to modern, civilized and european, however, seldom overlap. Signs of an MCE trinity are only spotted in the period 1870-1899, when colonialism and a rapidly changing industrial society produced new ideas about a superior and civilized European modernity. Outside this period, conceptual connections are present, but seldom between all three components of the trinity. Even if the trinity is assumed to be present, it was far from an equally sized triangle: Europe only marginally features in network representations of semantically similar words. Instead, it appears as a relatively insignificant word. Civilization and modernity hardly figure as spatial categories in newspapers, and in so far as they do, the most important concept is not Europe but the West. Conclusion Instead of an extensive and constant entanglement of modernity, civilization and Europe our research shows intermittent and alternating connections. Given that these results differ from research based on elite discourse, this paper demonstrates the need for digital research into conceptual interrelationships. In the case of the MCE trinity, this leads to a misreading of eurocentrism. Digital methods counter this fallacy and show how conceptual configurations assume different forms in different sources and social groups. "
	},
	{
		"id": 330,
		"title": "Italian Resistance Goes Digital: Event And Participant Extraction From Partisans’ Memoirs",
		"authors": [
			"Rovera, Marco",
			"Nanni, Federico",
			"Ponzetto, Simone Paolo"
		],
		"body": " I ntroduction In the digitization era, documents provided by historical archives are at prompt disposal of digital humanities researchers that could directly employ them in text mining analyses. However, the high degree of textual variability and domain specificity of such materials pose several methodological and technical issues for scholars aiming to automatically extract information from historical collections. This abstract reports on a recently concluded project that develop ed a system for extracting events, participants and their roles from a digitized corpus of Italian memoirs of Resistance members during the Second World War. In particular, in our work we have adopted and adapted resources, techniques and tools from research literature in information extraction to provide advanced semantic access to the collection. We chose events as structural concept for extracting and representing textual information as they offer a natural pivot, keeping together semantic information at different levels. We are also encouraged on this way by the findings of a recent work, which suggested, inter alia , the necessity to expand the inventory of event typesor TimeML). Textual dataset The textual dataset is composed of 25 digitized memoirs of Italian partisans. Since our pipeline relies upon a pattern-based method for the initial event extraction, we integrate the resource with two other subcorpora on the same topic, obtained from the web: all texts in the Italian Wikipedia category World War II Resistance movements and a set of biographies of Italian partisans from Wikisource . In addition, three more books have been digitized that compose the Memoirs-test subcorpus and are used for evaluation. Composition of textual corpus for event extraction. Extraction of events, participants and roles The methodology developed in our project combines Named Entity Disambiguation, Semantic Tagging and Role Labeling: as a first step, frame-like event patterns are collected across the whole corpus. Given the domain at hand, we mainly focus on movements of persons and artifacts, conflictual events and events related to organizations. Adopted Semantic Types. The event extraction pipeline consists of two macro-steps: first, NED is performed against a set of three gazetteers of Persons, Places and Organizations; also, all non-named entitiesare tagged with semantic classesby means of a semantic tagger based on word embeddings. The output of this first stage is then used as input for a pattern matching algorithm that, based on a previously collected dictionary of lexico-semantic patterns of argument structures, extracts event mentions from text, classifies them into discrete classes and assigns the detected participants a role with respect to the event itself. The pattern dictionary built for the purpose counts 246 lexical units, i.e., event-denoting lemmas: 124 verbs, 77 nouns and 45 multi-word verbal expressions; such lexical units map overall to 88 Event Classes, where the relationship between lexical units and event classes is many-to-many. The system is designed to separate high and low confidence event mentions based on the number of correctly labeled arguments. A detalied account of the adopted methodology can be found in Rovera et al., 2018. Evaluation The evaluation of the system is performed on a set of 300 manually annotated sentences taken from the Memoirs-test corpus, which has not been used for collecting patterns. In particular, we evaluate the performance of the system for a) event detection, where we achieve .78 Precision and .50 Recall, and b) in event classification, where we score .73 Precision. Visualizatio n In order to display the extracted information, we represent events, participants and roles as a network. Events and entities are represented as nodeslinked by edges, each labeled with the role that the entity plays in the correspondent event. Visualization of a network of events, entities and roles. The linking of a relevant number of named entities enables a rich visual representationof the extracted knowledge, along with the capability to build queries based either on a specific entity, on a class of events, or on a more complex combination of roles, events and entities. Conclusion By presenting our work at the conference we hope to foster the use of event, participants and roles for obtaining a semantically advanced access to historical corpora. "
	},
	{
		"id": 331,
		"title": "\"'How the World Jogges': Interconnectedness, Modularity and Virality in Seventeenth Century News\"",
		"authors": [
			"Ryan, Yann Ciaran"
		],
		"body": " Viral news is often associated with twenty-first century social media, but early modern news behaved in a similar way. News spread faster than would be expected in either a linear or diffuse model, much like the spread of information on Twitter or the global spread of contagious disease. Recent scholarship has shown that early modern news operated as a network. Crucially, it is theorised that to operate in this way, the network could be said to adhere to a Barabási-Albert model: a model for reconstructing networks using preferential attachment, which explains why in many real-world networks the vast majority of connections go to a small number of entities. Network analysis has been used by early modern scholars to examine correspondence networks between individuals, with recent work showing how network analysis can uncover both broad and specific patterns of communication.. This poster shows that the same methods can be used to examine an early modern network where the nodes are cities rather than individuals. This poster uses data from seventeenth news sources to examine one aspect of this network: modularity. Modularity is the tendency for a network to divide into individual components: it is a measure of the extent to which a network is a series of cliques or, on the other hand, a closely connected whole. Modularity measures and community detection algorithms can score any network on its degree of heterogeneity and then suggest the mostly likely divisions in that network. Understanding these divisions is key to understanding the impact and spread of early modern information. Between 1641 and 1649 London saw the development of a relatively free printed news industry. This included regular news from abroad: news mostly from Europe but occasionally further afield. It has been shown that this news moved in individual chunks, organised on the level of the paragraph rather than issue or even article. The regular, paragraphed structure of these early newsbooks means that structured data can be collected from the texts, either manually or extracted automatically. Utilising a manually-collected dataset of paragraphs of news, it is possible to analyse the underlying network of cities behind the information found in the newsbooks. The poster will demonstrate how the use of such a dataset with network analysis has led to the discovery of communities of news and information, specifically using network modularity and community detection to suggest the extent to which Europe could be divided into individual clusters of cities closely linked by the sharing of information, and how this can be used to understand the viral nature of early modern news. The network analysis shows that, at least from the perspective of London, Europes news network was highly modular. It can be divided into eight distinct, closely-connected communities, with twenty-eight key cities holding together the entire networked system. This modularity explains how Europes news system was highly viral and efficient. The communities found share different properties which are explored through this poster: sometimes regional and in close proximity to each other, but always joined by lines of communication rather than politics, language or religious confession. They come together to make a sketch of Europes communities of information. The poster presents maps, network diagrams and sample data to illustrate the suggested communities of information in Europe, as found from London newsbook data. It suggests reasons for the individual communities, and outlines the extent to which the communities can be seen to follow linguistic, confessional or political lines, or whether they deviate from these traditional divisions. The poster explores the extent to which understanding the network as a series of closely-knit communities can explain the reasons for the viral nature of news transmission, even in an early modern setting. "
	},
	{
		"id": 332,
		"title": "Analysis of Writer-Text-Translator Social Networks",
		"authors": [
			"Rybicki, Jan"
		],
		"body": " 1. Introduction This paper presents an attempt to help understand the complex relationships between writers, their texts and their translators. A text by a writer may be translated by a single translator, or rival translations of the same text may appear; or a writers one text may be translated by one translator, and another of his texts by another translator; all this creates a complex mesh of connections that begs to be analyzed as a social network. The fact that some translators limit themselves to just one source language and one target language – the usual situation – does not mean that exceptions do not exist: Polish-Jewish anarchist poet Joseph Bovshover translated Polish into Englishand English into Yiddish; Polish translator Ireneusz Kania translatesfrom 20 different languages. One of the main aims of this reconnaissance was to establish the degree of clustering by language, i.e. whether single-source-language communities are also interconnected. 2. Material and methods Data on writers and translators was obtained from UNESCOs Index Translationum, a large database of existing translations of texts from numerous domains. The data for this study was limited to literary translations into Polish , a total of almost 18,000 individual editions of novels or collections of short stories by 8290 authors and 6582 translators from 155 languages. Editions of the same translations by the same translators were treated as separate entities, since this allows to discover authors, texts, translators and translations of particular cultural significance. The network, prepared in Gephi, used the number of editions of texts of an author translated by a translator as weights. 3. Simple measures It is not surprising that a lions share of translations into Polish as recorded in the Index Translationum was from English-language literature: a total of 12190 editions, or 67.73%; this domination occurs in most cultures nowadays; in Poland, English has been the most-translated language since the 1930s. This is also visible in the most-translatedauthors: of the top 10, only Hedwig Courts-Mahler and Jules Verne wrote in a language other than English; of the top 5, only Jack Higgins was male, preceded by Barbara Cartland, Nora Roberts, Agatha Christie and Danielle Steel; the top 10 also included Lucy Maud Montgomery, Graham Masterton, André Norton and various Disney products. Authors deemed as classical, canonical or artistic only began to appear at the bottom of the second ten: Philip K. Dick came as 19 th , followed by Mark Twain, Jack London and Hans Christian Andersen; Dickens was 27 th ; Tolkien, 41 st ; Conrad, 46 th ; Ursula K. Le Guin, 54 th , beat Shakespeare, 56 th . German and French literature came far behind English and close to each other, with, respectively, 6.91% and 6.77% of the entire database, followed by Russian, Spanishand Italian. Apparently, Polands most prolific and ubiquitous translator is one Małgorzata Fabianowska, the translator of 73 editions by 71 authors, followed by Jacek Manicki, with 66 translators of such authors as Stephen King, Robert Ludlum and John Grisham. The fact than none of Polands celebrity translators appeared in any top positions shows the extent to which quantity does not equal quality in literary translation. 3. Social network analysis Figure 1 presents a three-dimensional network visualization of the data, produced with the Fruchterman-Reingold force-directed algorithmand edited with UCSF Chimera. Figure 1. Writer-translator network. Nodes are writers and translators; edges are translations. As is evident from the general description of the content of the database, the large community in the middle of the sphere shows a strongly-interconnected web of translations from the English. By comparison, other major literaturesare much less represented. There is precious little connection between the source languages. The debris at the external limits of the sphere are instances of one-time translators of texts by authors translated only once, and thus unconnected with the community of a given source language; instances of a single author translated just twice or thrice and/or by just one or two translators, etc. Figure 2 presents an example of this: Figure 2. A small community of two French authors, Peyremaure and Reznikoff sharing a translator, Olędzka; Reznikoff was also translated by Wasitowa. Whenever larger communities are less interconnected, it is feasible to follow the connections within. Figure 3 does this for a subset of the French community. Jean-Paul Sartre is thus connected via his translators to such writers as Raymond Queneau and Jacques Prévert, indicating that sharing translators might follow a chronological if not a generational or an ideological logic. But a somewhat more versatile translator, Jacek Trznadel, also links Sartre with Marquis de Sade. Numerous other interesting connections could be observed. 4 . Conclusions Social network analysis thus seems a good tool to examine relationships within a large database such as the Index Translationum, relationships which are not accessible through direct retrieval of its information. The examples of connections mentioned above obviously do not even begin to describe the extent to which visualizing these networks can be helpful to write, perhaps, an entirely new history of literary translation. Once similar network are drawn for other target languages, mathematical social network valuescould be compared between the literary cultures of these languages to produce quite a new type of distant reading of the phenomenon of literary translation. "
	},
	{
		"id": 333,
		"title": "Using a Feminist and Inclusive Approach for Gender Identification in Film Research",
		"authors": [
			"Samoilova, Zhenya",
			"Loist, Skadi"
		],
		"body": " Although there is a scientific consensus that gender is not binary, immutable, and physiological, it is still common to operationalize it in such a way. This can be said of both traditional as well as new sources of digitized or digitally born data. Not long ago, population surveys were treating sex and gender interchangeably requiring interviewers to ask a question about sex/gender only if it was not obvious from persons voice. Big data and Internet sources imposed a challenge of large amounts of messy data, where gender is often operationalized and coded with the help of automatic detection methods. Recently, there have been more attempts to critically assess and change these exclusive practices in both traditionaland digital methods. This contribution joins these efforts by describing our attempt to measure gender of film directors by relying on their own chosen self-representation. We also compare our results to alternative findings when binary manual and automatic gender detection methods are used. In communicating the comparisons, we visualize the data to invite others to critically reflect on current practices of gender operationalization. The research we report on in this poster stems from a study that explores a substantive question of temporal and spatial patterns of festival runsfrom a film studies perspective. The aim of the project is to explore festival runs of 1727 films selected from a sample of six relevant international film festivals starting from their premiere and ending with their commercial release or lack thereof. Gender of directors constitutes an important piece of information due to known discriminatory practices in film industry. Our data sample collected from six festival programs included 1727 international director teams. In our operationalization of gender, we focused on directors ways to use personal pronouns on available web resources. The web search was implemented manually by project assistants. If a person used male or female pronouns, a female or male gender was assigned accordingly. However, if a person used certain cues to indicate a self-identification with non-binary gender, a non-binary category was assigned. We report data on used cues, missing data, as well as challenges and limitations of the implemented web search. In addition, we compare the findings to manual non-consensual assignment of gender based on names and photos as well as automatic detection methods commonly used to assign gender in large databases. For the latter, we applied two widely used tools: Sexmachine implemented in Pythonand Genderize implemented in R. While Sexmachine primarily relies on name and country data, Genderize is a commercial application that also uses social media data. Results are communicated via visualizations guided by a feminist approach to information visualization and practice. This approach stresses the importance of examining the entire life cycle of the project against such principles as challenging binary categories, transparent communication of projects decision making, embracing horizontal systems of knowledge transmission, embedding data into a context, as well as visibility of labor. We conclude by discussing possible non-binary gender measurements at scale as well as their ethical and data privacy challenges. "
	},
	{
		"id": 334,
		"title": "From the Margins to the Center: A Method to Mine and Model Complex Relational Data from French Language Historical Texts",
		"authors": [
			"Sanders Garcia, Ashley"
		],
		"body": " In humanistic research, Named Entity Recognition is highly useful, but it mines surface data, rather than revealing the complex nature of relationships between these entities. Named Entity Recognitionextracts the names of people, locations, organizations, and, depending on the model, may also extract references to money, percentages, dates, and times, in addition to a miscellaneous class. Although this is certainly useful, NER does not represent the richness of the documents with which we work. For example, consider this fragment from a nineteenth-century French chronicle of Ottoman Algerian history: To further attach himself [to his ally], Pasha Hassan married his [allys] daughter, then he launched troops against the rebel…In just this short passage, which is not even a full sentence, we find several people referenced who are unnamed. If we look back at the text, we see that Pasha Hassans ally is Ben-El-Kadi of Kuku, Algeria, and the rebel is Abd-El-Aziz, but Ben-El-Kadis daughter is never named. This occurs frequently in historical source material. Those who remain unnamed are most often women, servants, slaves, and Indigenous people – the very people about whom scholars are most anxious to know more. This short paper presents a work-in-progress: a digital workflow and Python script to mine and model the relationships between extracted entities from French-language documents in order to grapple with the complexity of human relationships and cultures, as well as the perspectives of authors and their informants. This short presentation will share the complete information extraction code, its accuracy, the resulting visualizations, and a brief analysis from the case study. The method presented has applications far beyond French language and the history of the Middle East and North Africa. For instance, with some adjustments for language, this method would be highly useful in the analysis of The Twenty-Four Histories of China, the official history of the Chinese dynasties between 3000 B.C.E. and the seventeenth century. More broadly, this approach will be of use to scholars interested in identifying and studying relational data, social positions, and networks of both known and previously unknown actors, particularly those who remain unnamed in the source material. As a test corpus, this project uses four digitized, OCRed, and hand-cleaned nineteenth-century French chronicles of Ottoman Algerian history. The volumes range between 41,341 words and 170,737 words and cover the period 1567 to 1837 with a focus on Constantine, the easternmost province in Algeria. The challenge is to extract not only named entities and their relations to one another, but to extract unnamed persons and their relationships as well. In simple NER, the names Moustafa and Namoun, would be the only extracted data in the following sentence: Moustafa avait épousé une des filles de Namoun, but the daughter of Namoun who married Moustafa would not appear.The goal of this project is to uncover the positions and roles of women in Algerian society, so it is essential to locate and retrieve data about unnamed people. The built-in language models and extensibility of the spaCy natural language processinglibrary for Python makes it most suitable for this project. Specifically, spaCy enables researchers to define entities and build custom information extraction systems. Additionally, spaCys library features a French language model that has a built-in tagger, parser, and NER, unlike the Natural Language Toolkit or Stanfords CoreNLP Open Information Extraction system. To build an information extraction system with spaCy that pulls the desired relational data, we must first identify an extractable pattern by parsing and tracing the dependencies of a sample sentence, as follows: SpaCys visualizer also allows us to view the dependency parse tree using the following code and sample sentence. An examination of the parse tree above yields a pattern of parts-of-speech around the keyword épousé that we can use to extract the desired information about this relationship. Since we are interested in identifying the relationships between both named and unnamed people, we will look for specific patterns in parts of speech and syntax, as well as the location of proper nouns in relation to keywords. Based on an examination using the concordance method with the sample texts, the following keywords generated the best data: fils, fille, mariage, épous*, gendre, and beau pére. For example, the word fils, or son, yielded more consistent results for father-son pairs than the word pére, or father. From an examination of the word fils in context, as shown above, general patterns emerged. The patterns for fils and the proper output format for each pattern are shown below. These outlines then inform the Python script that uses spaCys library to extract the relational data. This script will be made freely available on GitHub following the DH 2019 Conference. Based on the examples and patterns above, the information extraction system derives relational data that easily translates into node and edge lists. In this case study, network analysis of the extracted data highlights how women, marriage, and kinship connections legitimated Ottoman rule. Initial findings suggest that Algerian women were key links in the chain that bound Algeria to the Ottoman Empire. "
	},
	{
		"id": 335,
		"title": "Twining Digital Humanities and Humanidades Digitales: A set of actual experiences from the South",
		"authors": [
			"Sanz, Amelia",
			"Fiormonte, Domenico",
			"Numerico, Teresa",
			"Priego, Ernesto",
			"Rodríguez-Ortega, Nuria",
			"Siapera, Eugenia"
		],
		"body": " In a post-digital era, when the digital is taken for granted in Westerncultures and a disembodied virtuality on screen seems the fabric of everyday life, it is time to ask ourselves why a growing number of scholars from the South remain suspicious about DH. Can the same tools and ingredients be used to prepare a cake in Vienna, Seville or Marrakesh? In other words, how could technological monism represent and preserve cultural and epistemological pluralism? Are DH policies colonial, hyper-colonial? Are we living the time when the colonized wants to become colonizer, or the time when the colonized begins to twine threads of Northern DH? The goal of this panel will be to show how Digital Humanities can have different meanings and reflect opposite needs, according to its geographical, socio-cultural, linguistic and economical contexts and discourses. So for example Humanidades Digitales/Informatica Umanistica does not mean exactly and cannot mean Digital Humanities. Moreover, it is a matter of facts: HD are developing without DH. We should be ready for this big bang. This is not a matter of colonial intention, but a matter of colonial effect: Northern criteria run towards the technological innovation and fail in terms of social impact, while Southern criteria seek for social innovation and fail as latest technological innovations. We can measure the distance to the Silicon Valley meridian or the distance to the Global South parallel. In our panel, we will try to map some successful experiences to identify practices and strategies in local projects from and in the South. We understand the term South as a metaphor, and not necessarily a space tied to a specific culture, economy or region. Boaventura de Sousa Santos in criticizing the Global South label writes : The South is here rather a metaphor of the human suffering caused by capitalism and colonialism at the global level, and a metaphor as well of the resistance to overcome or minimise such suffering. It is, therefore, an anticapitalist, anti-colonialist, and anti-imperialist South. It is a South that also exists in the global North in the form of excluded, silenced and marginalised populations, such as undocumented immigrants, the unemployed ethnic or religious minorities, and victims of sexism, homophobia and racism. South is also a representation of the expulsions of local interests and microcultures. As people suffer exclusion from their lands due to the extractive, predatory capitalism, local experiments and scientific cultures experiment exclusion because of the exploitation model of platform capitalism. From this perspective global DH risk to be absorbed and adopted by platforms in order to extract value and produce a global market of texts. Digitalization and datification of texts can be used to create a quantification method for the commodification of culture and meanings. The world of texts is on a verge of a huge transformation that works in the direction of a saturation of multiplicity and ambivalence of meanings, because respecting diversity and variability hampers quantification and measurability. The implicit proposal of the global DH guided by the capitalistic exploitation and standard building is to normalize all differences, in order to commodify content and communication, as it was done with natural resources. As Sousa Santos suggests the metaphor of south represents resistance and resilience against such normalization. The refusal of uniformity is often considered lack of efficiency, while it is a struggle in favor of diversity and multiplicity. This project conveys examples of alternative visions of digital methodologies. A team composed of scholars coming from Greece, Spain, Italy and Mexico, working together in as the Southern Digital Social Sciences and Humanitiesproject will present their reflections and experiences based on regional epistemological traditionsdealing with research on their digital disciplinary field in Southern countries, both upstreamand downstream. Technical and epistemological deficiencies and benefits of tools and codes will be discussed; examples of DH programs and examples of North-South, South-North and South-South publishing formulas. At the opposite end of mainstream narratives, we suggest as a starting point calling for the problematization and conceptualization of DH. So, we state that internationalization does not necessarily mean homogeneization, and standardisation does not necessarily mean Westernization. We propose to get back to the etymological meaning of « text » : « textum », the weave of threads in a fabric. This is to say a weave of codes and systems: western/northern technologiesand other ones well adapted to social, economic and political conditions, to their epistemological sovereignties. The cultural and epistemological bias of algorithms: examples and reflections Teresa Numerico and Domenico Fiormonte domenico.fiormonte@uniroma3.it teresa.numerico@uniroma3.it Università Roma Tre The reorganization of memory produced by the externalization devices has geopolitical, cultural and social effects that need to be treated and analyzed from various critical perspectives in order to be first understood, discussed and later transformed. We need to be aware that data are built and not given in its purity, so we have to discuss about how they are built and for which aims. An example can clarify the problem and offer some suggestions on how to find a solution. AI Machine learning algorithms risk applying biases and stereotypes to their knowledge inferences, based on statistical language associations. The ratio they use is the definition of statistical vector relationships between words, grounded on their reciprocal distance, measured within the training set of texts. The Stanford Natural Language Processing Group created one of the most famous algorithms to process words and to measure their distance, in order to grasp their semantic relationships. Its name is GloVe. GloVe is trained on various text corpora such as Wikipedia and Gigaword. The problem is that using the distance between natural language words to train an unsupervised learning algorithm there is the concrete probability of embedding in its knowledge base all the biases and stereotypes that are hidden within the data sets. According to Gillespie, the way the training set of data is conceived has a large unexamined impact on the result of the training of the related algorithms. The suggestion that AI algorithms are less biased than humans with respect to prejudices and stereotypes needs to be demonstrated; the evidence at the moment shows the exact opposite. It is crucial, in our opinion, to cast some doubt on the epistemological credibility, on the transparency of methods and on the fairness of the unsupervised training outcomes of the algorithms. We need to set a control strategy that can demonstrate the effectiveness and safety of results, and, above all, their assumed neutrality when treating sensitive data; for example, when we choose new employees or when we decide on the qualifications of a person for getting a mortgage, or, even worse, when we ask a black box algorithm to suggest the duration of the sentence for a convicted person. It is fundamental to discuss the organization of these algorithms and, above all, the effects of the implicit multi-layered biases embedded in training textual databases used as a reference standard. We think that we should be very cautious in trusting a blunt device for taking decisions in situations where it is unlikely that we completely understand its logical processes, as Norbert Wiener himself, already in 1950, wisely suggested. Open Access and the Global South: Alternatives to the Oligopolies of Knowledge in the Digital Humanities Ernesto Priego, Centre for Human Computer Interaction Design, City, University of London, UK [Ernesto.Priego.1@city.ac.uk] This contribution will present bibliometric evidence of the dominance of oligopolies of knowledge based in the Global North in the Digital Humanities, detailing the concentration of activity of academic communicationsvia for profit publishing companies based in the global north, by authors with affiliation with universities in the global north and in the English language. We will refer to the work done in the last five years, documenting and mapping this localised and mostly monolingual concentrationthrough alternative bibliometric methodologies, in order to draw attention to the correlation of the imbalances of this geopolitical concentrationwith closed modes of dissemination of high cost for institutions. I will discuss some successful or failed experiences in Mexico on the appropriation of open access mechanisms, in the Digital Humanities and other fields, in order to identify the challenges that this implies particularly for researchers in the areas of social sciences, arts and humanities, and specifically for those with affiliations in the global South. The Social Sciences and Digital Humanities of the South Project. : why and where Nuria Rodríguez-Ortega, Universidad de Málaga, España [nro@uma.es] In the first part of this presentation, I will carry out a reconstruction of the genealogy and history of the # CSHDSUR project, born in Malaga in 2013, to offer a framework that can help us understand its origins, the stage of the process we are in and the problems related to knowledge monopolies and cognitive colonialisms. The project was defined by two key aspects: firstly, trying to enlarge the social sciences and the digital humanities, which embraces other types of practices which are different from the strictly academic ones, such as those that take place in the citizen labs; and second, because of its critical orientation, aligning itself with the Digital Humanities approach that adopts the point of view of critical theory of culture, postcolonial theories, decolonial methodologies, etc. The second part of this presentation will be devoted to detailing the functionalities of a very specific tool rooted in Andalucía, in South Spain: the project EXHIBITIUM.com and the tool ExpoFinder. This is a case study of howan intensified critical consciousness can materialize into concrete alternative models,the notions of value and relevance in a located contemporary society implies the significant question of revising the axiological project of the Humanities of the 21st century,an epistemic and methodological de-hierarchization and transversality can emerge in the production of valid and contextually situated knowledge for all. The aim of this kind of initiative is not to oppose two extreme positions, not even to integrate ourselves in a global South, but to propose an issue on how to twine DH and HD. DH and Knowledge Production: The Case of Greece Eugenia Siapera, School of Communications, DCU, Ireland [eugenia.siapera@dcu.ie] We will present two events that are certainly co-eval in the case of Greece: the spread of the internet and digital media and the implementation of the most extreme austerity policies Greece has ever seen. All this takes place in a context that has its own idiosyncrasies, and for the most part still operates in a very closed, clientelist manner but which is now confronted by neoliberal demands for transparency, flexibility and privatisation. We will identify three central tensions/ambiguities: i): the political economy of knowledge production and the labour conditions of university staff and researchers; ii) the openness of digital publishing and demands to publish which are linked to a creation of publishing hierarchies;the role of the EU framework programmes and their approach to research funding. More concretely, the proposed paper will rely on in depth interviews with Greek researchers to outline problems they are facing withemployment status;accessing data archives;accessing the technologies and computational power necessary to process data. At the same time, the paper traces the ambiguities involved in that the very same technologies and methodologies of digital humanities and big data allow researchers to identify and concretely map the impact of protracted austerity on the production of knowledge. The presentation concludes with an identification of possible areas for intervention, including building research solidarities across and beyond the EU efforts to produce aresearch area in Europe; and data activism which will allow for a more equitable distribution of the tools that allow the production of knowledge across countries of the global south. However, in normative and political terms, the objective is not only an equitable distribution of data and knowledge, but a questioning of these very tools in creating new areas for subjecting to governance, state and corporate. In these terms, it is important to insist that a research paradigm from the south develops a different agenda to new forms of knowledge and the necessary tools to produce these. "
	},
	{
		"id": 336,
		"title": "Digital Folkloristics: Approaching Variation And Stability In Folklore With Computational Methods",
		"authors": [
			"Sarv, Mari",
			"Meder, Theo",
			"Kallio, Kati",
			"Janssen, Berit",
			"van Kranenburg, Peter",
			"Järv, Risto",
			"Mäkelä, Eetu"
		],
		"body": " Variation is a complex phenomenon engaging almost all aspects of folklore. Every cultural performance in daily life gets adapted to time and place, circumstances and audience. In this panel we we are going to explore complex phenomena of variation and stability in folklore on the basis of textual and musical representations of oral tradition with the help of digital and computational methods. In many cases, variations can be interpreted as intentional and meaningful. However, folklore seldom changes beyond recognition: there is always a part of narratives and songs that remains stable. Detecting in which points lies the stability in folklore sources, reveals to us what the very essence, the core of tradition is. Approaching the material from the other end, it needs to be analyzed how variation is produced, where the adaptability and creativity of folklore lies, and which are the meaningful possibilities for variation within the limits of tradition. As far as we are dealing with texts or melodies, we can determine in what respect oral performances can be labeled as traditional and to what extent folklore is the product of individual creativity and improvisational skills. After determining what parts of folklore remain the same, what changes and what parts are left out, we need to come up with an explanation: what does this all mean in the light of the culture of daily life? Our core material consists of narratives and songs: epics, poetry, myths and other folktales, life testimonies, and folk songs. Millions of folklore texts and performances, collected in the folklore archives and nowadays available in digital form can, together with the existing metadata, be used as data for finding out the regularities and irregularities in folklore - a universal kind of natural communication with its specific functions in society. Computational Analysis of Life Stories Theo MederIn 2013, a new project was started by the Humanitas Foundation, department Almere, which was called Levensboek. Volunteers from Humanitas would conduct interviews with elderly people who would tell their life stories. This life story was then recorded or edited by a volunteer and, with photos, then printed as a booklet in a limited edition. The booklets with life stories were mainly meant as testimonials for the children and grandchildren, other family and friends. One of the initiators of Humanitas, Veronica Stutvoet, contacted Theo Meder of the Meertens Institute with the question whether such Life Books were also interesting for archiving and studying. Since the study of contemporary folk culture is one of the core tasks of the Meertens Institute, Humanitas also decided to offer a booklet for the archive. Due to privacy legislation, a contract was added in which the narrators could indicate when the book could be studied freely. And from the Meertens Institute a list was drawn up with subjects that would interest the researchers, such as folktales, songs, games, festivities and rituals. The first Life Book was received in May 2013 in a festive manner: it concerned the book Met hart en zielby Mrs Elly IJsendijk. After proven success, the Humanitas departments in Apeldoorn and Zaandam also started to produce life books, and after five years 20 booklets were produced. In addition to a paper copy, the Meertens Institute also receives a digital copy on request, so that the stories can also be subjected to a computational analysis. This may include structure analysis, research into motifs or sentiment analysis. Research into gender is also possible; do women talk about other subjects than men? The storytellers were, without exception, born in the 1920s, 30s or 40s - meaning that some experienced the crisis years as a child, while some were born shortly after the Second World War. In any case, the war has left a mark on many children, even if they only heard the stories. The life stories are always linear: they often start with the parents, then the childhood, thewar, school, friends, education and profession, marriage, children and grandchildren, holidays, illnesses and deaths of loved ones. And yet the stories are always different, through the emphasis on certain themes, and through many unique personal experiences. Perhaps most revealing are the themes that allnarrators ignore or leave out. In my research, I analysed the digital life books on structure, sentiments, themes and the distribution of motifs, using tools such as AntConc and LIWC2015. Stability in folk song transmission Berit JanssenIn folk song traditions, melodies are circulated through transmission. In this process, parts of melodies may change, while other parts remain stable, meaning they resist change. Stability has been a long-standing point of interest in folk song research: how can stability be quantified, and can we predict which parts of a melody are stable? In the past, this question has been addressed through experimental research, in which artificial transmission chains were observed. While this direction of research is inspirational, the ecological validity of such approaches may be questioned. With the current computational means and rich digitized corpora of folk songs, we can study the results of real-life transmission of melodies by comparing variants of the same song. The current contribution describes such research on a corpus of 4120 Dutch folk song melodies. Two melodic units were investigated: folk song phrases and motifs. For the phrases, the goal was to predict the occurrence of a phrase in a family of related songs: a phrase occurring in many variants in almost identical form was considered more stable, and was expected to have different melodic properties from less stable phrases. To determine the occurrence of folk song phrases, a pattern matching method was developed in Python, which was optimized on a training set of annotated phrase occurrences. Several similarity measures were compared, and those approaching human judgements on phrase occurrences most closely were combined to detect phrase occurrences in the full set of folk songs. For the motifs, a set of motifs considered characteristic melodic material of 360 melodies was compared against random melodic patterns, with the expectation that the characteristic motifs would have different melodic properties from the random melodic material. We evaluated prediction success through Generalized Linear Mixed Models. The results show a number of successful predictors for stability of melodic segments in transmission: the length, position and number of repetitions in a melody, conformity to musical expectations, and the presence of repeating motifs can help us to predict whether or not a given melodic segment is stable. Both for folk song phrases and folk song motifs, the melodic predictors explain between 5% and 10% of the variation, constituting a medium-sized effect. Other factors might influence stability in folk song transmission: preference to copy performances of individuals based on their status in society, or preference to copy the most common variants of a melody. Given that such factors cannot be controlled in the current dataset, the extent to which stability can be explained purely on the musical properties of melodic segments is impressive, and shows that stability is certainly not a randomly occurring phenomenon, but arises from the resonance of melodic structures with our cognitive capacities to perceive and memorize music. Rule Mining for Melodic Cadences Peter van KranenburgThe availability of large collections of digitized folk songs enables an empirical approach to the study of various aspects of melodic structure. In this contribution, we focus on melodic patterns that are used to indicate a cadence, or end of phrase. Most existing approaches for modelling cadential patterns are either based on pre-defined rules or on statistical learning. Rule based approaches include Narmours Implication-Realization model, and Cambouropoulos Local Boundary Detection Model, which both are grounded in principles from Gestalt Theory. Statistical approaches include Rens Bods Data Oriented Parsing, Hurons ITPRA-model, and the IDyOM model by Pearce et al.. The current study takes a hybrid approach by employing a rule-mining algorithm to infer a model of melodic closurefrom a collection of folk melodies. There are many machine learning methods that could be used to learn models from data. The advantage of a rule-mining algorithm is that the resulting model is highly interpretable, as it consists of a series of rules. We employ a collection of more than 4,000 melodies in Western tonal idiom from the Meertens Tune Collections. Since the digitized melodies in these data sets include annotations of phrase boundaries, these are well suited to train cadence-detectors. The data set for rule mining consists of all pitch tri-grams from all melodies. The tri-grams are labelled as either cadential or non-cadential. We represent each tri-gram as a vector of feature values. Features include scale degrees of the three pitches, melodic contour, and metric weights. We use the RIPPER algorithmto perform the rule mining. The output of the algorithm consists of a series of rules to separate the cadential tri-grams from the non-cadential tri-grams. In a first run, we obtain a F-measure of 0.789 on a separate test-set. The three most important rules describe cadences on the first and the fifth degree of the melodic scale. The first rule states that a tri-gram which ends on the tonic, has a high metric weight for the third pitch, and has a descending contour is a cadential tri-gram. This rule reflects common knowledge from music theory. By closely examining the cases in which the discovered rules fail, we are able to identify possible other features to include. In particular, we find that the position of the tri-gram in a melodic phrase is of importance. Therefore, we include this in our feature set and perform a next run of the algorithm. The newly discovered model achieves a F-measure of 0.839 on a separate test-set. Adding the feature, clearly improved the discovered model. From this study, we conclude that cadence patterns obey general rules, and that it is possible to derive these rules from melodic data when including the right features. The advantage of a rule-based model is its interpretability in musical terms. Browsing the corpus of Finnic oral poetry Kati Kallio & Eetu MäkeläWith a versatile corpus of Finnic oral poems in several related languages and dialects and a wide variety of different orthographies, a central question is how to gather relevant items for each research setting. How to find similar poetic formulas or themes and trace intertextual relationships in a linguistically and poetically heterogenous corpus of oral poetry, and what theoretical possibilities does digital reading offer for the research? In this paper, we compare searches made with research interface Octavoand the present interfaces of two corpora of historical Finnic oral poetry in runo-song meterto the analyses that were made earlier manually with these collections, discussing both the practical and theoretical possibilities and implementations given by digital browsing possibilities. During the last decades, the folklore archives in Estonia and Finland have digitised two large sources of historical Finnic oral poetry, consisting of c. 181,000 poems in various dialects of small related languages around the Baltic sea: Karelian, Izhorian, Votic, Estonian and Finnish. The poems were recorded in 1564–1939 with various orthographical systems. Some words may appear in hundreds of different written forms. The stories and main characters may exist in various ways, with individual, local and regional peculiarities. The language may contain archaisms or special word forms, syllables and words used only in songs. The poetic system is complex and versatile, and there are no comprehensive dictionaries or ready-made parsers for the data. Yet, the research history provides a point of comparison. During the first half of the 20th century, a great amount of detailed studies on geographical variation of individual song types was made, with the aim of taking all the collected examples into account. Although the theoretical understanding of oral poetry has since changed, making these studies partly invalid, these studies are still relevant depictions of variation within the data. When compared with searches made with digital tools, they give a baseline for evaluating the possibilities and limitations of the present tools. The paper focuses on three examples: 1) Analysis by Väinö Kaukonenof the manuscript sources of oral poetry used by Elias Lönnrot when composing the Finnish national epic Kalevala. 2) Historical-geographical analysis by Martti Haavioof the vernacular Death Song of Saint Henrik, the medieval patron saint of Finland. 3) Typological-stylistic analysis by Matti Kuusiof the Karelian mythological Sampo-epics. Is it possible to find digitally all those variations and intertextual links — or more — of a particular theme or poetic formula that were gathered manually by the past researchers? This is approached 1) with word and collocation searches, 2) by checking the results by using a thematic index of the SKVR-corpus, and 3) finally comparing both strategies with the findings of earlier manual research. Potential of Stylometry in Studying Folkloric Variation: Content, Style, Language Mari SarvStylometry - a statistical method comparing sets and share of most frequent wordsin different texts - has been most notably used in the field of authorship attribution, but also in genre studies, in translation studies etc. The main idea lies in the assumption that individual style of an author is represented in the way he/sheuses the most frequent wordsor other units. In applying stylometry for the large historical corpora of literary writings one can detect development of style, which is not clearly distinguishable of the changes in naturallanguage use. The current paper addresses the potential of stylometry in studying variation in folklore texts. Stylometric analysis could possibly help us to find answers to many questions concerning the nature of folklore and variation inherent to it, say the individuality versus traditionality of performers/creators, similarities/differences of different folklore genres. In addition, stylometry could be used as clustering tool for detecting tradition areas within a bigger area, and even folkloristic text-types within the text corpora when focusing on content words in the analysis. At first glance stylometry seems to be an extremely useful and feasible method for getting better knowledge on variation in folklore; there are additional difficulties to solve though. First, the linguisticvariation and folkloric variation are inseparable and overlapping. Different words and word forms used in differentdialects do not have to mean differences in content aspects, like modes, genres, types. Non-standard language and non-standard orthographies present in folklore texts do not make the task of comparison easier either. Moreover, the folklore texts are usually not written down by performers themselves, but collectors who have left prints of their personal style into recordings. The complexity of variation in folklore makes it a challenge to tackle, and evokes questions if the variation we are able to detect using stylometry, form part of dialectal, stylistic or folkloric variation; is it individual, or reflects the peculiarities of genre, thematic or functional groups of texts. My experiments with multilingual corpora of folksongsin several Finnic languageson the basis of word forms have revealed that both, linguistic as well as content aspects play a role in clustering, reflecting main dialect boundaries in first instance, but revealing for example also regional predominance of lyric and epic mode in songs, and different thematic accents in regional groups. The network of characters in Estonian animal tales Risto JärvIf folklore is characterised by variation and milieu-morphological adaptation of characters, the adaptation of internationally spread animal tales will retain some established dominant characters – it is certain fixed characters that appear as certain types. The presentation observes the variability of animal characters, using network analysis. The study is based on the Estonian folk tale text corpus created by the Estonian Folklore Archives of the Estonian Literary Museum and the Department of Estonian and Comparative Folklore at the University of Tartu. The corpus contains 13,000 Estonian texts, of which approximately a fifth is made up by animal tales. While the Estonian folklore scholar Pille Kippar has noted in an earlier discussion of characters of animals talesthat the characters can be easily interchangeable within the limits of their stereotypes, I am analysing a sample of selected tale types to check how predominant such variability is, which characters in particular appear as interchangeable, and which regularities emerge in the variability as concerns versions of specific tale types as well as versions by particular storytellers. I analyse which setsof characters are most likely to vary within the tradition and whether there are causal relationships between this feature and the animal characters being active or passive. As several animal tales appear as cycles in the folklore tradition, which combine different tale types within one tale, also this characteristic is taken into account to detect whether any distinct features of character variability emerge in these cases. "
	},
	{
		"id": 337,
		"title": "Ranke.2 - A Teaching Platform for Digital Source Criticism",
		"authors": [
			"Scagliola, Stefania",
			"Guido, Daniele",
			"Fickers, Andreas",
			"Zaagsma, Gerben"
		],
		"body": " Professional historians have always questioned the origin, meaning and credibility of a historical source. Now that more and more historical sources are being digitised and published online, our traditional methods of source criticism need to be updated for the digital age. For this purpose the teaching resource Ranke.2 – Source criticism in the digital age – has been created at the Centre for Contemporary and Digital History in Luxembourg. The platform offers a series of lessons on how to critically assess and work with historical sources in digital form. This refers to both analogue sources that have been digitisedas to born-digital sources. By engaging with lessons on the Ranke.2 platform, students will learn: How digital objects are created. What changes when an analogue source is turned into a digital representation. How to question the concept of the original. How information is added to a digital object. How data is published online and made searchable. What impact search engines have on finding and selecting sources. The properties of different types of data. By problematizing the reliability of digital sources we aim to introduce students and scholars to a new form of digital hermeneutics. The poster that will be presented about Ranke.2 explains the relevance of digital source criticism, listing the key questions that should be posed to both analogue and digital sources. It also elaborates on the core Ranke2 pedagogical principles: 1. Differentiation in complexity and time required, to connect to different teaching contexts. This means lecturers can choose between the modules SMALL, MEDIUMor LARGE. 2. Offering teaching content in a variety of attractive interactive formats: colorful animations, quizzes, assignments for web research and tutorials for a hands-on workshop. "
	},
	{
		"id": 338,
		"title": "Curating and Archiving Linked Data Datasets from the Humanities - From Data of the Present to Data of the Future",
		"authors": [
			"Scharnhorst, Andrea",
			"Van Erp, Marieke",
			"Siebes, Ronald",
			"Gueret, Christophe",
			"Crick, Tom",
			"Tykhonov, Vyacheslav",
			"Coen, Gerard",
			"Smiraglia, Richard",
			"Doorn, Peter",
			"Van den Berg, Henk",
			"De Vries, Jerry",
			"Merono, Albert",
			"Ashkpour, Ashkan",
			"De Valk, Reinier"
		],
		"body": " Overview Digital Humanities are inseparable from digitalcollections. In research infrastructures, be it on national or international level, we see networks between established and new repositories, digital libraries, archives and other content-providers and research institutions. This panel addresses new challenges for the alliance of research communities and service institutions which emerge from new data formats, vocabulary standardization efforts, and collaborative practices. We focus on semantic web technologies and the adoption of LinkedData principles in humanities research, and more particular, data curation practices around them. The appeal of Linked Data to humanities can be explained by the inherent capacity of this new technology to link heterogeneous resources using shared and standardized schemas and vocabularies. This resonates with epistemic cultures in the humanities where scholars are used to work with different sources, viewpoints, and interpretations. Although, still early in the adoption process, the past decades have seen a number of projects which incorporate semantic web technologies into digital humanities methods. At the same time, there are intriguing pending questions about the long-term preservation of Linked Data datasets as complex digital objects. Examples of challenges are 1) the versioning aspects of the vocabularies and classifications, 2) the provenance of the metadata, 3) the attribution of authority for independently and heterogeneously createdcontent and 4) the selection process of vocabularies and schemas, especially when there are multiple options. This panel enables a discourse on higher level principles of working with and preserving Linked Data in the humanities based on reports of concrete experiences on the work floor. We discuss differences in data curation as conceptualized and executed during research and when preserving research data for the long-term. We further discuss how to align those different curation practices with the ultimate goal to making Linked Data used in digital humanities FAIR. In greater detail: Paper 1 discusses what to archive and how from a research perspective, and is complemented by Paper 5 which takes the perspective of a long-term archive; Paper 3 addresses automatic means to complement metadata and this way enhance search and discovery; while Paper 4 points to the danger of forgetting or losing data curation efforts in the past; and Paper 2 and 6 complement the panel with reports on issues of Linked Data curation based on experience from two different projects. Among them, the papers in this panel address the following three research questions: Q1: How to take care of Linked Data curation and management during research, and how to organize effective collaboration between Semantic Web technology pioneers and pioneers in Digital Humanities application of this technology?Q2: How to exploit the possibilities for efficiency and synergy of LinkedData in distributed networks between research, collections and overarching service providers?Q3: How to best bridge conflicting priorities between enabling researchbased on LinkedData technology and the long-term preservation of Linked Data datasets?Papers When to store what and how? Data curation challenges during research projectsIndexing Cultural Heritage Resources for Research and EducationAutomated Vocabulary Suggestion, Domain-Specific Linked Entity Recognition and Visually Faceted Verification404 Error - Resource Not Found: Why we Need to Rescue Endangered Knowledge Organization SystemsArchiving Linked Data Datasets - Experiences from a Humanities Research InfrastructureDutch Historical Censuses - Preserving Data for Research, the Wider Public and Future GenerationsAcknowledgement: This panel has been supported by the DiggingIntoKnowledge Graph projectand CLARIAH CorePapers When to store what and how? Data curation challenges during research projectsMany funding agencies require that data is archived after the end of the project and made available to the research community for further reuse [1]. There is also a lively discussion about making experimental data available in various research communities, for example in connection to publications [2]. In short, data curation challenges appear at different stages of the research and data life cycle and can create various tensions within research projects when they try to comply with this. This holds in particular for humanities projects that are not explicitly aimed at creating one or more datasets. In this paper, I provide examples from the intersection of Semantic Web and the humanities and illustrate data curation issues at three stages of the research process: 1) knowledge modelling, 2) exploratory experiments, and 3) user friendliness of Semantic Web technology. In each step, data curation is a prerequisite to data sharing and presents different challenges. Knowledge Modelling One of the main difficulties in digital humanities research is the gap between the digital and the humanities. Simply put, computers need clearly defined concepts and boundaries between concepts whereas in the humanities concepts are more fluid and the world is less black and white, but more a variety of shades of grey. Many computer science datasets are simplifications of the world, for example GeoNames [3] focuses on current location names and country boundaries, whereas countries and cities have grown, shrunk, merged and split and borders have changed through time. Concepts and entities also change over time. Present-day England is not the same as the England of 200 years ago. Current LOD resources fail to capture this complexity. Making the essence and limits of the data context explicit both for machines and scholars is a core challenge. Exploratory Experiments Linked data practices originate from computer science, where the research process differs from the humanities. A historian may start digging into an archive with a general research question, that will be further specified during the search in the archive. This may mean that documents selected at the start of the process as relevant may be discarded later on, and the type of information extracted from the documents to base analyses on may evolve over time too. A provenance trace of the search queries and resulting selections can only capture part of this information. It is therefore still an open question what kind of provenance trail is needed to be shared in the actual research process and in the long-term. Userfriendliness of Semantic Web technology Great strides have been made in making Semantic Web technology more accessible for non-experts. However, this technology is still not well integrated with the daily practices of humanists, barring the reuse and re-sharing of data according to linked data principles. The difference in knowledge between developers of semantic web research infrastructure and those conducting humanities research, even in interdisciplinary projects hampers discourse on data curation. [1]http://ec.europa.eu/research/participants/docs/h2020-funding-guide/cross-cutting-issues/open-access-data-management/data-management_en.htm [2] http://lremap.elra.info/ [3] http://geonames.org [4] https://druid.datalegend.net/ Indexing Cultural Heritage Resources for Research and EducationIn an increasingly digital world, e-infrastructure has become a key component of the daily life of researchers, underpinning a variety of academic activities. Over the period 2014-2020, the development of this varied e-infrastructure is supported by 850 M€ of funding from the European Commission. Those systems at the core of data-driven science are expected to not only contribute to scientific discovery, but to also play a larger role in society. This paper reports on the results of a specific national project [1], which directly engaged with cultural heritage content in the UK based on Linked Data principles. At its core is an open platform which indexes and organizes the digital collections of libraries, museums, broadcasters and galleries to make their collections and content more discoverable, accessible and usable to those in UK education and research. This includes culturally important images, TV and radio programmes, documents and text from world-renowned organizations such as the British Museum, British Library, National Archives, Europeana, Wellcome Trust and the BBC. In addition to linking content, the project also supported developers to create digital educational products that would inspire learners, teaching practitioners and researchers by using applications powered by the Research and Education Spaceplatform. This paper discusses the challenges faced by the project, the architecture developed for tackling them, and the lessons learned. In particular, we address challenges for consuming Web data; the problem of co-referencing; and most prominently the problem of licensing. In particular, we discuss how the lack of unambiguous declarations of copyright and license as metadata hinders the re-use of existing published data, and which methods have been tested so far to circumvent this problem. The paper closes with an inspection of other existing collections and platforms and a discussion on how they solve the above listed problems. [1] https://bbcarchdev.github.io/res/ Automated Vocabulary Suggestion, Domain-Specific Linked Entity Recognition and Visually Faceted VerificationAttempts to order our knowledge in a way that we can easily find what we are looking for is as old as the body ofknowledge grew. But, despite of sophisticated Knowledge Organization Systems, or new network-based principles, we all share painstaking experiences when trying to find back something via Google, knowing it is there, but simply not being able to find it via the keywords coming into mind. Homonyms, synonyms, generalizations, specializations, spelling variations and mistakes, language versions etc. are all complicating the keyword-based approach. Linked Data is one step forwardon solving some of the problems with keyword search. By having shared vocabularies created and maintained by experts on various domains, the digital items can be annotated with them and, in principle, easily retrieved by other experts from the same domain without being a skilled librarian as well. Unfortunately, there are still two main problems: - First, there is no single authority that for each possible domain defines a satisfying and indisputable annotation vocabulary. What we observe instead is a multitude of attempts, often tailored for momentous and local needs, and characterized by conflicting paradigms, missing subjects, and logical inconsistencies. In other words, one still needs to know which vocabulary is good enough and shared by a critical mass, if available. - Second, and even a more prevalent problem is annotating content is time consuming, requires specific skills and is not something with which one can boost ones career. Data curation as natural part of the research cycle is often invisible and undervalued. Where a researcher submitting a paper to a journal might still try her best to annotate it with some keywords, learning about Linked Data and then finding the proper URIs for a machine readable annotation is a bridge too far. This paper presents research which addresses these two problems by means of a automated and visually attractive support system. To tackle the first problem, we will investigate various quality aspects of the existing vocabularies by analyzing their expressiveness, relations and popularity via various metrics on the currently largest aggregation of Linked Data available: the LOD Laundromat. When successful, a domain expert can be confident that for her goals the best possible set of vocabularies and schemas are proposed to annotate her content. For the second problem we develop an integrated visual interface for a selection of Linked Entity Recognition tools developed in CLARIAH Core and apply the most appropriate combination for the best matching vocabularies and schemas according to the results from the first step. In other words, when a person wants to annotate a document, the first step will suggest the best vocabularies and the second step automatically detects entities in the document specified by these vocabularies. To summarize, by applying the expertise on library classification systems and integrating results from various projects, the task is to investigate and develop automated support for vocabulary selection and its domain specific entity recognition approach accompanied with intuitive domain specific visual verification support. The paper reports about first experiences with the system, and discusses further requirements for a wider implementation. 404 Error - Resource Not Found: Why we Need to Rescue Endangered Knowledge Organization SystemsThe name is the thing, and the true name is the true thing. To speak the name is to control the thing. - Ursula K. Le Guin, The Rule of Names Reducing ambiguity in research and improving interoperability of data and results are increasingly important themes within the field of Digital Humanities. According to the FAIR Data Principles in order to achieve such interoperability, it is necessary to use a formal, accessible, shared and broadly applicable language for knowledge representation and vocabularies that follow the FAIR principles [1]. As suggested by American science-fiction and fantasy novelist Ursula K. Le Guin in the above quote, we can only know a thing is a thing, and that it is the same thing that we are both speaking about, once we know and agree upon its name. Knowledge organization systemsinclude a variety of schemas that organize, manage, and retrieve information and knowledge. They range from ontologies, to classification schemes, thesauri, taxonomies, controlled vocabularies and semantic networks. Given their role in facilitating access to and retrieval of information, they play an important role in vocabulary standardization. Since KOS are constantly changing to reflect dynamic growth in knowledge, they present unique challenges in terms of maintenance and tracking the provenance of their changes. We can consider KOS as bridges allowing for shared meaning, since they connect terms with the appropriate concepts and knowledge. But what happens when these bridges are closed, broken or simply...lost? Over the course of a yearlong study, using a method of humanistic empirical association, this project has studied KOS in the wild, exploring their attributes, histories and evolutions. During the study, the 404 Error became a relatively familiar sight, highlighting the fragility and lack of stability ofthese systems, leading to questions about how to identify KOS which are at risk of disappearing. A lost KOS is a lost bridge to the meaning behind a term in a given context, place or moment in time. This paper aims to make recommendations for ensuring the long-term preservation of KOS, for example, ensuring that systems used in scholarly research today remain FAIR, and can be guarded against loss of meaning through semantic drift or the disappearance of the resource. A number of case studies will be presented in order to highlight the importance of these resources within the field of Digital Humanities and also to raise awareness about the benefits of their application. Finally a number of recommendations will be proposed related to common tools and mechanisms which need to be developed for their proper archival and maintenance, in order to make them FAIR. [1] https://www.force11.org/group/fairgroup/fairprinciples Archiving Linked Data Datasets - Experiences from a Humanities Research InfrastructureArchiving RDFdatasets may seem trivial. The notorious problem of format-outdating is non-existent, RDF can be expressed as essentially plain text in various specialized formats. No need to update this format to rapidly changing format specifications and ditto applications. The challenge of placing data and datasets in a proper context in order for them to be understandable and interpretable in time seems no greater than that for datasets with files of a more traditional format. In fact, less so because RDF data is self-describing in the sense it places itself in context by linking to a network of concepts and meaning - as long as its links are resolvable. It is this condition - the resolvability of links which in essence are references to other knowledge graphs - which makes archiving RDF still a challenge. In this paper, we look at this problem more carefully for a concrete case. We start with the question: What are RDF data and what makes RDF datasets different? We continue to ask: Do the nature of RDF dataimply that we must reconsider the cycle of producing, archiving and reusing data? According to well accepted definitions, RDF datasets package up zero or more named RDF graphs along with a single unnamed, default RDF graph and a RDF graph is a set of RDF triples. A triple or quad is, by its nature, an atomic statement; a RDF dataset can be fragmented into smaller chunks without compromising the validity or understandability of these smaller sets of triples. Does the fact that RDF datasets can be fragmented make special demands on the findability of these fragments and therefore on our search engines? Does it make demands on the way we must be able to retrieve these fragments and reuse them? The role of digital archives can be described as guarding the technical and cognitive interpretability of its assets over time. Technical interpretability can be secured by scanning and converting the formats of files in the archive at regular intervals. Cognitive interpretability, from the side of the archive, is usually covered by a one-time effort that comprises securing and placing the dataset in its due context. In this traditional scheme, the role of cognitive interpretation is carried out by humans. With RDF datasets, for the first time, the role of cognitive interpretation canbe carried out by machines. Does this inevitable machine interaction change conditions of archival interfaces and the datasets themselves? In this paper, we report our findings to build a pipeline from a tool developed for academic research, which offers the ability to access and edit data as RDF graph to a long-term archive. Acknowledgement: This paper has been partly funded by the CLARIAH CORE project, NWO grant number: 184.033.101. Dutch Historical Censuses - Preserving Data for Research, the Wider Public and Future GenerationsThis paper describes research data curation steps which put the Dutch historical censusesin a cultural and historical context by using semantic web technologies. We reflect in particular about data curation and archiving after the project has ended. The fact that the resulting RDF is an integrated dataset means that we can study the landscape of the historical censuses, the evolution of demography, labor and housing in the Netherlands for a period of two centuries. With the transformation into RDF, for the first time ever, we can ask questions on the Dutch historical census data as a whole. Due to its connections to external resources in the LOD cloudquestions that can only be addressed by combining all these data sources together can be answered. To give an example, we can now ask how many houses have been under construction across specific years at municipality level across the country. During the project, a pipeline was developed [1] from the raw data to an enriched RDF graph. Data have been harmonized along certain dimensions and perspectives, creating an iterative cycle between the domain specific analysis, annotation, enriching of the original data and its ingestion into new versions of the RDF graph. At the end of the project we applied a multi-tiered strategy to preserve data both for present research as well as for the future. Concerning the first, data is deposited in online repositoriesto stimulate further reuse. Additionally, the RDF data graph is available online [2] as SPARQL endpoint. A website has been created [3] to enable human interaction. Moreover, we followed a similar approach to store, manage, and publish queries using the GitHub infrastructure [4]. As a result, these can be collaboratively edited by humans, but also reused by machines for different purposes. Importantly, this includes the automatic generation of APIs [5], through which the whole RDF dataset can be easily accessed by users and standard HTTP clients. The enrichment of the RDF, through updates in all these resources, is an ongoing effort. Complementary, two coupled datasetsA first version of the Linked Data publication of the Dutch Historic Census, anda related dataset Geharmoniseerde census data, 1859-1920 which details the harmonization for the tables Plaatselijke Indeling as part of Volkstellingenyears 1859, 1869, 1879, 1889, 1899, 1909 and 1920have been deposited in the long-term, certified, data archive EASY. To address the problem of long-term preservation of Linked Data which by nature come with references to other resources, the depositincludes snapshots of all vocabularies, in their specific versions, on which the RDF database depends. [1] http://lod.cedar-project.nl/cedar/info.html [2] https://github.com/CEDAR-project/DataDump [3] http://virtuoso.clariah-sdh.eculture.labs.vu.nl/sparql [4] http://lod.cedar-project.nl/cedar/index.html [5] https://github.com/cedar-project/queries [6] http://grlc.io/api/cedar-project/queries [7] https://www.gemeentegeschiedenis.nl [8] http://dutchshipsandsailors.nl Acknowledgement: This paper has been partly funded by the CLARIAH CORE project, NWO grant number: 184.033.101. Hyvönen, E.. Publishing and using cultural heritage: Linked data on the semantic web. S.l.: Morgan & Claypool. Idrissou, A, Zamborlini, V, Latronico, C, van Harmelen, F & van den Heuvel, CMJM. Amsterdamers from the Golden Age to the Information Age via Lenticular Lenses. Short paper presented at the 5th DH Benelux Conference, 6 – 8 June 2018 Amsterdam, The Netherlands. Webresource: http://2018.dhbenelux.org/wp-content/uploads/sites/8/2018/05/Al-Idrissou-Chiara-Latronico_GoldenAgentsLenticularLenses_DHBenelux2018.pdf / https://web.archive.org/web/20190411142008/http://2018.dhbenelux.org/wp-content/uploads/sites/8/2018/05/Al-Idrissou-Chiara-Latronico_GoldenAgentsLenticularLenses_DHBenelux2018.pdf Mazzocchi, F.. Knowledge organization system: an introductory critical account. Knowledge Organization 45, no. 1: 54-78. Meroño-Peñuela, A., Ashkpour, A., van Erp, M., Mandemakers, K., Breure, L., Scharnhorst, A., Schlohbach, S., van Harmelen, F.. Semantic technologies for historical research: A survey. Semantic Web Journal 6, 539-564. DOI: 10.3233/SW-140158 "
	},
	{
		"id": 339,
		"title": "Between Data Mining and Human Experience – Digital Approaches to Film, Television and Video Game Analysis",
		"authors": [
			"Grotkopp, Matthias",
			"Scherer, Thomas",
			"Stratil, Jasper",
			"Agt-Rickauer, Henning",
			"Hentschel, Christian",
			"Bakels, Jan-Hendrik"
		],
		"body": " Within the field of digital humanities, studies of extensive corpora still focus, by and large, on a combination of quantitative approaches and the epistemology of distant reading. While such an approach can generate entire new sets of research questions and perspectives with regard to the macrostructures of certain media, formats and genres, the underlying principles of abstraction, accumulation and statistics fall short when it comes to questions of performativity, dynamics of perception or aesthetic experience. This circumstance becomes apparent in the field of temporal arts and media, especially if the respective research is shaped by a theoretical framework that draws on aesthetics and phenomenological approaches. In these cases, statistical data based on discrete entities are often of limited value. The very advantage of distant reading—stepping out of the tangible context of a certain point in time or space within a given work of art in order to get a grasp on overarching principles of the work as a whole or even larger corpora—turns into a dead end. While a subject matter that is being referred to in terms of a semiotic, semantic, or syntactic paradigm can be divided into discrete entities with a fixed value or meaning, the experiential quality of a certain detail within a phenomenological approach to aesthetics and performativity largely depends on the aesthetic composition as a whole. Accordingly, temporal arts and media—within the scope of this panel: film, television, web videos and video games—pose a challenge to the ways the isolation of features, the encoding of media texts, and the accumulation of data are being conducted within the current methodologies available in digital humanities. This panel features film scholars as well as computational scientists who have jointly developed a digital approach to compositional patterns in audiovisual moving images within the research group Audio-visual rhetorics of affect. While the first paper reflects upon possible intersections between phenomenological film theory and digital tools for video annotation and analysis, the second paper introduces against this theoretical backdrop and by means of a concrete analysis, a systematic framework for video annotation and data visualization with regard to compositional patterns in audiovisual media. The third paper addresses how this systematic framework for video annotation can be further standardized and automatized through semantic data structures and semi-automatic video analysis tools developed within the computational sciences. The fourth paper sketches out how this approach to semi-automatic video annotation and analysis can help achieve an empirical perspective on video game practice. Analyzing Audiovisual Images: Digital Humanities AND/OR Phenomenology? Matthias Grotkopp This paper aims to open our panel by making a case for an approach to digital film analysis that is grounded in film theory. How does one reconcile the inherent complexity of the object of study—the multimodal composition and temporal gestalt of audiovisual images—with a seemingly necessary reduction of complexity for data management structures? In practice, this question has mostly been avoided: digitized audio, images and audiovisual images tend to be treated as a wealth of easily accumulable metadata that almost by accident have data attached to them that also can be issued for human perception. These metadata naturally supply valuable information to media practitioners, media historians, and analysts of social, economic and cultural context. They present complex networks of who did what, when and where? which can be explored in myriad ways. But what happens if one is interested in questions like: What is the specific mode of aesthetic experience shaped in a film scene, and how does it relate to similar or contrasting scenes across a corpus of films? What is the temporal unfolding of metaphors and other meaning-making processes? What is the affective process of a scene and what is the summary emotional effect that a film aims to create? What is the poetic logic of a film or its desired rhetorical impact? This is where phenomenology comes in, since these are questions that can hardly be answered by quantitative measurement and statistical analysis of single parameters alone, like shot duration, color values, speech or tagging image content. Instead—to borrow a phrase from Maurice Merleau-Ponty—the task is to show how something takes on meaning—not by referring to already established and acquired ideas but by the temporal or spatial arrangement of elements. And it is about locating this emergence of meaning in embodied processes of reception where technologically animated audiovisual moving images are realized and reflected as reconfigurations of the parameters of perception and cognition. But what are the requirements for analytical tools and data evaluation that are based on such an approach, rooted in film theory and qualitative research, as compared to purely quantitative data mining? Here lies not only the practical problem of retaining and managing the complexity of the object, but also the theoretical problem that a) not all of the relevant parameters for analyzing audiovisual images can be quantified in a similar manner, that b) the temporal and spatial arrangement of multiple features is, for all intents and purposes, the primary property, and that c) these arrangements of features in the audiovisual data have to be conceptualized as realized in an embodied act of viewing. This paper will therefore argue for a method of combining fine-grained, time-code-based annotation following a standardized routine, with queries and visualizations that do not target these annotations as data-to-be-processed, but as patterns and dynamics of viewing-processes. Researching and Annotating Audiovisual Patterns – Methodological Considerations Jasper Stratil and Thomas Scherer Digital video annotation is often concerned with represented objects within the image, especially when it comes toautomated analysis of moving images. However, while the automatic detection of a car or a face may be useful for some purposes, it is not sufficient when analyzing the expressive qualities of audiovisual images: what feelings are evoked by the audiovisual compositions? How do processes of meaning making unfold? Following up on the theoretical implications of the phenomenological approach presented in the first paper of this panel, this paper is building on the fundamental difference between still and moving images, addressing a gap in video annotation practices: while a majority ofautomatic annotation routines focus on the single-frame extraction, film and media theory emphasizes audiovisual images as time-based media that unfold as an expressive movement. These temporal, cross-modal patterns are harder to detect and annotate than static phenomena deduced from individual frames. A pitfall ofautomatic analysis lies in a bias that prioritizes those dimensions that can be easily detected automatically, but this can lead to a reconstruction of an experience that is closer to computer vision than to human experience. A harsh, sudden and aggressive movement creates an entirely different experience than a soft gliding movement—similar to the unfolding of a melody, and this is highly relevant for the rhetorical dimension of audiovisual images. How, then, can this dimension of audiovisual images be annotated? Annotating movements that can be grasped at the level of compositional patterns requires a specific vocabulary that can only be acquired by combining theoretical and methodological discourses from film and media studies with the affordances of a systematic and machine-readable vocabulary in order to describe various different audiovisual dynamics. The paper presents first results concerning this challenge that were developed in a close cooperation between film studies and computational sciences. It addresses questions of segmentation, multimodal description, the balance between precision and time efficiency, as well as the development and application of a systematic vocabulary. These results will be demonstrated by reference to films concerning the global financial crisis. Through an exemplary film analysis the paper addresses the question of how this micro-perspective, i.e. the detailed reconstruction of compositional patterns, can be related to the macro-perspective of entire films. How is it possible to read these patterns within the vast amount of data necessary for the micro-analysis of moving images? How can the process of film viewing be reconstructed by identifying patterns in video annotations? And how can recurring patterns be identified and related to each other – within a film or even across a group of films? Figure 1. Exemplary view of a data set in ADVENE. The paper will exemplify a use case of systematic digital video annotations and their visualization as a research tool for empirical film analysis that enables a scaling of detailed film-analytical approaches for a larger group of films, as well as the sharing of annotation data among scholars. Standardization and Automation of Audiovisual Annotations Henning Agt-Rickauer and Christian Hentschel A holistic, scientific analysis of audiovisual patterns of staging in fictional and non-fictional video data requires dense localization and annotation of visual and auditory features within the video stream. Currently, the manual effort involved prevents analyses from going beyond micro-studies. On the one hand, this is partly because many annotation tools allow free-text annotations with arbitrary keywords and mapping the output of multimedia analysis tools to the desired target formats is costly. On the other hand, film analysis, which aims to identify temporal and cross-modal patterns, requires the fine-grained annotation of a large number of film-analytical aspects involving several hours of work per minute of film. The approaches presented here, therefore, pursue two main objectives: 1) developing a standardized semantic annotation vocabulary for digital film analysis and 2) developing semi-automatic classification approaches of audiovisual patterns. Manual video annotation performed by film scholars leads to highly valuable information that requires consistent management of video data, metadata, and annotation data in order to prevent them from being stored in isolated data silos and proprietary formats. We use Semantic Web technologies based on Linked Open Data principlesto publish, reuse, query, and visualize film-analytical data in order to share annotations with other film scholars and researchers from other disciplines. In this presentation, we will first show how semantic metadata of video files and movies help to create consistent video annotations for a large corpus. We will then show how concepts and terms from a film-analytical method are implemented as a semantic annotation vocabularyand integrated with open source video annotation software allowing domain experts to author and publish unambiguous Linked Open Data. The presentation will also provide some insights into retrieval and analysis applications based on a large number of published annotations. Automatic analysis of video streams aims at improving the speed of localization and extraction of audiovisual features mainly through the application of approaches from computer vision and machine learning. Shot boundaries and transition types can be automatically extracted by means of temporal video segmentation. Based on the extraction of content representing keyframes, the visual content of a shot can be further analyzed. Examples include extraction of dominant and salient colors, optical flow estimation for camera/object motion classification and visual concept detection for the identification of depicted objects and scenes. Finally, automatic analysis of the audio stream provides means for transcribing the spoken work into machine-readable text. The presentation will give a brief introduction into the main concepts behind these aforementioned approaches for semi-automatic annotation of audiovisual features with Linked Open Data. Whats Your Game? – A Digital Approach to Aesthetic Experience in Video Games Jan-Hendrik Bakels This paper suggests studying video games by means of digital film analysis in order to establish an empirical perspective on the act of playing, thereby addressing a crucial desideratum within game studies. Though not contested in the past two decades as it has been in the past, the division into a narratological and a ludological perspective on video games still shapes the basic theoretical coordinates within the field of game studies. The narratologist approach considers video games to be situated within a larger framework of arts and media that are defined by their storytelling potentials, often focusing on the supposed closeness of games to the concepts of interactive and/or non-linear narration. In turn, the ludologist conception posits video games within the larger framework of game theory, stressing structural aspects such as rules. Most contributions to game studies take this opposition into consideration by referring to both aspects of video games, with different emphases. This paper argues that, ultimately, both perspectives—at least from a phenomenological perspective—fall short to grasp the core of video game culture: the embodied experience of playing. Drawing on phenomenology as well as theories on kinaesthesia and rhythm in performative arts and media, the paper attempts to develop an empirical-analytical approach to video gaming as an act of appropriation on behalf of the player. From this point-of-view, narratives are not to be grasped on the level of representation; instead, they derive from the complex interplay of a human being making an embodied experience and that human beings capability of narrating this experience. On the other hand, a games rules and underlying structure merely shape the virtual potential of a game, not the way it is actually experienced while playing. In other words, if taken as theoretical vanishing points, both narratology and ludology fail to grasp video games at the level of actual playing: the first considers what is being taken away from a game after the act, while the latter rather reflects upon the game in a virtual state before it is actualized by the act of playing. Of course, kinaesthesia and embodiment have already been discussed in terms of video games before. Nevertheless, at least one methodological problem contributes to preventing such a perspective from being elaborated in a systematic manner: in order to address aesthetics, embodiment, and the actualization of a games potentialities at an analytical level, it is necessary to study actual acts of playing on the level of audiovisual images. But with most common video games lasting several hours, a comprehensive—not to mention: comparative—empirical study of games within the paradigm of media aesthetics seems to reach beyond the capacity of a single researcher or even research project. Against this backdrop, this paper will outline how the use of semi-automatic film-analytical tools developed in the past few years can provide the basis for a comprehensive and comparative study of specific acts of playing video games, thereby closing a crucial theoretical gap, as well as providing new impulses to the fields of narratology and ludology. "
	},
	{
		"id": 340,
		"title": "Classification of Text-Types in German Novels",
		"authors": [
			"Schlör, Daniel",
			"Schöch, Christof",
			"Hotho, Andreas"
		],
		"body": " Introduction When working with literary texts, a problem for linguists, literary scholars and for machine-based text understanding is the classification of text-types. The term text-type refers to a variety of different phenomena reaching from a superordinate view of genreto functionally motivated text-types as aggregations of structural or linguistic features. While the taxonomy, textual layer and functionality of different theories behind text-types may differ widely, a common concept behind these theories is the understanding of textual surface structures varying in their respective text-type. The text-types descriptive, narrative and argumentative emerge very frequently in many theories. Being able to automatically assign sentences to these text-types is therefore highly desirable when aiming to support quantitative literary studies. In this work we present our text-type dataset, a feature based machine-learning model and a deep-learning based model and show that both are able to classify text-types. Annotation Scheme Functionalizing the existing theories to a respective abstraction of surface phenomena, we came to our annotation guidelines: descriptive: the description of a physical object in its dimensions, parts, and/or properties narrative: the representation of a chain of events, actions or activities in its temporal progress driven by persons or other actors argumentative: the presentation, explanation and justification of an abstract idea in its logical context To consider possible shortcomings of these guidelines or the underlying model and to give the annotators an opportunity to indicate their uncertainty, we introduced additionally the label unknown. In our annotation tool, sentences were presented within a small contextual framing. Figure 1: Contextual frame for the sentenceto be annotated in our annotation tool Dataset We chose a random subset of 30 novels of the DROC corpusfor our experiments. From each novel we extracted a continuous segment of about 1% of the length. Each of the 1773 sentence obtained in this way was annotated by 3 annotators in order to judge the complexity of the annotation task and to identify a subset of sentences annotated with highly reliable text-type labels. Especially the argumentative text-type seemed to be difficult to annotate since the annotators often disagreed or showed uncertainty on the text-type e.g. for exclamations or other speech acts. When demanding full agreement among the annotators, the number of instances is reduced from 1773 to 830 sentences denoted as D S. A majority voteleads to 1503 labeled instances denoted as D M. Experimental Setup Feature Construction For the task of classifying text-types, we modeled several features targeting different surface levels: Bag-of-Words. As a baseline, we computed a simple count-based feature vector, representing how often which words occur per instance. Indicator-Words. Especially the argumentative text-type has frequent discourse-related indicator words. We used discourse particles, modal particles, interjections and punctuation marks. Word-Vectors. In contrast to sparse bag-of-words vectors, dense word-vectors like Word2Veccomprise semantic information. We used FastTextsince the character-based model is able to model compound words implicitly, which are very common in German. After unsupervised training of 100 dimensional Using 100 dimensions proved suitable in a initial experiment. word-vectors on the complete DROC corpus, a TSNE-based visualization of the vectorsrevealed a noticeable cluster of words of the descriptive text-type ranked via SD2-Zeta. Therefore, we modeled FastText based vector-semantics in two ways: averaging the word-vectors over all words within an instance and counting cluster-membership for words using a previously trained k-meansmodel. Germa-Net. We used GermaNetto generalize words in two ways: First, we followed each path towards to the most general hypernym and selected different hypernym levels as degrees of generalization. Second, we used GermaNets categorical structure for abstraction. Besides its taxonomic structure, GermaNet classifies words into 54 categories, e.g. the verb sagenbelongs to the category verb / communication. We use these categories as generalizations for each word to reflect e.g. descriptions of placesor people. Figure 2: TSNE-Visualization of FastText word-vectors. Blue color indicates that a word is more prominent for the descriptive text-type. The indicativeness was judged opposing the descriptive and other text-types via SD2 Zeta. Other text-types show similar results. Classification Task To examine the contribution of each feature, we conducted an extensive feature analysis using a Support Vector MachineRandom Forest classifier achieved slightly worse to similar results. as a classifier and compared its performance to a deep-learning based Recurrent Neural NetworkModel. The SVM was used with linear and RBF kernels, varying the hyperparameter C C ∈{1,10,100,1000} . Additionally we also evaluated the influence of the degree of generalization using GermaNet on the classification performance. For the RNN, we adopted the BiGRU model frombut introduced additional loss functions and model parameters as follows: We varied the number of GRU layers between 1 and 4 and the dimensions of the hidden layers between 100 and 400. We used the pretrained FastText model as initial embedding weights. We conducted a grid-search based parameter study for both classifiers to find the best model-parameter configuration. The model architecture is depicted in figure 3. Figure 3: BiRNN Model. Number of GRU Layers were varied between one and four in a parameter-study. Categorical-cross-entropy as loss and softmax as activation achieved best results. Table 1: Precision, recall, f1-score and accuracy for the SVM classifier on a random hold-out dataset from D S and D M Results We evaluate the models on sampled hold-out dataset fractions of 10% and report accuracy for both datasets, D S and D M. The majority baselineyields an accuracy of 0. 457 for D M and 0.398 for D S . The best performance of 0.864 mean For 20 repetitions on different hold out-sets accuracy ± 0.047 standard deviation for D s resp. 0.776 ± 0.022 for D M was achieved by the linear SVM classifier, C = 1, only using the average FastText word vectors as feature. This result supports our finding of rather distinct text-type word-vector clusters in our explorative analysis. However, we surprisingly find that additional features do not improve the result. The best feature combinations without averaged FastText vectors yield significantly For 20 repetitions on different hold out-sets, α = 0.05 using Pitmans permutation test as suggested bylower accuraciesand use the FastText cluster features. The best features without any FastText use bag-of-words and part-of-speech features for D Mand additionally GermaNet hypernym and category features for D S. The generalization-depth study aimed at finding the optimal degree of lexical generalization between the word itself and a very abstract and non-indicative root-hypernym. We therefore followed the hypernym path from each word to each root-hypernym and selected the highest and the lowest three hypernym-levels in a classification setup only using this feature. Our results indicate that the second hypernymis the most promising level of abstraction for our task. However, we are aware of the problem that hypernym relations for different words might each represent different degrees of abstraction, depending on the level of detail at which the taxonomy is modeled. The RNN model achieved significantly For 10 repetitions on different hold out-sets, α = 0.05 using Pitmans permutation test. lower accuracies for all model variations in comparison to the SVM. We believe that this is mainly a problem of too few training data since in theory, the RNN should be able to model the best performing feature, the averaged FastText vectors. In contrast to, our best performing model used categorical-cross-entropy as loss function and one GRU layer with a dimensionality of 400 on both datasets reaching a mean For 10 repetitions on different hold out-sets. accuracy of 0.801 ± 0.049 standard deviation for D S and 0.702 ± 0.029 for D M. All detailed results, including the results for our feature study are also available on GitHub https://github.com/cligs/projects2019/DH_TextTypes . Discussion and Future Work Our results show that an SVM as well as a deep-learning approach are able to classify text-types with an accuracy far beyond the baseline. To some extent, handcrafted features are able to compensate the small amount of training data for the D S dataset with an SVM and outperformed the best deep-learning based model. Since the performance of deep-learning based models heavily relies on a sufficient amount of training data, this outcome isnt very surprising and might be revised if more data becomes available. In comparison, the feature driven linear SVM classifier might also have an advantage when it comes to interpretability: The coefficient-weights of an SVM classifier can be interpreted as a whitebox modeland reveal interpretable insights into the decision process, whereas the decisions made by a neural network cannot easily be inspected, which is crucial for theory construction and deconstruction in thehumanities. For future work, we plan to examine in detail why the handcrafted features such as indicator-words or WordNet abstractions dont seem to be as useful as expected. We also plan to incorporate data augmentation methods and finally do a consolidating annotation run to have a bigger and cleaner text-type dataset. "
	},
	{
		"id": 341,
		"title": "Katharsis – A Tool for Computational Drametrics",
		"authors": [
			"Schmidt, Thomas",
			"Burghardt, Manuel",
			"Dennerlein, Katrin",
			"Wolff, Christian"
		],
		"body": " Introduction With his idea of Distant Reading, Morettiintroduced an important leitmotif in the Digital Humanities that has led to an ongoing discussion about quantitative methods in literary and cultural studies. We believe that the literary genre of drama is particularly well suited for quantitative analyses and hence adapt the concept of Drametricsas a term for the distant reading of dramatic texts. In addition to the actual dialogs, dramatic texts contain other structural elements that can be easily quantified, such as the characters of the play as well as an explicit act and scene structure. Keeping these features in mind, it is hardly surprising that we find a number of recent studies dedicated to the quantitative analysis of drama. At the same time, there have been quantitative approaches to the analysis of drama that date far back into the pre-digital age. As an example for early approaches to quantitative analyses of drama, we would like to refer to the ideas of Marcusmathematical poetics, which also contains interesting approaches for quantitative drama analysis. Solomon Marcus Mathematical Poetics Marcus suggests the scenic presence of characters as a basic computable measure of a play, which, for each dramatic text, can be visualized by means of a configuration matrix. The matrixcontains one row for each character of the play, and one column for each scene. Whenever a character appears on stage, the value 1 is entered into the corresponding cell; if a character is not present in a scene 0 is entered as a value. Figure 1: An example configuration matrix visualizes the appearances of charactersthroughout the 15 scenes of the play. Configuration matrices can be used to compute various quantitative aspects of a drama, for instance: the scenic distance and proximity of characters and even specific relationships between charactersas well as the overall configuration density of plays. The configuration density is calculated by dividing the number of cells holding a 1 by the total number of cells. In other words, the configuration density indicates how many of the potential character appearances have actually been realized. It can be understood as a measure of a plays population density. When every character appears on the stage in every scene, the play has a theoretical maximum configuration density value of 1. During the 1970s and early 1980s, several studies applied Marcus mathematical approach for the analysis of texts, always dealing with very few samples of text. In these studies, configuration matrices proved to be useful in text analysis, as they fasten and simplify the overview of a characters first or last appearance, co-presence or avoidance with other characters. Some years later, Ilsemanntook on the ideas of Solomon Marcus to explore Shakespeares plays in a quantitative way. Ilsemannused the frequency and lengths of characters speeches as further parameters and found that the configuration density is an important aspect of genre-distinct quantitative patterns for comedies, romances, tragedies and history plays. In 2005 and 2008, Ilsemann used the frequencies and distributions of speech lengths to discuss authorship attribution in Shakespeares plays. The Katharsis Tool In order to be able to automatically analyze quantitative aspects of dramatic texts according to Marcus character configurations and Ilsemanns analysis of speech lengths and frequencies, we have created Katharsis, a tool for computational drametrics. The Katharsis tool comprises a parsing component that extracts and calculates various quantitative parameters as suggested by Marcusand an analysis component that searches for dramatic texts of a certain author, genre, timeframe, etc. Currently, a test corpus of approx. 100 German drama texts from the TextGrid Repository Available online via https://textgridrep.org/is available for analysis. The texts are available as TEI-XML, allowing for the extraction of metadataand speeches with the corresponding speaker and structural information. Note that the tool can be extended with further plays from other authors and genres if the texts are encoded in TEI-XML. Furthermore, the quantitative metrics are independent of the language. Figure 2 shows the Katharsis results for a search for dramatic texts by Friedrich Schiller. Users can download any quantitative information displayed in the screenshot in JSON format for individual analysis. Figure 2: Summary of quantitative information calculated by Katharsis for dramatic texts by Friedrich Schiller. With the help of Katharsis researchers are able to examine a specific drama in more detail. The tool provides an interactive configuration matrix to explore character appearances and speech statistics for each configuration. Figure 3: Katharsis snippet of the interactive configuration matrix for the play Maria Stuart, by Friedrich Schiller. Furthermore, Katharsis produces a table and several interactive bar charts to analyze the distribution of speakers and speech statistics on the structural levelsand the progression of these metrics throughout the course of a play. Figure 4: Average length of speechesthroughout all acts of the play Maria Stuart by Friedrich Schiller. Another segment of the tool shows statistics concerning the comparison of speakers like speech statistics and the distributions of scenic presence. Furthermore, following Marcusapproach, specific character relations derived from the configuration matrix can be explored. For each character of the play, the tool displays relations to other characters which may be of the type dominate/dominated, alternative, independent or concomitant. The last component concerning the analysis of individual dramatic texts follows Ilsemannsidea to examine the distribution of speech lengths in the play. We calculated the speech length by counting the number of words. Users can analyze an interactive histogram and a curve chart. Different speech lengths can be included in the visualization dynamically to narrow down the range of speech lengths for more in-depth analysis. Finally, Katharsis can be used to analyze and compare self-created collections of plays by means of various quantitative aspects. The comparison of different genres and authors is a pre-configured comparison. Figure 5 illustrates a comparison of speech lengths for Goethe and Schiller showing that Goethes most frequent speech length is seven while Schillers is rather low with only four words. This might be one reason why the plays of Goethe never were that successful on stage like those of Schiller. Figure 5: Comparison of the relative distribution of speech lengths for the plays of Goethe and Schiller. The Katharsis tool is available online and can be tested as a live demo in any current web browser: http://lauchblatt.github.io/Katharsis/index.html Case Studies on Quantitative Drama Analysis In this section, we illustrate the usefulness of Katharsis by means of short case studies: An important computable aspect of dramatic texts are the encounters of characters on stage in different configurations. A case study that used Katharsis on 13 tragedies, 17 comedies, one tragicomedy and one Schauspiel of the German authors Andreas Gryphius, Christian Weise, and Gotthold Ephraim Lessing verified the hypothesis that there is a trend for comedies to have higher configuration densities than tragedies. For dramatic German texts from 1600 to 1800 the mean length of speeches in comediesis lower, whereas the total number of speeches is higher, which means characters in comedies seem to interact in a more dialogic manner. Figure 6: Average length of speeches in comedies and tragedies of the corpus. Figure 7: Average number of speeches in comedies and tragedies of the corpus. This seems plausible with regard to some characteristics of tragedies and comedies already known: Tragedies more often feature monologues because they provide the ideal occasion to reflect on jealousy, hatred, guilt, plans of murder, or suicide. A general lack of communication, or communication difficulties, may be associated with the fact that generally fewer characters share the stage. In comedy, however, protagonists more often encounter each other. Typical comic effects such as confusions between characters or characters exchanging roles as well as speeches delivered at spectators, are staged in the presence of several characters and may result in a rather high configuration density. Future Work: Sentiment Analysis for Drama To enhance the applicability of Katharsis as a tool for computational drametrics, we are currently preparing to include basic sentiment analysis techniquesas an addition to mere structural parameters. While sentiment analysis has been particularly popular in the field of computational linguistics, the approach is also gaining popularity in literary studies. So far, we have evaluated different sentiment analysis techniques for the context of historic, German language plays. A first Katharsis prototypethat implements sentiment analysis for 12 German plays by Gotthold Ephraim Lessing is available online: http://lauchblatt.github.io/Katharsis/sa_selection.html In the long term, we plan to combine character-to-character sentiment analysiswith the existing configuration matrices, thus not only transferring Marcus approach of mathematical drama analysis to a digital tool, but rather enhancing it by using additional parameters such as character sentiment. "
	},
	{
		"id": 342,
		"title": "Ethics and Legality in the Digital Arts and HumanitiesThe DARIAH-EU Working Group ELDAH",
		"authors": [
			"Scholger, Walter",
			"Hannesschläger, Vanessa",
			"Kuzman Slogar, Koraljka"
		],
		"body": " Ethical and legal issues in Digital Humanities research The European Research Infrastructure Consortium Digital Research Infrastructure for the Arts and Humanitiespromotes open access to methods, data, and tools, and stands for responsible conduct. As an infrastructure for Arts and Humanities researchers, it strives to deal with all issues arising in the quickly progressing domain of digital research. DARIAHs new ethics and legal strategy - e.g. the establishment of a dedicated Open Science Officer and the Working Group on Ethics and Legality in Digital Arts and Humanities- is an effort to establish openness as a de-facto standard in digital research and a contribution to ensure a high standard of legal and ethical awareness wherever researchers from communities connected to DARIAH are concerned. This should facilitate better access to tools and data for researchers, as well as to recommendations and best practices for the conduct of their research. ELDAH is dedicated to addressing the needs of the research and education community regarding the topics of legal issues and research ethics. In the age of digital information technology and the constant availability of information through the internet, it is not only important to have democratic access to knowledge, but also essential to consider issues of access, transparency, reusability, and attribution. Of particular interest and concern to cultural heritage institutions and humanities scholars alike are issues of copyright on, provision of, and access to digitized source material. There is a recognizable political drive in the European Union to facilitate free and public access to cultural heritage and research data hosted at publicly funded institutions. However, the lack of legal harmonization and the diverse and often unclear national legislations on the use and provision of resources by public cultural heritage or research and education institutions has been prohibiting a much broader engagement. The recent EU regulation on Data Protectionalso caused a lot of insecurity and, due to lack of information and counseling, prompted frantic activity that often went beyond actual legal requirements and presents significant difficulties to conducting research. Therefore, ELDAH provides reliable information for humanities scholars facing legal questions and uncertainties in their research, not from a legalist but from a community- and practice-oriented perspective. Beyond this legal scope, issues of ethical research practices and scholarly conduct are central to the Humanities and Social Sciences, perhaps even more so in a largely digital, internet-based research context: Different ethical issues become salient as the researcher develops research questions, seeks and gains access to individuals and/or information, manages and protects personally identifiable information, selects analytical tools, and represents the data through dissemination, in published reports, conference presentations, or other venues.A dedicated Working Group Currently comprising about 40 members from 18 countries, covering a large variety of disciplinary fields, ELDAH produces recommendations, training and information materials on IPR, open licenses and Open Science in general, and offers workshops on these topics to scholars in the context of DARIAH events across Europe. Additionally, ELDAH works in close collaboration with similar groups among the large European research infrastructures - like the Legal and Ethical Issues Committee of CLARIN ERIC, the Consortium of European Social Science Data Archivesor the Europeana Networks Association - and pertinent working groups of national organisations - e.g. the Open Science Network Austriaand the Association for Digital Humanities in German-speaking Countries- developing recommendations that will transcend national or infrastructural boundaries. A common perspective? This poster will inform the audience about the main activities and topics covered by the ELDAH Working Group. It will present an opportunity to discuss the audiences questions and concerns regarding legal and ethical challenges of their work, to share recommendations, but also to collect valuable input on the current prevalent concerns of the DH community and to connect with working groups and initiatives on these topics beyond ELDAHs current Eurocentric scope. Topics like scholarly conduct, data protection and openness are universal themes in digital humanities research culture, and there is a growing demand for a common broader, international perspective. "
	},
	{
		"id": 343,
		"title": "A Finite-State Approach to Automatic Greek Hexameter Analysis",
		"authors": [
			"Schumann, Anne-Kathrin",
			"Beierle, Christoph",
			"Blößner, Norbert"
		],
		"body": " Abstract This paper presents a fully automatic approach to the scansion of Ancient Greek hexameter verse. In particular, we describe how finite-state automata can be used to discriminate between the 32 variants of Ancient Greek hexameter. We evaluate the performance of our annotation algorithm against hand-annotated scansion data. The project code is available online https://github.com/anetschka/greek_scansion. . Introduction Greek literature has, for centuries, served as a paradigm and model for literary writing all over Europe. The epitomes of Ancient Greek literature, the Odyssey and the Iliad, are epic poems that share the same metre: hexameter. Hexameter annotation is crucial for large-scale and data-driven investigations dedicated to these poems, and automatic annotation algorithms open up new opportunities for research in this field. Ancient Greek hexameter verses can be described as regular sequences of long and short syllables, with the length of each syllable being determined by the length of the syllables vowel. Long and short syllables are organised in feet of the following form: Dactyl: The foot is composed of three syllables, the first of which is long, while the others are short. Spondee: The foot is composed of two long syllables. Six feet make a complete hexameter. Feet 1-5 can be either spondees or dactyls, only the last foot is restricted with respect to its metric form: It is composed of a long syllable and the so-called anceps the length of which is variable. Figure 1 is a generic depiction of the resulting 32 variants of Ancient Greek hexameter. Due to the free flow of either dactyls or spondees, hexameter can accommodate varying syllable countsand produce a broad range of rhythmic effects. Section 2 of this paper provides an overview of related work. Section 3 describes the annotation algorithm. Section 4 gives the evaluation results and section 5 concludes this paper. Figure 1. Generic hexameter scheme. Vertical bars separate feet. Horizontal bars indicate long syllables, bows indicate short syllables. Vertical stacks of symbols indicate that both variants are possible. X marks the anceps. Related Work An early, rule-based approach to the semi-automatic scansion of Greek hexameter has been developed by Höflmeier. Höflmeier combines two different kinds of knowledge to resolve hexameter verses: Local linguistic rules that establish which vowels are short, and which vowels are long. Knowledge about the overall structure of the verse for the resolution of partially annotated verses. The approach is semi-automatic since the resolution of verses that possibly exhibit complex linguistic phenomena such as synizesisis delegated to the user. A similar approach has later been proposed by Pavese and Boschetti. An advanced study in the automatic scansion of metric poetry is the work by Greene, Bodrumlu, and Knightwho use weighted finite-state transducers, trained on a small corpus of manually annotated data, to analyse Shakespearean sonnets. The authors report accuracy values of up to 81.4 %. An interesting approach to the problem of Greek hexameter scansion is presented by Papakitsos. Papakitsos performs syllabification and then employs a search strategy to identify dactyls, that is, the verses are not analysed left-to-right. Rather, the search starts in the fifth foot where dactyls are particularly likely. Once the appropriate – for the established number of syllables in the verse – number of dactyls has been identified, the search terminates. Dactyls are, again, identified by means of local linguistic rules. The search, however, is strongly dependent on the correctness of the syllabification. For instance, if the verse under analysis has been found to consist of 13 syllables, the search algorithm will look for exactly one dactyl. Papakitsos reports a recall of 0.98 and a precision of 0.80. A rule-based implementation of a fully automatic Greek hexameter scansion algorithm has been published by Hope Ranker https://github.com/epilanthanomai/hexameter. . This algorithm uses an ensemble of weighted finite-state transducers to resolve the feet one by one. Alternative approaches to the automatic analysis of metric poetry employ machine learning. In these studies, the problem is usually modelled as syllable-wise classification. For instance, Estes and Henchemploy a Conditional Random Fields classifier to analyse Middle High German epic texts, reaching an f-measure of 0.90. Zabaletakreports on a very wide range of experiments, but achieves the best results with a combination of a sequential model and deep learning for the classification of English, Spanish, and Basque verses. N-grams, positional and length features as well as linguistic markers are used to train the models. Finite-State Approach to Hexameter Analysis Our approach to the scansion of Ancient Greek hexameter is based on the same two types of knowledge that were already used by Höflmeier: Local search: We use 5 local linguistic rules to determine whether a pair of syllables can safely be considered long. Global analysis: We exploit knowledge about the overall structure of Greek hexameter to complete partially annotated verses, that is, verses that could not fully be resolved with the help of the linguistic rules. Moreover, the local search step follows the strategy of Papakitsosin that it searches for a fixed number of spondees that result from the syllable count established during syllabification. Figure 2 shows a visual representation of our scansion algorithm. The algorithm scans epic Greek text verse by verse: Pre-processing consists mainly of lower-casing and the removal of diacritics. Moreover, we have implemented a syllabification algorithm that uses regular expressions to identify syllables and to establish the syllable count of the verse. The local search and all following steps are then handled by dedicated deterministic finite-state automata. There are specialised FSAs for verses of 13, 14, 15, and 16 syllables and a simpler FSA for all remaining cases. In the local search step, the active FSA performs a targeted search for a given number of spondees, using 5 simple linguistic rules. If enough spondees are found, the plausibility of the solution is checked. Otherwise, the verse is passed to the global analysis step. The FSAs were implemented using an existing Python library https://github.com/pytransitions/transitions . . For global analysis, we use a non-deterministic finite-state transducer. In this transducer, each syllable corresponds to a state, and alternative solutions are modelled by means of alternative paths. The FST is weighted, but since we did not have access to an appropriate training corpus, we were not able to learn transition weights from data. Instead, they were set manually following the description provided by Papakitsos. The FST was implemented using the Helsinki Finite-State Tools https://hfst.github.io/python/3.12.1/index.html. . If the plausibility check fails, the verse is passed over to error handling to compensate for potentially erroneous syllabification. Global analysis then completes the verse. The plausibility of the result is checked again. Depending on this result, the FSA will transition to its final state, that is, either success or failure . If the verse, however, passes the plausibility check immediately after the local search step, the FSA transitions directly to the success state. Figure 2. Visual representation of the scansion algorithm. Evaluation We have evaluated the performance of both our syllabification and our scansion module against hand-annotated verse data. The annotations were carried out by two advanced students of Greek philology, discrepancies and errors were clarified by means of group discussions. For syllabification evaluation, we randomly chose a set of 171 versesfrom both the Odyssey and the Iliad. For scansion evaluation, we randomly selected 346 verses from a broader range of Ancient Greek texts. Table 1 provides an overview of this data set. For syllabification, we achieved a syllable-wise accuracy of 0.98. Verse-wise accuracy reached 0.82. Scansion correctness was evaluated by means of precision, recall, and f-measure with the following results: Precision: 0.95 Recall: 1.00 F-measure: 0.98 The evaluation scripts are included in the open-source code package of our software. Table . Evaluation data. Conclusion In this paper, we have presented a fully automatic approach to the analysis of Ancient Greek hexameter text. Automatic annotation tools are crucial for data-driven investigations in Greek philology. Our algorithm integrates various kinds of linguistic knowledge into a set of finite-state automata and thus makes use of well-defined concepts in the field of computational linguistics, while remaining transparent to philologists. Our evaluation results are competitive. Future work will be dedicated to the exploitation of the resulting annotations for research in Greek philology. "
	},
	{
		"id": 344,
		"title": "Syriac Persons, Events, and Relations: A Linked Open Factoid-based Prosopography",
		"authors": [
			"Schwartz, Daniel L."
		],
		"body": " Introduction This paper explores the development of a prosopographical database for the field of Syriac studies called SPEAR: Syriac Persons, Event, and Relations. Syriac is a dialect of Aramaic used in the Near East and South and Central Asia between the 3rd and 8th centuries and continues to be used liturgically by Christians in the Middle East and India as well as expatriate communities in Europe and North America. This project employs a factoid-based approach to prosopography. Where most factoid-based prosopographies organize data in a relational database, SPEAR encodes prosopographical data from primary source texts in Text Encoding InitiativeXML using a customized schema designed to facilitate linking this propopographical data to other linked data resources and for serialization into RDF. The Srophé Application employing eXist DB ingests the TEI documents and allows for visualization and querying of data. From text to factoids SPEAR has looked for inspiration to the Prosopography of Anglo-Saxon Englandand other factoid prosopographies coming out of the Department of Digital Humanities at Kings College London. Where traditional print prosopographies distill short narrative summaries of known information about historical individuals, factoid-based prosopographies collect and tag discrete pieces of information asserted in primary source texts. The result is a text-based persons database. Encoders work text-by-text to encode relevant prosopographical material that can be sourced to that text such as name variants, genders, occupations, physical characteristics, personal relationships, and historical events. Each factoid is encoded in a unique <div> element with a TEI customization to add a @uri attribute, thus assigning a Uniform Resource Identifierto each factoid <div>. Every factoid also includes one or more <bibl> elements pointing to a Syriaca.org bibliography URI for a specific edition of the work. TEI encoding provides an XML structure and the shared semantic content of elements and attributes. Sample SPEAR factoid Linked open data framework The SPEAR data model integrate the TEI encoding of prosopographical data with the Linked Open Dataframework of Syriaca.org. Syriaca.org provides URIs and disambiguating information for entities in the field of Syriac studies: persons, places, works, and bibliography. This system of URIs facilitates the production of a data graph out of the discrete pieces of prosopographical data contained in each factoid; the data about a particular person, the references to a particular location, or the persons with a particular occupation. SPEAR uses the relationship ontology produced by Standard for Networking Ancient Prosopographieswhen possible. Modern ontologies generally do not accommodate important ancient social relationships, such as those constituted by slaveholding. Employing SNAP relationships enables SPEAR to encode the relationships present in the source base according to a standard used by related projects likely to be interested in the data produced by SPEAR. Transforming personal information gleaned from ancient textual historical sources into machine-readable and queryable data has required the development of a robust controlled vocabulary for systematically describing personal information and events. Out of a desire to use a vocabulary familiar to scholars in the field, SPEAR modified and expanded the keyword list used by the Comprehensive Bibliography on Syriac Christianity, a list that has evolved over almost fifty years to describe scholarly work in the field of Syriac studies. Though in an early stage of development, Syriaca.org has encoded the Syriac Taxonomy in TEI, assigned URIs to each term, and provided minimal hierarchical structuring. This rich encoding and RDF serialization will facilitate faceted browse and search by multiple fields. It will also allow the display of links between individual factoids and scholarly bibliography described with the same controlled vocabulary. From factoids to text SPEARs use of LOD facilitates another important aspect of the project, the ability for users to return to the primary sources from which factoids have been derived. SPEAR bibliographic references use the system of DTS:URNs developed by the Homer Multitext Project to cite specific portions of a text. A shared URN standard is also employed by the Digital Syriac Corpus, a partner project offering TEI encoded Syriac texts. Each factoid page containing a URN in the bibliography displays the portion of text corresponding to the reference. The page also includes a link to take users to the full Corpus text as well. A factoid viewed in HTML Conclusion SPEAR shows how a prosopography project can employ TEI, field-specific scholarly standards, and Linked Open Data to produce a highly structured and semantically rich database that maintains close ties to the texts from which it is derived. The work speaks to recent discussions on the need for TEI to engage more fully with semantic web technology. Future developments for this project include: encoding data from additional texts, developing the current taxonomy into a structured domain ontology for use by the field of Syriac studies more broadly, exploring different approaches to data visualization, and developing protocols for serializing this data into the RDF standards of SNAP, the Advanced Research Consortium, and CIDOC-CRM. "
	},
	{
		"id": 345,
		"title": "Where Our Responsibilities Lie: People, Method, and Digital Cultural History",
		"authors": [
			"Schwartz, Michelle",
			"Crompton, Constance"
		],
		"body": " As cultural historians, should our responsibility be to the people in our historical data set or to our methodology? Is it possible to, as they say, have it both ways, and if so, what do Digital Humanities methods offer us as we seek to responsibly represent political history? In digitizing and digitally remixing a primary source data, should we value data collection consistency or value recovering information that the original methodology could not capture? We plan to report on the data collecting practices of Lesbian and Gay Liberation in Canada, a Canadian history project. As Stan Ruecker and Stéfan Sinclair have argued, the work of the humanities consists of adding value to cultural artifacts through interpretation and analysis, but what is the most sound way to gather together the materials that will permit the creation of this added value? The best practices may lie in our collective past. It has been 10 years since Tom Scheinfeldt posted his pivotal blog post Sunset for Ideology, Sunrise for Methodology?, reminding us that that late 19th and early 20th century scholarship was dominated...by methodological refinement and disciplinary consolidation...Serious scholarship was concerned as much with organizing knowledge as it was with framing knowledge in an ideological construct. Digital humanities scholars have answered Scheinfeldts call: the field has revived the great 19th century work in bibliography, prosopography, chronology, philology and lexicography. We want to move further and respond to Elijah Meeks call to expand our explicit discussion of the methods and methodologies we use to collect and organize the data we augment through interpretation and analysis. The Lesbian and Gay Liberation in Canadaproject is comprised of 34,000 records about the people, places, events, and organizations of the gay liberation movement from the formation of the first homophile group at the countrys largest university in 1964 to the start of the AIDS crisis in 1981. The project builds on Donald McLeods monographs , Lesbian and Gay Liberation In Canada: A Selected Annotated Chronology. The base text consists of event records organized by date and then by location, with each summarizing a moment in liberation history. We have augmented the text both through digitization and the addition of data from archival sources and oral history The chronology text was encoded in TEI, to identifyevents, people, places, and dates. The TEI was then converted via XSLT into Cypher, the language that underpins Neo4j, a popular graph database. In addition to facilitating our own analysis of the data, the database underpins our node-based public history web app at lglc.ca. We are starting the second of the project: direct data augmentation. The base text that underpins our project is a chronology that was amassed primarily through archival research conducted in the periodical studies tradition. This method left gaps in the data, since periodicals tend to favour those groups newsworthy enough to be covered in the periodical press; wealthy enough to advertise in the periodical press; and situated close enough to the press to benefit from print advertisements. Sensing that there were voices on the margins, racially, socio-economically, and geographically, missing from the periodicals-only collection method, we have devised an experiment to decide on which method will best help us fill these gaps. We know that oral history has fruitfully been used in the field, so we have assigned one of three potential methods to three research assistants: The first assistant is using library authorities paired with secondary sources to create a prosopography, or collective biography, of the over 3,500 people mentioned in our chronology in order to find statistical norms within the political movement and to map activist relationships. This will help us ascertain where the likely gaps are in our periodicals-only based representation of the political movement. The second assistant is continuing with the periodicals and archives-only method, but turning to periodicals that were dedicated to documenting the concerns and activities of groups currently marginalized in our base text; The third assistant is conducting oral testimony interviews to capture events not covered in our chronology. In this short-presentation, we report back on the findings of this research into methodological best practice and will demonstrate the affordances of the alpha version of our public history site. As we are entering the augmentation phase of the project, we are keen to get feedback and best practice recommendations from the ADHO membership. "
	},
	{
		"id": 346,
		"title": "An Interactive, Multi-layer Edition Of Jacob Bernoulli's Scientific Notebook Meditationes As Part Of Bernoulli-Euler Online",
		"authors": [
			"Schweizer, Tobias Julius",
			"Alassi, Sepideh",
			"Mattmüller, Martin",
			"Rosenthaler, Lukas",
			"Harbrecht, Helmut"
		],
		"body": " Introduction The Meditationes is the scientific notebook of the mathematician Jacob I Bernoulli, a member of the Bernoulli dynasty from Basel. The notebook consists of 367 pages; more than 90% of the 287 entries deal with issues in mathematics and physics. Parts of the Meditationes have been published in six volumes over the past decades according to rather varying standards. Our project will provide a complete edition of the manuscript for the first time, providing facsimiles, transcriptions, translations, and comments. The edition is part of Bernoulli-Euler Online, a platform for early modern mathematical texts. BEOL makes texts available to historians of science and will provide tools for working with the resources available on the platform. Transcription of the Meditationes Regions and normalized text The transcription of the Meditationes is based on the digital facsimiles of the manuscript. For each entry – a Meditatio –, regions are defined on the manuscript page and transcribed individually. Regions are categorized as title regions containing the entrys number, text regions, mathematical notation regions, and mathematical figure regions. Regions can be connected by cross references. Figure 1: Part of Meditatio 165 Figure 1 shows a part of Meditatio 165. Two regions are shown. The region at the bottom of the pagecontains a cross reference sign #that reoccurs in the region in the pages margin. This means that the text in the margin belongs to a specific place in M165-07-T. When presenting the transcriptions, we offer a multi-layer graphical user interface. The diplomatic layer shows the single regions aligned with their transcriptions. When specifying a region in the GUI, the associated transcription is shown to the user. The next layer shows a text that is easier to read with the margin region directly integrated into the text of region M165-07-T. The normalized text reflects the Meditatios logical structure: it is a sequence of paragraphs that can span regions and pages. The example below shows the normalized text that combines the transcription of M165-07-T and M165-08-TN: [...] hoc est, { quia d d x . d x : : d y 3 . I n t . d y 3 , erit [...] proportional‹es› ‹?› } d d x . d x : : d y 3 . I n t . d y 3 & quia etiam [...] Regions are geometrically defined and shown as overlays. The regions can be converted to IIIF image URLs. The International Image Interoperability Frameworkdefines a URL-based syntax for accessing images, allowing for interoperability among image repositories.With IIIF, regions can be easily viewed as image files, defining their format, resolution, and even rotation. Rotation is useful when text has been added vertically in a margin, as in M165-08-TN, which is easier to read when it is rotated 270 degrees clockwise. Philological and mathematical text The transcription of the Meditationes is based on two markup systems: XML for the philological aspects of the text, and LaTeX for mathematical notation. LaTeX is the input format preferred by the specialists working on the project, and can be automatically converted into MathML if necessary. To represent both the mathematical and philological aspects of the text, LaTeX mathematical notation is embedded in the XML-based transcription. The XML transcriptions can be transformed to TEI/XML by means of XSL transformations. In some cases, the surrounding context has to be taken into account when rendering the LaTeX, e.g., if text is underlined or struck through. This can be achieved by dynamic macro insertion to avoid redundancy in the transcription source files. Figure 2: Title Text of Meditatio 54 Figure 2 shows the title text of Meditatio 54. In the manuscript, philological text and mathematical notation are not differentiated: both kinds of text are underlined. The excerpt below shows a part of the transcription source. When transforming the XML to HTML using an XSL transformation, macros are inserted dynamically if needed. In case of abc, the macro \\underline{abc} is inserted dynamically. <hi rend=underline>In <choice><abbr>ang<am>.</am></abbr><expan>ang<ex>ulo</ex></expan></choice> <formula notation=tex>abc</formula>. ductâ obliquè <lb/> ... </hi> Philological text can be converted to HTML for rendering in a web browser. The MathJax library is used to render mathematical notation. MathJax generates HTML and CSS from LaTeX or MathML, and can be instructed to render mathematical notation as underlined or struck through. When using LaTeX macros directly in the source, custom macros are used that are replaced when transforming the XML source code to HTML. This way, the usage of MathJax-specific macros can be avoided, making it possible to use other rendering environments, such as a LaTeX-based typesetting system for print output. We have found Pandoc to be a useful tool for changing the implementation of a macro. The XML transcription files are preprocessed: for each formula, the LaTeX is passed to Pandoc, which replaces custom macros with the implementation to be used in the production environment. https://pandoc.org/MANUAL.html#latex-macros Translations For each Meditatio, translations in English, German, and French are available. Translations mainly refer to the normalized text, and can be aligned with the regions by their IDs. To avoid redundancy, complex formulae are referenced rather than re-encoded. In some cases, additional macros are needed for language support. <formula notation=tex id=M151-09-M_1> \\left. \\begin{array}{lcr} &amp; &amp; BE \\\\ \\language{Trigono}{Dreieck}{triangle}{triangle} &amp; &amp; \\frac{4}{3}a \\\\ \\language{Quadrato}{Quadrat}{square}{carré} &amp; &amp; a \\\\ \\language{Pentagono}{Fünfeck}{pentagon}{pentagone} &amp; &amp; \\frac{4}{5}a \\\\ \\language{Hexagono}{Sechseck}{hexagon}{hexagone} &amp; &amp; \\frac{2}{3}a \\\\ \\language{Heptagono}{Siebeneck}{heptagon}{heptagone} &amp; &amp; \\frac{4}{7}a \\\\ \\end{array} \\ \\right\\} \\language{ erit }{abschneidet, so wird }{, one has }{, on aura } CE \\language{ vel }{ oder }{ or }{ ou } Ce\\ = \\left\\{ \\begin{array}{r} \\frac{1}{3}a \\\\ 0 \\\\ \\frac{1}{5}a \\\\ \\frac{1}{3}a \\\\ \\frac{4}{7}a \\\\ \\end{array} \\right\\} </formula> The excerpt above shows LaTeX code for a formula referenced from the translation file. Depending on the target language, the macro \\language{...}{...}{...}{...} has to be replaced with the correct argument. In case of the English translations, the first argument is taken into account, in case of German the second and so forth. This can be accomplished by using Pandoc to preprocess the XML. Integration into Bernoulli-Euler Online The transcriptions, facsimiles, translations, and comments of the Meditationes will be imported into Bernoulli-Euler Online, a web-based platform for studying early modern mathematics. The platform already contains correspondencerelating to members of the Bernoulli dynasty and Leonhard Euler. The material in BEOL has been imported from existing editions, both print and digital. For BEOL, a comprehensive index of persons and bibliographical items had to be created. This index makes it possible to search for persons and bibliographical items in a global manner, overcoming the previous divisions into different parts of the edition.BEOL will serve as a single entry point for historians of science interested in the works of members of the Bernoulli dynasty and Leonhard Euler. The BEOL platform is based on Knora, a generic framework for storing, sharing, and working on primary sources and data in the humanities. All operationsare performed through calls to the Knora API over HTTP. This API will enable clients to integrate the material available on BEOL into other platforms and vice versa. BEOL contributes to the development of Knora. Project-specific requirements are generalized so they can be implemented as generic components. These components can then be used by projects other than BEOL. "
	},
	{
		"id": 347,
		"title": "The European Literary Text Collection (ELTeC)",
		"authors": [
			"Odebrecht, Carolin",
			"Burnard, Lou",
			"Navarro Colorado, Borja",
			"Eder, Maciej",
			"Schöch, Christof"
		],
		"body": " Introduction The COST Action Distant Reading for European Literary History is a collaborative, interdisciplinary network which aims to facilitate the creation of a broader, more inclusive and better-grounded account of European literary history and cultural identity. The network consists of European researchers from different disciplines and research fields such as computational linguistics, corpus linguistics, andliterary studies. Currently, over 100 researchers from 30 different countries are working together in the Action. With the present poster, we would like to present our strategy for developing a key output of the project, the corpus which serves as an empirical basis for our project. Basic idea of ELTeC The Distant Reading Action aims to develop the resources and methods necessary to change the way European literary history is written. To that end, the Action will create a diachronic, multilingual, medium-sized open access benchmark corpus of novels from 1840-1919, called ELTeC. A working group dedicated to Scholarly Resources is collecting sets of 100 novels in at least 10 European languages. These novels are encoded according to the recommendations of the TEIand made freely available via GitHub and Zenodo. ELTeC is the basis for work taking place in the Distant Reading network on evaluating and improving methods and tools supporting text annotation and analysis for Digital Literary Studies. It also provides the materials to work on theoretical concerns and use cases from European literary history also taking place in the Distant Reading network. Hence, ELTeC is a key activity in the Distant Reading network. Corpus design The key challenge regarding the corpus design and annotation schemas for ELTeC is the need to handle the tension between valid and meaningful criteria and an operationalisation of these criteria for texts from many different cultures, a challenge typical of large-scale DH projects. The corpus design is therefore metadata-based by defining a set of parameters instead of relying on literary canons. Therefore, we integrate famous, canonical as well as forgotten, non-canonical novels. We represent the variation of production and aim to maximize the variety within each time period. The corpus design of ELTeC is similar to reference corpora designed to serve as an empirical base for different research approaches. We have put a focus on the composition of the corpus. The corpus will be balanced with regard to parameters which include language, publication date, author gender, length and reprint counts. The corpus also includes smaller varieties of a language in order to be able to consider the high variance of the vernacular languages across Europe. Cf. for further details our white papers on Canonicity and corpus design parameters in the ELTeC context and Sampling criteria for the ELTeC. Corpus annotation The ELTeC is encoded using TEI XML, customised for Distant Reading methods and tools. Our main goal has been to identify a small core set of textual features which can be readilyidentified in existing digital transcriptions, or easily and consistently provided by new transcriptions. Unlike many Digital Scholarly Editing projects, the intention is not to support a rich representation of the full complexities of an original source, but rather to represent in a consistent and economical way an agreed minimum of the textual features most relevant to Distant Reading practices. We distinguish a basic encoding, a richer TEI encodingand a linguistic encoding. A single TEI ODD is defined, from which we derive schemas and documentation for each level of annotation. The encoding scheme is formally documented using the TEI ODD system. See further and . In 2019, we have started to develop a customization for level 2, which includes linguistic annotation. At level 0, only chapters, headings, and paragraphs are distinguished. At level 1, more semantic encoding is introduced, for example distinguishing the functions of highlighting, such as to mark foreign words or emphasis. Additional markup for corrections, footnotes and some structural features is also included. Contributors to the ELTeC may choose to submit materials using either level of encoding, depending on what is most appropriate for them. Data sources, collaboration and publication We integrate already existing textual resources, whether already TEI-encoded or not, e.g. from Project Gutenberg, TextGridor the Cervantes Virtual Library. Project members also digitize and encode new texts, especially non-canonical novels, and make them available for research for the first time. As far as possible, ELTeC corpora include a digital edition of the first edition of each novel. Collaborative corpus creation, documentation and publication of ELTeC is happening on our GitHub organisation, where each language collection is stored in a single repository with all available encoding levels. For each language, a specific group of researchers is responsible for selecting and annotating the novels. However, as we are working on an open platform, the remaining members of the project as well as other colleagues are welcome to review and improve the text selection and annotation. Acknowledgements Distant Reading for European Literary Historyis a COST Action funded by the COST Association through the Horizon 2020 Framework Programme of the EU. "
	},
	{
		"id": 348,
		"title": "Modelling Text-Genetic Relationships",
		"authors": [
			"Van Hulle, Dirk",
			"Schäuble, Joshua"
		],
		"body": " The discipline of genetic criticism regards text as a dynamic rather than static object and tries to put the text back into motion, opening it to the moving constellations that presided its genesis. Consequently, such a dynamic perspective implies an equally dynamic model of representation. A traditional danger of manuscript research is that the researcher gets lost in the details of the archival material. Over the last decades, numerous editing projects explored the potential to capture text-genetic processes digitally. Most of these digital editions and archives, such as WoolfOnline, the Jane Austen Fiction Manuscripts, the Shelley-Godwin Archive, present the textual genesis in two ways. First by arranging the extant source documents in a stemmatologically established order, giving the reader an insight into the chronological document succession from a first note or draft to a first edition. Secondly, the individual documents are transcribed and critically annotated in rich detail, giving the user an insight into what each extant version looked like on paper. In addition to these features, some editions also integrate extra tools, such as a writers correspondence or an authors own reading in the form of a digital or virtually reconstructed library. What all these projects have in common is a hierarchical data model. Documents are stored in collectionswhich could well be visualized as directed rooted tree-graphs. In such a tree the edition as the overarching structure represents the root element,collections represent its child elements and the individual documents are descendants. Below the document level, the tree continues in strictly hierarchical TEI encodings that lead us down to the smallest annotated unit of a document – a phrase, a word, sometimes a letter – nested in XML brackets. Up to this point, in terms of data structures, the edition can be visualized as a single coherent tree structure. There are three common practices to describe the textual genesis against the background of this hierarchical representation of the material. Stemmatological metadata describe relationships that break this strict tree structure. The documents are arranged in a chronological sequence that might well vary from the documents physical order in the collections as represented by the tree. Yet, these genetic relationships always link elements on the same level of the trees hierarchy – the document level. They neither allow to zoom in on deeper levels of the tree nor to derive information on how text units on these finer levels of granularity are genetically interconnected. Annotating the textual genesis within individual documents, e.g. with text-genetic TEI encodings, allows us to link nodes of the treeacross the hierarchical tree structure. Additions, deletions and substitutions are assigned to groupswhich are put into sequencesin the metadata. Just like the stemmatological metadata, these structures represent genetic paths that break the hierarchy, yet in this case they do not allow us to zoom out along the tree or the stemmatological document relations. We cannot draw conclusions about how the sequential making of an individual version is connected to the textual genesis across multiple versions. Collation software such as CollateXand the upcoming HyperCollatedetects the textual variance between different text versions and models these differences in so-called variantGraphs. Collation software allows us to capture paths that represent the textual variance between the documents on the granularity level of the token – across the tree hierarchy of an XML encoding. Without stipulating any genetic interpretation, these graphs raise questions such as how was a sentence/phrase altered syntacticallybetween draft A and draft B?. The graph does not give explicit answers. Instead, it neutrally visualizes the variant and invariant text tokens between a selection of versions. Collation is limited to capturing connections between the documents, yet not on the hierarchical level of the document, but on the level of the token, which may well be smaller than any TEI annotation. Again, this approach does not allow us to zoom out. We cannot derive information from the stemmatological order, nor from the witness-specific genesis. In all three cases, additional graph-structures are annotated across to the underlying hierarchical tree, which is itself a graph. Each one of these structures provides an alternative navigation for a particular level or subtree of the work and thus each of these structures represents a different aspect of the works textual genesis. Only very few projects, such as the Faust Edition and the Beckett Digital Manuscript Project, incorporate all three structures and even those projects have not managed to merge them in a way that allows the user to seamlessly navigate over all genetic information. What is missing is a comprehensive model that allows to navigate seamlessly between the different types of genetic paths, to zoom in and out on writing processesand to connect external source texts to their use in the drafts. Ideally, we should be able to implement this model in an easily accessible and extensible research environment, allowing the scholar to capture, organize, visualize and analyze genetic paths of all described types. Building on the system of genetic paths as developed in HyperLearnthe proposed paper presents a digital way of modelling text-genetic relationships in an eXist-db based research environment for genetic criticism, henceforth referred to as a Manuscript Web. Such an MW in the form of a customizable web application allows textual scholars to organize their document-collections consisting of facsimiles, TEI transcripts and bibliographical metadata, in four different module types:virtual libraries,collections of notes,drafts andpublished editions. A Manuscript Web also starts from a project tree, but unlike the projects described above it enables users not only to capture genetic paths across the given tree hierarchy, but to search the respective modules to which they belong and to store relationshipsbetween all identifiable elements/hierarchical levels of the project tree. The model thus enables users to zoom in and out between macro- and micro-genetic levels, as well as between exo- and endogenesis. For example, to link endo- with exogenesis, a scholar may connect an entire section of a TEI-encoded notebook to an identified source in the authors virtual library to indicate that this notebook section contains reading notes from the related source. On a microgenetic level, individual notes from this section may be linked to paragraphs, phrases or interlinear additions in a manuscript draft. On a higher level in the hierarchy level, this draft may be linked to the following draft in the stemmatological sequence. Such one-to-one relations can be captured individually and regardless of the granularity level. Where the source- and target-references of independent relations overlap, they form paths and genetic graphs across the corpus. Since all these graphs refer to elements of the underlying project tree, this tree provides a navigational backbone that allows the user to zoom in and out on the genetic information. From any document within the environment the user can access all genetically related entities to answer questions such as what does this particular paragraph look like in the next draft? or which literary sources inspired this paragraph?. The aim of the proposed model is to enable users to connect what is usually merely juxtaposed. Most digital archives and scholarly editions offer the traces of a works genesis as digitized items, side by side. What this paper proposes is a way to enable not only scholarly editors, but also users to discover and record the connections between these textual traces. The ability to record these connections facilitates a more comprehensive understanding of a works genesis. Put[ting] the text back into motion, as Contat et al. described it, implies a dynamic model that allows users to turn the different genetic traces or stills – so to speak – into the motion picture of the genesis. With the proposed model, zooming in on the smallest level of textual change no longer entails the danger of getting lost in the labyrinth of the digital archive thanks to the possibility to zoom out again at every stage in the enquiry and see the bigger picture. Finally, the paper shows how this dynamic model facilitates not only research into one single works genesis, but also comparative genetic criticism of several authors works. Up till now, comparative studies have been relatively rare in the field of genetic criticism, because every authors writing method is characterized by idiosyncrasies. By modelling the text-genetic data in such a way that they become more comparable, the proposed model will contribute to the development of comparative genetic criticism. "
	},
	{
		"id": 349,
		"title": "A Paper Full of Things. Quantitative and Qualitative Approaches to Early Modern Newspaper Advertisements.",
		"authors": [
			"Serif, Ina",
			"Reimann, Anna"
		],
		"body": " At the end of the seventeenth century, so-called intelligence newspapers emerged in big European cities like Paris or London, springing up all over the continent in the course of the eighteenth century. Instead of political news, they contained mainly classified ads with the purpose of connecting people offering and people seeking something. These advertisements covered a wide scope, like real estate, work and travel opportunities, specific services, information, as well as all kinds of things: things for rent and things for sale, lost and found items, second-hand goods, newly invented or well-known medical products, and imported goods like coffee and tea, to name just a few examples. Many of these ads were placed by non- or semi-professional sellers, but also professional suppliers such as craftsmen, traders and shops used this new, but soon well-established, communication platform to inform the reading public about their products and to find customers. Therefore, these intelligence newspapers are, on the one hand, a particularly interesting source for examining the micromechanics of local markets, on the other, helpful for analyzing connections between local, transregional and increasingly global markets of goods in the early modern period – and they are also quite unmanageable for a singleresearch endeavor with regards to the sheer mass of data. Our newly started project takes one particular intelligencer, the Basler Avis-Blatt, as a case study for the application of different digital tools and methods to this source type. Up to now, intelligence newspapers have not been taken into account as a whole beyond text recognition, and even then mostly in small samples. This is due to the overwhelming amount and diversity of the different ads, as the intelligencers appeared periodically for years or even decades. The here presented source, which is preserved in its entirety, appeared weekly, later even daily, between 1729 and 1844 in the city of Basel; this sums up to 6391 issues with about 50 000 pages and over 750 000 single ads. By using computational methods and digital tools, we want to facilitate an extensive and comprehensive analysis of intelligence newspapers, combining quantitative with qualitative approaches. After having built a digital collection, using IIIF mechanisms for presentation and annotation, the whole corpus was made available as full text, and the single pages have then been enhanced with page xml after automated layout recognition/page segmentation. Every page is segmented into single ads, matching text with layout units. In doing so, data and text mining can now focus on the smallest and most important entity of the corpus: the ad itself – a major step compared to the analysis of unsegmented full text. To generate structured data for the development of a comprehensive database for further analysis, the segmented ads are classified into ad typesand content types. So far, the classification has been done manually, which already allows first quantitative outcomes, pointing towards content-related research questions; after having established a ground truth of classified ads, supervised machine learning will be used to test automated or semi-automated ad classification. Unsupervised machine learning will serve as a sensor for slipped patterns and as a corrective measure to question the constructed annotation types and to evaluate categories. The procedure of cascading classification makes it possible to handle the advertisements with different areas of interest and to make a preselection of those that are to be classified further, following different specialized research questions – e.g. the category animals, adding dog, duck, or donkey; as the classification process of the ads is nearly infinite, collaboration during the project as well as after its completion is a central aspect. With the aim of publishing the digital collection and the underlyingtext, and also making the database accessible, the intelligence newspaper in question will be made available for other researchers and a variety of possible research questions. The proposed poster will present the different approaches to handle and evaluate the large source and the deriving data sets. It shows workflows established in the initial stage of the project, first results obtained with text mining, and impressions of the advantages of combining a variety of digital tools and computational methods with content-related questions in the analysis of early modern newspapers. It also presents perspectives on further research possibilities and research questions emerging from the project. "
	},
	{
		"id": 350,
		"title": "Al-Ṯurayyā, the Gazetteer and the Geospatial Model of the Early Islamic World",
		"authors": [
			"Seydi, Masoumeh",
			"Romanov, Maxim"
		],
		"body": " Information about places whose locations are not easy to identify with certainty and whose names may vary because of cultural and historical contexts are of great importance to historians. Places of cultural meaning or administrative units meet the needs of historians, rather than physiographic landforms on which many existing digital gazetteers and data models focus. Al-Ṯurayyā The name al-Ṯurayyā, Pleiades in Arabic, is a tribute to Pleiades Gazetteer, which was the main source of inspiration at the early stage of development of this project. The previous version is developed at Tufts University. provides an extensive gazetteer of the early Islamic Empire with over 2,000 toponyms and almost as many route sections from Georgette Cornus AtlasIt covers from western provinces in Spain and North Africa to Eastern province, Sind. —where the primary attribute of collected objects are their geographical coordinates and their place in the Empires administrative hierarchy. Beyond the gazetteer, al-Ṯurayyā implements a spatial model that visualizes settlements, routes, itineraries, regions, and networks; additionally, it can perform specific queries that are meant to help to analyze specific historical events and phenomena through resulting visualizations. Following the idea of Linked Data, al-Ṯurayyā is designed to be connected to the primary and external sources. Its data model takes into account connectivity, spatial relations, and the additional evidence provided by the historical context from the primary sources. Al-Ṯurayyā can be used with other data sets of the same structure thus acting as a gazetteer and a geospatial model for different historical contexts. The minimum components of the gazetteer entry is defined by:settlements—currently with toponyms in English transliteration and Arabic;geographic location—latitude, longitude;administrative classification of settlements— metropol, town, village, etc., which present the administrative organization of the Empire. Places: The gazetteer provides a search panel to find and visualize a place and access the relevant information that the data model provides. As in Figure 1 , searching for Baghdad in both Arabic and transliterated Latin characters yields a list of matches and highlights the position of a selected match in red on the map. The type of a settlement is represented in the visualization of places by different sizes of circles, which are assigned according to their type. According to al-Ṯurayyā data model, the toponymic data is linked to contextual information including the records from primary sources The current version provides the descriptions only from: Abū ʿAbd Allãh al-Ḥimyarī. Rawḍ al-miʿṭār fī ḫabar al-aqṭār. Ed. by Iḥsān ʿAbbās. 2nd edition. Bayrūt, 1980 . and links to secondary sources, which is shown together with the technical details when a toponym is selected. The descriptions from the primary sources are matched automatically and the percentage value in parenthesis next to each record indicates the match certainty. Figure 1. Toponym search Figure 2. Technical information on a selected place—coordinates, URI, region, sources, type, names, etc. Figure 3. List of records from primary sources relevant to a selected toponym Figure 4. Detailed description of a selected toponym from primary sources Figure 5. References to external sources Routes and Itineraries: The spatial model of al-Ṯurayyā currently offers two main modules that compute and visualize routes and itineraries of various complexity, using pathfinding algorithms, and networks of reachability from the selected center, using the network on of settlements places and routes. Figure 6 - Selected route section information Relevant features of the geospatial model are as follows: Pathfinding: The model computes paths between a source and a destination, the shortest and optimal routes. The shortest path implements Dijkstras algorithmwhile the optimal path computes the next shortest path with the higher number of waystations along the way. For example, in the shortest pathin Figure 7 , the sparsity of waystations makes the path dangerous across the Syrian desert while the optimal one—including higher number of stations—leads around the desert through the populated regions. Itineraries: One can plot an itinerary by selecting stopsalong the way to map more specific routes. This feature customizes the pathfinding computation model by considering the places that should be included in the itinerary. Figure 8 plots Nāṣer-e s Khosraws itinerary from Nishapur/Naysābūr to Cairo/Fusṭāṭ, as described in h is own travelogue titled The Book of Travel. This model suggests not mentioned locations that he might have visited. Network Flood: To represent reachability, al-Ṯurayyā introduces a method to model the network of settlements that are reachable from a starting point within a certain number of days of travel. Network flooding shows the reachability as well as limitations of the reach from a selected center, which in historical contexts are of great value to find answer for questions related to spread of power, explore the viable geographical limits of a state with the seat of power in a given center, and visually measure the prominence of specific urban centers. In the context of this model, we represent network flood by coloring the places on the map based on their distance from one/multiple centerthat the user dynamically chooses. Each color represent a network of places reachable within the same dayof travel. For example, Figure 9 depicts Marw al-Šāhiǧāns network within ten days in which locations in red, orange, yellow, and green are reachable within ten, twenty, thirty, and fifty days respectively and places in pale colors are unreachable according to the underlying route network and criteria. Network of multiple centers represent the same reachability concept from multiple centers, which can be used to represent itinerary courts. Figure 7. Shortestand optimalpaths from ‌Baġdād to DimašqFigure 8. Plotting the itinerary from Naysābūr to Fusṭāṭ from Nāṣer-e Khosraws The Book of Travel Regions: Al-Ṯurayyā models and visualizes regions, using the underlying network. The initial view makes the complete view of the whole area that the data covers and the overlap of the colored points properly shows the shape, density, and the extent of each province in the period in question, avoiding the modern idea of borderlines. In the Regions panel, the visualization highlights the selected region depicting its geographical position and administrative extent with all its settlements and routes. Figure 9 - Network flood wit h Marw al-Šāhiǧān as a center Figure 10. Network flood of two centers Figure 11. Modeling a region Al-Ṯurayyā is designed to serve as a starting point for the visual analysis of spatial data in written documents, and as a tool for answering meaningful, complex research questions about how the geography of premodern empires was shaped and conceptualized. For future developments we are planning to provide tools for data verification and contextualization. "
	},
	{
		"id": 351,
		"title": "Modelling Poetic Similarity: A Comparative Study of W. B. Yeats and the English Romantic Poets",
		"authors": [
			"Shang, Wenyi",
			"Zhang, Jingzhou",
			"Huang, Win-bin"
		],
		"body": " Introduction Observing poetic similarity is fundamental for identifying interrelationships and poetic influences among authors. When investigating poetic similarity, intertextuality is always regarded as the most significant factor, and it is also an index that can be calculated computationally. In terms of verse, homogeneous formal elements like rhyme, the basic rhythmic structure, and meteramong poems can also act as crucial indicators of poetic similarity. The goal of this research is to design a framework to quantitatively measure poetic similarity with digital methods, which can dig into a vast number of data and suggest the interrelationship of different authors works. This research focuses on the world-renounced Irish poet William Butler Yeats. The complex poetic influence he received deserves scrutinized investigations and the possible influence of the English Romantic poets is examined here. In this study, influence refers to the shaping power of a precursor poet on a later poets poetic style and poetic genre, which can be traced and observed. Specifically, a model is constructed to compare Yeats and the English Romantic poets, exploring their similarities in three aspects:intertextuality;formal elements, including rhyme, meter, and enjambment;sentiment. Methods Fig. 1. Framework of model As Fig. 1 shows, the model includes a preprocessing stage and four parallel quantifications: intertextuality calculation, rhyme and meter detection, enjambment calculation and sentiment analysis. The raw data are crawled from the Bartleby collection, including 130 works of Yeats from three collections, 216 works of Blake, 119 works of Byron, 53 works of Keats and 469 works of Wordsworth. For intertextuality, after tokenization, lemmatization, and filtering stop words, considering the remaining lists of words generated from Yeatss works as target and those generated from the English Romantic poets as source, we compare them in turn in the unit of phrases, a segment of text demarcated by a semicolon or a colon. All source-target phrase-pairs that share at least two distinct words are recorded. Next, each recorded phrase-pair is weighted according to the following formula: Here, fand fare the frequency of each matching word in its target and source phrase divided by the length of the phrase respectively, and dt and ds are the distance of the farthest matching wordin their target and source phrase. Phrase-pairs with words of lower frequency and those with closer distance are privileged because these indicate stronger possibility of intertextuality, which is set as the summation of every phrase-pairs score within them divided by the product of their lengths. Finally, the rate of intertextuality of Yeats and each English Romantic poet is defined as the average value of each verse-pair with non-zero value. For formal elements, CMU Pronouncing Dictionary is exploited to identify syllables, stresses and rhyme words. The results of the identification are recorded as strings and are compared to every standard rhyme type and meter style to calculate their Levenshtein distances, and the rhyme and meter types of the target verse are guessed and defined accordingly. For enjambment, after line segmentation, the proportion of enjambments is calculated as the number of the lines divided by the number of the lines that contain ,/./!/? immediately before the line breaks. For sentiment analysis, Python library TextBlobis used to define the emotional tendency of the verses. Each verse is inputted into the system, and a parameter ranging from -1to 1is outputted. The emotional tendency of each poet is calculated as the average of that parameter in all of his works. Results Table 1. Rates of intertextuality of Yeats and the English Romantic poets Table 1 shows that the rate of intertextuality between Yeats and Blake is the highest. The results of significance test show that the difference between the rate of intertextuality of Yeats-Blake and Yeats-any other Romantic poet is statistically significant at a significance level of 10-6, which shows that the intertextuality between Yeats and Blake is remarkably higher than those between Yeats and the other English Romantic poets. Since a higher rate of intertextuality shows a stylistic rather than generic imitation, the results indicate that Blake may have exerted a stronger influence on Yeatss poetry than the other poets studied. Table 2. Formal elements of Yeats and the English Romantic poetsTable 2 shows that the distribution of rhyme types in Wordsworths verses is relatively the most similar to that of Yeats, and he also has the closest proportion of enjambment with Yeats. In terms of meter style, Yeats has a very similar distribution with Blake. Table 3. Sentiment of Yeatsand the English Romantic poets *The abbreviations stand for names of the collections: The Wind among the Reeds, Responsibilities and Other Poems, The Wild Swans at Coole, respectively. After the sentiment analysis, two major findings are observed from Table 3:Yeats has the smallest value, while Blake has the second smallest;Sentiment parameters of Yeatss different collections ascend in a chronological order. Conclusion This research successfully builds a model to quantitatively measure poetic similarity. The results show that Blake, among the English Romantic poets, is the most similar to Yeats both in terms of intertextuality and sentiment. With regard to formal elements, Yeats resembles both Blake and Wordsworth. This studys possible contribution to Yeats scholarship is to quantitatively measure and prove the prominent influence of Blake on Yeats poetry, and concretely shows Yeats relationship with such movements as Romanticism. Furthermore, the framework designed by this research can be applied to investigate poetic similarity or intertextuality among other poets or poems, thus making contribution to literary studies in general. We believe that by the means of investigating massive data of poetic similarity, the influence of chanciness in literary interpretation can be substantially weakened. Digital methods can serve as powerful tools to detect latent literary attributes, raising significant topics that can inspire further studies. "
	},
	{
		"id": 352,
		"title": "Constructing A New Science Framework In Japanese Historical Studies Through Digital Infrastructure",
		"authors": [
			"Shibutani, Ayako",
			"Goto, Makoto"
		],
		"body": " We are developing a new digital infrastructure to serve as a comprehensive digital network of Japanese historical resources. Using the system, we are constructing a new science framework for Japanese historical studies. This paper introduces our system, which is called Knowledgebase of Historical Resources in Institutes. As one of the khirins prospects, we show that disseminating scientific information of historical resources can promote advanced collaboration between relevant Japanese and international research institutes. The English version of the khirin is also launched from April in 2019. The khirin uses linked data and the International Image Interoperability Framework. Metadata are described in each data source to connect relevant data with Resource Description Frameworkin khirin. Using these links, the complexity of plural catalogues can be absorbed. Assigning a URL to each resource enables access to original resources so diverse data such as fact data and resources can be connected at the same time. Through a SPARQL endpoint, the system allows one to connect resources with anyone worldwide. Followed by the khirin, we are constructing a scientific information platform to support storage and management of metadata using RDF, IIIF, institutional repositories, and Digital Object Identifiersystems. Regarding studies of historical paper materials, scientific data contains morphological features such as the thickness, weight, and sunomeof papers, the formation of fibres, and the tenryoused in paper making. Surveys of paper materials are essentially non-destructive; paper surfaces are observed with the transmitted and reflected lights of microscopes. Some researchers have already examined numerous historical paper materials; however, other researchers do not have easy access to numerical data that can be utilised in comparison processes. In addition, the preliminary morphological analyses in palaeography are carried out simply with visual examination by specialised researchers without experience in scientific approaches. This type of study has very low reproducibility. In our study, applying scientific resource data to the khirin enables researchers to access original data conveniently when searching for primary evidence. Here, we use a historical paper document, Oda Nobunaga Shuinjo, as an example. The document shows that KameiKorenori might be qualified to be an owner of the Izumo province following his superior military valour on 7 September 1581. When we search this document in the khirin, keywords to relevant materials regarding Oda Nobunaga are provided in the search results. On the detailed screen, the catalogue information for Oda Nobunagas materials provides links for accessing detailed scientific data. These links enable users to access other relevant documents that relate to the document Oda Nobunaga Shuinjo. If a link destination provides datasets of paper components, we can search for other materials with similar components. As the access path to data resources regarding historical paper materials, making use of the diverse information in the khirin enables users to access a wide variety of resource information. We can thus construct a new theoretical framework to use in advanced studies. Our study is also standardizing the scientific methodology of historical materials research by conducting the following approaches: qualitative analysis of historical papers focusing on paper component details to compare our findings with the classifications granted to historical materials in previous studies; and reconstruction of papermaking methods from various production/consumption areas and time periods using DNA biomarker analysis. Objective classification and quantification of the morphological features of paper materials are required to interpret their origins. In addition, large datasets can improve the development of scientific studies of historical materials, providing a reliable dataset that may benefit researchers worldwide. We are examining a large number of digital images such as microscopic photos and catalogue information images. Through these results, our study can promote the advanced application of utilizing a database to cross-search historical research data and relate it to other historical paper materials in local, national, and international institutes. The khirin system can access to original resources that follow other relevant resources from other institutes, together with their diverse datasets of scientific information such as morphological features and material components. Making this association between one resource and others is a way of using Linked Data. It enables worldwide researchers to access to primary evidence. Showing some examples, our poster shows how other projects and initiatives will be more advanced by our scientific framework with the khirin. Search results are also described with relevant information and materials in other institutes to our resource information and other collections. "
	},
	{
		"id": 353,
		"title": "Grounding Paradigmatic Shifts In Newspaper Reporting In Big Data. Analysing Journalism History By Using Transparent Automatic Genre Classification.",
		"authors": [
			"Smeenk, Kim",
			"Bilgin, Aysenur",
			"Klaver, Tom",
			"Tjong Kim Sang, Erik",
			"Hollink, Laura",
			"van Ossenbruggen, Jacco",
			"Harbers, Frank",
			"Broersma, Marcel"
		],
		"body": " Introduction This paper shows how the systematic and quantitative study of genre in large digitized newspaper collections sheds light on the development of journalism discourse. We argue that genre conventions that can be discerned in a newspaper text signal the underlying discursive norms and practices of journalism as a profession. However, digital newspaper archives do not contain fine-grained genre information on the article level. As such, this paper adopts a machine learning approach to add genre labels to newspaper articles. Classifying genre in a standardized and reliable manner is challenging, though, because genre is a typical example of a latent content category, which needs considerable interpretation. To ensure the reliability of the results and to evaluate the machine learning approach, it is crucial to make the methodological impact of various machine learning pipelines transparent. This paper therefore discusses how the distribution of news genres developed over time and how transparent classification empowers journalism history researchers to benefit from the computational methods for doing large-scale content analysis. Genre and journalistic discourse The grand narrative of journalism history, prevalent in journalism historiography, claims that journalism from the end of the 19 th century has moved away from partisan journalism and opinion-oriented reporting practices, towards critical and autonomous journalism that is event-centred and fact-based. For the Netherlands specifically, this journalistic development has taken place between the 1950s and the 1990s as part of the depillarization of society. Since genre connects textual features to journalisms underlying discursive norms and routines, a historical analysis of newspaper genres can elucidate how journalisms professional practice has developed over time. We focus particularly on modal genres, which are defined based according to their formal features, e.g. an interview. This definition has been dominant in how journalists and academics have defined journalistic genres in the Dutch context. We have formulated 16 classes of journalistic genres. Figure 1 : The distribution of 16 news genres used in this study in a selection of newspaper articles of the Dutch newspaper NRC. Genres of interest to our study are represented by bright colors while other genres are colored grey. The large difference in genre frequencies caused the machine learner to focus on predicting frequent genres correctly while ignoring the others. In order to avoid this problem, we use a balanced training set in which all genres occur equally often Transparent automatic genre classification Classifying the genres of newspaper articles retrieved from digital newspaper archives raises multiple conceptual and methodological challenges. Firstly, genres are prototypical constructs: some articles do not necessarily match the characteristics of the genre perfectly, nor can they always be neatly delineated from other genres. Furthermore, the distribution of genres over specific topics is skewed, e.g. reports are often about sport matches and interviews about human interest. However, topic is not the defining feature of modal genres and it may introduce unwanted bias in the automatic classification process. To make an informed decision on the right pipeline and understand its inherent biases, we have developed a dashboard that allows the scholar to explore the underlying decision-making process of the machine learning pipeline. On this platform, transparency is achieved through data visualisations that show the performance of the classifier per genre, by offering article-level and classifier-level explanations for the performance, as well as by providing comparison between various machine learning pipelines. The workflow The platform supports data preprocessing and feature extraction, pipeline configuration, training and testing of the pipeline, pipeline evaluation and comparison, and hypothesis testing. Regarding the data pre-processing, we remove quotes from the documents using regular expressions, to avoid confusion between practices of sources and journalists. For the feature extraction in order to represent the data, we consider three methods: Bag-of-wordsfeatures, manually curated linguistic features, such as the number of sentences and the number of first person pronouns, and pooled features combining word features with linguistic features. As for pipeline configuration and training, the platform currently supports 7 machine learning algorithms: GradientBoost, LightGBM, Multi-Layer Perceptron, Naïve Bayes, Random Forest, Support Vector Machines and XGBoost. Pipeline evaluation The machine learning pipelines are trained on a balanced data set containing 960 annotated articles from 9 different newspapers evenly spread over time, and across topic domains. Through an iterative process, which involves comparing pipelines that use different pre-processing settings in addition to various machine learning algorithms and assessing their underlying decision-making criteria given by interpretability tools, the best pipeline is chosen to perform the genre classification. We measure the performance by commonly used classification metrics such as precision, recall, and accuracy. The accuracy scores vary between 0.41 and 0.70. This paper shows, however, that choosing a pipeline based on accuracy score alone does not necessarily result in choosing the best pipeline for our research purpose. Not all genres are equally difficult for a machine learning algorithm to distinguish. Service messages, such as television guides or stock market information, are particularly easy, for example. Yet, these are not the genres we are most interested in for our research questions, since these do not reflect the developing journalistic practices we are interested in. Pipelines that score higher accuracy scores because they are outperforming others on classifying these less relevant genres, are thus not the best choice for our purpose. More relevant are pipelines that are performing well on the genres we are most interested in. We thus need to evaluate the pipelines beyond their accuracy scores. Furthermore, looking into the explanations, we noticed that some pipelines tend to do topic classification instead of genre classification, as can be observed by the recurrence of topic-related words in the feature importance rankings for some genres. The predictive features for the decisions of other pipelines, however, do line up more closely with how genres are defined in journalism studies: first personal pronounsbeing predictive for an article to be a column, for example. The pipelines that combine a bag-of-words approach with manually curated linguistic features give the most promising results. They are able to identify both distinctive words for certain genres - such as to be continuedfor literary fiction - and the relevance of linguistic features - such as the number of adjectives which can be predictive for the reportage. Furthermore, these pipelines can recognize words that point at the direction of relevantfeatures that had not been considered yet, contributing to the understanding of genre in journalism history by uncovering patterns that were not known before. Based on the pipeline evaluation and comparison, the preferred pipeline for our studies is chosen. Figure 2 : Local feature explanation by LIME. On the left it shows the probability that the article is a particular genre. On the right it shows the bag of word and curated linguistic features of this article that have been most predictive in classifying it as column, and which have been suggesting that it is not a column. The features, especially the first personal pronouns, meand mij, line up with the journalism studies understanding of column as an article that is focused on the personal impressions of the author Application of transparent automatic genre classification to journalism history In order to gain more insight into the development of genre in journalism history, we aim to gain insight in how the machine learning pipelines output compares to the distribution of the manually coded golden standard data. In other words, the goal is to observe if the machine learner can reproduce the genre distribution for the years for which we have gold standard data. From previous research, we have inherited a data set with approximately 10.000 manually annotated articles for two constructed weeks in 1965 and in 1985 for four newspapers: Algemeen Handelsblad/ NRC Handelsblad, De Telegraaf and De Volkskrant. This data set is used as a golden standard for the distribution of genres in 1965 and 1985. The study compares the distribution of genres in this data set to the distribution of machine-labelled data. In this way, we test the trustworthiness of the results: does the distribution of the machine-labelled data correspond to the distribution noted in the gold standard data? The future work for this study is to apply the most trusted machine learning pipeline to large-scale unlabelled data to comprehend the development of genre distribution between 1950 and 1995. The results of such work will shed light on the shift from opinion-oriented to fact-centred news in Dutch journalism, marking advances in Dutch media history. "
	},
	{
		"id": 354,
		"title": "Modelling Conflicts Between Characters in Present-Day Dutch Literary Fiction.",
		"authors": [
			"Smeets, Roel",
			"Sanders, Eric",
			"van den Bosch, Antal"
		],
		"body": " Critiques of literary representation Literary studies has a long tradition of analysing texts from an ideological perspective. Inspired by feminist, postcolonialand Marxiststrands of thinking, these so called critiques of literary representation have been focusing on hierarchies between genders, ethnicities, and classes in literary texts. One way in which these hierarchies can be traced is through comparatively analysing representations of characters with different demographic backgrounds. For the field of Dutch literature, a diverse range of detailed close readings have been conducted analysing the relative importance of certain represented identities as opposed to others. In recent years, quantitative methods such as social network analysis have made it possible to study character representation on a larger scale. Insights from e.g. network theory can lead to a broader understanding of the power dynamics between characters. Important aspects of these dynamics are positiveand negativerelations between characters, as bonds and conflicts in networks are indicative of hierarchical oppositions between represented identities. In order to gain an empirically informed understanding of character hierarchies in present-day Dutch literary fiction, the present paper models conflicts for all 2137 characters in a corpus of 170 novels that were submitted to one yearof the Libris Literatuurprijs, one of the most prestigious literary prizes in the Dutch language area. It draws on extensive metadata from earlier research in which gender, descent, age, education and profession of all these characters were gathered, as well on more recent research in which relational informationbetween these characters was collected. Methodological design Social networks for each of the 170 novels are semi-automatically extracted using the co-occurrence approach described in Smeets & Sanders 2018. These networks are used to model conflicts in two ways, the first of which focuses on conflicts between two characters, the second on conflicts between three characters. Conflict scores In earlier research, all characters were ranked with Pythons NetworkX libraryfor five basic network centrality metrics: degree, betweenness, closeness, eigenvector, and Katz. Each of these rankings are an indication of a certain aspect of a characters relative importance in the story. For every dyad of enemies in the corpus, it is detected who the higher ranked character is. For each of the five centrality metrics, a characters conflict score is incremented by 1 in case he/she is higher ranked than his enemy. Finally, a multiple linear regression analysis is carried out to test the extent to which a characters gender, descent, age or education is a predictor of his/her conflict score. The outcome of the regression analysis serves as an indicator of which represented identities are the more powerful ones in the conflict. Socialbalance The social balance theorypostulates that there is social balance in a triad when either all three nodes are friends, or when two friends share the same enemy. Conversely, it postulates that there is social imbalance when all three nodes are enemies, or when two enemies share the same friend. This is used as a theoretical framework for modelling conflict dynamics between subnetworks of three characters in the corpus. For every enemy/friend triad, it is automatically established whether it is socially balanced or imbalanced according to the theory. It turns out that the majority of triads, 69%, is socially balanced as opposed to 31% of socially imbalanced triads. Among these two general categories of social balance and imbalance, fully positive and fully negative triads are most present. In light of authoritative narratological theories, the prevalence of social balance is remarkable, as conflict is commonly esteemed to be one of the driving forces behind narrative action. For the analysis of conflicts in individual novels, this observed pattern can be used as a general framework to contextualise and evaluate the particularity ofbalanced triadic subnetworks. One such a contextualisation will be demonstrated by evaluating a single triad in light of the overall pattern. Figure 1. Absolute distribution of socialbalance for all enemy/friend-triads in the corpus divided by typeContribution to the field In this paper the two models of conflict will be used to disentangle the complexities of power dynamics in character representation. We will assess the possibilities and challenges of our approach for critiques of literary representation that mainly use qualitative close reading methods. It will be argued that conflict situations co-shape the ideological representation of characters in literature, and the importance of a data-driven and empirically informed approach to character representation will be highlighted. . Keywords: conflict, social network analysis, Digital Literary Studies, Dutch literature "
	},
	{
		"id": 355,
		"title": "Clearing the Air for Maintenance and Repair: Strategies, Experiences, Full Disclosure",
		"authors": [
			"Smithies, James",
			"Ciula, Arianna",
			"Otis, Jessica",
			"Cheslack-Postava, Faolan",
			"Holmes, Martin",
			"Arneil, Stewart",
			"Newton, Greg",
			"Mulliken, Jasmine"
		],
		"body": " Introduction The digital humanitiescommunity has reached an inflection point. Conceptual issues related to DH are now routinely discussed, and a significant body of literature about DH tools and methods exists, but very little is said about the challenges of maintaining the projects and tools that result from DH activity. The truth is that many teams are struggling with decades of accumulated technical debt, and the natural process of technological entropy. Lack of openness about the problems of sustaining and, when appropriate, archiving DH projects is natural: our problems expose weaknesses at the heart of our community, make us feel insecure when comparing ourselves to other teams, prompt us to question our relationship with funding agencies, and raise questions about the sustainability of our core activities. But we need to discuss the issues so we can increase our understanding, share best practices, and advocate for change. Long-term technical maintenance can be daunting, but good planning, carefully considered processes, transparent and healthy relationships with administrators and IT departments, and some common sense can resolve most issues. Our panel brings together four DH software engineering teams and initiatives, based in the United Kingdom, United States, and Canada, with responsibility for over 350 projects built over two decades. The panel aims to clear the air, by openly discussing the problems we face and detailing the security, maintenance, archiving, and sustainability solutions we have put in place to resolve them. The wide range of issues across the international DH community mean that we can only initiate a conversation, but we hope our commitment to full disclosure, coupled with a degree of geographical breadth, will help set the tone for a new era of collaboration and information sharing. Our long-term goal is to foster a culture of sustainability across the DH community, and respect for those engaged in the essential work of maintenance and repair. Our four panelists will describe the work of two DH centres and two archiving and sustainability projects, detailing the wide range of approaches they have adopted to manage technical but also operationaland financial complexities. Common themes are apparent across the four teams, related to forward planning and consideration of application and data life-cycle management, but differences exist too. Sometimes differences relate to technical philosophy; sometimes differences relate to the exigencies of local funding and operational realities. In all cases, however, our technical development is informed by a firm belief – resulting from hard experience – that maintenance and repair are integral to the art of making. Our panel members believe the next phase in the evolution of DH will require greater attention to the practical, epistemological, and methodological imperatives of maintenance and repair. Paper One: We Need to Maintain 100 Projects, Without Funding? Really? A Pragmatic Approach to Archiving and Maintenance Kings Digital Labwas launched in November 2015. It was established to improve Digital Humanitiessoftware engineering process and quality, and ensure digital research is scalable and sustainable. It works in partnership with a DH department, and other departments across the Faculty of Arts & Humanities at Kings College London, and comprises a Director, Project Manager, Systems Manager, and a team of 10 analysts, UI/UX designers, and engineers. The team inherited ~100 projects when it was established, built using heterogeneous technologies over two decades. The vast majority of those projects had no funding for maintenance and were running outdated operating systems and software as a result. Several had experienced minor hacks, and the infrastructure they used was approaching end of life. The Lab estate, although representing a significant corpus of high-quality DH research attracting ~250,000 unique users per year, constituted significant risk to DH at our university, and security risk to the wider university network. Over the last three years the team have undertaken a full audit of its projects and integrated archiving, sustainability, and research data management to its Software Development Lifecycle. An internal process triaged the projects according to security risk, scholarly value, cultural and cultural heritage value, brand value, and maintenance cost. Principal Investigatorswere contacted and given a range of options to consider, from archiving their site to upgrading it and placing it under a Service Level Agreement, usually of between 2 and 5 years. Approximately 50 siteswere quickly archived, with the remainder being upgraded and moved to SLAs progressively over 18 months. A major infrastructure upgrade was implemented over the same period, providing 50TB of disk space, ~1TB RAM, and enterprise backup systems capable of supporting significant growth. The process of discussing projects with PIs and finding funding to upgrade and maintain projects was often difficult. The Faculty supported every project led by one of their staff members, and offered cost-recovery rates to all other projects, but many PIs had difficulty reconciling themselves to the need for ongoing maintenance and support or found it difficult to gain support from their administrators. At the time of writing all but a handful of projects have been resolved, but the process highlighted significant gaps in understanding across the DH community along with policy issues related to funding of DH projects. Archiving and sustainability issues have been reduced to an acceptable level in the Lab, but risks remain, and ongoing management will be required. Archiving and sustainability are now integral to the Labs software engineering process, from requirements elicitation during concept development to archiving or ongoing maintenance in the post-funding period. The Labs processes are aligned, in turn, to the Universitys Research Data Managementprocess and an effort is being made to align technical design with the national web archive. Paper Two: Running God Knows How Many Versions of PHP: The Challenges of Successfully Sustaining Digital Projects In 2019, the Roy Rosenzweig Center for History and New Media is celebrating its 25th year, including a long legacy of both creating and sustaining digital history projects. Founded on the principle of democratizing history, RRCHNM is committed to creating open-source software and sustainable digital projects; millions of people use center-designed software such as Zotero, Omeka, and Tropy, while tens of millions visit the centers various project websites every year. Nor is this engagement limited to the centers most recent accomplishments; one of the centers older projects, the twenty-year-old History Matters, is still receiving over three million website hits annually. While this widespread and continuous engagement with center software and websites is a testament to the intellectual value of the centers digital scholarship, it also intensifies the sustainability pressure inherent in all digital projects. The centers earliest projects involved bespoke code which requires considerable labor either to maintain or to later transition to a content management system, e.g. Omeka, which was built in part to streamline the creation and management of the centers digital projects. Even projects running on a CMS still require upkeep, as new versions of the system or its components are released; external links break when websites restructure or eliminate content; and older technologies such as Adobe Flash are deprecated. Furthermore, the servers of which these projects live eventually reach the end of their hardwares lifespans and the projects must be migrated to new servers to survive. These technological challenges can be further exacerbated by logistical conditions that arise in a environment where collaborations happen across departments and institutions, projects have temporally limited funding support, and project team members are either soft-funded staff positions or graduate students who are—usually, though not always—transient members of an institution. While transdisciplinary and cross-institutional collaboration, grant funding, and graduate student involvement are generally seen as positive aspects of the digital humanities as a field, they lead to logistical challenges with respect to sustainability. The decentralization of knowledge across institutions and teams, as well as personnel discontinuity over time, leads to challenges in documenting and maintaining contextual knowledge around projects as they age, often requiring personnel to reinvent the wheel during the already challenging and unfunded process of technologically updating old projects. Despite these challenges, RRCHNM has sustained its digital projects over the course of its existence and—thanks to the institutional support it receives from its associated department and college, as well as the leadership of its previous and current directors—is well positioned to continue sustaining its digital projects for the foreseeable future. This paper will discuss both the institutional conditions that enable RRCHNM to sustain its digital projects as well as the technical and logistical challenges that it must overcome when continuously sustaining a wide variety of digital projects created by hundreds of people over the course of two and a half decades. Paper Three: Ruthless Principles for Digital Longevity Project Endings is a SSHRC funded collaboration which aims to provide practical solutions to issues attendant on ending a project and archiving the digital products of research, including not only data but also interactive applications and web-based publications. Endings is a collaboration between the Humanities Faculty and the Library at the University of Victoria, and endeavours to align the aims of faculty researchers producing projects and the archivists who will eventually be responsible for curating their work. Using both practice-based methods and scholarly research, Endings is already producing recommended approachesand diagnostic toolsthat will assist scholars in ensuring that their project will be completed, archivable, functional, and available well in to the future. The project has conducted a survey with 128 project leaders, and conducted 28 follow-up interviews to gain insight in to the practical issues faced by DH scholars. Simultaneously it has been actively working on ending several existing in-house projectsusing the a set of principles developed from our work. These principles focus on reducing technological overhead and applying software development best practices to the planning and construction of a projects digital outputs. Our methodology is based on paring back the range of technologies used to the absolute minimum, and building completely static web materials with no dependence on any server-side technologies. The Endings project divides digital projects into five primary components: data, products, processing, documentation, and release management. We aim at longevity primarily for data and products, but believe that this goal requires careful attention to processing, documentation and release management. We are developing preservation principles for of these factors, and this presentation will discuss key components of the principles along with their justification and practicality. Many of these principles are uncontroversial. For instance, principle 1.1, Data is stored only in formats which conform to open standards and which are amenable to processing would not be surprising to anyone. Others are more demanding and are likely to meet strong resistance from some members of a project team; programmers may be unsettled by the demand that there be no dependence on external libraries: no JQuery, no AngularJS, no Bootstrap, or puzzled by the requirement that every page contains all the components it needs, so that it will function without the rest of the site if necessary, even though this means duplicating information across the site. This ruthless set of maxims can make rapid development and deployment more difficult, but the principle of hard now, easy later is the only real guarantee of digital longevity for projects which, while they may be curated, are never likely to be actively maintained over the long term. Paper Four: Balancing Innovation & Persistence in Digital Scholarly Publications In the past few years, several university presses have been awarded funding by the Andrew W. Mellow Foundation to meet the needs of digital humanists and social scientists who are pushing the bounds of traditional print publishing practices and seeking to output their arguments in a form that matches their methodologies. These multimodal, multilinear, open-access, web-based publications follow a parallel editorial and production workflow as traditional scholarly monographs, and as peer-reviewed scholarly works, carry the same weight in consideration of tenure and promotion for the scholars who create them. Stanford University Press is pushing farther than others by allowing authors to choose their own web-based platforms or builds rather than offering ones designed in-house. This openness introduces challenges for preservation and persistence of the publication. But rather than limit the potential for innovative expression, SUP is investing in exploring possibilities in digital preservation for complex interactive works. This program has seen the publication of four unique projects to date that could not possibly be rendered as print books but whose place in the scholarly record is just as important to their fields of study. It is important then that they can endure the specific threats that web-based digital content faces. The publisher needs to mitigate the potential fragility of these formats by keeping a handle on the complexity, but they must also support the innovative and courageous strides scholars are making as they rightly challenge the politics and limitations of traditional print publishing model. Essentially, they must balance innovation with durability. To intercept the dangers to persistence introduced in digital formats, SUP implemented a careful multi-phase set of guidelines for preservation and archiving. These start with pre-production technical recommendations that encourage but do not force authors to choose platforms and design elements that are durable and archivable. They also initiate a three-pronged preservation strategy during production and immediately after initial publication that can ensure viable access to and experience of the work once inevitable obsolescence creeps in to the live hosted product. These include documentation and digital repository deposit, web archiving, and emulation. While devoting resources not usually deployed in a university press context, SUP is also talking to authors before, during, and after the development and publication of their work and learning that expectations for their works longevity vary significantly. While most acknowledge and even embrace the risks and ephemerality of the digital, they also put trust in the Press to ensure the kind of persistence associated with traditional publications. Some expect to share the responsibility of persistence and others would be satisfied with a time stamp that covers academic milestones related to tenure and publishing expectations. A candid dialogue about existing and developing preservation practices in such programs will hopefully both assure scholars that publishers are aggressively pursuing preservation as part of the responsibility of publication, and also invite DH authors to share their expectations and concerns with the challenges digital presentation formats present. "
	},
	{
		"id": 356,
		"title": "A Corpus-linguistic Approach to the Analysis of Latin Morphology",
		"authors": [
			"Smith, David Neel"
		],
		"body": " The study of a historical language like Latin requires a corpus-linguistic perspective. Since we cannot appeal to native speakers of Ciceronian Latin, medieval Latin or pre-classical dialects known from early inscriptions, our knowledge of the language depends on the surviving documents we choose to study. Several excellent Latin morphological parsers already exist.All of them are designed as comprehensive systems applicable to Latin generally, however. Some support adding new vocabulary items, but scholars and teachers studying texts with non-standard morphology or orthography have few options. Normalizing the text directly contradicts the corpus-linguistic mandate to understand the language as it is attested. Forking one of the existing open-source systemswill appeal to few Latinists. This paper describes an alternate methodology. It differs from current approaches in two ways: by automating the building of parsers tailored to particular corpora, and by identifying all components of a parsers output with canonically citable URN values. While it would be possible to build a parser covering all known digital Latin texts, parsers targeted at the language and orthography of specific corpora can reduce the ambiguity of analyses to instances of true morphological ambiguity. In a corpus of Plautus, for example, the surface form anime can only be the vocative singular of animus. In a diplomatic edition of manuscripts of the Latin Psalms, e might represent the orthographic equivalent of classical ae so that anime could be genitive singular, dative singular, nominative plural or vocative plural of anima. A comprehensive morphological parser would have to accept all these possibilities for analyses of anime. A classical Latin parser, on the other hand, could accept only ae as valid first-declension endings; the lexicon for a Latin parser of the Psalms does not need an entry for animus, since that word does not appear in the Psalms, so the only ambiguity it would identify is the identical form of four case-number combinations of the first-declension noun anima. By using URNs to identify all components of an analysis, we can readily combine analyses from multiple parsers. CITE2 URNs identify collections of discrete objects.. Since we do not use string values to identify forms or lexical entities, when we analyze Plautus with a classical Latin parser and a diplomatic edition of the Psalms with a parser specific to that corpus, the analyses of each parser can identify the lexical item anima with the URN urn:cite2:hmt:ls:n2612. Our parsers can thus recognize that anime in the Psalms and animae in Plautus are equivalent forms of the same lexical entity, but the token anime represents different lexical entities in the two corpora. This approach opens up new possibilities for research and pedagogy. For editors of diplomatic editions, automated morphological analysis is invaluable for validating manually edited texts, but it is only possible when the orthography, lexica of stems and inflectional rules can all be defined for the corpus being edited. Morphological data can enrich familiar analytical models such as social networks. Parsing of named entities is often limited because they are precisely the kind of vocabulary missing from standard lexica. If we construct a social network of persons appearing in the same passage and associate with each name its grammatical case, the resulting graph not only distinguishes clusters of co-occuring figures, but indicates what syntactic role they fulfill in relation to each other. The approach described here invites a beginning-language pedagogy preparing students to read a particular corpus. It is customary to analyze a digital text in order to determine what vocabulary should be stressed in introductory language courses. In deciding how best to sequence topics, we can also analyze the frequencies of forms and of specific inflectional rules. If supine forms are rare in our target corpus, we might choose not to emphasize them. But we can go further, and evaluate which particular inflectional classes should be emphasized. Does every variation of third-declensions i-stems appear in the corpus were preparing students to read, or should we devote more time to other topics? In this approach, the central technological component is not a Latin parser, but an open-source system for building corpus-specific parsers. It is modelled in part on the Kanónes system for building Greek morphological parsers, but extends and generalizes some of its ideas. As with Kanónes, a digital humanist manages a set of structured text files. A build process managed with sbtreads the text files, translates them into source code in SFST-PL, and applies the Stuttgart Finite State Transducer tools to compile binary finite state transducers for working with Latin morphology. The package includes a suite of utility scripts in Scala that can be run from the sbt console and parse the SFST output into an object model. They include scripts to parse a wordlist, summarize a corpus morphologically, and export the morphological analyses to a tabular format suitable for use with tools such as a RDBMS. Three data sets define the parser for a corpus. First, a plain text file defines the orthography by enumerating all Unicode codepoints allowed in parseable tokens. Second, a set of delimited-text tables defines a lexicon of stems, recorded in the defined orthography. Each stem is uniquely identified, and associated with a URN for a lexical entity, as well as an inflectional class. Third, a further set of delimited-text tables defines the inflectional rules that apply to the corpus. The rule is uniquely identified, includes an ending recorded in the defined orthography, and is associated with one of the same inflectional classes used in the tables of stems. Sample data sets illustrating how to organize the data for a complete parser include diplomatic editions of Latin manuscripts, Latin texts digitized from print editions from the Tesserae Project, and a corpus comprising all paradigms in Allen and Greenoughs Latin Grammar. Other than a text editor to create or modify the data files, the system has only two technical requirements: sbt, and the SFST toolkit. Directly using the included scripts is the simplest way to analyze or export results, but the scripts in turn rely on a JVM code library imported by sbt that can be used with any JVM language. DH projects that want to use the parsers output differently can use the code library to ingest the parsers output and have direct access to the data through high-level abstractions. While it is less likely that digital humanists will choose to expand on the set of included transducers, the organization of the SFST system supports this, too. The included transducers are chained in a standard design pattern: data transducer -> acceptor transducer -> analysis transducers The data transducer is the Cartesian product of all stems with all rules. The acceptor transducer filters these so that only combinations of rules and stems belonging to the same inflectional class remain. Analysis transducers suppress some categories of information to provide a mapping from an incomplete set of data to a full analysis. A final transducer that suppresses all analytical information and keeps only stem+ending strings therefore maps surface forms to a full analysis. Alternatively, a transducer that suppresses all data except a URN for a lexical entity and symbols for person, number, tense, mood and voice provides mappings like first singular present indicative active of urn:cite2:hmt:ls:n25153 -> jacio. The theme of DH2019 is complexities. The approach to morphological analysis presented here respects the complexity of Latin as it is attested in millenia of surviving documents. Managing simple text files to build corpus-specific parsers with analytical output identified by URNs, we can bring a more nuanced corpus-linguistic perspective to research and teaching with digital corpora of Latin. "
	},
	{
		"id": 357,
		"title": "Capturing the Geography of 1900s Britain as Text: Findings from the GB1900 Crowd-Sourced Gazetteer Project",
		"authors": [
			"Southall, Humphrey",
			"Aucott, Paula"
		],
		"body": " Introduction The purpose of the GB1900 project was to transcribe all text strings appearing on the second edition of the Ordnance Surveys County Edition six inch-to-one mile mapspublished between 1888 and 1914 and covering the whole of Great Britain. The only exclusions were primarily numerical data easily obtained from modern digital mapping, i.e. spot heights, the depths of lakes and distances on road signs. The scale of this task, and the limited effectiveness of optical character recognition when applied to text on maps, made crowd-sourcing the most appropriate methodology. Figure 1: Excerpt from County Edition Six Inch mapThe project grew out of Cymru1900, a collaboration between the National Library of Wales, the University of Wales Centre for Advanced Welsh and Celtic Studies, the Royal Commission on the Ancient and Historical Monuments of Wales and Peoples Collection Wales, with funding from the Welsh Assembly. Cymru1900 launched in October 2013 and remained live until it was replaced by GB1900, but was much more successful at obtaining initial transcriptions than the matching confirmatory transcriptions required to finalize each string. GB1900 involved additional partners at the National Library of Scotland, supplying a digital map mosaic covering the whole of Great Britain, and the University of Portsmouth, providing additional development time to revise the software to encourage confirmations, then hosting the revised system. GB1900 launched in September 2016, incorporating all existing transcriptions and user registrations from Cymru1900, and was closed down at the end of January 2018, by when it was very hard to find new text to transcribe. The overall project history, sources and software is described in Southall et al, while the crowd-sourcing process and the motivations of volunteers are explored in Aucott et al. The GB1900 Datasets Following some manual work to resolve c. 30,000 problematic transcriptions, final datasets were made available for download in July 2018, from Portsmouths web site A Vision of Britain through Time: http://www.visionofbritain.org.uk/data/#tabgb1900 Three distinct datasets were created. Firstly, a raw dump consisting of all the uncleaned tables from the MongoDB database behind GB1900, excluding only the table holding user registration details and including all the different transcriptions of each string; this is offered under a CC0 license, enabling anyone to do what they like with it. Secondly, the main cleaned dataset, containing just one agreed transcription of each of 2.55m. strings, together with OSGB and WGS84 geographical coordinates. Thirdly, an abridged version from which the commonest strings judged not be place names have been removed. The latter two datasets are offered under CC-BY-SA licenses. The abridged dataset is presented as a gazetteer, meaning an inventory of the names by which people identified particular places: towns, villages, hamlets, woods and so on. As such, it is possibly the most detailed gazetteer of Britain ever created, and certainly the most detailed specifically historical gazetteer. It is further described in Aucott and Southall. However, more than half the transcriptions are not place names but are still of very considerable interest, and our main focus here. The three most commonly occurring names in the abridged dataset are Manor House, Manor Farmand Mount Pleasant. Conversely, the most common unabridged terms are F.P., meaning a footpath, W, meaning a welland P, meaning a pump. The original justification for including these in the transcription process was the difficulty of defining place names with enough precision and clarity to be really consistently applied by the volunteers. Figure 2: Locations and types of windmill, from GB1900 The complete and abridged datasets are currently made available simply as CSV files, for easy uploading into databases or viewing in spreadsheets. Although we have written elsewhere about the importance of presenting gazetteers as linked data, the datasets consist primarily just of strings and an associated coordinate, plus the names of containing modern local authorities and parishes added via point in polygon look-ups, so these data are not linked. However, we are exploring how the GB1900 data may be linked to the DEEP gazetteer created from the Survey of English Place Names, which has a complex semantic structure but currently contains locations for only 4.4 per cent of the included places. Capturing geography in words In some senses, the County Series maps are not large scale maps but small scale plans: they contain no symbology or key, just the outlines of buildings and other features; and a great deal of text. As a result, transcribing the text, and recording the locations where it appears, captures most of the maps meaning, their semantic content. Capturing all those detailed outlines from the maps would not greatly extend our geographical knowledge, while greatly complicating both the transcription software and the volunteers task. It would however have been desirable to capture the fonts and sizes used for each text string, as that does incorporate some additional meaning. Figure 3: Chalk Pits in south east England, from GB1900 We can learn much about the detailed geography of Britains people over the last two centuries from the census. However, sources for the broader study of the evolving cultural landscape are more limited. We have worked extensively with the Land Utilisation Survey of Great Britain from the 1930s, but this provides only a very broad brush overview of land use, and little about what makes particular places special. Figures 2 and 3 provide two different examples from GB1900 data of how humans both used and shaped the landscape, making particular places distinct. Figure 2 shows how windmills were a common feature of the flat landscapes of eastern England, and give the lie to those who see modern windfarms as a new intrusion. Figure 3 of course partly just shows the distribution of chalk in the underlying geology, but the concentrations, especially into the Hampshire-Surrey borders, also tell us something of agricultural improvement. How GB1900 is being used The data are already being used by project partners. The National Library of Scotland have implemented a search facility using it on their open access online map collection for their six inch to the mile maps. Searching by place names from GB1900 finds the location on the six inch map and the visitor can then choose to view another map for the same location: https://maps.nls.uk/geo/explore/ The Royal Commission on the Ancient and Historical Monuments of Wales have a List of Historic Place Names in Wales which must be consulted for all new developments in Wales. Currently using data from Cymru1900, it will be updated to include GB1900: https://historicplacenames.rcahmw.gov.uk/ The authors are incorporating a GB1900 search facility into the Vision of Britain system. This links GB1900 data into the existing historical information structure and for the first time will offer greater detail for places within parishes: http://www.visionofbritain.org.uk/expertsearch#gb1900/ Researchers independent of the project have also been experimenting with the dataset, including Jim Clifford, a historian at the University of Saskatchewan who looked at breweries, distilleries and pubs: GB1900 Works Website Joe Rosehas been working on industrial sites and has produced a 33 volume listing of Scottish mills, linking them to other locational information. His dataset is being used by a current Glasgow PhD student. He is also working on a desktop application. This link shows an early extraction of quarries: https://geo.nls.uk/maps/gb1900quarry/ Conclusion The County Series maps are the largest scale at which the Ordnance Survey mapped the whole of Great Britain, larger scales existing for towns and for more densely populated rural areas. Most of their meaning is in the text that GB1900 has extracted, and while it would be hard to claim this is a literary work, it is a remarkably detailed record of both physical and cultural landscapes, which we can now present in summary form. "
	},
	{
		"id": 358,
		"title": "Encoding Ancient Greek Music",
		"authors": [
			"Sowerby, Zachary David"
		],
		"body": " <DH-Heading1>1. Introduction</ DH-Heading1> This paper presents a project to encode and analyze ancient Greek music. Notated music of Ancient Greece survives only in about 40, often quite fragmentary, sources spanning a chronological range of 500-600 years. Only one song exists that can be considered complete. The information preserved in these documents, however, can be quite rich. It informs us about lyric, pitch, rhythm, meter, section breaks, dynamics, and instrumentation. Documents containing notated ancient Greek music survive primarily in the form of papyrus fragments. A smaller but significant number appears in inscriptions, while a limited amount of the music of Hadrians court musician, Mesomedes of Crete, survives in a manuscript tradition. Due to the accidents of survival, the majority of these compositions date from the Roman period, although a significant number of Classical and Hellenistic fragments, such as a fragment from the Orestes of Euripides, survives as well. They range geographically from Greece to Anatolia to Egypt, and include tragedies, paeans, hymns, comedies, instrumental works, theoretical exercises, and one early Christian hymn. Huge challenges for the study of this corpus are its fragmentary nature and its geographical, chronological, and topical extent. The goal of this project is to implement a design permitting digital study of this corpus, and to provide a framework for research and pedagogy. The digital corpus must be able to encode the same information across a variety of source types but also take the different aspects of the documents histories into account so that understanding can be made in context. 2. The Digital Corpus Our digital editions should capture the semantics of ancient Greek musical documents. They should support computational analysis and presentation directly from the information recorded in the ancient documents. To do this, they must address two fundamental challenges: recording ancient Greek musical notation, and coordinating musical notation, lyrics, and other analytical data sets. It is not possible to record ancient Greek musical notation using existing systems for the digital encoding of music such as MusicXML and the Music Encoding Initiative. The Music Encoding Initiative, for example, is excellently suited to encoding modern western music notation. Its XML scheme can be extended with modules for particular kinds of notation. However, MEI assumes that music can be encoded in terms of pitches as notated in familiar staffed systems. The limits of this assumption are most evident in MEIs guidelines for encoding neumes, which are forced to avoid encoding unstaffed neumes entirely. For this reason, Waller et al.developed the Virgapes notation for encoding medieval plainchant, which they presented at DH2018. We need instead an encoding scheme that expresses the semantics of ancient Greek documents. The Unicode block for Ancient Greek Musical Notation, for example, does not satisfy this requirement, since its codepoints focus on graphemes rather than sememes, with the result that visually identical but semantically distinct symbols are encoded to a single codepoint. We therefore define an original encoding establishing a one-to-one correspondence between the symbols used in Greek musical texts and their digital encoding. Most surviving Greek musical documents are vocal. Our editions of the musical notation must therefore be coordinated with editions of the lyrics. We adopt the model mentioned above, by Waller et al. Musical notation and lyrics are transcribed in separate TEI XML documents aligned by canonical citation using Canonical Text ServicesURNs. We extend the model of Waller et al. by adding further editions. Any aspect of analysis which is not able to be easily captured in an existing edition is simply captured in an aligned edition. For example, Greek texts had a pitch accent that is explicitly written in modern printed editions, and for a source document can be reconstructed by the editor. Similarly, meter can be inferred from the length of syllables in the text. Both accent and meter are recorded in separate documents and aligned by CTS URN with the diplomatic editions of the lyrics. 3. Systematic Data Extraction, Computational Analysis, and Presentation Along with its digital editions, the project includes a suite of programs to extract and analyze data from the editions. This endeavor operates by the principle that the corpus and the manipulation of the data therein are to be entirely separate. Data within the corpus can be systematically extracted for computational analysis or presentation. Since all editions of a single document are aligned, correlated patterns in different types of editions can be identified and analyzed. For example, accent information and pitch information can be compared, to study the known phenomenon in ancient Greek music by which melodic pattern tends to correlate with accent. Instances of agreement and of conflict can be separately analyzed in relation to the other editions to achieve an understanding of how and why agreement or conflict might occur. For presentation, for example, composite information from the aligned editions can be exported as a text file which can then be read by a custom-coded application created with the music program Max MSP. This aural analysis provides the choice to realize the music according to a variety of tuning systems championed by a variety of competing ancient theories, such as those of Aristoxenus and Ptolemy, for which the program is specially equipped. 4. Open Source All material from the project will be available in a public repository on Github. New editions and code from contributors can expand the project, creating an ongoing initiative to advance the study of this topic. This frameworks explicit modeling and coordination of a large and complex corpus of information underscores the theme of Complexities. The automated generation of aural analysis with Max MSP takes a novel approach of audio synthesis by systematic data extraction, and introduces a range of options for presentation and teaching. "
	},
	{
		"id": 359,
		"title": "Collating Medieval Vernacular Texts: Aligning Witnesses, Classifying Variants",
		"authors": [
			"Camps, Jean-Baptiste",
			"Ing, Lucence",
			"Spadini, Elena"
		],
		"body": " Introduction Aligning different versions of the same work is both a computational and a philological challenge. In particular, the collation of witnesses of an ancient or medieval text poses specific difficulties due to the coexistence of macro-structural and localised variants, including a large number of formal variants. We present an experimental computer-assisted workflow for aligning several witnesses and classifyingvariants. Formal and substantive variants are examples of categories especially relevant for languages which are unstable in their graphic system, as are medieval languages. The case studies are in Old French, and, marginally, Old Spanish. The distinction between formal and substantive variants enables to treat them separately. Stemmatology, for instance, will be mostly interested in the former, while, for linguistic analysis the latter are needed. In automatic collation, based on full transcription of the texts to be compared, the formal variation is generally preserved, but temporarily nullified by means of normalisation or fuzzy match: this enables an accurate alignment of the texts and at the same time the preservation of the original forms. How to handle variation Medieval texts, especially in vernacular, often exhibit important variation. At the phrases or words levels, syntactic or graphic variations account for diachronic and diatopic differences, varying scribal practices and the plurality of graphematic standards. This makes it difficult to align sequences between texts, when they have very few letters in common, e.g., Cait del fuere | Chiet dou fuerre | Kiet du feurre. Difficulties due to spelling or flexional variation only add up to already existing variations in word order or substance. Consider the following example taken from Chrétien de Troyes Chevalier au lion: H Li frans li dolz ou ert il donques P Li frans li dous ou estoit donques V Li franz li doz ou ert il donques F Li frans li dols ou ert il donques G Li biaux li preuz ou estoit donques A Li preus li frans u est il donques S Li preus li frans u ert il donques R Li frans li dols u ert il donques M Li frans li preus ou est il donques Spellingand flexional variantsgo along with substitutions, additions/deletions, or permutations. In such a case, clearing out spelling and flexional variation might help in resolving the other difficulties. This paper offers a new approach to the normalisation task made possible by the developments in the field of NLP and the resources now available for medieval languages, following the steps described in fig. 1. Processing workflow The initial step is the acquisition of the text, from the digital image, done by a combination of manual transcription, automated handwritten text recognition, and post-correction. The raw text thus obtained is then structured and stored in an XML/TEI based format. All these tasks are performed before the normalisation step, here represented by lemmatization and linguistic annotation, done with the help of neural network-based taggers/lemmatizers. Traditionally, normalisation consists of the preparation of the texts for alignment and might imply lowercasing, removing punctuation or editorial markup, as well as the temporary removal of formal features. Our proposal is to move to an automatic normalisation performed using NLP tools. Each tokenis annotated with linguistic information such as part of speech, lemma and detailed morphological information. This kind of normalisation is only possible when suitable resources are available. For Old Spanish, Freelingprovides a specific module. For Old French, we used the data provided by the Geste corpus, annotated with lemmas, as well as POS and morph tags according to the Cattex scheme. With this data, we trained a neural tagger/lemmatizer suitable for variation-rich languages. On the test set, accuracy reached 94.5 and 95% for lemmatization and POS-tagging, and was in the range 94-98.5% for different morphological features. After normalisation, the texts enriched with linguistic information can be used to perform the alignment. Variation in structure, order or content in medieval texts is favoured by the existence of active textual transmissionand by processes of rewriting, prosification/versification, etc. Changes in the order of the structural entitiesare also common. In order to collate these displaced entities, a phase of macro-structural alignment might be needed. This process can be done by a combination of direct expertise and tools conceived for detecting paraphrase, text reuse or computing similarities. The very collation is then made by using the collation program CollateXin its Python version. CollateX uses multiple alignment algorithms, suitable for the comparison of more than two witnesses; its modular structure, based on the Gothenburg model, enables the user to intervene on each module separately and to add new ones. Automatic categorization of variants All these software bricks can be integrated in a more complex pipeline up to the the final output. The modular structure of CollateX enables us to adjust the alignment and the visualization phases, in order to take into account the linguistic annotations added to each token. The alignment is performed directly on the annotation, used as a normalised form. In the creation of the output, some rules are added to compare the original forms with the annotation and to assign a category to the variant. For example, the category formal variant is assigned to aligned tokens which have the same annotations but different original forms, such as: mielz, miels, miaus. Additional rules can be used for classifying variants into finer-grained categories, using linguistic annotation. Possible classification of variants using linguistic annotation, with examples of possible subcategories and cases. The broad paradigmatic subcategory encompasses synonyms, cohyponyms, hypero-/hyponymes or holo-/meronyms; the semantic subcategory is reserved for lexemes who do not hold this type of relation between them. Conclusions and Further research This paper presents some early results of an ongoing research on automatic collation and categorization of variants. Performing normalization using NLP tools not only speeds up the task, but also makes the identification of fine-grained categories possible. The case studies show the strong and weak points of this proposal and of the technical solutions for its implementation. Eventually, this research forces us to reflect upon the importance of having software components which are open and modular, in order to improve them and to include them in computational pipelines. "
	},
	{
		"id": 360,
		"title": "Topographies of Digital Modern (Foreign) Languages research",
		"authors": [
			"Spence, Paul Joseph",
			"Brandao, Renata"
		],
		"body": " Modernlanguages have been under pressure in a number of countries for some time now, with reduced provision in many English-speaking countries, and options increasingly focusing on English elsewhere. In the English-speaking world, a long series of reports has catalogued these challenges at all levels of education, often using words like crisis and urgencyand it has been suggested that the field of Modern Languages needs to re-configure itself to meet new cultural/media landscapes. Meanwhile, as the digital humanities have expanded and increasingly entered mainstream debates about the future of the humanities, there has been a growing focus on its terms of representationand commitment to diversity. On a geocultural level, various initiatives have sought to address the immense Anglophone linguistic and cultural disparitieswhich digital culture has amplified — and which DH has often, albeit unwittingly, reinforced within its own areas of influence — but there has been relatively little consideration given to possible interplay with ML, which seems strange given ML expertise in studying multilingual diversity, translation and transcultural perspectives. This paper starts by tracing interactions between ML and DH based on detailed empirical research, and argues that there is much to gain from connecting current debates about the future of the Modern Languageswith concerns about the linguistic diversity deficit in the digital humanities. Topography So far there has been little systematic analysis of interactions between ML and digital humanities, something the Digital Mediations strand on the AHRC-funded Language Acts & Worldmaking project aims to address. Through a series of landscape surveys – questionnaires, interview surveys, literature reviews, resource reviews and curricular studies - we have mapped the topography of digital ML research and found that the engagement of Modern Foreign Languages with digital is both wider and deeper than is widely understood by DH or indeed by ML itself, although recently it has started to surface more prominently through initiatives such as the digital hispanisms panel at the Association of Hispanists of Great Britain and Ireland conference in 2018or the 2019 MLA forum on Digital Humanities & Modern Languages. Digital mediation in ML is particularly strong around language learning/teaching, where researchers and teachers have an elevated understanding of the potential for authentic, multimodal, learner-centred, transcultural and polylingual interaction, but the focus of our research is principally on research intolanguages and their related cultures, rather than language learning. Our research first sought to establish the topography of ML-DH interactions through an open survey, interview study with ML respondentsacross a range of different roles, course reviewand resource review. We have found that ML researchers have an elevated sense of digital affordances, but, generally speaking, low engagement with the kind of advanced digital methods represented by DH, beyond areas such as digital editing, linguistics, electronic literature and web archive studies of language communities. There were some interesting trends in both survey and interview studies: while transcultural and plurilinguistic expertise is valued, national and disciplinary boundaries are deemed less and less relevant to ML research; large scale data approaches are judged to be increasingly important in facilitating interesting comparisons of cultural reception and influence across many languages; digital is becoming increasingly important as a research object in itself; and there is support for the idea that digital needs tobe integrated more closely into ML practice, rather than being something different in future. Towards a DHML agenda? How does this connect to DH specifically? How can we start to theorise the connection between the digital humanities and ML, and what opportunities are there for deeper, bidirectional and equitably conceived/constructed collaboration? In November 2015 a writing sprint on the theme of Modern Language and the digital, later published as an open access multi-author collaborative article on The Shape of the Discipline edited by Claire Taylor and Niamh Thornton, opened up debate about this. Conversation ranged from themes such as data-driven approaches to modern languages or the ML research process, to users/interfaces in ML and digital ethnography and while some contributors argued that DH often has too narrow a conception of digital, the article set the scene for greater possible collaboration between the field of ML and DH. In an article for Digital Humanities Quarterly, Thea Pitman and Claire Taylor expanded on this theme, arguing that ML bridge a gap between traditional DH praxis and linguistically- and culturally-specific cultural studies approaches to digital materials. They went on to argue that more collaboration is needed to optimise the potential of both disciplines through what they call a critical DHML approach. Mutual disruption Debates about DHs own communication practices have rightly received a fair amount attention in recent years, with various attempts to address linguistic and cultural diversity, but it has been much less common for linguistic diversity to be explicitly harnessed as part of DHs research agenda. There is a substantial body of DH research involving language, but the overriding dynamic has historically been digital humanities disrupting ML-based research practices, rather than the other way around, and this naturally shapes discourse around the ML-DH relationship. DH2018, in many ways an exemplary conference in terms of representation, with ample discussion of cultural diversity and reappropriation of digital spaces – such as the keynote Weaving the Word, by Janet Chávez Santiago – addressed this balance to some extent, but even here the emphasis was largely on digital transformation of ML rather than vice versa, and where ML perspectives surfaced explicitly, it was generally in relation to pedagogy. What if we were to change the direction of flow, or to look for challenges which are mutually transformative? As a result of our research we propose some areas where DH might benefit from a greater, and more explicit, adoption of ML sensibilities, which can be summarised as follows: The international classroom. Greater attention to the concept of the international and plurilingual classroom in DH pedagogy - including diversification of the DH curriculum, and the use of virtual communications to explore multilingual interactions between DH student cohorts, potentially in liaison with ML programmes. Linguistic diversity as research topic. The growing consolidation of supercentral languages and English as the hypercentral language has major implications for language diversity, a phenomenon which will increasingly become a major challenge for DH and scholarly communications as a whole. Digital humanists are already active in some great projects to protect endangered, minority or heritage languages, but there is a need for greater awareness of, and commitment to, linguistic diversity in the actual construction of DH resources and this is an area ML where can make an important contribution. Linguistic/cultural interfaces. Early digital experiments like the work of Jiménez, Underberg and Zorn adopted key Andean concepts such as complementary duality and the tripartite division of time and space into web design. Closer attention from DH to how resources are constructed linguistically and culturally would help foster greater awareness about how resources work transculturally within the field. Greater recognition and organisation of multilingual resources. With the exception of corpus-based resources such as CLARIN, it is not easy to find language-based DH research online, even on otherwise excellent resources such as the EADH project list. Multilingual methods and tools. We know digital methods and tools are not culturally neutral, and yet much of the discussion about them still tacitly assumes that they operate globally and monolingually– we need more formal work in DH to explore how they operate differently in and across different linguistic and cultural contexts. Conclusions Languageshave been a key area of focus for the Digital Humanities from its origins as a field, and yet DH has rarely explored what this means in any real depth for its own research and pedagogical practices. In this paper, we have outlined a rationale for greater focus on ML-DH collaboration and have proposed some key elements of a possible future common research agenda. Topografias da pesquisa em LínguasModernas Digitais LínguaModerna têm estado sob pressão em vários países já há algum tempo, com oferta reduzida em muitos países de língua inglesa, e preferências cada vez mais focadas no inglês em outros lugares. No mundo anglo-saxão, uma longa série de relatórios catalogou esses desafios em todos os níveis de ensino, frequentemente usando palavras como crise e urgênciae sugerindo que o campo da Língua Moderna precisa se reconfigurar para conhecer novas paisagens culturais e midiáticas. Enquanto isso, à medida que as humanidades digitais se expandiram e entraram cada vez mais nos debates basilares sobre o futuro das humanidades, houve um foco crescente em seus termos de representaçãoe compromisso com a diversidade. Em nível geocultural, várias iniciativas têm procurado abordar as imensas desigualdades linguísticas e culturais anglófonasque a cultura digital ampliou - e a qual as HD tem muitas vezes, embora inconscientemente, reforçado dentro de suas próprias áreas de influência - mas há relativamente pouca consideração dada a possível interação com a LM, o que parece estranho dada a experiência da LM em estudar a diversidade multilíngue, tradução e perspectivas transculturais. Este artigo começa por traçar as interações entre LM e HD com base em pesquisas empíricas detalhadas, e argumenta que há muito a ganhar ao unir os atuais debates sobre o futuro da Língua Modernacom as apreensões sobre o déficit de diversidade linguística nas humanidades digitais. Topografia Até o momento há pouca análise sistemática das interações entre a LM e as humanidades digitais, algo que a vertente Mediações Digitais do projeto Language Acts & Worldmaking financiado pela AHRC pretende abordarAtravés de uma série de pesquisas de panorama - questionários, entrevistas, revisões de literatura, revisões de recursos e estudos curriculares - mapeamos a topografia da pesquisa digital de LM e descobrimos que o engajamento das línguas estrangeiras modernas com o digital é mais amplo e profundo do que é amplamente entendido pelas HD ou mesmo pela própria LM, embora recentemente tenha começado a emergir de forma mais proeminente através de iniciativas como o painel Digital Hispanists na conferência da Associação de Hispanistas da Grã-Bretanha e Irlanda em 2018ou o fórum 2019 MLA sobre Digital Humanities & Modern Languages. A mediação digital na LM é particularmente forte em torno da aprendizagem / ensino de línguas, onde pesquisadores e professores têm uma compreensão elevada do potencial de interações autênticas, multimodais, centradas no aluno, transculturais e polilinguais, mas o foco da nossa pesquisa é principalmente sobre pesquisa em línguase suas culturas relacionadas, ao invés da aprendizagem de línguas. Nossa pesquisa primeiro buscou estabelecer a topografia das interações LM-HD através de uma pesquisa aberta, estudo de entrevista com profissionais de LMem diversas funções, revisão de cursoe revisão de recursos. Descobrimos que os pesquisadores de LM têm um elevado senso de oportunidades digitais, mas, em geral, baixo engajamento com o tipo de métodos digitais avançados representados pelas HD, afora de áreas como edição digital, linguística, literatura eletrônica e estudos de arquivo na web de comunidades linguísticas. Há algumas tendências interessantes em ambos os estudos de pesquisa e entrevista: enquanto a experiência transcultural e plurilinguística é valorizada, as fronteiras nacionais e disciplinares são consideradas cada vez menos relevantes para a pesquisa de LM; considera-se que as abordagens de dados em grande escala são cada vez mais importantes para facilitar comparações interessantes de recepção cultural e influência em muitas línguas; digital está se tornando cada vez mais importante como objeto de pesquisa em si; e há apoio para a ideia de que o digital precisaintegrado mais de perto na prática da LM, ao invés de ser algo diferente no futuro. Para uma agenda da HDLM? Como isto se conecta especificamente as HD? Como podemos começar a teorizar a conexão entre as humanidades digitais e a LM, e quais oportunidades existem para uma colaboração mais profunda, bidirecional e equitativamente concebida / construída? Em novembro de 2015, um writing sprint sobre o tema Modern Language and the digital, mais tarde publicada como um artigo multi-autoral colaborativo sob o título The Shape of the Discipline, editado por Claire Taylor e Niamh Thornton, abriu este debate. A conversa variou de temas como abordagens baseadas em dados para linguagens modernas ou o processo de pesquisa de LM, para usuários / interfaces em LM e etnografia digital, e enquanto alguns contribuintes argumentaram que as HD frequentemente tem uma concepção muito estreita do digital, o artigo estabeleceu o cenário para uma maior colaboração possível entre os campos de LM e HD. Em um artigo para a Digital Humanities Quarterly, Thea Pitman e Claire Taylor desenvolveram este tema, argumentando que a LM cria uma lacuna entre a praxis tradicional de HD e abordagens de estudos culturais linguisticamente e culturalmente específicos para materiais digitais. Elas argumentaram que são necessárias mais colaborações para otimizar o potencial de ambas as disciplinas através do que elas chamaram de abordagem crítica de HDLM. Interrupção mútua Os debates sobre as práticas de comunicação das HD receberam justamente atenção nos últimos anos, com várias tentativas de abordar a diversidade linguística e cultural, mas tem sido muito menos comum para a diversidade linguística ser explicitamente aproveitada como parte da agenda de pesquisa das HD. Há um corpo substancial de pesquisas em HD envolvendo a linguagem, mas a dinâmica dominante tem sido historicamente que as humanidades digitais atrapalham as práticas de pesquisa baseadas em LM, e não o contrário, e isso naturalmente forma o discurso em torno da relação LM-HD. A DH2018, em muitos aspectos uma conferência exemplar em termos de representação, com ampla discussão sobre diversidade cultural e reapropriação de espaços digitais - como a palestra Weaving the Word, de Janet Chávez Santiago - abordou este equilíbrio até certo ponto, mas mesmo aqui a ênfase foi em grande parte na transformação digital da LM, e não vice-versa, e onde as perspectivas da LM surgiram explicitamente, foi geralmente em relação à pedagogia. E se mudássemos a direção deste fluxo ou procurássemos desafios mutuamente transformadores? Como resultado da nossa pesquisa, propomos algumas áreas nas quais as HD podem se beneficiar de uma adoção maior e mais explícita das sensibilidades LM, que podem ser resumidas da seguinte forma: A sala de aula internacional. Maior atenção ao conceito de sala de aula internacional e plurilingue na pedagogia das HD - incluindo a diversificação do currículo das HD e o uso de comunicações virtuais para explorar as interações multilíngues entre os coortes de alunos das HD, potencialmente em ligação com programas de LM. Diversidade linguística como tema de pesquisa. A consolidação crescente das línguas supercentrais e do inglês como linguagem hipercentral tem implicações importantes para a diversidade de idiomas, um fenômeno que se tornará cada vez mais um grande desafio para as HD e para as comunicações acadêmicas como um todo. As humanistas digitais já estão ativas em alguns projetos de porte para proteger línguas ameaçadas, minoritárias ou de herança, mas há uma necessidade de maior conscientização e comprometimento com a diversidade linguística na construção real dos recursos HD e esta é uma área que a LM pode fazer uma contribuição importante. Interfaces linguísticas / culturais. Os primeiros experimentos digitais, como o trabalho de Jiménez, Underberg e Zorn, adotaram conceitos-chave Andinos como dualidade complementar e a divisão tripartite do tempo e do espaço no design da web. Uma atenção mais próxima das HD para a forma como os recursos são construídos linguisticamente e culturalmente ajudaria a promover uma maior conscientização sobre como os recursos funcionam transculturalmente dentro do campo. Maior reconhecimento e organização de recursos multilíngues. Com a exceção de recursos baseados em corpus, como o CLARIN, não é fácil encontrar pesquisas de HD baseadas em idiomas, mesmo em recursos excelentes, como a lista de projetos EADH. Métodos e ferramentas multilíngues. Sabemos que os métodos e ferramentas digitais não são culturalmente neutros e, no entanto, grande parte da discussão sobre eles ainda pressupõe implicitamente que eles operem globalmente e monolingualmente– nós precisamos de mais trabalho formal nas HD para explorar como eles operam de forma diferente em diferentes contextos linguísticos e culturais. Conclusões As línguastêm sido uma área chave de foco para as Humanidades Digitais desde sua origem como campo de estudo, e ainda assim HD raramente explorou o que isso significa em qualquer profundidade real para suas próprias pesquisas e práticas pedagógicas. Neste artigo, delineamos uma justificativa para um maior foco na colaboração LM-HD e propusemos alguns elementos-chave para uma possível agenda de pesquisa comum futura. Topografías de la investigación digital en lenguas modernas Las lenguas modernasllevan tiempo bajo presión institucional en varios países. En países anglófonos se ha ido reduciendo la provisión de lenguas modernas sensiblemente, mientras que en varios otros países las opciones tienden a limitarse a estudiar inglés como lengua extranjera. En el mundo anglófono se ha producido una serie de informes sobre esta situación, catalogando estos retos a todos niveles de la educación, a menudo utilizando palabras como crisis o urgenciay se ha llegado a sugerir que la lenguas modernas deben reconfigurarse para nuevos panoramas culturales y mediáticos. Mientras tanto, las humanidades digitales se han expandido y se empiezan a entrar en debates más extendidos sobre el futuro de las humanidades, con el resultado de que se ha hecho cierto enfoque sobre sus términos de representacióny su compromiso con la diversidad. A nivel geocultural, varias iniciativas han buscado afrontar las inmensas desigualdades lingüísticas y culturales hacia lo anglófonoque han sido amplificadas por la cultural digital – y que las humanidades digitales a menudo, aunque sin querer, ha agudizado bajo su propio dominio de influencia – pero se ha considerado poco la posible interacción con el campo de las lenguas modernas, lo cual resulta sorprendente, dada la experiencia de este campo en materia de diversidad plurilingüe, traducción y perspectivas culturales. Esta presentación empieza por trazar las interacciones entre las lenguas modernas y humanidades digitales, apoyándose en investigación detallada empírica, y argumenta que se puede ganar mucho por conectar debates actuales sobre el futuro de las lenguas modernas con el déficit en términos de diversidad lingüística en las humanidades digitales. Topografía Hasta ahora, ha habido poco análisis sistemático de las interacciones entre las lenguas modernas y las humanidades digitales, algo que intentamos abordar con la investigación en la rama `Digital Mediations` del proyecto `Language Acts and Worldmaking` financiado por el AHRC. A través de una serie de estudios – cuestionarios, entrevistas, revisión literaria, revisión de recursos digitales y estudios curriculares – hemos trazado la topografía de la investigación digital en lenguas modernas y hemos encontrado que el compromiso de las lenguas modernas con lo digital es más amplio y más profundo de lo que se aprecia, tanto en las humanidades digitales como en las mismas lenguas modernas, aunque últimamente ha empezado a salir al público más a través de iniciativas como el panel de `digital hispanisms` del congreso Association of Hispanists of Great Britain and Ireland en 2018o en el foro MLA 2019 sobre Digital Humanities & Modern Languages. La mediación digital de las lenguas modernas es más fuerte en el aprendizaje o la enseñanza de las lenguas, donde los investigadores y educadores tienen una comprensión elevada de la potencia para interacción auténtica, multimodal, transcultural, plurilingüe y centrada en la/el estudiante, pero el enfoque principal de nuestra investigación ha sido sobre la investigación en lenguas y culturas modernas, más que en la enseñanza de lengua. Nuestra investigación ha intentado establecer la topografía de las interacciones en lenguas modernas y humanidades digitales a través de una encuesta abierta, entrevistas con personas en varios papeles asociados con las lenguas modernas, una revisión de programas educativosy revisión de recursos. Hemos encontrado que los investigadores tienen un sentido elevado de las oportunidades digitales, pero en general poco compromiso con el tipo de métodos digitales avanzados más familiares en las humanidades digitales, más allá de ciertos campos como la edición digital, la lingüística, la literatura electrónica y los estudios archivo web de comunidades lingüísticas. Hemos observado tendencias interesantes, tanto en la encuesta como en las entrevistas: mientras que se valora la habilidad transcultural y plurilingüe, las fronteras nacionales y disciplinares cobran menos importancia en la investigación en lenguas modernas; se aprecian cada vez más los enfoques con datos a gran escala para comparar la recepción cultural y la influencia a través de varias lenguas; lo digital empieza a cobrar mayor importancia como objecto de investigación proprio; y se apoya cada vez más la idea de que lo digital tiene que ser estrechamente integrado en la práctica de las lenguas modernas, en vez de configurar algo nuevo en el futuro. Hacia una agenda digital en lenguas modernas ¿Como podemos conectar todo esto con las humanidades digitales concretamente? ¿Cómo podemos empezar a teorizar sobre la conexión entre las humanidades digitales y las lenguas modernas, y qué oportunidades hay para una colaboración bidireccional y concebida/construida con igualdad en la relación entre ambos campos? En noviembre de 2015 se abrió un debate sobre esto con un `writing sprint` acerca del tema de `Las lenguas modernas y lo digital` que después dio el resultado de un artículo colaborativo multi-autor de acceso abierto con el título `La forma de la disciplina`, editada por Claire Taylor y Niamh Thornton. El debate tocó temas como el enfoque sobre datos digitales o el proceso de investigación en las lenguas modernas, los usuarios y las interfaces y la etnografía. Mientras que algunos de los autores sostenían que las humanidades digitales a menudo demuestran una concepción demasiado estrecha de lo digital, el artículo proponía una colaboración mayor entre ambos campos. En un artículo para Digital Humanities Quarterly, Thea Pitman y Claire Taylorsiguen este hilo, aseverando que las lenguas modernas forman un puente entre la praxis tradicional de las lenguas modernas y un enfoque más cercano a los estudios culturales hacia los materiales digitales. La disrupción mutua Ha habido debates importantes sobre las prácticas comunicativas de las humanidades digitales en los últimos años, que en varias ocasiones han intentado abordar cuestiones de diversidad lingüística y cultural, pero es menos común ver la diversidad lingüística formar parte explícita de la agenda de investigación de las humanidades digitales. Existe un cuerpo significativo de investigación en humanidades digitales sobre las lenguas, pero el discurso histórico dominante es de una dinámica donde las humanidades digitales transforman las lenguas modernas, y no vice versa. DH2018, que fue ejemplario como congreso en muchos aspectos en términos de la representación, y donde se facilitó debate amplio sobre la diversidad cultural y la reapropiación de espacios digitales – como, por ejemplo, la presentación plenaria de Janet Chávez Santiago `Weaving the Word` - abordó este desequilibrio hasta cierto punto. No obstante, incluso en este caso, el énfasis fue más sobre la transformación digital de las lenguas modernas que al contrario, y las perspectivas explícitamente hacia las lenguas modernas generalmente se centraron en la pedagogía. ¿Y si cambiáramos el flujo de esta dinámica, para buscar retos que llevasen los dos campos a una transformación mutua? Como resultado de nuestra investigación, queremos proponer áreas donde las humanidades digitales podrían beneficiarse de una adopción mayor y más explícita de sensibilidades hacia las lenguas modernas, resumidas de tal manera: El aula internacional. Se requiere una atención mayor hacia el aula internacional y plurilingüe en la pedagogía de humanidades digitales, que incluye la diversificación en el curriculo, el uso de comunicaciones virtuales en las interacciones plurilingües entre varios grupos de estudiantes, potencialmente a través de programas formales de enseñanza en HD. La diversidad linguística como tema de investigación. La consolidación creciente de las lenguas `supercentrales` y el inglés como lengua `hipercentral` tiene implicaciones importantes para la diversidad linguística, un fenómeno que será un reto importante para las humanidades digitales y la comunicación académica en general. Los humanistas digitales ya son activos en proyectos para proteger las lenguas minoritarias, comunitarias o apeligradas, pero se necesita una mayor atención sobre, y compromiso con, la diversidad lingüística en la construcción de recursos HD, y esto es un área donde las lenguas modernas pueden aportar algo importante. Las interfaces lingüísticas y culturales. Los experimentos tempranos como el trabajo de Jiménez, Underberg y Zorn adoptaron conceptos andinos de la dualidad complementaria y la división tripartita entre el tiempo y el espacio en el diseño web. Una atención mayor a cómo los recursos en HD están construidos ayudaría a fomentar mayor sensibilidad sobre cómo los recursos funcionan a nivel transcultural en el campo. Mayor reconocimiento y organización de los recursos plurilingües. Con la excepción de recursos de corpus como CLARIN, no es fácil encontrar recursos filtrados por lengua, incluso en recursos excelentes como la lista de proyectos de EADH. Métodos y herramientas plurilingües. Sabemos que los métodos y las herramientas no son culturalmente neutros, y sin embargo gran parte del discurso sobre ellos implica una suposición tácita de que estos métodos operan de manera global y monolingüe. Se necesita más trabajo en HD para explorar cómo funcionan de manera diferente en distintos contextos lingüísticos y culturales. Conclusiones Las lenguashan sido un área de enfoque clave para las humanidades digitales desde sus orígenes como campo, y sin embargo las humanidades digitales rara vez ha investigado lo que esta diversidad implica para sus prácticas investigación y pedagogía. En esta presentación, hemos trazado unas líneas generales de una propuesta para fortalecer la colaboración entre lenguas modernas y humanidades digitales y algunos elementos de una agenda común de investigación. "
	},
	{
		"id": 361,
		"title": "Bootstrapping Project-specific Spell-checkers",
		"authors": [
			"Sperberg-McQueen, C. M.",
			"Huitfeldt, Claus"
		],
		"body": " Spell-checking software is well established in consumer applications but often unexploited by data-creation projects in the digital humanities. We argue that spell-checking provides a relatively straightforward way to findtranscription errors even in texts written in idiosyncratic or inconstant spelling. Working hypotheses We believe that spell checking is feasible, useful, and underused in DH projects. More specifically: Fewer than half of DH projects transcribing existing materials use spell-checking technology. Standard word-in-isolation spell checking can find transcription errors. Project-specific spelling dictionaries can do better than off-the-shelf dictionaries for writing by idiosyncratic or inconstant spellers older language forms no longer supported in off-the-shelf dictionaries non-standard and minority languages which lack off-the-shelf dictionaries Project-specific filters may be necessary to create a checkable alpha textbut are feasible. Modeling spell-checking, modeling languages In conventional word-in-isolation spell-checking, the language model is trivial: all acceptable forms are equiprobable, a form is acceptable if and only if listed in the dictionary, unknown forms have probability zero, and any token with probability zero is a probable error. To find alternative spellings, a Levenshtein distance of oneor moreis sometimes used. A combination of phonetic encoding and Levenshtein distance can sometimes be helpful. Recent work on spelling correctionuses more elaborate models to detect real-word errors. In this paper, however, we assume the simple model of text as a sequence of equiprobable known forms. Challenges in using spell-checking For spell-checking in DH projects, some complications arise. Transcribers normally seek to reproduce the spelling of the exemplar, not to correct it. When standard spelling dictionaries are used to check material which consistently violates orthographic norms, they will erroneously flag some correctly transcribed misspellings and miss unconscious corrections by the transcriber. Off-the-shelf dictionaries reflect current norms for widely spoken languages. Older language varieties and under-resourced languages often lack dictionaries. The language transcribed may have no standardized orthography; spelling may vary by scribe or line-by-line. XML documents may contain material not to be spell-checked, or material in different languages or varieties. We believe these complications can be addressed. For idiosyncratic spelling, the solution is to use a project-specific dictionary, not a standard dictionary, so that correctly transcribed misspellings will be accepted and inadvertent corrections flagged. Inconstant spelling makes spell-checkers miss transcription errors that substitute one accepted form for another. But spell-checking can still catch other errors.Producing project-specific dictionaries from scratch requires some work, but our experiments suggest that even modest effort can produce spelling dictionaries that will detect existing transcription errors without excessive noise. For dealing with XML, its helpful to write filters to extract the desired word forms. Fortunately, this is normally straightforward. A small pilot study Several practical questions arise: How can project-specific dictionaries be constructed? What should they contain? How much work is involved? How big must the dictionary be: to catch as many actual errors as possible? not to flag correctly transcribed words erroneously? How much project data is necessary to obtain a dictionary of that size? We have explored these questions using material from the Wittgenstein Archives at the University of Bergen and from Liam Quins digital version of Alexander Chalmerss General Biographical Dictionary. For each project, we selected test material: for Wittgenstein, two small texts taken from non-final versions of the corpus; for Chalmers, 10,000 words from volume 25. For Wittgenstein, we checked the normalized-spelling text, identifying word forms which violate German orthographic norms. The Wittgenstein project defined standard orthography as that of Dudens Rechtschreibung, but admitted some idiosyncratic spellings consistently used by Wittgenstein. For Chalmers, we proofread the sample against the page scans. With programmatic filters we extracted an alpha text. We constructed dictionaries of various sizes by compiling lists of correct forms from different subsets of the project corpora. In principle, project-specific dictionaries should be built by proofreading texts one-by-one; to streamline the pilot project, we took the shortcut of checking wordlists against off-the-shelf dictionaries. This does not visibly affect the statistical results shown later, but it does mean that for Chalmers some mistranscriptions were missed and some bad flags thrown. We checked the test samples against those dictionaries. For each test, we counted the number of correct and incorrect tokens in the sample flagged or left unflagged by the spell checker. Results of the pilot study Constructing project-specific dictionaries The simplestmethod is to start with an empty dictionary and spell-check texts from the projects corpus one by one. For each word flagged by the spell checker, either add it to the dictionary or correct it in the text.With an empty dictionary, the spell-checker will at first flag every form in the text. To avoid the tedium of dealing with so many bad flags, it may be worthwhile to list the most frequent forms in whatever part of the corpus is available for consultation, check them manually, and seed the dictionary with them. For Wittgenstein, a dictionary of 1300 forms covers about 90% of the running tokens in the text, flagging only one token in ten. What to include and exclude Ideally, the dictionary should include all forms which actually occur correctly in the corpus and no forms which are transcription errors. This ideal is unattainable for two reasons. First, the same form may occur both as a correct transcription and as a mistranscription; it cannot be both included and excluded. Second, as the corpus grows, there will always be some correct forms not yet found in the dictionary, and thus always some erroneous flags. The optimal solution is to balance the relative inconvenience of undetected errors and false flags against the relative frequency with which each form is correct or mistranscribed. If undetected errors and bad flags have equal weight, then a form should be included in the dictionary if any occurrence is more likely correct than not. If we would rather see ten bad flags than miss one mistranscription, then a form should be included only if it is ten times more likely to be correct than incorrect. The projects preferences determine the threshold to be met. If spelling habits vary from document to document, it can be useful to make both a project-wide dictionary and document-specific dictionaries for texts with distinctive usage. When forms intentionally excluded from the dictionary do occur correctly transcribed, they can be marked with sic or similar markup and excluded from the alpha text, to avoid throwing bad flags for them. With these complications, the rule for forms flagged by the spell-checker becomes: If the form is correctly transcribed and meets the projects correctness threshold, add it to the project dictionary. If the form is correctly transcribed and meets the threshold in the current document but not elsewhere, then add it to the document-specific dictionary. If the form is correctly transcribed but does not meet the threshold, then tag it with sic or the equivalent. If the form is incorrectly transcribed, correct it. Dictionary size Error detection does not require a big dictionary. Indeed, because of real-word errors, bigger dictionaries often find fewer actual errors than smaller dictionaries, as shown below for Chalmers. Small dictionaries, however, throw too many bad flags. Fortunately, the number of bad flags falls dramatically as dictionary size rises, as shown below for Wittgensteinand Chalmers. Whether spell-checking feels useful or pointless dependson its signal:noise ratio; in our data dictionaries of about 15,000 forms reach aratio of 1:10. How big a corpus must be processed to produce a dictionary of suitable size? It varies, but as the plots below show, something more than 200,000 tokens are needed for a dictionary of 15,000 forms. Conclusions and future work Spell checking can find transcription errors in real-world data, even though transcription errors are logically distinct from misspellings and even when the spelling is idiosyncratic or inconstant. Developing a project-specific dictionary takes little time and can be expected to improve the results of proofreading. Even very small project-specific dictionaries can be useful. Work remains to be done to extend the pilot study to more materials, to support interactive correction of texts, to improve on the XML support offered by existing spell-checkers, and to explore the application of more sophisticated models of text to support word-in-context spell-checking in lieu of word-in-isolation spell-checking. "
	},
	{
		"id": 362,
		"title": "Word Embeddings for Processing Historical Texts",
		"authors": [
			"Sprugnoli, Rachele",
			"Moretti, Giovanni"
		],
		"body": " In the last years, word embeddings have become important resources to deal with many Natural Language Processingtasks. Several pre-trained word vectors have been released generated with different algorithms but all based on a huge amount of contemporary texts, mainly news and Wikipedia pages but also Twitter posts and crawled web pages. The interest towards this type of distributional approach has recently emerged also in the Digital Humanities community as proved by the organization of dedicated workshopsand the publication of scientific articles on vectors built from historical or literary texts for tracking semantic shifts. This submission aims at expanding current research on historical word embeddings by presenting a set of English vectors pre-trained on a sub-part of the Corpus of Historical American Englishwith three different algorithms. The subset of COHA we have chosen includes 36,856 texts of all the four available genrespublished between 1860 and 1939 for a total of more than 198 million words. We chose this specific time frame because we have a collection of travel writings of the same period of publication on which we planned to perform several NLP tasks as the one presented in the Application section below. In particular, this collectioncontains both travel reports and guides published in a period of radical transformation in travel habits thanks to several technological, economic and sociological factors that led to the decline of the Grand Tour and the emergence of leisure-oriented travels. As for the applied models, we used the GloVe, fastText and Levy & Goldbergs approaches. By adopting these three models, we cover different types of word representation: GloVe is based on linear bag-of-words contexts, fastText on a bag of character n-grams, and Levy & Goldbergs model on dependency parse-trees. Before applying these models, we lower-cased all the texts; tokenisation and dependency parsingwere then performed with Stanford CoreNLP. The training was done by considering all words appearing at least 10 times in the COHA sub-corpus and a context window size of 10. In the first phase, words are mapped to their frequency count, then a context vocabulary is created taking into consideration the context window. Our pre-trained word embeddingshave 300 dimensions and are publicly available online. Table 1. Examples of the most 7 similar words as induced by different embeddings HistoFast HistoGlove HistoLevy Word2Vec gay merry, gayest, joyous, gaiety, gayly, gayety, light-hearted merry, bright, joyous, cheerful, brillant, happy, flowers merry, gorgeous, joyous, rosy, lively, bright, cheerful lesbian, bisexual, lgbt, lesbians, women, sexual, gays, homoxesual dancing dance, danced, dances, dancers, dancer, walzing, dancin dance, playing, danced, singing, music, dances, dancers singing, bathing, skating, swimming, feasting, wrestling, chattering dance, singing, dances, songs, dancers, ballroom, featuring woman girl, madwoman, lady, irishwoman, husband, maid, she girl, man, women, wife, she, husband, mother girl, man, damsel, gentlewoman, englishwoman, youngster, creature man, person, girl, child, women, children, men Table 1 shows the top 7 similar words, in terms of cosine similarity, of a given set of target wordsas found in our three historical word embeddings and in Word2Vec trained on contemporary data. Among the words reported in Table 1, the main meaning shift is observed for gay, for which the reference to homosexuality is not present in the historical vectors. As for dancing, it is worth noticing that historical vectors brings out typical terms of the considered periodand that the dependency-based approach induce similarities having the same syntactic role: instead of finding words having high domain similarity, Levy & Goldberg model finds words with high functional similarity, thus words behaving like the target word. Terms rarely used in contemporary texts are detected for the target word woman as well: e.g. madwoman, damsel, gentlewoman . Social roles such as maid and wife does not appear in the list of the most similar words in Word2Vec, replaced by the neutral term person . Visualization of the embeddings of woman. Image created with the Embedding Projector https://projector.tensorflow.org/ Application Place names automatically detected and then visualized using Carto, https://carto.com/ Our embeddings can be useful resources for the development of NLP tools aiming at processing historical texts with neural architectures. For example, we applied them to the recognition of place names of different typesin English historical travel writings on Italy. The deep learning architecture we adopted, using a small set of in-domain training data, the HistoGlove embeddings and no feature engineering, outperformed both the CoreNLP CRFmodel retrained with the same dataset and the same neural architecture employing bigger vector spaces pre-trained on contemporary texts. Our best model achieves a precision of 86.4, a recall of 88.5 and an F-measure of 87.5. Figure 2 displays the place names, related to the center of Florence, automatically detected in the tenth chapter of Florence and Northern Tuscany with Genoa. "
	},
	{
		"id": 363,
		"title": "Towards a national collaborative network: Spatial Humanities Netherlands",
		"authors": [
			"Stapel, Rombert",
			"Zijdeman, Richard",
			"van Steensel, Arie",
			"Beek, Wouter",
			"Mac Gillavry, Edward",
			"Spaan, Bert",
			"Vermaut, Thomas",
			"Mol, Hans"
		],
		"body": " Over the past decades the Netherlands has fostered a rich variety of projects in a field we would today refer to as spatial humanities. Such projects include long-running infrastructural undertakings, e.g. the municipality boundaries of NLGISand cadastral maps of HISGIS Netherlands. With the rise of Linked Data in recent years, the field of spatial humanities has gained a strong momentum in the Netherlands by cultural heritage orientated tech-companies creating smart geo-tools. Yet, the field is fragmented and there is little coordination regarding best-practices, tools, and vocabularies. By creating this local network, we want to move away from previous situations in which individuals from different institutes and companies are working in isolation; tackling similar issues related to historical GIS in the Netherlands. Instead we aim to create an approachable, friendly port-of-entry for new and existing researchers and students in the field, which could foster day-to-day, informal collaborations: especially in those daily settings for which the threshold to participate in existing, global networks is, because of their distance, too high. A key accelerator towards a spatial humanities network is the national infrastructural project CLARIAH, creating a critical mass of researchers and computer science engineers working with Linked Data. Examples are Amsterdam Time Machine, Historical Leiden in maps, and Adamlink. Furthermore, existing spatial humanities projects, such as the Historical Atlas of the Low Countries, OpenGazAm, and Golden Agents will also be integrated in the network. The backbone of the network is formed by four research institutes of the Royal Netherlands Academy of Arts and Sciences, the Fryske Akademy, International Institute of Social History, Huygens ING, and Meertens Instituut. With the input of afore mentioned tech-companies, and other cultural heritage partners, the aim is to move towards a national spatial humanities platform, for exchange and collaboration, within and outside the Netherlands. To ensure the latter, the network communicates with Pelagios and the World-Historical Gazetteer for the exchange of infrastructural knowledge, data models and vocabularies, benefitting researchers worldwide. "
	},
	{
		"id": 364,
		"title": "Cooking Recipes of the Middle Ages: Corpus, Analysis, Visualization",
		"authors": [
			"Steiner, Christian"
		],
		"body": " Cooking traditions, whether they are regional or in a larger context, are one of the most distinguishable items of European culture and an important part of European identities. But how did they become to what we know them now? How did they develop and what were their influences? During the last decades, research arrived at two important conclusions on these questions. Firstly, there are no quantitative studies on the origin and formation of regional cuisines in Europe. Secondly, manuscripts containing thousands of cookery recipes first appeared in the Middle Ages, which can be consequently regarded as the birth of modern European cuisines. On the European continent Latin, Middle French and Early New High German recipes provide the majority of culinary transmission. The project is preparing the cooking recipe transmission of France and the German speaking countries, which sums up to more than 80 manuscripts and about 8000 recipes, for the analysis of their origin, their relation, and their migration through Europe. The comparison of French and German food history is especially suited for this task as France always had a culturally formative influence on German speaking peoples. Cooking recipes are culturally charged transient texts, which are best diachronically and spatially analyzed by strongly relying on digital humanities methods. However, understanding these recipes, their context and their transmission is not an easy process. The technical terms that describe ingredients, utensils, procedures, and customs of the time are a challenge even for history scholars who specialize on the topic. Thus, the texts need not only be transcribed and edited but also semantically enriched so that further analysis like machine aided comparison of ingredients or cooking processes can add to standardized philological research like the collation. The base of our data will be customized TEI/XML documents with a schema aiming at facilitating the semantic annotation of cooking recipes in general. The core of our digital research strategy is the Semantic Web and the idea of Linked Open Data. We are in the comfortable position within the Humanities that we are mainly dealing with food ingredients i.e. animals, plants and fungi, all fields of research that are provided with a sophisticated amount of already established ontologies and Linked Data For an overview of ontologies covering these topics see http://www.ontobee.org/, http://aims.fao.org/, https://ndb.nal.usda.gov/ndb/, https://agclass.nal.usda.gov/about.shtml, http://zbw.eu/stw/version/latest/thsys/70498/about.de.html all of which are connected to the Linked Open Date Cloud by proving its data in one or more serializations of OWL and/or RDF. including the general knowledge bases of Wikidata and DBPedia. Ontologies are also already being successfully used to representand analyzecooking recipes, albeit with different focuses and granularity of data. In the digital research-approach, we partly rely on text similarities but largely on the occurrence of ingredients, preparation instructions and time, tools, serving suggestions and medicinal, cultural as well as religious implications in the texts. The sheer quantity of semantics within one historic recipe shows how complex a semantic annotation of these texts is, leaving apart all philological observations that can be found in the material. Besides the ability to find links between resources that were not known before, a main argument for the decision to put Semantic Web technologies at the core of the project, was to ability to work outside of language barriers. Through the use of concepts in the sense of a notion, an idea rather than a term we are trying to overcome historical and language constraints i.e. the German word for potato Erdapfelappears in a manuscript from about 1488) long before the potato was imported from South America to Europe, giving us proof that the concept of Erdapfel must have been a very different onefrom todays notion. For this reason, we needed to introduce a workflow that reflects the recipes philological and semantic complexity. A precompiled list of medieval plant names and their translations to modern English and German as well as their medieval variant dictions http://medieval-plants.org collected by Helmut W. Klug, the principal researcher of the Austrian part of CoReMa, gave us the opportunity to start with aautomated alignment of Wikidata concepts with the Reconciliation Service API https://github.com/OpenRefine/OpenRefine/wiki/Reconciliation-Service-API provided by OpenRefine http://openrefine.org/ . Once each term has a concept connected with it, these concepts are used to enrich the ingredients within the actual recipe texts in the TEI documents. However, as stated above, a crucial point of this process is the human interpretation of the enriched entities and the decision for a concept. Therefore, the auto alignment procedure can only be viewed as a means to an end. Once the entities of each recipe are equipped with concepts, the projects analysis can reveal concurring or deviating eating habits, text migration as well as the influence of neighboring countries on their respective cuisine. The vast implementation of ontologies in the natural sciences allows us to establish connections from historical eating habits to modern concepts of food and generate new knowledge for the domain of food history. The research data will also be the basis for spatial and temporal visualization and statistical evaluation. The storage, analysis and dissemination of the projects data is handled by the data repository GAMSdeveloped by the Austrian Centre for Information Modelling in Graz. http://gams.uni-graz.at/ Within this infrastructure aimed at long-term preservation, the triplestore Blazegraph is accessed through a web service for the storage and retrieval of RDF triples which allows us to query our projects databases of medieval recipes in France and Austria as well as all connected concepts in the Linked Open Data Cloud. "
	},
	{
		"id": 365,
		"title": "A Kind of Magic: Migrating a Large Digital Edition of Letters into a New Infrastructure",
		"authors": [
			"Steiner, Elisabeth",
			"Vasold, Gunter",
			"Saric, Sanja"
		],
		"body": " Introduction The poster will introduce the approach taken to migrate a large-scale digital letter edition with accompanying material to a new technical infrastructure. The underlying projectis concerned with the work on the scientific estate of 19 th century linguist Hugo Schuchardt. The primary objective is the edition of the scientific correspondence, an endeavor being underway since decades. The database comprises nearly 6500 full-text transcribed letters with facsimiles and editorial comments. In addition to a large bibliographical database of primary and secondary literature, a former funding period also produced thesauri for persons, places and subjects. The reason for the migration primarily lies in the increasing difficulty to maintain the proprietary infrastructure, which has been developed and extended for more than two decades. Levels of Migration Organizational The migration process entails moving a large part of the technical infrastructure from the Institute of Linguistics to a second institution of the university specialized in DH and long-term preservation. In future, one institute will be responsible for scholarly data generation and producing the SIP), whereas the second institution is concerned with the archival and publication. Nevertheless, there has to be close cooperation between the two partners to create the necessary organizational and technical interfaces to accomplish this. Technical The existing proprietary infrastructure is only partly documented. The project is a pioneer in electronic publishing efforts, thus unfortunately having to deal with a lot of legacy infrastructure and code. The standardized digital objects created during the migration will be integrated into an existing trusted digital repository for long-term preservation operated by the second partner. Since the funding for the migration phase is limited to one year, the main focus will be to create an automated workflow for the integration of further data, which requires as low future maintenance as possible. Content related The first step of the migration is to create appropriate data models for all resources and to pull the data from the current infrastructure. The full-text transcriptions are mostly already in TEI format, but not a consistent one. The new schema will also be interoperable with the DTAbase format and ediarum. Furthermore, especially thesaurus data is not stored in an acknowledged standard format and has to be transferred to more suitable formats, mostly SKOS. Due to the high recognition of the project in the community, special attention must be payed to keep existing URLs working and to avoid broken links and confusion on side of the community. With regard to data reuse, correspondence data will be delivered in CMIFto the CorrespSearchaggregation service; furthermore, the data will be published on a freely accessible OAI endpointvia PMH. Visualization and Analysis The migration process will also include an update and consolidation of the user interface and dissemination options. A new presentation layer based on Bootstrap 4will be implemented, presenting all data sources from the project accompanied by static content in a fully responsive way. Digital facsimiles will be presented in an IIIF-compliant viewer. After finishing the migration of the research data, new visualization and analysis tools like a timeline visualization, a map tool and statistical analyses of the correspondence metadata can be easily integrated using the existing APIs and services of the digital repository. The intent is also to exploit the thesauri to a larger extent in resource discovery and search mechanisms. Conclusion The poster will highlight the complexity of migrating a well-known and already large-scale digital edition project to a new infrastructure, touching on every of the aforementioned aspects and showing our solutions. Nevertheless, this challenging process also entails the chance to add extended functionality and dissemination options to the currently rather outdated web representation and further enhance data quality, reuse and interoperability in a way not possible one or two decades ago. Furthermore, the poster will once more highlight the need for domain specific trusted repositories or data centers. The case of the project in question illustrates how proprietary standalone infrastructures must fall into disuse because of impossible maintenance work. "
	},
	{
		"id": 366,
		"title": "Qualitative Space of Poetry",
		"authors": [
			"Stell, John"
		],
		"body": " Introduction Writing often takes place or is displayed on a two dimensional surface, but many of the digital techniques for the representation of language reduce two dimensions to one. This reduction leads to many powerful tools in corpus linguistics, which depends heavily on patterns involving words appearing in linear sequences. However, it is widely acknowledged that language is not wholly captured by a purely sequential representation. Asnotes in the context of poetry: Texts that are presented as spatialarrangements in order to activate the visual composition on the page as an element of signification have a long tradition. More widely than poetry, layout in the two dimensional space of a single page can carry meaning – alignments both horizontal and vertical of lines of text, and placement of blocks of text in relation to images and diagrams are a key feature of multimodal documents. Frameworks for such documents have been proposed by Bateman and others. Further areas where layout is essential, including comics and graphic novels, have also been studied. It is striking that although representing two dimensional structure in documents of many kinds is clearly relevant to the digital humanities, existing work has made very little use of the techniques of qualitative spatial representation that have been applied in artificial intelligence over more than the past two decades. This paper describes work currently in progress to apply these particular techniques as a means of representing some aspects of the two dimensional structure of poetry layout. Qualitative Relations in Poetry Layout The idea of qualitative spatial representationis that spatial relationships, such as next to, alongside, bordering, overlapping, and many other topological notions, can be represented computationally using logic. An introduction to the technical foundations of QSR is provided by. The motivation in artificial intelligence is that humans use common sense spatial concepts in everyday situations rather than the numerical coordinates which predominate in most computational representations of space. The origins of QSR go back to philosophical interests in an account of space fitting human experience, such as the theory of extensive connection proposed by. To explain how QSR can be applied to the space of poetry in its printed form we will consider some examples. The following is the result of some standard image processing techniques applied an image of Southeys Cataract of Lodore printed in 1823. The text here occupies four pages but initially we only consider relationships between successive lines of text. Layout in Southeys Cataract of Lodore This poem is a well-known example of visual arrangement as the form follows the cascade of water: initially having a variety of directions before a long descent ending in progessively widening course as it reaches the bottom. Many related examples are known and described inand. Two further examples are given in Figure 2. Layout in poems by Eavan Bolandand by Alasdair Gray. On the left of Figure 2 is Eavan Bolands poem CODE. An Ode to Grace Murray Hopper 1906-88 maker of a computer compiler and verifier of COBOL. fromconsists of four blocks in which lines start successively further to the right, interleaved with three blocks in which all lines are justified on the left. On the right of Figure 2 is Alasdair Grays poem First of March 1990consisting of six blocks of text sharing a roughly similar shape. In each block the lines end successively further to the left down the page, although the right hand ends do not display any easily identifiable regularity. Although the examples have been presented visually, the reduction of poetry to simplified images does not give a computational means of comparing one poem to another. Given a corpus of tens of thousands of poems, how can the space of different shapes be understood? How can one query such a corpus for poems of a particular kind of shape? How could shapes be described, and how might one map changes to favoured shapes over time? These questions suggest a visual kind of distant readingbut one we have begun to investigate using QSR rather than geometrical methods. Application of Allens Interval Relations One basic QSR technique comes from the work of Allenin qualitative relationships between intervals of time. If we consider possible ways two intervals may relate, one answer is the 13 relationships indicated on the left in Figure 3. This shows 7 ways the uppermost interval of each pair relates to the lower one. The last 6 relations have inverse relations. Although the inverse of before might be called after, we use the initial letter of each of the 6 followed by i to indicate the inverse. The application of this scheme to a sequence of lines appears on the right in Figure 3. The Allen relations applied to layout This means the spatial arrangement can be represented by: di, di, di, e, e, e, d, d, d. Of course, this loses many features, such as the left-right and up-down symmetry, in this case. However it does capture the structure of: lines becoming shorter on each side, then continuing down the page at a fixed width and finally expanding on both sides. We have developed prototype software by coding in Python which determines qualitative relationships between lines and between blocks of text. This uses standard image processing techniques to segment images of pages into text lines as rectangular regions. Qualitative relationships are then calculated from the positions of these regions. In continuing work we are exploring other systems of spatial relationships, since the ones provided by Allen are only one example of many that are mentioned in. We note thatuses spatial relations between bounding boxes in video frames to detect patterns making up events. It is an exciting possibility that QSR can describe patterns making up visual poetic structure in an analogous way. "
	},
	{
		"id": 367,
		"title": "Analysing Visual Communication in Theory and Practice with Archetype",
		"authors": [
			"Stokes, Peter Anthony",
			"Marques de Matos, Debora",
			"Jakeman, Neil"
		],
		"body": " Brief description The goal of this workshop is to teach participants how to use Archetype while introducing them to the theoretical and practical issues around describing and categorising handwriting and other visual communication in a digital context. This will be done primarily through practical application of the software to a range of different cases including not only left-to-right alphabets but also hieroglyphs, ideographs and others as well as right-to-left and top-to-bottom scripts. Attention will also be paid to decoration and other forms of graphic communication, insofar as it comprises repeated visual elements that can be compared by Art Historians or others much as components of letters are compared by palaeographers. A screenshot of Archetype as used in the DigiPal project Archetypeis a generalised system for the online presentation of images with structured annotations and data which allows users to analyse, describe, search for, view, and organise detailed characteristics of handwriting or other material in both verbal and visual form. Designed primarily for the palaeographical analysis of historical handwriting, it was first developed at Kings College London for the Digital Resource and Database for Palaeography, Manuscript Studies and Diplomaticproject, funded by the European Research Council, and has since been extended by the Kings Digital Lab particularly through the Models of Authority and Exon Domesday projects. Available as FOSS, it now used in around two dozen research projects, most of which are small private studies but some of which are the result of large funded research projects. Further details are available from the URLs and bibliography below. Applications to date range from medieval Latin charters to modern draft manuscripts; parchment to inscriptions on stone and coins; writing in Hebrew and Greek to Chinese or Cuneiform; and decoration in manuscripts, tapestries and paintings. Results can be interrogated via a web API or through XML exports with fully customisable templates. For this workshop, participants will be introduced to the principles and practice of the software and will discuss some of the underlying theoretical issues. They will learn to understand and use the Archetype model for writing and other graphic communication, including how to customise the software for their own purposes. They will see different ways in which the software can be and is being used, not only for small and large research projects but also for teaching and public engagement. Example materials will be provided, but participants are strongly encouraged to bring their own materials and research questions to experiment with. Indeed, the workshop will end with an opportunity for participants to work on their own materials, individually or in small groups, while sharing the challenges, problems and solutions that they face with the leaders and other participants. Screenshot of Archetype applied to decoration Two key themes will underlie the workshop. The first is that Archetype is designed primarily to enable manual annotations of images. This emphasis on manual annotations is in deliberate contrast to word spotting, character segmentation and OCR/HTR: the goal is not to annotate every occurrence of every symbol, but rather to enable the researchers to make their own decisions and to communicate those decisions to others, a principle very much in the spirit of algorithmic accountability which is often difficult if not impossible to achieve with machine vision and deep learning. The second key theme of the workshop is that Archetype involves structured annotations, and this distinguishes it from other image annotation systems. To use Archetype requires creating a model of the handwriting or other visual communication, reflecting on which visual elements are significant to the research questions at hand. This then enables users to carry out requests such as show me examples of letters with ascenders written by a given scribe; show me examples of the hands of different people drawn in a given set of manuscripts; or show me images of personal names in this document. However, defining an appropriate model is a significant challenge that we will discuss and experiment with in practice. Comparison and connections will also be made to some other relevant ontologies and descriptive models such as IconClass, the Biblissima and CRMtex adaptations/extensions of CIDOC-CRM, and the IDIOM ontology for Mayan script. Screenshot of Archetype as used in the Exon Domesday project Participants can expect to end the day with a local installation of Archetype which they will have customised during the workshop, and the skills to extend their Archetype installations in a structured manner that will lend itself to effective research. The workshop will focus on how to use Archetype on participants own computers for their private use, but the results could later be installed on a server for team work and/or publication. The default home page of Archetype before customisation Workshop leaders Peter A. Stokes, École Pratique des Hautes Études, Université PSL peter.stokes@ephe.psl.eu Peter has around fifteen years of experience in applying digital methods to palaeography and manuscript studies, including leading the team that developed Archetype. He has a degree in Computer Engineering and a joint degree in Classics and medieval English, and a PhD in palaeography. He has led or co-led several large projects in digital humanities, including an European Research Council Starting Grantand two Arts and Humanities Research Council grants, as well as a Leverhulme Early Career Fellowship in digital palaeography. He has published on palaeography and digital humanities, as well as name-studies, lexicography, and Anglo-Saxon charters. Debora Marques de Matos, University of Münster debora.matos@uni-muenster.de Debora is a researcher at the University of Münster, Germany. With a background in art history, Hebrew palaeography and digital humanities, she currently leads The Other Within, a project that crosses research areas such as iconography, big data, and computer vision. Before that, she conducted research on the material transition from manuscript to print, and her PhD thesis was dedicated to the production of illuminated Hebrew books in Portugal. She has extensive experience with Archetype applied to Hebrew script and book iconography. Neil Jakeman, Kings Digital Lab, Kings College London neil.jakeman@kcl.ac.uk Neil has been working on DH projects at Kings since 2011, having contributed both as a Developer and more recently as a Research Software Analyst. He has an MSc in Geographical Information Systems and Environmental Managementand a BSc in Geology. Neils experience with geospatial systems complements the technological approaches used in the annotation of images and he is now acting as an ambassador for Archetype to push the framework into new domains and applications such as Numismatics, Iconography, and Art History. Target audience and expected numbers The target audience is people interested in structured annotations applied to visual communication, including interest in problems of modelling and communicating such content. The core application is palaeography and epigraphy, including not just alphabetic languages but also hieroglyphs, ideographs, and so on, but the tool is also useful for those interested in manuscripts, inscriptions and other forms of writing more generally, as well as for art history. It is also relevant to those interested in modelling writing and models for deeply structured annotation. Previous experience has shown considerable interest from PhD students and librarians as well as from those in traditional positions of teaching and research. Archetype and its immediate predecessors have been presented several times at DH and have received significant interest. Expected numbers for this event are therefore around ten and potentially up to twenty participants. Length and format Length: One day Format: The workshop will be very hands-on, with participants required to work actively on their laptops throughout the day. The general format will be that the workshop leaders present a new element of Archetype, both in terms of its theoretical rational and its practical usage, and then attendees test this out on the example material that will be provided to them. This pattern will largely be repeated throughout the day. At the end of the day, however, participants will be given time to start developing their own project with the help of the workshop leaders. A tentative outline is provided. Please note that this will be adjusted according to time and in order to accommodate participants interests and existing skill-level. Theoretical introduction: Principles and models for describing handwriting and written communication. Getting started: setup, annotating images, adding texts. Adding Images and Hands. Adding Manuscripts. Describing Graphs; adding new Characters, Allographs, Components and Features. Textual markup; linking text and image. Customising the framework: adding static content, changing menu structure, customising the home page. Advanced customisations: customising the search pages; using the Web API for custom searching; introduction to the customisation framework. Participants working time: an opportunity for participants to work on Archetype according to their own interests, with the support of workshop leaders and each other. Wrap-up discussion and further steps. Requirements for attendees Attendees are required to bring their own laptops with administrator rights to install new software. Archetype depends on Docker which works with Linux, MacOS or Windows, but it is somewhat limited on Windows and so Linux or MacOS are strongly recommended if possible. Those running Windows must also ensure in advance that their system supports virtualisation, as Docker and therefore Archetype will not function without it. Attendees will be expected to install the Docker and Archetype software in advance of the workshop. Instructions are available at https://hub.docker.com/r/kingsdigitallab/archetype/, and attendees should contact the workshop leaders directly if they encounter any problems in the installation process. Attendees will be provided with sample materials to work with; however, they are encouraged to bring their own materials to use with the system. To do this, they should come with one or more other documents in mind, including digital images of the documents and ideally some research questions to investigate. Most of the day will be working with a web-based GUI interface, but the advanced customisation session will involve working with Python, XSLT and command-line interfaces. Participants who are not already familiar with these are still extremely welcome and will receive the necessary support from the workshop leaders, but at least a basic knowledge of these will allow participants to gain more from this session. "
	},
	{
		"id": 368,
		"title": "EScripta: A New Digital Platform for the Study of Historical Texts and Writing",
		"authors": [
			"Stokes, Peter Anthony",
			"Stökl Ben Ezra, Daniel",
			"Kiessling, Benjamin",
			"Tissot, Robin"
		],
		"body": " Throughout the history of DH, a great deal of effort and many research projects have been dedicated to the study of historical documents, particularly around linguistic annotation and critical and facsimile editions in TEI, but also in automated image analysis, handwritten text recognition, layout analysis, named entity recognition, and more. Furthermore, software implementing innovations in these fields is increasingly allowing for interconnections through Web APIs, a process that is further enabled by standards such as the International Image Interoperability Frameworkand Distributed Text Services. All this strongly suggests that these different components can now usefully be brought together into a single interconnected infrastructure to allow for the online preparation, analysis and publication of texts, editions, images, and annotations. This is the goal of the eScripta platform which is currently under development at Université Paris Sciences et Lettres. A prototype platform is already available and will be demonstrated at DH2019. Although focussed on researchers at PSL, the code is already Open Source and freely available, and, unlike most comparable systems, the trained models for HTR are also freely open and available through the Kraken model repository. The platform is part of a larger funded project dedicated to the history and practice of writing. The host institution has an exceptional concentration of specialists in very diverse forms of writing, including manuscripts, documents and inscriptions not only in Latin, Greek, Hebrew and Arabic but also Egyptian, Cuneiform, proto-Semitic, Aramaic, early Chinese, Old Vietnamese, Indic scripts in all forms, Batak, Javanese, Pyu, Tocharian, and many others. This wide expertise and range of use-cases means that the project presents an important opportunity to develop a deeply cross-cultural and cross-disciplinary infrastructure that allows for multigraphic and multilingual work of exceptional breadth. A key existing component of this platform is Kraken, a module for handwritten text recognitionthat allows automatic analysis and transcription of manuscripts and printed books. It has been developed specifically for multi-lingual documents and non-Latin scripts including non-alphabetic writing-systems and right-to-left, top-to-bottom, bottom-to-top and bidirectional writing. Kraken has already been integrated into the platform and provided with a new and significantly improved interface for text entry and correction, as well as enhanced line-detection for complex layouts. eScripta Web interface to Kraken, ready to enter Ground Truth transcription. Uncorrected results of newly-developed line detection on complex layout for right-to-left text. A further key pre-existing component of the platform is Archetype, for deeply structured annotations to enable the exploration, study and communication of palaeographical and art-historical content; it is already being used in completed projects including DigiPal, Models of Authority and Exon Domesday. The default Archetype home page before customisation. Archetype applied to the Exon Domesday book showing occurrences of the gallows markArchetype used in the Models of Authority project to show images of a given titlein the chartersArchetype used in the Exon Domesday project to display the text, transcription and translation of the book. Other important components to be added include linking to external services such as those provided by D ivaServices. These are to be complemented with an ergonomic interface for manual transcription and data-entry for different types of digital and printed editions, as well as translations, commentaries, and linguistic analyses. This component will again comprise a dialogue between manual linguistic annotation such as POS tagging for the preparation of training data with Pandora, and a module for post-correction using Pyrrha. The resulting texts will be disseminated through TEI Publisher or similar. The Pyrrha interface for post-correction of automatic annotation The Digital Mishnah, built on TEI Publisher A further element is deep or allographic transcription, combining Archetype and Kraken with the interface already described. This will facilitate the production of facsimile editions and also training the computer for automatic transcription, alignment of existing texts with images, wordspotting, and text-image searches of the sort encouraged by Archetype. This combination of quantitative and qualitative approaches will also allow, for instance, the clustering of letters in a manuscript based on deep annotation, or analysing the distribution of allographs throughout an inscription or corpus. This becomes particularly powerful when combined with databases of manuscripts that are dated, geolocalised and/or written by the same scribe, such as Medium and Pinakes which are also planned for integration. As well as problems around internationalisation and the many conventions and needs of such a broad project, another challenge here is how and to what degree these different components can be combined given their different data-models, purposes and assumptions. A Conceptual Reference Model is therefore being developed which will constitute a further important output of this project, based on the Biblissima ontologyand CRMtex. "
	},
	{
		"id": 369,
		"title": "Improving OCR of Black Letter in Historical Newspapers: The Unreasonable Effectiveness of HTR Models on Low-Resolution Images",
		"authors": [
			"Ströbel, Phillip Benjamin",
			"Clematide, Simon"
		],
		"body": " Introduction The quality of Optical Character Recognitionis a decisive factor for the application of text mining techniques on historical newspapers. OCR for texts published in black letter is particularly challenging due to several factors: the low distinctiveness of characters, the change over time regarding vocabulary and spelling, the use of small font sizes, and the oftentimes poor paper quality. Holleyargued that in light of the poor OCR quality in newspapers, a focus on manual crowd-correction is more promising than investments in software development. Although automatic OCR post-correction can improve the quality of the text, the methods often lack precision, are not robust enough, or require a lot of in-domain training data. The problems are manifold and complex, but recent progress in neural OCR techniques promises significant improvements. These OCR models often outperform commercial systems like ABBYY FineReader . However, the training of a neural system using open-source softwareis demanding. Integrated handwritten text recognition and annotation platforms like Transkribus facilitate the creation of a ground truth, as well as the training and application of neural and corpus-specific models for OCR. Transkribus was initially designed to decipher manuscripts see . It allows the manual transcription of uploaded documents so that they can be used as training material for Handwritten Text Recognitionmodels. A useful feature of Transkribus HTR models is that the recognition of printed texts works just as well as that of manuscripts. A few dozen of corrected pages are sufficient for high-quality OCR results. In this study we illustrate how to drastically improve OCR quality for black letter in newspapers with a modest amount of manual work for ground truth creation. The integration of HTR model training into the Transkribus platform enables Digital Humanists to leverage the performance of neural OCR without having to tackle unnecessary technicalities. In our experiments we additionally address the following questions. Robustness: Are HTR models reusable for material that varies in digitisation quality. Transferability: How well does a model perform on another newspaper than the one it was trained on? Data and Experiments We use PDFs with medium-resolution images produced in 2005 from scanned microfilms of the German-language Neue Zürcher Zeitungfor our experiments. The OCRed text stems from ABBYY FineReader XIX , which was ABBYYs product for 19th century black letter recognition at that time. The first experiment evaluates the differences between three OCR systems:FineReader XIXresults from 2005,ABBYY FineReader Server 11results see , available within Transkribus ,Transkribus HTR model. Figure 1 shows example output from our three OCR systems. Example excerpts with low-quality OCR from two pages of the NZZIn our second experiment we apply the HTR model trained on medium-resolution images to high-resolution imagesfrom 1899 digitised anew from paper in order to test the transferability of the model. We also analyse the performance of the HTR model in two other publications. Creation of a ground truth and HTR model training The NZZ had been published in black letter from 1780 until 1947. We chose one title page per year at random from this period and loaded the image extracted from the PDF into Transkribus. We used the Transkribus internal FRS11 to recognise the text in the images and manually corrected words and baselines. The resulting ground truth of 167 pages contains 304,286 words and 43,151 lines. Depending on the amount of text on a page, the correction of a page including the baselinestakes between 1 and 2.5 hours. We used 90/10 split for training and testing the model. Evaluation We use the bag-of-words F1-measure metrics of PRImA TextEval 1.4 for evaluation. The F1-measure is the harmonic mean of precision and recall. Precision gives the percentage of OCRed words that are part of the ground truth, while recall measures the percentage of ground truth words that were found by the OCR system. By applying a bag-of-words approach, possible differences in layout recognition cannot distort the results. Results Figure 2 shows the evaluation on all pages from the test set. The FRS11beats the FRXIXthroughout. Our HTR model scores 97.0%on average and achieves significant improvements over both ABBYY products. Comparison between original FRXIX, FRS11, and Transkribus HTR. The application of our HTR model to five high-resolution images of newspaper pages from the NZZ shows accuracies of at least 98% and an average improvement of 4.24% over FRS11. Comparison between FRS11 and Transkribus HTR model on five high-resolution images from 1899. In terms of the transferability of our HTR model the average F1-measures of 98.6%for the Bundesblatt and 98.9%for the Neue Zuger Zeitung over five pages each show that although the model has been trained on the NZZ, it is able to score equally high on different publications. The FRS11 reaches 92.4%for the Bundesblatt and 88.4%for the Neue Zuger Zeitung, showing the superiority of our HTR model. Conclusion We have shown that Transkribus is an excellent tool for creating HTR models for the OCR of newspapers typeset in black letter. Even with a limited amount of training data, our HTR model consistently outperforms state-of-the-art commercial software. Our HTR model trained on medium-resolution images digitised from microfilm still performs better than commercial software when applied to high-resolution images derived from paper originals. Given the availability and abundance of digitised historical material in the form of PDF files with poorly OCRed text, our findings showcase how digital humanists can improve their source material for text mining with a reasonable effort. Acknowledgments We would like to express our gratitude to Günter Mühlberger and the Transkribus team for their support in training HTR models and partially correcting baselines of our ground truth. Moreover, we thank Camille Watter and Isabel Meraner for their help in the transcription process. This research is supported by the Swiss National Science Foundation under grant CR-SII5_173719. "
	},
	{
		"id": 370,
		"title": "The Digital Humanities Certificate Option: What's At Stake?",
		"authors": [
			"Sturgeon, Stephen"
		],
		"body": " This short paper will trace the roots of the digital humanities certificate option as it is now most commonly conceived, beginning with Lisa Spiros 2010 post Opening Up Digital Humanities Education and then summarizing how her ideas were developed by scholars such as Lynne Siemens and Kara Kennedy in journal papers. It will then move on to examining how these ideas were put into different kinds of practice at institutions like Texas A & M University, the University of Nebraska-Lincoln, and the University of Virginia, and finally it will turn to the authors own experiences as the developer of a digital humanities workshop series who then participated in an attempt to standardize it for university accreditation as part of a certificate program. Issues explored along the way will include faculty collaboration and labor equity, fair intellectual representation, the complexity that conversations about these assume, and conditions for librarians and campus partners to agree to when developing curricula together. "
	},
	{
		"id": 371,
		"title": "Integrated DH. Rationale of the HORAE Research Project",
		"authors": [
			"Stutzmann, Dominique",
			"Currie, Jacob",
			"Daille, Béatrice",
			"Hazem, Amir",
			"Kermorvant, Christopher"
		],
		"body": " Abstract – HORAEis a cross-disciplinary research project studying religious practices and experiences in the late Middle Ages and Renaissance as evidenced by the medieval bestseller, Books of Hours. Developing tools in artificial intelligence, computer vision and image analysis to read a very large and diverse corpus of medieval manuscripts, in Natural Language Processingto identify textual units and structures, and in book history and religious practices, HORAE also implements diverse tools developed in the DH community. This paper presents HORAE as a research Gesamtkunstwerk that tackles a common challenge of complexity, uncertainty and granularity with different tools from different fields. By broadening the perspective in a genuinely cross-disciplinary research, we discuss the position of DH within the humanities. Introduction HORAEis a cross-disciplinary research project studying religious practices and experiences in the late Middle Ages and Renaissance through Books of Hours, the number one medieval best seller. It involves three partners in humanities and computer science: the Institut de Recherche et dHistoire des Textes, the private company TEKLIA and the Laboratoire des Sciences du Numérique de Nantes. More than 10,000 of these books survive and the production of such a large number of manuscripts is a major phenomenon and witness to profound changes in late medieval society, on cultural, religious, and industrial levels: speculative book production rather than on commission for specific clients, devotio moderna and imitation of clerical practices by lay people, customization of devotional objects, etc. Books of Hours are at once deluxe items of social display and intimate objects of devotional intensity, used for ones salvation. Their textual content is immense, yet scarcely studied. HORAE combines the research and expertise of all three partners: in artificial intelligence applied to computer vision and image analysis to automatically read a very large and diverse corpus of medieval manuscripts; in natural language processingto parse and identify textual units and structures; and in book history and religious practices. The project aims to create an integrated chain from image treatment to producing new knowledge by placing the end user at the center of developments and focusing on ergonomics and data visualization. Aims, corpus, relevance The aims encompass:re-using the many digitized manuscripts which are online but underused;new open-source software for Handwritten Text Recognition;tools for segmentation and plagiarism detection, adapted to the transcription of medieval manuscripts produced by the machine, in order to identify the texts in Books of Hours;identifying and editing unpublished texts;visualization of manuscript clusters which share the same textual characteristics, either in the order of the different parts, or in the order of their smaller textual units;studying the diffusion and circulation of devotional and liturgical texts at the end of the Middle Ages in order to better understand the cultures and faith in the 13th c.-16th c. Big, linked, open, usable data. With its aims and methods, HORAE addresses the largest corpus of manuscripts in medieval studies so far. It changes research methods in auxiliary sciences and tackles the challenges of big data. Books of Hours have been little studied until new because they are too numerous, too complex and too standardized: now, their very number, repetition and complexity allow this project to develop new and efficient technologies and methodologies, and to gain new knowledge about the Middle Ages. The suitability of liturgical data for fruitful historical analysis was recently illustrated by data from calendars. Libraries, infrastructures, and digital humanities. Several hundred thousand medieval manuscripts and millions of medieval archival documents are preserved worldwide. The textual wealth of these, mostly unpublished, resources is far from exhausted. It is for this reason that HTR and NLP are needed. Books of Hours have been massively digitized because they are often heavily illuminated and of interest to art historians; in spite of this, these books, as texts, are massively underused. By making new use of documents first digitized for other audiences and other purposes, HORAE demonstrates the opportunities created by open-access digitization using the IIIFprotocol https://iiif.io/ . Community relevance. The late Middle Ages and Renaissance were times of extreme religious tension within communities. Direct comparison with modern times is of course impossible, and history does not solve current problems. However, studying how religious practices contributed to shape communities and served as common identifiers, and how texts were circulated as a basis for specific religious tendencies, can help produce relevant comparisons. This can inform sociology of religion and is of clear contemporary relevance. Methods and realizations Based on previous and preliminary works, we have built a first reference corpus of Books of Hours worldwide, including more than 7,000 items, which will be enhanced during the course of the project. In accordance with previous observations, they mostly originate in Italy, France, and the Low Countries, but their liturgical use shows the extent of the influence of Rome. Fig. 1: Map of 7,000+ preserved Books of HoursFig. 2: Place and date of production of 2,400+ Books of HoursFig. 3: Liturgical destination of 2,300+ Books of HoursIn this corpus, we have selected 500 manuscripts that are available through the IIIF protocol. We have successfully applied a page classifier to distinguish bindings, blank pages, calendars, full page or half page miniatures, historiated initials, and plain text pages and published the results as IIIF manifests at https://github.com/oriflamms/HORAE. We then segment the page into lines of text. Next, we read or recognize the text with the same techniques which were applied successfully in the HIMANIS project and now allow full text search to 199 volumes, the largest corpus of medieval manuscripts yet indexed. They rely on deep learningand a dedicated ground truth. NLP techniques are then applied to match the automatically-generated text with editions, in order to identify the different texts and parts of texts that are present in the manuscripts. Our automated transcription is sufficiently accurate to create a mid-level approach at text level and to correctly identify texts despite the remaining errors. The authors develop new techniques, but can also rely on existing software such as Tracer, Collatex and ITEAL. In future steps, we will compare the contents of manuscripts in order to understand historical patterns and identify and edit hitherto unknown texts. Fig. 4: Page classifier: ¾ page miniatures and historiated initials from Books of Hours preserved at the Houghton Library, Harvard UniversityFig. 5: Automated reading of a medieval manuscript as IIIF annotationsFig. 6: Visualization of correspondence between the biblical text of the Psalms and the content of a given Book of HoursChallenges: uncertainty, sequence, and granularity The following methodological challenges should be explained. Uncertainty. HTR produces an error-prone text. Text identification based on methods of text reuse detection should be able to ascertain the textual source and thus create correct data from fuzzy, incorrect and automatically-generated data. Sequential text A+B ≠ B+A. Books of Hours present sections whose ordering may represent only a social practiceor may distinguish their liturgical use, i.e. their geographical validity and applicability. Most techniques of distant reading tend to erase the sequential order of sentences and words. We thus need accurate sequential tables of contents and to identify which variations are relevant for research purposes. Granularity. Books of Hours are built as a sequence of many sections with subsections in a highly hierarchical manner, with conflicting logicaland physicalstructures. Any sequence of a coherent section of several texts can be subsumed in the first sentence of the first text, and words can be represented by their initial letters. This circumstance creates numerous ambiguities, and it is also extremely difficult to ascertain if some parts of the text were omitted in writing but supposed to be said or if they are not meant to be there at all. Therefore, in order to establish a correct table of contents, we have to record what is there and what is not, but also to interpret what should be there. Fig. 7: Simplified structure of a Book of Hours Fig. 8: Granularity of textual content Integrating DH in the humanities Gesamtkunstwerk There are many definitions of DH. We stress the parallel between DH and auxiliary sciences. They are diverse, have a large focus on epistemology, and need specific training, curricula, and data infrastructures. They are also dispersed and taught in several faculties. Equally, they strive for autonomy when their results and tools are used by other disciplines, sometimes without any interest in their construction, relations and inner coherence. HORAE integrates several fields of research in digital humanities with a common goal. It makes an extensive use of DH-specific methods and tools. Having a strong focus on manuscript studies and text transmission, it combines digital research methods and historical study in a mutually-enriching process. It is therefore connected to both traditional auxiliary/fundamental sciences and DH. Beyond that, it implements technologies developed without connection to the humanities. The alignment of research questions from different fields makes it possible to have non-humanities and non-DH partners engaged in mutually-beneficial research, in which no partner is only a service provider to the others. As a consequence, it also disintegrates DH as a specific field by extracting a set of questions, research, and tools, as the new normal in the humanities. With this perspective, we suggest that DH could benefit from analyzing experiences in fundamental sciences to reflect on their fluid position within the humanities. "
	},
	{
		"id": 372,
		"title": "Tikkoun Sofrim – Combining HTR and Crowdsourcing for Automated Transcription of Hebrew Medieval Manuscripts",
		"authors": [
			"Kuflik, Tsvi",
			"Lavee, Moshe",
			"Stökl Ben Ezra, Daniel",
			"Ohali, Avigail",
			"Raziel-Kretzmer, Vered",
			"Schor, Uri",
			"Wecker, Alan",
			"Lolli, Elena",
			"Signoret, Pauline"
		],
		"body": " The French-Israeli project Tikkoun Sofrim is part of a network of projectsaimed at developing a future framework for comprehensive and systematic textual availability, editions and deep annotation ofmanuscripts via a pipeline that combines HTR with crowdsourcing the corrections and validation by scholars. End uses are scholarly editions, library service, automatic annotation, distant-reading and big-data studies. The sample The project focuses on Tanhuma-Yelamdenu Midrashim, late rabbinic exegetical works without full scholarly critical edition, even thought vulgate editions in two recensions [1,2] and translations exist [3]. Unlike other classical works, they were not subject to formal canonization, but went through a long period of fluid transmission [4]. Manuscripts differ significantly from each other on the micro- and macro-levels. The texts reflect different possible hierarchies following the biblical text, liturgical rites or divisions in early print editions. Hence, the envisaged data structure will need to go beyond a simple canonical structure to support flexible accessibility to the texts both from library and critical edition perspectives. HTR The current Layout analysis is based on the z-profile-method for column detection [5] and the heartbeat-seam-carve algorithm for line detection [6], well adapted for literary manuscripts with regular layout. Textual ground truth was created by manual transcription from scratch or through the adaptation of transcriptions sent by scholars who had worked on manuscript-parts. Using the open-source kraken engine [7], we trained models for four large manuscripts[8]. The CERs vary between 2.8%, 2.9%, 6.9%and 8.9%. The likely reasons for the weaker results for Vatican and Geneva were the use of low resolution imagesas well as binarization issues, curved lines and superposed words. These issues will be dealt with in an upcoming, much improved layout analysis based on state-of-the-art CNNs part of kraken@eScriptorium at PSL [9,10,11]. Crowdsourcing Our crowdsourcing platform opened on Feb. 13th, 2019 [12]. To maximise our reach, we offer a designated UI for mobile and desktop. The technological stack used for development was selected to simplify future code contribution and collaboration with other platforms: Vue.js / Google-Firebase for the operational site and MySQL for analytical purposes. Since classical rabbinic literature is still thriving in contemporary ritual and religious study cycles, we based community recruitment and engagement on links to websites for daily studies following annual and other cycles of studies. The launching was set to the beginning of Deuteronomy in 929 , a community daily Hebrew Bible chapter project, and we assigned fitting pages to each biblical chapter. While the link with 929 proved very fruitful for a first broad exposure to the public, by and large ongoing traffic mainly came from intensive facebook activity in relevant groups and pages. Attracted first by hilarious mistakes in the automatic transcription, users continued into long term engagement. Other crowd-engagement methods involved presentations in schools, synagogues, universities and homes for senior citizens Israel, France and the US. Two articles in the press, one of which included the URL of our website, greatly augmented the number of users in week 6 [13,14]. Results Until April 26, we received on the average 5005 lines per week: Mega-usersand super-users, presenting 4.6% and 19.3% of the number of user, provided almost 80% of the total amount of transcriptions. Based on the submitted transcriptions, we produced an alignment on the word level with collatex [15,16]. With a majority vote, we established a preliminary complete text for the 590 pages of Geneva 146. According to manual check, the automatic reconstruction results in about two mistakes per page, i.e. an accuracy of ca. 99.8%, most of which concern markup for additions or deletions. A transcription based on collating contributions of 5 users/line. Conclusions and outlook We now have the tools and know-how for high-quality automatic transcription of Hebrew manuscripts. Crowdsourcing is a promising mechanism for correcting remaining errors. Future versions of the platform should include embedded community engagement, feedback elements to ensure continuity and optimize social media engagement activity. The project [17] provides PoC for future large scale production of manuscript texts, based on combined deep learning and crowdsourcing mechanisms. These will be based on optimizing the performance of HTR engines and crowd task-assignment, using crowd based transfer learning and weak supervision . "
	},
	{
		"id": 373,
		"title": "One More Time With Feeling: Revisiting XPointers to Address the Complexities of Promptbook Encoding",
		"authors": [
			"Takeda, Joey",
			"Roberts-Smith, Jennifer"
		],
		"body": " Though the Text Encoding Initiativehas long supported the use of the XML fragment linking mechanism known as XPointers, they are seldom implemented or recommended as a method of linking TEI documents. Hugh Cayless has argued that TEI pointers are an underappreciated mechanism, the victim of a Catch-22: the specifications are difficult to grasp, and as a result, most people never bother to try using [them]. Though Cayless proposals for new definitions of TEI pointers are compelling, five years later, his diagnosis holds true: the definitions of the pointing schemes have not been updated and there is still a paucity of examples of XPointers in TEI projects. This short paper responds to Cayless by outlining the use of TEI pointers in the Waterloo-based Stratford Festival Onlineproject, which aims to encode the Festivals world-class collection of theatrical promptbooksin TEI. Through this real-world example of the necessity of XPointers in the SFO project, we make the case that they may also be a viable option for other TEI projects. Promptbooks are difficult objects to encode because they are simultaneously both textual artifacts and also instructions for actualizing the aesthetic decisions that define a particular production of a play. Over the course of a rehearsal process, the stage manager creates a book to remind themselves what they will need to do during each performanceto ensure the performance is consistent. The stage manager records the what of the bookalong a timeline showing when each action needs to happen; the when recorded in the book takes the form of words spoken by actors. The text of a promptbook is a performance timeline along which the events of a performance are sequenced for the stage managers reference. Promptbooks therefore contain three ontological categories of content: 1) a timeline; 2) descriptions of performance events; and 3) annotations made by the stage manager, which link events to the moments on the timeline when each should occur. Current TEI Guidelines for Drama cannot accommodate this ontological complexity. To complicate matters further, stage managers use a range of idiosyncratic marks to link performance events to moments in the textual timeline, and these moments may in turn be literally moments, as brief as the space between two syllables pronounced by an actor, or may conversely be periods as long as several lines of spoken text. Figure 1. Some systems for linking events to moments in the timeline of spoken text in the promptbook from the Stratford Festivals King Lear. In response to these challenges, our research team is developing an approach to promptbook encoding that uses two data filesthat are linked by stand-off markupand XPointers. Our choice to use XPointers derives from earlier experiments with standard techniques of standoff markup. We initially attempted to use <anchor> elements, repurposing the TEIs double end-point-attachment method. However, this approach makes the event document unusable without the text document. Since one of the projects goals is to map events to other electronic versions of the same base text, we rejected <anchor> elements. We also considered tokenizing each character of the text document so that an encoder could specify a range of characters. This method does not require an encoder to add elements manually to the performance text document and could enable the creation of user-friendly markup interfaces. However, linking to individual <c> elements in the text document requires that the performance text be edited completely before running a tokenizing algorithm, whereas the projects workflow requires encoding the text and event documents simultaneously; and adding <c> elements significantly increases the size of the document, makes it difficult for human encoders to read and seriously inhibits any further editing of the document. By contrast, the TEI pointer framework provides a non-intrusive and event-centric method of addressing segments of the text document, because it does not rely on the encoding of the text document. There are multiple TEI pointer schemes that can handle stable element ID references, character position references, and string matching. To be sure, using TEI pointers in a standoff document is not a mutually exclusive practice with encoding <anchor>s or tokenizing the source text; instead, TEI pointers offer the project further options for addressing segments of text beyond ID references, including XPath, string matching, range selection, and left and right point selection. For instance, TEI Pointers can take advantage of the matchfeature, which allows encoders to specify a string to address based off a canonical referencethat two projects share as an interoperable data-point, like a canonical line number. This is not to say that TEI Pointers are inherently more interoperable than other methods,but, instead, provide another mechanism for addressing contiguous and adjacent nodes, which is difficult to do purely with IDREFS. Consequently, we are developing a method, using Apache Ant and an XSLT function library, of parsing TEI pointers and associating the text document with the standoff markup. Since our approach to resolving XPointers requires a two stage process, this paper does not describe how to resolve XPointers in standard just-in-time setups; instead, we demonstrate a method of handling XPointers that can be integrated as an intermediate process in a static build pipeline, the infrastructural benefits for creating sustainable and archivable digital projects will be dealt with in further detail elsewhere. Since XPointers address segments that are not necessarily stable or canonical and links between the standoff document and the source document can easily break, we have also developed a suite of diagnostic tests to ensure that the XPointers used in the project are, at minimum, resolvable. Both the sample implementation and the diagnostic code, along with documentation discussing the development of and differences between XPointers and TEI Pointers, will be available via Github as a proof-of-concept for the feasibility of using XPointers within TEI projects. We hope that other projects—particularly those dealing with standoff annotation structure or other situations where the source text is not available for editing—can make use and, ultimately, improve the TEI Pointer system for use across various domains. "
	},
	{
		"id": 374,
		"title": "“Alle Begjin Is Swier\": The Use Of The Frisian Web Domain Web Data For Digital Humanities Research",
		"authors": [
			"Teszelszky, Kees"
		],
		"body": " We have seen an steep growth in the amount of general Top Level Domains outside the country code Top Level Domainsafter the Internet Corporation for Assigned Names and Numbersallowed the introduction of new generic top-level domains on June 26, 2008. The most interesting generic TLD from the point of web archiving in the Netherlands was the introduction of the Frisian top-level domain .frl in 2014. Fryslânis a region of the Netherlands with its own minority language and culture. Around 354,000 people have the Frisian language as their native language. Since 2014, no less than 14,000 URLs have been registered with this domain extension, of which 60% is a website. Many of these websites have Frisian language content on it. This poster will describe the plans, the preliminary results and the research potential of The Koninklijke Bibliotheek – National Library of the Netherlandsto map, harvest and create a web data set out of the Frisian web domain starting with the .frl TLD. KB-NL as a national library has collected born digital material from the web since 2007 through web archiving. It makes a selection of websites with cultural and academic content from the Dutch national web. Most of the sites were harvested because of their value as cultural heritage of the Netherlands, including the Frisian heritage and culture. KB-NL works together with Tresoar, the repository of the history of Fryslân, to select and harvest born digital content of the Frisian web for its web archive. I will describe the methods and experience with mapping, selecting and harvesting websites of the Frisian web domain. I discuss also the characteristics of web materials and archived web materials and will explain the use of these various materialsfor digital humanities research. A future harvest of the Frisian web domain will provide future researchers with an unique born digital data set of a minority language which can be combined with other similar data sets of the Frisian language. "
	},
	{
		"id": 375,
		"title": "Taking Digital Humanities to Guatemala, a Case Study in the Preservation of Colonial Musical Heritage",
		"authors": [
			"Thomae, Martha E.",
			"Cumming, Julie E.",
			"Fujinaga, Ichiro"
		],
		"body": " The goal of this project is the preservation and dissemination of Guatemalas colonial musical heritage by applying music information retrieval tools to a group of Guatemalan manuscript sources while maintaining the original sources in their homeland. This group of sources is written in mensural notation, a music notation style used in Europe throughout the Late Middle Ages and the Renaissance. The only known Guatemalan mensural repertoire is contained in two extant manuscript sources:the Santa Eulalia manuscripts, copied in the late sixteenth century; andthe Metropolitan Cathedral choirbooks, copied during the sixteenth to eighteenth centuries. The Santa Eulalia manuscripts, a collection of fifteen books originally found in a north-west region of Guatemala, are no longer in the country. Therefore, the set of choirbooks kept in the Metropolitan Cathedral of Guatemala City contains the only mensural repertoire that remains in the country, and so it is of utmost importance to treasure and preserve them for future access. In this paper, we will explain the different steps that will be taken to guarantee the preservation and accessibility of the Guatemalan Cathedral choirbooks. The musical heritage from the colonial period of Latin American countries is not well known around the world. Even though colonial music is rooted in the early Western music traditions, little is known about how these traditions and repertoires evolved in the Spanish colonies of the Americas. The lack of high-quality digitization of music documents from these countries and era limits access to people able to visit the site that has the original sources. Furthermore, the use of old music notation restricts performance of the music to the knowledgeable experts. In this paper, we will use recent music encoding technologies to address the accessibility issues of the mensural colonial repertoire of the Americas. The barriers to accessibility in the body of Guatemalan mensural music can be summarized into:the lack of high-quality digital images,the notation style, andits layout. Firstly, high-quality images are necessary for the music to be readable. While low-quality images can still be read by humans, high-quality images are necessary to encode the music in an automated way using techniques such as Optical Music Recognition. Furthermore, high-quality images allow for digital restoration, enhancing details nearly invisible in the original sources. Secondly, mensural notation differs significantly from our current music notation system in that the duration of notes is not absolute, but rather context-dependent. This characteristic makes it impossible for non-experts to accurately read this music. Lastly, the parts corresponding to each voiceare written in separate areas of the page instead of being vertically aligned as in a modern score format. Together the context-dependent nature of the notation and the separate-parts layout hinder the appreciation of the polyphonic texture of the music. It is only until musicians acquainted with the notation sing the various parts together or until an expert transcribes the music into a modern score that these textures can be really perceived and enjoyed. The layout of the original music deters its study even for experts, because it is hard to visualize the vertical relationships between notes sung simultaneously in two different voices, something that only becomes clear when the music is presented in score format. In this paper, we will present the step-by-step process that will unravel the accessibility barriers of this repertoire, resulting in the digitization and encoding of the repertoire as musical scores in a machine-readable file format. This procedure involves the use of the following tools: A Do-It-Yourselfbook scanner. Given their antiquity, these books cannot be digitized face-down using a common flatbed scanner. On the contrary, they need to be digitized from the top using a book scanner and, ideally, using a v-shaped book cradle to lower the stress on the book spine. Currently, there is no book scanner in Guatemala that has the appropriate dimensions to digitize these books. Since buying a large book scanner is out of our possibilities, we will build our own by emulating their book cradle, lights, and camera configuration. Optical Music Recognitionsoftware applications. Just as Optical Character Recognitionallows the machine to read the characters written in the page, OMR systems read the music symbols from a music document. OMR software that will be used to read and encode the sources include: Pixel.js, Gameraand a machine learning model developed by Pacha and Calvo-Zaragozathat was trained with Spanish mensural music. The resulting data from the OMR process will be encoded in the MEIformat. The mensural notation Scoring-Up Tool. This set of scripts convert the part representation of the score into a modern rendering, providing a more conventional and modern representation to musicians and lay people. We hope that the digitization and encoding of the music documents with the aforementioned techniques will allow for the preservation and dissemination of Guatemalan mensural music—a historic musical heritage that otherwise may be lost, forgotten, or damaged. The result should facilitate its dissemination, study by musicologists, and appreciation by the general public, especially for repertoires that are virtually unknown abroad. Furthermore, we expect this research to be used as a model for the digitization of the mensural repertoire of other countries that were once part of the colonial past, such as the Spanish Colonial Empire. Music Manuscripts US-BLI: Bloomington, Indiana University, Lilly Library MS 1–15. GCA-Gc: Guatemala City, Catedral, Archivo Capitular 1–4. "
	},
	{
		"id": 376,
		"title": "Atlantic Journeys through a Dutch City's Past: Building a Mobile Platform for Urban Heritage",
		"authors": [
			"Thompson, Mark L.",
			"van der Woude, Joanne"
		],
		"body": " The complexities of the relationship between Europe and America—historical, commercial, ideological, cultural—are so manifold that digitally rendering them may seem quixotic. Yet our mobile software application Amerigo sets out to do just that: explore meaningful, multi-layered connections through space and time between the Netherlands, America, and the broader Atlantic World. In its current phase, Amerigo uses a single location as its jumping-off point—the city of Groningen in the north of the Netherlands—and maps its role in Atlantic colonial networks during the Dutch Golden Age. It charts these relationships through an interactive digital narrative interwoven into the space of the city itself. The lead character in Amerigos first storyline is Rock Brasiliano, a Groninger who went to Dutch Brazil in the mid-seventeenth century and then became one of the archetypal pirates of the Caribbean. As users of Amerigo walk through the city, they will also encounter other memorable characters from the period, including a crypto-Copernican scientist who helped to found the University of Groningen; his mother, a Mennonite martyr; rival botanists arguing over West Indian specimens; shopkeepers vending tobacco from the English Chesapeake; and a Tupi Indian from Brazil training to become a Reformed minister. These characters—all based on real historical individuals—vividly illustrate Groningens place in a multi-nodal Atlantic network. Indeed, this complex set of relations, seemingly too vast for one persons grasp, becomes tangible and comprehensible as Amerigos users choose their way through the story-space of the city. Why is early modern Groningen an especially promising locale for such an experiment? In the last twenty-five years, study of the Atlantic World as a complex, interconnected space has drawn together once disparate histories of early modern Europe, the Americas, the Caribbean, and West Africa. At the same time, scholars and activists in Europe have brought increased attention to the colonial past embedded within their own countries and cultures, especially with regard to slavery and the slave trade. Meanwhile, heritage institutions have begun to highlight dimensions of this past, as well—often sparking controversy as triumphal accounts are challenged or replaced by complex, critical, multi-centered narratives. Finally, individuals and institutions are developing new forms of digital heritage through websites, games, and tours that shed light on these historical connections. History and heritage, always closely linked, have only become more tightly connected as they move into the digital sphere. How these producers, consumers, and pro-sumers of digital history, culture, and heritage interact, however, is an open question. A key location for all of these trends and processes is the contemporary city, which serves at once as a site for the production of modernity and a monument to its lost past. Human geographers, urban planners, and cultural theorists have long employed qualitative and quantitative methods to examine how people use, understand, and construct urban space in time. In more recent years, media scholars have brought new attention to mobile narrative and locative media as subjects of study, and heritage research has focused on the significance of contemporary cities as locations for the production, consumption, and study of heritage. Meanwhile, the efforts of cycling and pedestrian advocates have converged with cities attempts to promote urban tourism and led to more walkable, bikeable urban centers that lend themselves to newforms of historical exploration and cultural experience. Groningen, a busy, dense university town interlaced with shopping streets and bike paths, turns out to be an excellent location for the study of all of these domains. In 2019 our Groningen-based consortium—consisting of academic researchers, multiple heritage institutions, and a private app developer—began building Amerigo. The application will serve different groups in different ways. As a mobile tour app produced by a commercial software company, Amerigo will enable a broad audience of users to explore the city-space of Groningen through its complex historical relationships with the Americas and the Atlantic World and to see its present and past in a new light. As a platform for the production and distribution of historical and cultural research, Amerigo will provide scholars and students with a central portal for connecting to archival collections and resources, conducting collaborative research and teaching, and sharing their complex results with the public through interactive narratives. And as a tool for the study of user behavior in relation to digital heritage, Amerigo will allow the members of the consortium to study the complex problems of how different audiences move through the space of the city, how they engage withthe curated content, and how they connect with one another through the application. It is here in this last area—measuring and testing how users interact with a digital heritage platform linked to a real urban space—that we foresee having the most significant impact in the digital humanities. Amerigo offers a new way of exploring these complexities together. In our short paper we discuss the preliminary findings from our project up to the present. This discussion will include a brief overview of the paths we have takenup to the current state of the application as of July 2019. We will also discuss our efforts to integrate development of the app with an experimental undergraduate course that was led by one of the researchers in the spring of 2019. By the time of the conference, we aim to have an early version of the application available for testing, and we are eager to receive feedback from attendees. Ultimately, our paper is intended not only to offer a portrait of a digital heritage project in progress but also to foster creative and critical thinking about how we might use digital methods to engage with our complex past. "
	},
	{
		"id": 377,
		"title": "Blocumenta: An Experimental Art Project on the Blockchain",
		"authors": [
			"Thwaites, Denise",
			"Pailthorpe, Baden"
		],
		"body": " From Bitchcoin and CryptoKitties to distributed ledgers for nuclear non-proliferation, feverish adoption and experimentation with blockchain technology is matched only by the promissory hype that accompanies it. This paper presents the aims and background of Blocumenta - an experimental blockchain-based contemporary arts project that deploys creative methodologies to investigate the poetic potential of blockchain, while exploring alternate models for funding, archiving and historicising the creative practices of our era. Since the Satoshi Nakamoto Bitcoin White Paper in 2008, blockchain has ignited public debate, carving out a distinctive place in the cultural imaginary. However, current research recognises that the celebrationist discourse surrounding the Blockchain Revolutionmust be matched with rigorous critical interrogation to ensure the positive development of this technology for human societies. The Blocumenta project thus distances itself from the crypto-libertarian blockchain evangelism that is commonly associated with this field of tech development. Instead it builds upon research emphasising the cooperative properties of this technology, and its potential to enable decentralized governance of community assets. Departing from an initial stage of creative engagement and aesthetic experimentation across local arts and tech communities in Sydney and the Yirrkala, São Pauloand Guangzhou, Blocumenta interrogates whether a distributed, autonomous and trustless contemporary art archive can in fact overcome the limitations of centralised and often exclusionary art historical narratives, or whether the cultural and historical biases simply find new iterations and determinations that can be expressed through this emergent technology. Blocumenta does this through the design and implementation of a distributed autonomous arts organisation, that prototypes: [1] A new technical and economic model for cultural exchange addressing material inequalities that pervade artistic production, using a cryptocurrency crowdfunding structure that builds upon existing financial, social and cultural applications of blockchain, including Bail Blocand Game Chaingers; [2] A distributed applicationand blockchain architecture for artists to document, present and archive their cultural practices, cooperatively governing this cultural asset that serves as an alternate resource to circumvent the digital clutter and corporate biases of the Web 2.0. A future-oriented cultural heritage project, Blocumenta distinguishes itself from current explorations into blockchains promise as a tool to revolutionize artistic practices and communities, as seen in programs delivered by EU funded initiatives such as DAOWOand State Machines. Its distinction is found in a commitment to decentralizing the historic eurocentrism of global contemporary art, and in arguing that conversations regarding the cultural development of Blockchain as a decentred technology should equally strive to realise a globally decentred human approach to cultural heritage. Likewise, Blocumenta serves as a counterpoint to long standing approaches to the digital museum, which are underpinned by an ethos of open access to data, as exemplified by institutions such as SFMOMA. Rather than providing a window into the content and organisation of traditional collecting bodies, Blocumenta intervenes in the composition of an immutable archive governed by artist collectives based across the Global North/South divide, experimenting with archival protocols and formats that challenge the stasis and rigidity long associated with the museum as mausoleum. Through its decentralized governance structures, it raises questions such as: What kind of work merits archiving? How could the immutability of blockchain archiving provoke a different kind of digital activity, through its trustless accountability? How might such technical architectures create distinctive exhibition, performance and reception spaces for contemporary art practices, from new media to performance to literature? Finally, in this paper we will consider how Blocumenta exemplifies a shift away from a digital age characterised by an ethos of free and open access to content that is, in reality, underpinned by hidden centralised informational systems mediated by corporate interlocutors. Building upon Birchalls discussions of shareveillanceand Glissants concept of opacity, we will question whether digital systems in which archival participation and access must be earned through the cooperative labour of a decentralized community of stakeholders, may be an appropriate antidote to the implicit power dynamics of traditional archives, as well as the chaotic transience of Web 2.0 digital culture. Subverting the original aims of the quinquennial survey of contemporary art Documenta, Blocumenta challenges the homogenising and colonial processes associated with the encyclopaedic museum, archive and exhibition, envisaging new forms of cultural heritage grounded in dynamic collaboration and equitable exchange between members of a globally distributed community. "
	},
	{
		"id": 378,
		"title": "AVinDH SIG Workshop",
		"authors": [
			"Tilton, Lauren",
			"van Gorp, Jasmijn"
		],
		"body": " DH2019s theme Complexities asks the field to focus on DH as the humanist way of building complex models of complex realities, analysing them with computational methods and communicating the results to a broader public. Understanding the diversity of the human experience means turning to diverse sources, which includes the different forms in which people produce knowledge and communicate ways of knowing. One way that people make and communicate meaning is through audio and visuals. Forms such as drawing, film, and radio construct, convey, and circulate our complexities including beliefs, values, and histories. Analyzing these forms has become an increased object of study within DH; a field which has traditionally focused on text. With this move from the periphery to the center of audio and visual, scholars are turning to computational methods to increase the discovery, analysis, and accessibility of AV materials. The AVinDH SIG Workshop will bring together DH scholars interested in learning how to analyze AV materials. The day-long workshop will include: 1. Hands-on Sessions that share approaches and methods for computational image and audio analysis. The workshops will cover approaches to computational analysis of audio and visual. The workshops are designed to offer strategies across the analysis pipeline from collecting data, assessing the quality of AV data, video annotationand image analysis. Two sessions will be running simultaneously. The AVinDH SIG decided to include two tutorials with tools developed by CLARIAH since the conference is located in the Netherlands. This will allow multiple members of the teams who have built these tools to attend, which would be cost prohibitive if all of the members had to travel, and support DH members who are interested in learning about these tools. The other two sessions are from scholars based in the United States. 2. Lightning Shorts where participants can share briefly share projects, with a focus on works in progress. The Lightning Shorts session will focus on current research in AV DH. Each participant will have three-minutes to introduce their work. The goal of the session is to share a range of work in the field in the same space so that participants can quickly become aware of each others work as well as identify those their scholarship is in conversation with. It also allows more participants than traditional models like paper sessions. Participants will be invited to register ahead of time and also will be able to sign-up the day of the workshop if there are time slots remaining. 3. Closing Session that provides a recap of the day. The AVinDH SIG Workshop builds off of several years of effort to center AV in the field. In Lausanne, Krakow, and Montreal, the AVinDH SIG led an all-day mini-conference. The workshops were primarily a conference format in which participants submitted proposals, were reviewed, and then selected to present their research. Attendance has steadily increased. The AVinDH SIG is now shifting toward hands-on workshops. DH scholars have been forging new computational approaches and building software as well as platforms to facilitate computational analysis of AV due to the rapid advancement in deep learning and computer vision. Due to this shift, the AVinDH SIG offered a half-day workshop on computer vision with images in Mexico City. The sold-out workshop indicated significant interest in hands-on workshops. For 2019, AVinDH SIG is looking to blend the two models. We are proposing a day-long workshop for 2019 because workshops require significant time to be effective; there has been an exciting increase in DH tools and software in this space; and, the SIG would like to support work in audio and visual analysis. The lightning shorts still offer a space for people to present their research and share projects with the community. The closing session will be a recap of the day and closing remarks. Schedule: https://avindhsig.wordpress.com/workshop-2019-utrecht/ "
	},
	{
		"id": 379,
		"title": "Exploring Audiovisual Corpora in the Humanities: Methods, Infrastructure, and Software",
		"authors": [
			"Tilton, Lauren",
			"Arnold, Taylor Arnold",
			"Bergel, Giles",
			"Van Gorp, Jasmijn",
			"Noordegraaf, Julia",
			"Melgar, Liliana",
			"Williams, Mark",
			"Bell, John",
			"Ordelman, Roeland",
			"Poell, Thomas"
		],
		"body": " Image analysis is increasingly central to the field. DH scholars are a critical part of interdisciplinary teams developing infrastructure for analyzing audio-visualculture at large scale. The first paper titled Computer Vision Software for AV Search and Discovery describes novel approaches to increasing discovery of AV content in cultural heritage institutions by focusing on four search modalities; how computer vision can complement and augment traditional metadata-based content management; and, use a new multimodal approach to search in which correspondences between the audio and visual content of videos are learnt from unlabelled data. Analyzing Moving Images at Scale with the Distant Viewing Toolkit then turns to how digital humanists can use DVT and why we should be involved in creating image analysis infrastructure. The paper will begin with an overview of DVT, a software library that summarizes media objects through the automated detection of stylistic and content-driven metadata. The paper will then turn to how and why humanities scholars need to be a part of questioning and then retraining algorithms that address our scholarly concerns and commitments. Training computer vision algorithms on historic images requires new training sets and metadata. The third paper,  The Media Ecology Projects Semantic Annotation Tool: Collaborative Synergies to Train Computer Vision Analysis will discuss SAT, which enhances the precision of existing annotation methodologies by adding geometric targets within the frame, and also provides the infrastructure to instantiate an iterative process of computer vision algorithm refinement. The paper will detail an innovative multi-partner project to collate, annotate, and mobilize for algorithmic research significant curated metadata related to WWI footage at the U.S. National Archives. From user to co-developer: Strategies for a User-centered Approach to Building Media Analysis Infrastructure will then discuss re-centering users in the development and implementation of infrastructure. The research communities expected to use these platforms must be collaborators from the beginning so that their needs are incorporated, else broad and generalized infrastructure will serve few needs well. They ground their argument in a case study, which discusses a year-long pilot program to engage scholars in the development of Media Suite; a media studies toolkit for audiovisual research that is a part of the Dutch national research infrastructure CLARIAH. In line with ADHOs commitment to amplifying diverse voices, the panel is intentionally designed to represent different institutional, national, and funding structures. There are two U.S. based teams: one is from an elite, private Carnegie Research 2 university in the Northeast and the other is from a small, private liberal arts college in the South. The other two papers reflect work from public research-intensive universities in Europe. Funding includes government grants, commercial partnerships, and private philanthropy. Participants represent a range of fields including American Studies, Media Studies, and Statistics. The panel also includes three women presenters, a demographic regularly underrepresented in technical research, particularly computer vision. The panel is therefore designed to bring different perspectives on how to approach audiovisual DH infrastructure. Computer Vision Software for AV Search and Discovery This paper will describe current work in making audiovisual content searchable by using computer vision to identify features in images. The talk will, first, describe ongoing collaborations with cultural heritage sector partners that implement the current state of the art in computer vision software and, second, outline new research that improves the characterisation of AV content. The focus will be on VGGs collaborations with BBC and the British Film Institute. The intention behind these partnerships is twofold: first, to assist maintainers of large audiovisual datasets to benefit from proven computer vision techniques; and second, to provide VGG with opportunities to refine and advance the state of the art in the field through working on tasks such as facial recognition across large datasets. As the BBC resource is publicly accessible it will provide the main case-study for the first part of the paper. The BBC resource implements four search modalities. It contains over ten thousand hours of broadcast news and factual programming, recorded over five years from several British TV channels, amounting to over 5 million keyframes. The paper will demonstrate each search mode; describe the methods upon which each is based; and note the performance of retrieval in relation to the state of the art. The paper will give an account of how the resource is indexed and maintained; how the collaboration is structured; and how the methods might be integrated into either public-facing discovery tools or in-house AV management systems. Turning to the partnership with the British Film Institute, the paper will outline how computer vision can complement and augment traditional metadata-based content management.The BFI dataset combines a digital video archive of films and television programmes described through the use of custom metadata. While visual recognition and metadata-based search each have their own strengths, the paper will outline their strength in combination – metadata providing training data and ground truth, while visual recognition provides a means of identifying inconsistencies or errors in metadata. The paper will discuss the practicalities of integrating the two modes in a collection-management environment as well as the importance of a development ecosystem based on open source software, documented standards, and a support community. It will propose that the digital humanities community could take a leading position in building such an ecosystem for computer vision. Last, the paper will outline a new multimodal approach to search, in which correspondences between the audio and visual content of videos are learnt from unlabelled data, with the result of localising sound-production and improving visual categorisation. The paper will explore some of the potential uses of identifying these correspondences in digital humanities research projects. Analyzing Moving Images at Scale with the Distant Viewing ToolkitThe paper will address how digital humanists can use the Distant Viewing Toolkitand why we should be involved in creating image analysis tools. The paper will begin with an overview of DVT—a software library that addresses the challenges of working with moving images by summarizing media objects through the automated detection of stylistic and content-driven metadata. It algorithmically approximates the way humans process moving images by identifying and tracking objects, people, sound, and dialogue. The DVT software library allows users to input raw media files in a variety of formats. The input files are then analyzed to detect the following features:the dominant colors and lighting over each shot;time codes for shot and scene breaks;bounding boxes for faces and other common objects;consistent identifiers and descriptors for scenes, faces, and objects over time;time codes and descriptions of diegetic and non-diegetic sound; anda transcript of the spoken dialogue. These features serve as building blocks for the analysis of moving images in the same way words are the foundation for text analysis. From these extracted elements, higher-level features such as camera movement, framing, blocking, and narrative style can be derived and analyzed. After an overview of the features, we will then discuss the output formats and how non-technical users explore and visualize the extracted information. This discussion will center on a specific application to the research question of how to organize and search a large collection of local television news programs. While explaining the usage of the toolkit in addressing humanities questions, we also address how and why humanists should be involved in creating image analysis methods. We will discuss how and why humanities scholars need to be a part of retraining algorithms in order to build computer vision methods that address our concerns and intellectual commitments. Specially, we will talk about how we are applying transfer learning to tweak the open-source computer vision algorithms to better function on moving images from across the 20th century. The toolkit utilizes and adjusts the architecture of three open source programming libraries: dlib, ffmpeg, and TensorFlow. Within these frameworks, novel computer vision and sound processing algorithms extract the required features. Specifically, the project draws from VGGFace2 for face detection; YOLOv3 for object detection; AudioSet/VGGish for converting sound to text. Our work in building DVT consists of modifying and stitching together these six models for our specific humanities-centric needs. Digital humanists are well positioned to bring our critical lens to computer vision in order to facilitate archival discovery and visual culture and media studies scholarship. Thanks to the support of a 2018 United States National Endowment for the Humanities Office of Digital Humanities Advancement Grant, a fully working development version of DVT has been made available on GitHub. The Version 1.0 release is scheduled for Fall 2019. The Media Ecology Projects Semantic Annotation Tool: Collaborative Synergies to Train Computer Vision Analysis This paper will introduce new work in the development of The Semantic Annotation Tool, an NEH-funded component of The Media Ecology Projectthat affords the creation of precise time-base annotations of moving images. MEP is working to realize a virtuous cycle of archival access and preservation, enabled by technological advance and new practical applications of digital tools and platforms. In a fundamental sense this is a sustainability project regarding moving image history as public memory. By fostering innovations in both granular close-textual analysisand computational distant reading, MEP is a collaborative incubator to develop 21st-century research methods. We will report on our innovative multi-partner collaboration to collate, annotate, and mobilize for algorithmic research significant curated metadata related to vast collections of World War I footage at The National Archives and Records Administrationin the U.S. The key collection to be focused on for this presentation is footage produced by the U.S. Signal Corps, which represents the first major motion picture endeavors by the U.S. military. Within MEPs emerging research environment, Mediathread has allowed the documentation of written descriptions and tagging through full-frame time-based annotations. Deploying the Semantic Annotation Toolenhances the precision of our already granular annotation methodology by adding geometric targets within the frame and real-time playback of annotations with sub-second resolution. SAT also provides the backend infrastructure to model and instantiate an ongoing, iterative process of computer vision algorithm refinement that features 1) manual time-based annotations that help to train deep learning algorithms; 2) the production of many additional time-based annotations via these algorithms; 3) the evaluation and refinement of the new annotations by means of further manual annotation work within the SAT environment; and, 4) the application of the resultantly refined annotated dataset as the new training set. This paper will detail a first-draft test case of such an SAT workflow. The collection of Signal Corps WWI footage has been selectively curated into a test dataset of 300 films. Within this dataset we have developed via manual annotation a training set of annotations from 50 films for use in training the Distant Viewing Toolkit, which is being developed by a team at another U.S. university. The training metadata is derived by translating into SAT the metadata from two unique collections of existing metadata: 1) the shot log of archival footage utilized in a recent 3-part PBS American Experience program about the history of WWI, and 2) selected notecards of precise shot-specific metadata produced as part of the documentation by The Signal Corps itself. The SAT will be demonstrated to afford a unique and unprecedented collaboration between digital humanities scholars and scientists, professionalfilmmakers, and cultural memory archives in the advancement of DH audio-visual analysis toward enhanced video search and multimedia information retrieval. From user to co-developer: Strategies for a User-centered Approach to Building a Media Analysis Infrastructure Digital humanists experiment with information technologies and develop tools, either using the scholars own skills or in close collaboration with data and system experts. Toolmaking has taken place within the digital humanities for over seventy years. Thus, the design of systems and the understanding of their underlying processes are inherently at the core of scholarly investigation. According to Patrik Svensons, this engagement assumes different forms including using information technology as a tool, as an object of study, as an exploratory laboratory, as an expressive medium, and as an activist venue. Thus, being a digital scholar implies not only being a user of a tool or an evaluator of its performance, but also being at the center of a tools design from the beginning. This paper describes one way in which scholars can directly participate in the construction of digital humanities infrastructure by means of a case study. We will focus on the Media Suite, a media studies toolkit for audiovisual research, part of the Dutch national research infrastructure CLARIAH . In certain contexts, there has been a move to create overarching infrastructure for the sciences, humanities, and social sciences at the national and international level. The aim is to coordinate efforts in data sharing, interoperability, access, and tool provision according to standards. Thus, personal, bottom up engagement with tool building has given way to a more supervised, top-down way of tool and data provision. At the same time, these generalized infrastructures are expected to be usable in specific, collaborative research settings. The question for these infrastructures of if we build it, will they come?becomes the fundamental paradox of how to attract scholars with specific research questions to use tools and data that is generic and aimed to serve broader groups. Consequently, to avoid the risk of broad digital humanities infrastructures creating a separation between scholarsand infrastructure developers, they need to devise ways to more directly involve the communities that they aim to serve. In order to do this, CLARIAH has set up a pilot scholar program in which scholars were invited for one year to offer feedback and help developing the infrastructure. This paper describes the strategies we used for co-developing the CLARIAH Media Suite with the six pilot research projects that were involved, including: design sessions, a living demonstration scenario document, a chat room connected to the development issues in the Github repository, a summer school, and the participation in a series of demonstration meetings organized by the overall CLARIAH pilot project program. In addition, this paper presents the results of the evaluation of the six research pilot projects. Since there is a lack of common evaluation frameworksin digital humanities projects, we evaluated the pilot projects using a questionnaire and interviews based on these topics categories: research outcomes, Media Suite improvements for research support, the scholars new skills, and dissemination outcomes. "
	},
	{
		"id": 380,
		"title": "Poetry In Motion: Quantified Self Data And Automated Poetry Generation",
		"authors": [
			"Tonra, Justin",
			"Davis, Brian",
			"Kelly, David",
			"Khawaja, Waqas"
		],
		"body": " Eververse is a project which synthesises perspectives from disciplines in the humanities and sciences to develop critical and creative explorations of poetry and poetic identity in the digital age. Deploying tools and methods from poetic theory, data analysis, and Natural Language Generation: the automatic production of natural language output from a non-linguistic data source. Eververse uses data from quantified selfdevices to automatically generate and publish poetry which correlates to the wearer/poets varying physical states. Context One of the more common ideas to be found in different statements and theories of poetry presents the poet as a creative vessel or conduit, admitting the sensory input of the world into their bodies and minds, and producing poetic output in turn. For Walt Whitman, every atom of being and sensation was an appropriate inspiration for poetry, and his works exhaustively catalogued the varieties of personal and public experience in nineteenth-century America. In poetics such as this, art increasingly collapses into the being and identity of the artist. Poet W. B. Yeats famously articulated this conundrum, asking How can we know the dancer from the dance? The production processes of print, which intervened between the poet and their desire for immediate and spontaneous expression, are circumvented in the digital age, as poets use the internet and social media to create a poetry that is vast, instantaneous, horizontal, globally distributed, paper thin, and, ultimately, disposable. The internet is shaping contemporary poetry with these characteristic forms, and by incorporating the modalities of images, gifs, video, and sound. Eververse seeks to explore networked technologies and their affordances to articulate new and novel means of being a poet in the digital age. Project description Eververse sends biometric data from a Fitbit fitness tracking device worn by the project PI/poet to its custom-built poetry generator. This generator utilises NLG techniques to output poetic text published in real time, and 24/7, on the Eververse website. The form and content of the poetic output is designed to change according to different physical sensations and experiences in the poets waking and sleeping life. Following Charles Olsons injunction that the line comes...from the breath, from the breathing of the man who writes, Eververses poetic lines decrease in length as the poets heart rate increases and breath contracts. Similarly, the response to the randomness of the dream sleepstate is an increased irregularity in the poetic form. Content, too, reflects these variations, as heightened-sentiment vocabulary is produced to reflect the emotional intensification of an increased heart rate, while the dream sleepstate generates surreal images and vocabulary. Technical description The Eververse application consists of three main modules. The first module interfaces with the Fitbit device and its data through its Application Programming Interface. The activity data of the poet wearing the device is then sent, in JSON form to the NLG module referred to as the generator. This generator carries out a number of steps in order to generate and return a poetic couplet based on a conceptual model of states based on the activity information contained within the passed JSON data. The number of words and the frequency of the generated couplets correlate with the heart rate of the poet, whereas the textual content of the couplet is generated from the input corpus which is fed to the generator. Thematic links exist between the content of the input poetic corpora and the conceptual topics of physiology, and sleep and the bodycorpus comprises poems on the topic of the body; all poems are previously published and none is composed by the Eververse poet. In order to disassemble and reassemble the corpora for publication, they are arranged in a reverse ngram matrix and further shaped into a frequency lookup table by Poesy, a Markov Model-based Natural Language Poetry Generator. The lookup table is used to create verse lines and a python library, Pronouncing, is deployed to rhyme the verses. In short, our method takes a language model approach similar toalthough we do exploit some semantics, specifically alignment of couplets with Fitbit activity states. The generator is written mainly in the Python programming language using the micro web framework, Flask. It consists of a web interface to display the generated poetry and an administrator interface that is used to define heart rate parameters for different zones and to determine the form and content of the verse that corresponds to these zones. The extensible approach we use to build the poetry generator means the project can easily incorporate additional biometric data types along with their associated corpora in future. The public user interface created to display the generated poetry relies on a number of Open Source JavaScript libraries. These libraries enable display of generated textand retrieval of data from the web applications API and user interface animations. The dynamic background images are created in realtime, and utilise the activity data as an input to affect their appearance, representing a visual correlate to the generated poetry. Multiple versions of this interface were created for deployment on the web, in a live performance environment, and for display in a standalone exhibition setting. Each interface was adapted to take into account the context in which it would be experienced, for example, differences in how, or if, user interaction was required, and addressing the differing requirements for text size, line spacing, and overall page layouts. Conclusion The presentation of this paper will report on the technical work completed to develop Eververse, while reflecting on the implications of the project for issues in poetics, authorship, and automated literature generation. In addition, the presentation will describe deployments of the project in web, exhibition, and live contexts, concluding with a brief live demonstration. Accessible supporting materials for the conference presentation will be made available in English, Irish, Italian, and Urdu . "
	},
	{
		"id": 381,
		"title": "Climate Event Classification Based on Historical Meteorological Records and Its Presentation on A Spatio-Temporal Research Platform",
		"authors": [
			"Wu, Shang-Yun",
			"Wu, Cheng-Han",
			"Pai, Pi-Ling",
			"Wang, Yu-Chun",
			"Tsai, Richard Tzong-Han",
			"Fan, I-Chun"
		],
		"body": " Introduction The impact of climate change has become more and more obvious. How to understand its cause and effect to find a way to deal with it has become an important research topic. To trace the occurrence and impact of climate disasters, many clues can be found in the rich records left by historical materials. Chinas Three Thousand Years of Meteorological Recordsextracts meteorological descriptions from 8,228 historical sources and organizes these descriptions by regions and dates. This important work contributes to the analysis of the temporal and spatial characteristics of meteorological phenomena. The East Asian Historical Climate Databaseis compiled based on chorographies and official histories. This study develops an event classification method based on the meteorological records in the early Qing Dynastyin this database. By representing classical Chinese texts into word embedding vectors and the k-means algorithm, we overcome the difficulty of analyzing classical Chinese and not having enough training data. We then integrate the classification results with the map and timeline to develop a Spatio-Temporal search interface, which facilitates climatologist to access and analyze data according to the three dimensions of time, area and event categories. Methodology The main task of this study is the meteorological text classification, which is composed of three steps: preprocessing, text representation generation, and k-means clustering. We use 36,123 historical meteorological descriptions in the East Asian Historical Climate Database as the input data. As shown in Table 1, each record contains six fields. To make meteorological data more suitable for machine learning semantics and classification of meteorological events, we first pre-process meteorological data, includingreplacing GanZhi, year, month and number with G, Y, M, and N, respectivelyremoving place names and punctuation symbols because they have nothing to do with classification. Then, we use the word2vec algorithm to convert each meteorological record written in classical Chinese into 200-dimensional embedding vectors and then use the k-means algorithm to divide all embedded vectors into k groups. We use the validation set to find the most suitable k value of 45, and evaluate the clustering results with the labeled 9,530 records. The overall accuracy is 82%. Table 2 details the 45 clusters. From Table 1, we can see that many clusters contain the same climatic events, but after careful examination, these groups are still slightly different. Taking clusters 2, 29and 37as examples, although all three clusters can be roughly classified as flood hazards, it is found that most of the texts of cluster 37 refer to seasons. The records of cluster 29 mostly refer to building damage, while the records of the other two groups do not. Table 1. The table of the East Asian Historical Climate Database ID year province county/ city meteorological description source 1795-11 1663 Anhui Province Guichi County 秋，池州大水，城井行舟。【鄉父老云：與明萬曆三十六年水相似。】 Flood in Chi-zhou in autumn. Villagers said, The flood is similar to the one happened in Wanli period in Ming Dynasty. Volume twenty-nine of Chi-zhou Local Gazetteers, published in Kang-xi periodin Qing Dynasty 1795-12 1663 Anhui Province Huangshan City 七月望，邑令陳恭備建學官，是日雷雨大作，千山如注，田間水數尺，忽浮飛木於縣東門。 In mid of July when Chen Gong, the county magistrate, was going to build a school, a thunderstorm happened. Flood was a few feets high, and floodwoods flew at the east gate of the county. Volume eight of Tai-ping County record, published in Chia-ching period in Qing Dynasty 1795-13 1663 Anhui Province Shitai County 秋大水，父老云：與萬曆三十六年水勢相似。 Old man commented about the autumn flood: It was said to be similar to the one happened in the Wanli period in Ming Dynasty. Volume two of Shi-di Local Gazetteers, published in Kang-xi period in Qing Dynasty 1795-14 1663 Anhui Province Dongzhi County 秋大水，十一月始退。 The autumn flood began to recede in November. Volume seven of Dong-Liu Local Gazetteers, published in Qian-long period in Qing Dynasty 1795-15 1663 Jiangxi Province Jiujiang 大水，潰堤數處，禾黍盡沒。 Flood broke levees and crops were all be drowned. Volume fifty-three of De-Hwa Local Gazetteers, published in Tong-Zhi period in Qing Dynasty 1795-16 1663 Jiangxi Province Ruichang County 秋八月，大水入城。 Flood crashed the city in August t Volume one of Rui-Chang Local Gazetteers, published in Kang-xi period in Qing Dynasty Table 2. The 45 clusters and their semantics Cluster_0: Crop Failure Cluster_15: Abnormal Animal Behavior Cluster_30: Social Problem Cluster_1: Wind Cluster_16: Drought Cluster_31: Rain Cluster_2: Flood Cluster_17: Social Problem Cluster_32: Famine Cluster_3: Crop Failure Cluster_18: Crop Failure Cluster_33: Flood Cluster_4: Insect Cluster_19: Crop Failure Cluster_34: Crop Failure Cluster_5: Rain Cluster_20: Drought Cluster_35: Crop Failure Cluster_6: Flood Cluster_21: Undetermined Cluster_36: Rain Cluster_7: Famine Cluster_22: Social Problem Cluster_37: Flood Cluster_8: Social Problem Cluster_23: Famine Cluster_38: Undetermined Cluster_9: Social Problem Cluster_24: Rain Cluster_39: Flood Cluster_10: flood Cluster Cluster_25: Disease Cluster_40: Drought Cluster_11: Flood Cluster_26: Crop Failure Cluster_41: Rain Cluster_12: Flood Cluster_27: Insect Cluster_42: Crop Failure Cluster_13: Crop Failure Cluster_28: Wind Cluster_43: Rain Cluster_14: Social Problem Cluster_29: Flood Cluster_44: Crop Failure Table 3. Examples of records in Cluster 2. 廣東省，大水 Guangdong Province, Great Flood. 安徽省，大水 Anhui Province, Great Flood. 河北省，大水 Hebei Province, Great Flood. 福建省，六月，大水 Fujian Province, June, Great Flood. 江西省，七月，大水 Jiangxi Province, July, Great Flood. 江西省，七月十五，大水 Jiangxi Province, 15th July, Great Flood. 江西省，七月，大水 Jiangxi Province, July, Great Flood. 湖南省，五月，大水 Hunan Province, May, Great Flood. 湖南省，五月，大水 Hunan Province, May, Great Flood. Table 4. Examples of records in Cluster 29. 貴州省，四月十二日，大水，壞心心橋欄杆。 Guizhou Province, 12th April, Great Flood, The railing of the bridge was broken. 福建省，大水，東橋折水至西門橋頭十字街。 Fujian Province, Great flood, The folding water was from the east bridge to Ximen Bridge Crossroad. 福建省，五月初十日，南鄉大水入城。 Fujian Province, 10th May, Nanxiang water flooded into the city. 廣東省,夏五月，大水，城垣崩陷八處，共六十餘丈。 Guangdong Province, in summer and May, Great flood, The eight places of the city, total 60 feet, collapsed. 江西省，秋八月，大水，蘆溪市宗濂橋圮。 Jiangxi Province, in autumn and August, Great flood, the Zonglian Bridge collapsed in Luxi City. 甘肅省,霪雨，塌傾城垣。 Gansu Province, the city walls collapsed after a long period of rain. 江蘇省,泗州城陷沒，漕堤決。 Jiangsu Province, the city of Suzhou collapsed, the dike was broken. 廣東省,夏五月，大水，秋溪鯉魚溝堤潰。 Guangdong Province, in summer and May, Great flood, Qiuxi carp ditch dike burst. 江西省,秋七月，大水連漲七次，西溪橋鷹咀石俱沖圮。 Jiangxi Province, in autumn and July, floods rose seven times, and Yingzui Stone of Xixi Bridge collapsed. 廣東省,大水，城北平地可通舟。 Guangdong Province, Great floods, the city flat land was navigable. 河北省,堤下口決。 Hebei Province, Breach of the embankment. Table 5. Examples of records in Cluster 37. 浙江省,秋大水。 Zhejiang Province, Autumn, Great flood. 安徽省,夏秋水。 Anhui Province, Summer and Autumn, Great flood. 湖南省,秋大水。 Hunan Province, Autumn, Great flood. 安徽省,秋大水。 Anhui Province, Autumn, Great flood. 江蘇省,秋大水。 Jiangsu Province, Autumn, Great flood. 湖南省,夏水。 Hunan Province, Summer, Great flood 浙江省,夏秋大水。 Zhejiang Province, Summer and Autumn, Great flood. 浙江省,秋大水。 Zhejiang Province, Autumn, Great flood. 江西省,春大水。 Jiangxi Province, Spring, Great flood. Front-end interface Then, we use the Time and Space Infrastructure of Chinese Civilizationto present meteorological events on the map interface based on the year and location of the climate events in the historical meteorological records, providing an interface for researchers. Our system is located at http://iisrserv.csie.ncu.edu.tw:5000/English. The main features of the interface include a scrolling timeline, a pop-up condition selection window, and an instant response map. When the user selects the conditions, the map will immediately display the records satisfying the conditions. If the cursor is moved over the location on the map, the page below the map will show the meteorological records of the location in the timeline interval. Figure 1. System interface. Case Study To show the usage of our system, we look into the meteorological records during 1650 to 1700, which is the late stage of the Little Ice Age, to investigate the phenomenon of climate change in Qing Dynasty of China. First, we choose the temperature event category. Among these temperature records, there are 68 extreme cold climate records. We can see that this kind of phenomenon was located from tropical zone to tepid zone in China, therefore, we can conclude that extreme cold records appear not only in middle- to high-latitude areas, but also in lower latitude area. Figure 2. Areas with Low-Temperature Record from 1650 to 1700. Extreme cold weather usually comes with disasters. We choose rain as our variables as step two. Among the records, there are 379 records related to snow. To deeply look into the records, besides directly meteorological phenomena of snow, we further collect the indirectly meteorological phenomena data, such as three days in a row, frost-damaged trees, ...etc. From Table 6, we can see that during the Little Ice Age, the extremely cold climate led to disasters is more frequent than that during Non-Little Ice Age. Table 6. Numbers of Data related to snow during Little Ice Age and Non-Little Ice Age in Cluster rain. Meteorological Phenomena Data Little Ice Age （1650-1700） Non-Little Ice Age （ 1745-1795 ） Rain 3,014 1,618 Snow 379 119 Snow, more than 10 days /month /ten days in a row three days in a row or more 46 5 Snow, frost damaged trees 8 2 Snow, birds, animals and human beings freeze to death 17 6 When investigating the meteorological phenomena during Little Ice Age, we find out rare snow records in Taiwan. As shown in Table 7, these records are in terrain area, such as Chia-Yi county and Tainan City, which could be seen as a strong evidence of the extreme climate during Little Ice Age. Table 7. Rare snow records in Taiwan. year province county/ city cluster meteorological description data sources 1683 Taiwan Province -- Temperature, and rain 冬十一月，雨雪。是夜冰堅厚寸餘。從來臺灣無雪無冰，此異事也。 In November, winter, it snows and rains. The ice is more than an inch thick that night. Its strange because there has not snowed since ancient times. Volume ten of Taiwan Local Gazetteers, published in Kang-xi periodin Qing Dynasty 1683 Taiwan Province -- Rain, crop failure, temperature五月，大雨。霪雨連月，鄭氏之土田阡陌多被沖陷，有高岸為谷之歎。冬始雨雪，冰堅厚寸餘。臺土氣熱，從無霜雪。 In May, it rained heavily. After a long period of rain, the fields of Zhengs family were crashed, and the high bank became valley. It began to snow in winter, and the ice was more than an inch thick. Normally the field was hot, and there was no frost and snow in Taiwan. Volume nine of Rebuilt Taiwan Local Gazetteers, published in Kang-xi period in Qing Dynasty 1683 Taiwan Province Chia-yi County Flood, rain, crop failure, temperature夏五月，大雨水，時霪雨連月，鄭氏土田多沖陷，有高岸為谷之歎。冬十一月，始雨雪，冰堅厚寸餘。諸羅有霜無雪，是歲甫入版圖，地氣自北而南，信有矣。 In May, it rained heavily. After a long period of rain, the fields of Zhengs family were crashed, and the high bank became valley. In November, Winter, it snowed and iced over. Normally the field was hot, and there was no frost and snow in Tsu-lo. It was believed that the climate was from Northern area, starting from the year Tso-lo became Qings territory. Volume twelve of Tsu-lo Local Gazetteers, published in Kang-xi period in Qing Dynasty 1683 Taiwan Province Tainan City Flood, rain, flood, rain, crop failure, temperature, temperature, temperature, drought, rain 春，鯽魚潭涸。夏五月，大雨水，田園多沖陷。六月，澎湖潮水漲四尺。秋八月壬子，鹿耳門潮水漲。冬十有一月，雨雪冰。是臺地氣暖，從無霜雪，是歲八月甫入版圖，冬遂雨雪，冰堅寸許，地氣自北而南，運屬一統故也。 In Spring, crucian pond dried up. In May, Summer, it rained a lot, and fields are mostly crashed. In June, the tide at Penghu rose four feet high. On August 18th, the tide at Luermen rose. In November, Winter, it snowed and iced over. Normally the field was hot, and there was no frost and snow in Taiwan. However, started from August when Taiwan was under Qings authority, it rained and snowed in Winter, and the ice was more than an inch thick. The climate was from the Northern Area. It is because of territorial unity. Rebuilt Taiwan Local Gazetteers, published in Qian-long period in Qing Dynasty Conclusion Although technology has improved nowadays, it is still hard to predict the weather. In order to understand the impact of climate disasters and find a way to deal with it, tracing the historical climate event could be a solution. This study establishes a principle to classify meteorological phenomena based on Chinas Three Thousand Years of Meteorological Records, develops a Spatio-Temporal research platform, and build an instant response front-end interface. By the climate case study, we collect the users feedback and improve the front-end interface, as well as enhance the precision of a mass of meteorological data analytics. Although the research platform only contains the meteorological data from 1647 to 1795, we hope to expand the capacity of the database and establish a mature Spatio-Temporal research platform in the future. "
	},
	{
		"id": 382,
		"title": "Event Extraction on Classical Chinese Historical Texts: A Case Study of Extracting Tributary Events from the Ming Shilu",
		"authors": [
			"Tsai, Richard Tzong-Han",
			"Lu, Yi-Hsuan",
			"Wang, Yu-Chun",
			"Fan, I-Chun"
		],
		"body": " Introduction Events are important historical research targets. Major events are composed of a series of smaller atomic events that involves several people and their interactions at a specific time and location. For investigating Chinese historical events, imperial historical records are one of the most reliable sources. There are two main challenges of studying imperial historical records. The first is that the records are usually quite lengthy. To systematically trace the evolution of an event or a subject such as Qing conquest of the Ming, historians need to mark up all mentions in relevant paragraphs in the Ming Shilu, The Ming Shilucontains the imperial annals of the Ming emperors. which is a tedious, time-consuming job. The second obstacle is that there is no publicly available text-mining tool that can extract person, location, time, and event mentions from Ming dynasty historical records. The aim of this work is to begin development of text-mining tools that tackle these two challenges. One common way to represent an atomic event is as an event frame containing an event trigger and several event arguments. An event trigger, or predicate, is a key action that evokes the event. An event argument is a word/phrase in the sentence related to the predicate. Event frames are therefore called predicate-argument structuresin computational linguistics. Event arguments are distinguished from one another by their relations to the event trigger. Such relations are referred to as semantic roles, including main arguments such as agent and patient, as well as adjunct arguments, such as time, manner, and location. Every predicate has its own frameset, that is, the set of sematic roles. The sentence 大寶法王 差 喃哈鎖南 can be represented by a PAS in which 差is the predicate, 大寶法王is the sender, and 喃哈鎖南is the person sent. The task of marking up atomic events in texts can be treated as a PAS recognition problem. PAS recognition comprises two steps:locating the event trigger;recognizing arguments and labeling them with semantic roles. Since the second step is the key, PAS recognition is referred to as semantic role labeling. In this paper, we aim to identify tributary events in the Ming Shilu automatically by using SRL techniques. We first compile a set of tributary verbs and define framesets for each verb. Then, we formulate SRL as a sequence labeling task and employ the state-of-the-art algorithm, conditional random fieldsLafferty25252510John D. LaffertyConditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence DataICML01?>, to tackle this task. This work represents the first machine-learning-based attempt to extract atomic historical events from Classical Chinese as well as the first effort to solve this problem using SRL. Predicate Selection and Frameset Construction Selecting predicates and defining their framesets are the prerequisites of SRL. To reduce human effort, we first employ the clustering-based classification method Tsai22222210Richard TsaiWeisoEvent: A Ming-Weiso Event Analytics Tool with Named Entity Markup and Spatial-Temporal Information LinkingDH2017?>to identify paragraphs related to tributary events. Next, to facilitate the selection of predicates and their corresponding framesets, we label these paragraphs with NEs. We select 12 tributary verbs as predicate candidates. Manually defining the frameset for a verb requires linguistic expertise and is time consuming. We base our framesets on those in PropBank, http://verbs.colorado.edu/chinese/cpb/ the most widely used frameset database, revising them according to verbs appearance in the Ming Shilu. For each verb v, we revise its PropBank frameset by checking on average 100 tributary paragraphs containing v and NEs. Semantic Role Labeling After compiling the frameset database for tributary verbs, we perform SRL for such verbs. First, we search all tributary paragraphs for these verbs to locate event triggers. Our SRL system then recognizes arguments with semantic roles from such paragraphs. The SRL task here is formulated as a sequence labeling problem in which the system should output the most probable sequence of labels when given a sequence of tokens. Although deep learning methods such as Qian201728328328310Qian, FengSha, LeiChang, BaobaoLiu, LuChenZhang, MingSyntax Aware LSTM model for Semantic Role LabelingProceedings of the 2nd Workshop on Structured Prediction for Natural Language Processing27-322017Association for Computational LinguisticsW17-4305http://aclweb.org/anthology/W17-4305http://dx.doi.org/10.18653/v1/W17-430510.18653/v1/W17-4305?>have demonstrated their effectiveness in the SRL task for modern Chinese, they are still impractical for classical Chinese because there are no word and sentence segmentation tools available for the Ming-Shilu. Without segmented sentences, we cannot train a word embedding model, making deep-SRL impossible. Therefore, we employ the CRF model, which achieves the competitive performance in the SRL task for modern Chinese Tan200928228228210Y. TanX. WangY. ChenChinese semantic role labeling using CRFs and SVMs2009 International Conference on Natural Language Processing and Knowledge Engineering2009 International Conference on Natural Language Processing and Knowledge Engineering1-5computational linguisticsinformation retrievalsupport vector machinesChinese semantic role labelingCRFSVMdocument retrievalmachine translationquestion answeringinformation extractionLabelingNatural languagesData miningGoldPerformance evaluationText recognitionTeethTrackingSemantic Role LabelingConditional Random Fields200924-27 Sept. 200910.1109/NLPKE.2009.5313827?>and does not require segmentation information We use the relative position in a chunk and that in an NE as features because these types of information are very informative for predicting semantic role tags. For this step, we use Tsai et al.s NER system Tsai22222210Richard TsaiWeisoEvent: A Ming-Weiso Event Analytics Tool with Named Entity Markup and Spatial-Temporal Information LinkingDH2017?>. Since there is no chunking tool or annotated corpora for classical Chinese, we design a chunking algorithm that requires no training data: NER is used to mark NEs in all paragraphs. NEs and strings between NEs are treated as chunks. All chunks with their counts are recorded in a list l. For each chunk c i and its superstring with the same prefix or suffix c j, if c is count is larger than c js over a threshold N, c is count is increased by 1 and c i is cut off from c j. Step 2 is repeated until there is no new chunk generated. For each paragraph, all strings between NEs are segmented by the maximum matching algorithm with the final l as the reference dictionary. Experiments We select 500 paragraphs from the 2,874 paragraphs containing tributary events in the Ming Shilu for our experimental dataset. 12 verbs are selected as event triggers. All paragraphs are labeled with PASs by two experts262617Cohen, JacobA Coefficient of Agreement for Nominal ScalesEducational and Psychological MeasurementEducational and Psychological Measurement37201evaluation inter-annotator-agreement1960http://epm.sagepub.com/cgi/reprint/20/1/37cohen1960?>κ =.89). All experiments are conducted using 10-fold cross validation. The data set is divided into 10 subsets, and the holdout method is repeated 10 times. Each time, one of the 10 subsets is used as the test set and the other 9 subsets are put together to form a training set. Then the average error across all 10 trials is computed. The parameters are set to the default of CRF++. Table 1 shows the argument-recognition performance, which achieves an F1-measure of 82.90%. Table 2 shows the PAS-recognition performance. Our system achieves an F1-measure of 48.45%. Although the argument-recognition performance is satisfactory, correctly identifying every argument in a given PAS is very challenging. Therefore, PAS-recognition performance is somewhat less than ideal, though it is still comparable to the performance of other systems on modern Chinese Xue200823232317Nianwen XueLabeling Chinese Predicates with Semantic RolesComputational LinguisticsComputational Linguistics225-2553422008http://www.mitpressjournals.org/doi/abs/10.1162/coli.2008.34.2.22510.1162/coli.2008.34.2.225?>. Table 1: Argument-recognition Performance Precision Recall F1-measure 81.08% 84.79% 82.90% Table 2: PAS- recognition Performance Precision Recall F1-measure 48.45% 48.45% 48.45% Analysis and Application In this section, we present a detailed breakdown of PAS recognition results for the major predicates to show the practical performance of our system to historians. In the Analysis—Extracted Tributes and Rewards Items subsection, we categorize the most common tributary/rewarding items extracted by our system. In the Analysis—Extracted Envoys subsection, we show some top-frequent envoys extracted by our system. At last, we briefly illustrate the application of our system to historical studies. Table 3: Tribute Categories Category Frequency Examples Animals, 羊,馬,象,人Local Products 178 方物Luxury Goods 58 金,珊瑚,文綺,玉石Buddhist Items 40 香,香爐,佛像,舍利Weapons 29 盔,甲冑,劍,刀For rewards, the extracted 401 terms can be divided into 7 categories, shown in Table 4. Table 4: Reward Categories Category Frequency Examples Money 667 銀兩,鈔Cloth 580 布,紗,素叚,綵緞Banquets 479 宴Clothing 318 衣服,蟒衣Buddhist Items 53 佛像,袈裟Imperial Ceremonial Objects 41 冠帶,誥印,誥命Animals 24 馬,駝,羊,牛Analysis—Extracted Tributes and Rewards Items We have analyzed the atomic events whose predicates are related to tributes, such as 朝貢/chao gong、貢/gong、進/jin、獻/xian, and rewards, such as 賜/ci、賞/shang、給賞/gei shang. For tributes, the extracted 175 items are classified into five categories, shown in Table 3. According to Tables 3 and 4, we can see that the tributes are quite different from the rewards. Animals and slaves are the most popular form of tribute, while money is the most common reward. The tributes and rewards reflect the diplomatic and trade relationships between Ming and its tribute states. Interestingly, we found that rewards sometimes included oxen, which was unexpected for tribute states that did not extensively practice agriculture. For example, we examined the paragraphs containing the reward oxen and found that the reward target was泰寧衛/Tai-Ning-Wei, a Ming-allied Mongolian tribe that lived a nomadic lifestyle. This reward of oxen is evidence suggesting that Tai-Ning-Wei may have practiced agriculture due to Ming influence Yingtai2424245Gu YingtaiMingshijishi benmo20Yingtai2424245Gu YingtaiMingshijishi benmo20?>. Analysis—Extracted Envoys There are 4,002 tributary events which contain 3,064 envoys. 塔卜歹/Tabudai, a Tai-Ning-Wei chief, visited Ming officials 33 times between 1500 and 1555. The secondly most frequent envoy is升合兒/Shengheer, another Tai-Ning-Wei chief. He paid tribute to the Ming authorities 25 times between 1537 and 1611. This shows that Shengheer lived more than 74 years, and may suggest that he was actually several different men. In addition, we can also observe chiefs who sent envoys to the Ming in extracted tributary events. There are 488 events in which envoys were sent and 313 distinct senders. The leader who sent envoys the most is孛來罕/Bolaihan, the governor of朵顏三衛/Duo-Yan-Three-Wei. He sent envoys 20 times to the Ming between 1502 and 1537, such as塔卜歹/Tabuda, 納哈出/Nahachu, 納挨/Naa. Application With a well-developed SRL tool, historians can quickly identify historical events and terms labeled with roles in events from Classical Chinese literature. The extracted PAS information can be further used to support large-scale analysis of historical events, such as the distribution of events overall or for some person or some place. "
	},
	{
		"id": 383,
		"title": "Defining and Debating Digital Humanities in China: New or Old?",
		"authors": [
			"Tsui, Lik Hang",
			"Chen, Jing"
		],
		"body": " In the global context, no single unified definition of digital humanitiesis possible. The scholarly context that DH was defined and debated in the Greater China region is starkly different from that in Western academia, owing to the unique features of humanities data in Chinese, especially for texts. With special focus on the context and cultural politics of the conditions in which DH emerged and the contestations it encountered, our paper unravels the complex issue of DH emerging as a scholarly field in China from a historical standpoint. In this state-of-the-field investigation, we draw from our experience in running major DH projects in Chinese studies, as well as in building DH communities in China by founding and editing a popular academic blog on DH and moderating online discussion groups on the Chinese social media app WeChat. The Classical and modern Chinese language presents technical challenges in digitizing, organizing, and mining Chinese texts, for instance, in word segmentation. The amount and heterogeneity of Chinese characters increase the difficulty of mining Chinese texts. Despite this, DH is hardly a novel approach to Chinese academia. During the 1980-2000s Chinese humanities experts have begun producing computational and corpus linguistic, statistical, and GIS analyses in history, literary, and geographical studies. Examples include stylistic research on the 18 th century novel The Story of the Stone. Such projects have employed cutting-edge research methods, but did not gain enough currency to be accepted by mainstream humanities scholars. During this prehistory of DH for Chinese humanities, there were also large-scale digitization projects on Chinese materials, especially the historical compendium that are the most basic: Siku quanshu [Complete library of the four treasuries] and the Twenty-five Standard Histories. Database companies, libraries, university departments, and governments have made substantial investments in digitization in the form of R&D for commercial databases, cultural heritage projects, funding for libraries, research grants, etc. After pioneering scholars Jieh HSIANG, JIN Guantao, and WANG Xiaoguang introduced the concept to China around 2009, most mainstream humanities experts have been skeptical or have even rejected the academic practices that DH entails. One common misunderstanding about it among Chinese scholars concerns how research questions can directly yield different answers from employing DH tools and methods. Some historians assume that DH research is solely about using quantitative methods. The value and potential of qualitative inquiry in DH, especially in media studies or visualization-related studies, is rarely noticed. For these researchers, the use of digital tools is merely for acquiring research materials; the further utilization of data that is conventionally recognized as DH research is seriously lacking. Suchconceptions and practices have prevented Chinese scholars from moving from digitization to deeper-level knowledge discovery and methodological renewal, until the 2010s. In the field of Chinese history, quantitative historians, historical geographers, and researchers in prosopography and social networks have gained most from digital advances. All in all, the paradigm shift in China is slowly taking place, but it is a delayed one given the amount of preparation from the prehistory phase. Taking into consideration that many of the elements of DH already existed in Chinese academia, DH should not be taken as a completely novel paradigm for the humanities in China. Since the late 2000s, DH, Electronic evidential scholarship, and quantitative history have all become academic buzzwords. Among these digital-related concepts, junior scholars and students in China are increasingly exposed to DH. Several major Chinese universities have set up DH centers that involve the study of digital humanities, or at least its promotion. More institutions have secured funding for research projects that include the building of academic databases. Proprietary databases are already plenty and are dominating the field in China—most scholarly data is owned by commercial companies and made available to scholars through institutional subscribed databases. Given this amount of investment and input, DH is underdeveloped for the Chinese-speaking world, if we try to trace DH as an emerging trend. According to our analysis of the main theoretical and manifesto-like texts on the DH paradigm in China and the main ongoing projects, we contend that DH has become a canopy term for Chinese scholars to reconceptualize, recatergorize, and repackage old projects and academic practices from the prehistory phase examined earlier. Several longstanding text and biographical databases have recently gained widespread attention, partly thanks to online communities on social media. These include the Database for the Study of Modern Chinese Thought and Literatureand the China Biographical Database. Disciplines are highly institutionalized and centrally governed in China; academic boundaries are rigid. Incentive systems discourage researchers from engaging in DH research, which often disrupts traditional forms of academic publishing. As a result, young DH scholars in China tend to collaborate with like-minded peers that are outside, rather than within their own disciplines. Mainstream senior scholars have only begun reflecting on digital history and the use of databases in history journals since 2016. To tackle these challenges, several major Chinese universities have set up DH centers and their libraries are beginning to develop DH librarianship. There are also international efforts to construct an infrastructure for connecting digital resources and systems for Chinese history. Since 2015 there have been acclaimed conferences to discuss the scholarly trend of DH, but capacity building events such as workshops that cover vital DH skills are still rare, given the growing demand. Humanities curricula in China usually do not provide the basic skill set that is useful for budding DH work, forcing interested learners to be proactive and daring. Weak support for digital literacy among humanities scholars in China also account for the dearth of DH expertise. Institutions within universities that possess the capacity to provide digital training and research support for such humanities scholars are rare, although libraries have begun to recognize the need to strengthen such efforts. "
	},
	{
		"id": 384,
		"title": "Operationalising Ambiguity; Mapping the Structural Forms of Comics",
		"authors": [
			"Turton, Alexander Robert"
		],
		"body": " Introduction In response to the conferences theme of Complexities and the humanist way of building complex models of complex realities, this paper will report the findings of my doctoral research into how different structural forces meet, reroute and disrupt one anotherin Alison Bechdels graphic memoir, Fun Home. By encoding multiple understandings of the same source material at the data creation stage of the project, and by creating tags that reflected a plural, partial and situatedconceptualisation of Truth, it was possible to operationalise the resulting parallaxand build a richer model of the comics rhizomatic rhetoric. This made it possible to better understand the critically under-theorised issue of how different structuring forms operate on the text and one another. In addition to reporting these specific findings, this paper will communicate the investigative method by which I reached them in the belief that it has a broader application for Digital Humanities projects that must work with non-indexical or otherwise ambiguous sources. Background and method The narrative of a comic is organised by structural, spatial, temporal, and thematic logics, but analysing how each of these forms interact with, and organise, one another is challenging not only for non-digital scholars but also for digital ones. This is because each of these categories can be understood, defined, and therefore represented in a database, in multiple, equally plausible and valid, ways; it is unclear which particular understandingof which particular form might prove to be the most productive in explaining the patterns and relationships that emerge from the data. The recent emergence of digital and empirical approach to comicsreflects an understanding that the volume of information found in a comics panels exceeds the capacity of human memory and, therefore, requires prostheticising into digital memory if scholars are to progress from, and in some cases even apply, non-digital criticism and theory of the systemor languageof comics. A key facet for digital approaches to comics, and an almost unique opportunity in terms of distantly reading literature, is the mediums mode of articulating time, the panel. Panels assemble signifiers at a discrete site, giving creators minute control over what – both in terms of contentand container– is present for, and therefore co-locates with, each disclosure of narrative information, each event, each phase of a scene. This means that themes and references can be rhetorically woven into the diegesis. In Fun Home, for example, Bechdels father was killed by a Sunbeam bread truck, and loaves of this particular brand are scattered through the text at otherwise seemingly innocuous moments, inextricably linking them to that pivotal scene. This rhetorical affordance, however, isnt limited to the content of panels. It extends to the structures that organise that content, too, the forms that execute the parsing of narrative information, the unravelling of a texts arguments. Not only does each signifier occur at a discrete site, but that site is also placed in varying relations with other sites. Some of these relations are more literally structural and include linearity, as well as being part of a particular row and page, and row layout and page layout. Others of these relations are commonalities they have in terms of diegetic chronology, event, scene, thematic concerns, location or source of information. Each of these different kinds of organising logics, or lines along which the text can be folded, operate on a comics content, its words and its images; they are in relationship with them. Further, the combination of words and images found in comics means that one narrative track, often the words, can be used to anchor the narrative, allowing the other track to move around freely, and without extra-diegetic explanation, in time, space, or subject; the medium exists in a state of almost perpetual montage and this produces a richer and more networked rhetorical texture. As well as choosing what is present for the disclosure of information, the author has precise control over the pacing of their narrative and can rhetorically interweave different chronologies and different sources of information. By sources of information I particularly mean archival documentation. Whilst drawing on archival evidenceis a feature of most graphic memoirs, particular attention and acclaim has been directed towards its usage in the scholarly and popular reception of Bechdels memoir. Indeed, over the course of the memoir, Bechdel draws on photos, maps, diaries, letters, dictionaries, tape recordings, annotated copies of family books, and newspapers. But where these existing scholarly analyses stop is when it comes to linking this usage to, or interrogating its relationships with, the other structuring forms at play in the memoir. Most strikingly, given it is the very mode of articulating narrative, this intersects with another gap in the critical context – and one by no means unique to Fun Home – a sustained study of the rhetorical effect of page layouts. Critical attention to page layouts tends to be reserved for unique and remarkable layouts, even though the signification of the majority of panels is operated on by more standardised, and reproduced reproducible layouts. It is these gaps in isolation, and in their relationships to one another, which my doctoral work, and this paper, addresses. These critical gaps can be understood partly as a result of the analogue mode of scholarship used to investigate such an information-rich medium, and partly as a result of the idiolectical nature of usage, meaning these features cannot be precisely formalised across texts by theorists. However, another significant cause of this gap – and one which persists in larger-scale digital approaches to comics due to their need for a single cross-corpus markup scheme – is that these organising logics cannot be formalised into a single classification scheme without doing significant violence to the source. This is where my method differs from other, larger, digital approaches to comics. Not only are the contents of panels ambiguous signifiers, but so are the structuring forms that operate on and organise these contents; both can be understood in multiple and equally valid ways, and by restricting my project to a single text I was able to encode several of these interpretations and play them off against one another. Not only do singular definitions do violence to the source in their positivism, but they forego the potential of leveraging different conceptualisations against one another to see which definitions of which features interact with the data in meaningful ways, create rhythms and harmonies, and furnish us with consistent interactions. By encoding different interpretations of these organising forms, and comparing the strengths of the resulting correlations, relationships, and collisions, it is possible not only to enrich our understanding of how they organise the book, but also which definitions of them are more appropriate in the books system. Rather than getting caught in a feedback loop of fore-projection,this method offers a mode of investigating sources when not only is it unclear which features will prove significant or interesting, but also what conceptualisations of those features will, and leveraging this ambiguity for investigation. Conclusion By building a complex model of [a] complex realit[y], and by asking how different structuring forms are arranged, arrange, and arrange one another in Alison Bechdels graphic memoir, Fun Home, this paper will demonstrate the investigative power of multiply tagging ambiguous data outlined, and advocated for, in Towards Feminist Data Creation, and will report the specific findings of this doctoral research as a counterpoint to the larger projects undertaking digital comics research. "
	},
	{
		"id": 385,
		"title": "Collaborative Decision Making and Food Security: Digitizing Indigenous Knowledge of Rural Farmers in Uganda.",
		"authors": [
			"Tweheyo, Robert"
		],
		"body": " This paper discusses the value of indigenous knowledge and practices providing sustainable food security. Indigenous knowledge and practices play a crucial role in maintaining and enhancing genetic diversity, reduce environmental damage and boost resilience to climate change. Innovations based on traditional knowledge provide climate smart alternatives that can significantly increase food productivity and incomes of local communities, while ensuring sustainability and maintaining genetic diversity, as required by SDG2 . The Food and Agriculture Organizationacknowledges the breakdown of traditional systems and how this has affected environmental degradation, undermining the long-term viability of pastoral livelihoods. Access to adequate food is a basic human right. It is a catalyst to the realization of all other human rights, such as the right to education and good health, to mention only two. Therefore, attaining national and household food security is of fundamental importance. However, achieving food security and making it sustainable, remains a global challenge. Although Uganda is well known as an agricultural-based economy, sustaining food security has become challenging, owing to the poor economic status of the country. The current situation is that of low productivity, persistent hunger and malnutrition. This leads to much human suffering and substantial low productivity ; USAID, 2016 ;. ICT can play major roles in improving the availability of indigenous knowledge systems and enhancing its blending with the modern/ scientific and technical knowledge . Digitizing indigenous knowledge bridges the geographical and perceptual gap between communities . Mark and Rensselaerargue that ICT is vital in sustaining and stimulating communities traditional ways of knowing. According to , ICT could indeed act as a source of empowerment and knowledge exchange by enabling young people, old, employed and unemployed to exchange traditional and modern knowledge. It creates a platform for interaction among members of the community in form of collaborative processes . Due to the complexity of decision making processes, collaborative decision-making plays an essential role in the design and communication at all levels in problem solving processes. Collaborative decision-making refers to a situation where different people working together toward achieving a common goal come together to exchange ideas on how to achieve the stated goal. It is defined by as a joint effort toward a common goal; a process in which stakeholders with different perspectives of a problem, can constructively explore the differences and can search for solutions that go beyond their own limited visions. It ideally involves a free exchange of ideas to allow creation of most innovative and strategic decisions . Within collaborative decision making, there are many processes and best practices that can be employed and shared to ensure the best outcomes. A collaboration process provides a mechanism for engaging stakeholders in an effort to identify and address food security problems . In the case of rural food insecurity, relevant stakeholders need to be involved to appropriately brainstorm, i.e. share their experiences on how they apply indigenous knowledge . Following a design science research philosophy in an engaged scholarship research paradigm, a digital platform for managing and sharing indigenous knowledge that enhance food security is developed as an intervention base. This study was conducted in the selected areas of rural Uganda where agriculture is predominantly the major source of livelihood. A literature review was carried out to gain a generic understanding from different stakeholders perspectives and to get deeper insights of the challenges rural farmers face and decisions they take to overcome the challenges. In the exploration phase, it was observed that rural farmers operate in complex circumstances characterised by poor coordination and collaboration, lack of information and knowledge sharing to learn about what other farmers do in order to resolve food security decision making challenges. The exploratory study findings also revealed that the context in which rural farmers make food security decisions was complex and unstructured. The platform enables farmers and stakeholders to share knowledge and experiences regarding household food security. The platform provides a collaboration suite that offers an interactive environment for knowledge sharing and decision-making. It provides steps for engaging farmers and stakeholders in food security decision-making. The collaboration suite describes steps that are crucial for collaborative decision-making. The artifact offers an environment for collaborative decision-making among farmers and stakeholders. It is a platform where farmers and stakeholders freely exchange ideas on food security improvement and come up with innovative and strategic decisions. Collaborative decision-making enables farmers and stakeholders to share knowledge and information on best and worst practices as experienced by farmers. The initiator of the topic for discussion can invite people of his choice to the chartroom to discuss about the proposed topic by giving their views in the brainstorming session. As noted by , collaboration promotes sharing of experiences and practices in a specific context. During collaboration decision-making, farmers are encouraged to brainstorm ideas, tell their stories on indigenous knowledge experiences from which the best ideas are agreed upon by consensus and shared understanding. The purpose of sharing indigenous knowledge experiences by farmers is to learn from each other the best practices. Domain experts like extension workers and agriculture officers can provide technical advice by joining discussions on the platform. Collaboration stimulates comprehensive thinking and is a means of identifying best practices for addressing food security problems . In the digital platform, CDWs take the role of intervention facilitators. The role of intervention facilitator is to provide support to rural farmers with no or low education background on how to use the platform. A facilitator gives assistance to farmers by using the intervention schemata to provide technical assistance to farmers and stakeholders who may want to participate in the discussions. A screen shot of the collaborative decision-making The digital interface provides collaborative platform for farmers and stakeholders engaged in a discussion about food security issues in a brainstorming manner. Every registered farmer can post his/her opinions suggesting ways in which food security could be made better using indigenous knowledge. The link allows any registered user to initiate a topic of discussion and to invite the views of other farmers and stakeholders. It represents a decision-making style in a way of brainstorming by instant messaging. It offers an opportunity for engaging farmers as domain practitioners in food security and key stakeholders in a group discussion. Collaboration builds teamwork and enables knowledge and experience sharing as a means of identifying alternative ways for addressing food security problems, see ; . The digital interface has a provision for farmers to give their views regarding issues of their concern and also to participate in a discussion by chatting using the chat room that is provided on the dropdown menu bar. Upon clicking the farmers views icon, one is able to see the views of others and can join the discussion by scrolling down and writing his/her views in give your comments space provided at the bottom of the page. In the chartroom, any registered user can initiate a topic for discussion and can invite people of his/her choice to chat with in a way of sharing knowledge. In the interface, CDWs take the role of facilitator. Their role as intervention facilitators is to guide farmers and stakeholders on how to collaborate in decision-making process . They give instructions on how to brainstorm, generating alternative ways for improving food security. The platform was instantiated and evaluated by the users and domain experts in the field of food security and information systems. It was perceived as a usable and useful ICT interface for sharing indigenous knowledge among farmers. This study provides both descriptive and prescriptive solutions to the problem of food insecurity in rural communities. Furthermore, it contributes to digitized and contextualized interventions to solving food insecurity in developing countries. "
	},
	{
		"id": 386,
		"title": "Navigating the Complex Landscape of Digital Humanities Methods and Tools with the OpenMethods Metablog",
		"authors": [
			"Tóth-Czifra, Erzsébet",
			"Berra, Aurélien",
			"Leão, Delfim",
			"del Río Riande, Gimena",
			"Larrousse, Nicolas",
			"Maryl, Maciej",
			"Moranville, Yoann",
			"Morselli, Francesca",
			"Wuttke, Ulrike",
			"van Zundert, Joris"
		],
		"body": " A paramount challenge in present-day humanities knowledge production is to communicate research results in ways that aligns with increasingly digital research workflows. The OpenMethods metablog aims to explore and deliver a solution for this need in a Digital Humanitiescontext. It provides a platform to bring together all formats of openly available digital publications. To this end the OpenMethods metablog provides a convenient and easy way for DH experts from around the globe to select, propose, curate, and highlight online published content. Suitable online content may be proposed by Community Volunteers. The OpenMethods platform is intentionally interdisciplinary and multilingual to facilitate a timely disclosure and spread of knowledge and to raise peer recognition for the related research results. The group of DH experts, known as the OpenMethods Editorial Team, currently comprises 23 editors from 11 countries. The platform has been developed in partnership with the DARIAH community since it is an offspring of the DARIAH Humanities at Scale project. OpenMethods offers a collaborative model of open scholarly communication that is growing out of community practices and needs in DH. OpenMethods goes beyond traditional paper-based communication practices in several ways, as explained below. 1. OpenMethods is dedicated to the critical discussion of DH tools and methods. Digital tools and methods are genuine research outputs whose academic recognition is still lagging behind monographs and journal articles that serve as the highest value currency in current academia). In addition to spreading the knowledge and raising peer recognition for existing digital resources, a core mission of the project is also to facilitate the culture of reuse of these materials. 2. OpenMethods is inclusive with a variety of content types like blog posts, videos, preprints, podcasts, etc. These are becoming increasingly important aspects of scholarly work) as they are not only accelerating discussions within and outside of academia but are also flexible enough to follow the dynamic and multimodal nature of DH methodology discourse. 3. One of the key aim of the platform is helping researchers in navigating through the rich and dynamically evolving DH landscape to find the most relevant tools and methods for their research. This is achieved via a novel form of expert community review. As an enrichment of the preselected valuable Open Access publications, successful nominations are categorised with tags based on TaDiRAH and a brief introduction in English is added to each post in which one of our Editors explains the relevance of the republished content to the DH practices. The multilingual character of the platform allows for the representation of multiple languages and cultures in the DH discourse yet currently this possibility is not exploited to its full extent. 4. The platform not only propagates the culture of reuse but has been built on reused tools itself. It is based on a WordPress CMS and the PressForward plugin. It enables us to create a simple workflow for our experts: almost all steps of their workcan be undertaken within this plugin. Besides, we are constantly seeking ways to put DH tools in service of more effective content discovery and enrichment. For this purpose, we have created plugins to achieve interoperability with the entity recognition NERD service and the research discovery platform ISIDORE. Our goal is to reach and engage the widest possible range of DH communities, from scholars taking the first steps towards going digital to DH experts who are shaping specific research areas as representatives for particular methods. In achieving its goals OpenMethods faces many difficulties. In our poster we wish to highlight a number of these challenges: Reaching a critical number of readership as a result of focused outreach strategy. Finding solutions for long-term incentivisation of the editors, ensuring that editors are recognised for their contributions, and sustaining a viable pool of reviewers. Establishing bidirectional exchange between traditional journal publishing and novel components in scholarly communication such as blogging. Our poster presentation will allow us to solicit the widest possible feedback from DH communities. To this end it will not only explain the aims and strategies of OpenMethods, but will also include an interactive demo. We also wish to encourage the conference attendees to join and expand the OpenMethods network, explore its potentials for advancing their own research methods and participate in the development of the platform. "
	},
	{
		"id": 387,
		"title": "Exploration of the Seventeenth Century Japanese Authors’ Writing Style Using a Quantitative Approach",
		"authors": [
			"Uesaka, Ayaka"
		],
		"body": " Introduction This study aims to exploration of the seventeenth century Japanese authors, Saikaku Ihara, Dansui Hōjōand Ichirōemon Nishimura, writing style from a quantitative point of view. Saikaku was a national author whose novels were published in seventeenth century in Japan. As he gained a national audience, Saikaku was pressured to write on demand and in great volume. At first, he wrote only one or two novels a year, however in the two years from 1687 to 1688 he published twelve books, with a total of sixty-two volumes. Saikakus style and approach also changed at this point. One recent hypothesis has stated that he wrote twenty-four novels, however it remained unclear which works were really written by Saikaku except Kōshoku Ichidai Otoko, Shōen Ōkagami, Kōshoku Ichidai Onna, Kōshoku Gonin Onna, while research on his works has proceeded, these fundamental doubts about his authorship remain. Dansui was Saikakus student. Morihas argued that Saikakus novels are an apocryphal work mainly written by Dansui except Kōshoku Ichidai Otoko. Many researchers have raised questions about the authorship of Saikakus posthumous novels, because these novels were edited and published by Dansui after Saikakus death. Ichirōemon was known for imitating the work of Saikaku. However, Ichirōemon was too conscious of Saikaku, and tried to preserve the traditional style of kana zoushi. Thus, Ichirōemons novels were nothing like as good as Saikakus　. Previous studies In our previous studies, we have analyzed Saikaku and Dansuis novels, and have clarified the following points by extracting their writing style using principal component analysis and cluster analysis:A comparison of the Saikaku and Dansuis novels showed ten prominent features: the grammatical categories, words, nouns, particles, verbs, adjectives, adverbs, adnominal adjectives, grammatical categories bigrams and particle bigrams; andUsing these features, we analyzed Saikakus four posthumous novels. We found these four posthumous works indicated same features of Saikakus novel, therefore we concluded that most part of these four posthumous novels belonged to Saikaku. In this study, we explored among Saikaku, Dansui and Ichirōemon using a quantitative approach to inspect a relationship of their works. Data for this study We used Saikakus twenty-four works digitized from Shinpen Saikaku zenshū), Dansuis three works digitized from Hōjō Dansui shūand Ichirōemons five works digitized from Nishimura Bon Shōsetsu Zenshū. Since Japanese sentences are not separated by spaces, we used Mecab-0.996 and Unidic for early modern spoken Japanesemade by Center for Corpus Development, National Institute for Japanese Language and Linguistics in this study. Table1. Title of three authors novels Author Title Saikaku Kōshoku ichidai otoko Saikaku Shōen ōkagami Saikaku Wankyū issei no monogatari Saikaku Kōshoku gonin onna Saikaku Kōshoku ichidai onna Saikaku Saikaku shokoku hanashi Saikaku Honchō nijyū hukō Saikaku Nanshoku ōkagami Saikaku Budō denraiki Saikaku Kōsyoku seisuiki Saikaku Hutokoro suzuri Saikaku Nihon eitaigura Saikaku Irozato mitokoro setai Saikaku Bukegiri monogatari Saikaku Arashi ha mujyō monogatari Saikaku Shin kashōki Saikaku Honchō nijyūhukou Saikaku Seken mumezanyō Saikaku Ukiyo eiga ichidaiotoko Saikaku Saikaku okimiyage Saikaku Saikaku oridome Saikaku Saikaku zokuturezure Saikaku Yorozu no humihougu Saikaku Saikaku nagorinotomo Dansui Chuya yōjin ki Dansui Budō hariai ōkagami Dansui Shikidō ōtuzumi Ichirōemon Otogi bikuni Ichirōemon Sōgishokoku　monogatari Ichirōemon Sayogoromo Ichirōemon Shin otogibōko Ichirōemon Shinchikusai Analysis and results In this study, we compared Saikaku, Dansui, and Ichirōemon by the word, particles, particle bigrams, character unigram, character bigrams and character trigrams using principal component analysisto see the differences in each author. We conducted PCA and each authors depicted independently in the words, Japanese particles, Japanese particle bigrams, character unigram, character bigramsand character trigrams. Figure 1. PCA results for the word Figure 2. PCA results for the particles Figure 3. PCA results for the particle bigrams Figure 4. PCA results for the character unigram Figure 5. PCA results for the character bigrams Figure 6. PCA results for the character trigrams Discussion and conclusion When comparing the words, Japanese particles, Japanese particle bigrams, character unigram, character bigrams and character trigrams using PCA, Saikaku, Dansui and Ichirōemons novels made each groups. Moreover, as said in qualitative research, Saikaku and Dansuis novel shown closer and Ichirōemon shown different characteristics, especially Sayogoromo. We on-going digitize Dansui, Ichirōemon and the other writers text data. In the future analysis, we will add works and the other writers for comparisons the relationship of the seventeenth century Japanese authors works. "
	},
	{
		"id": 388,
		"title": "The Augmented Criticism Lab’s Sonnet Database",
		"authors": [
			"Ullyot, Michael"
		],
		"body": " Introduction / Importance The sonnet is a prodigious poetic form. Since its invention in the 13th century by Giacomo da Lentino, hundreds of poets have written many thousands of sonnets in European literary languages. It was popularized by Petrarch in the 14th century, translated by Wyatt and Camões in the 16th, and reformulated by poets from Shakespeare to Rilke to Frost. The experimental poet Raymond Queneau has even written a machine-generated sequence whose lines can be recombined in a hundred trillion different ways. This project has begun to compile every extant sonnet into a database < acriticismlab.org >, in order to quantify their features through time. Those features include dates, languages, authors, diction, sentiments, named entities, and form. My research question is straightforward: just what is a sonnet? Definitions have tended to focus on its form: 14 lines of rhymed ten-syllableverse. Subtypes, including the Petrarchan and the Shakespearean sonnet, are often based on rhyme scheme. But another definition is based on generic rather than formal features: a first-person reflection or dialectical self-confrontation, often with a voltafrom problem to resolution. To what degree, then, is the sonnet a form or a genre? What subtypes will a comprehensive, quantified taxonomy reveal? I am pursuing these inquiries by gathering as many known specimens of sonnets as possible, and then quantifying my analysis of their metadata. This includes metadata at the level of tokens and lines; of clauses and sentences; of rhyme-unitsand complete sonnets; and of their published sequences. There are many features of these units that can be encoded, largely through automated natural-language processing. Tokens can be lemmatized and tagged with their parts of speech; their order and frequencies can be modelled as topics; their syllables per line can be counted; their rhyme with other tokens can be represented. The only human-dependent encoding the database includes at present leverages the expertise of anthology editors: orthography, punctuation, authors, dates, and copyright. The sonnet genre must be localized in its diction. Some words appear more frequently than others, particularly in the sonnets early centuries of first-person lovelorn reflections: words like love and she and suffer and so on. So, too, do words describing the sonnets own composition: words like ink and lines andthis. But genre can be quantified at the level of the sentence, as other scholars have discovered by analyzing topics and principal components in Shakespeares sentences. This project will determine what generic features the sonnets words and sentences reveal. Methods The ACL Sonnet Database has standardized its texts according to the TEI guidelines, making them available to basic query functions and JSON object serialization. Thus far it contains 1880 Englishlanguage sonnets, including 445 transcribed from a single print anthology. My students and I have populated this repository first with English-language sonnets because they are numerous enough to offer a test case for machine-enabled research in any natural language. The database also maintains a Python class for connecting to its data via the RESTful API, automating much of the data parsing for analysis with software like the Natural Language Toolkit. Initial student-driven inquiries began with close readings of ten sonnets from the anthology to identify quantifiable features. Students have charted the frequency distributions of the sonnets rhyme schemes; enjambment; rhetorical figures; and topics, including rhetorical questions and references to celestial objects and classical muses. Results At this proof-of-concept stage, the database offers results only in these limited domains, and on this limited dataset. At the time of the DH2019 conference, it will have many more thousands of sonnets. I will report on their quantifiable formal characteristics, including rhyme schemes, meter, line lengths, sentence lengths, word frequencies, part-of-speech distributions, and ngrams. I will also report on topic models and the sonnets principal components distributed through time, author nationality and gender, and other salient subdivisions. Discussion Anthologies of sonnets are sufficient for preliminary student-driven inquiries, but to generate insights into the sonnet writ large, a wider net is necessary. I have begun conversations with machine-learning specialists to train a neural network to recognize sonnets in undifferentiated text files, based on the formal and generic characteristics of sonnets isolated by anthology editors. To prepare for this phase my approach will be two-pronged: to give students another dozen anthologies for further transcription; and to use that expanding repository of sonnets as a training set for a machine-learning process that will identify similar poems in a corpus of 70,000 English texts printed before 1700, the Early English Books Online - Text Creation Partnershipcorpus. Early sonnets establish conventions to which later English sonnets respond, so they are a valid place to begin this inquiry. That process has already begun with a subset of 18,000 XML files from the EEBO-TCP corpus containing the <l> element, denoting lines of poetry. My lead programmer, who built the database, will write an algorithm that parses these undifferentiated elements into clusters of 14-line sequences, on the provisional assumption that all 14-line stanzas or poems bear a family resemblance to the sonnet.I will begin with 14-line sequences in order to identify the extra-formal characteristics that are twinned with this form; only then can I unshackle the detection algorithm from the constraints of form, to see which other poetic units bear the nearest affinity. "
	},
	{
		"id": 389,
		"title": "From a Reference Book to Research Data: Literary Bibliographies as Sources for the Data-driven Research",
		"authors": [
			"Malínek, Vojtěch",
			"Umerle, Tomasz",
			"Wciślik, Piotr"
		],
		"body": " Rise in data-driven studies Data-driven research on bibliographical data in the field of literary studies is on the rise from the beginning of the 21st century with researchers such as Franco Moretti, Matthew Jockers, Katherine Bode, or Hoyt Long and Richard Soleading the way of empirical and theoretical research and discussions. Notable and established methods of data-driven research – like distant reading, or macroanalysis– have gained considerable recognition in the scholarly field. However, recently the quality and representativeness of datasets for performing such research have come under scrutiny. Bode calls for data-driven research that is based on datasets that are richer, better documented, curated, and more systematic in their scope. While Bode addresses the scholarly community in the first place, this paper, written from the perspective of data producers, recognises the challenges facing existing literary bibliographic resources in the age of data-driven research. Basing on the experience with two large bibliographic databases, this paper assesses how literary bibliographies adapt to the need for advanced data uses, and how application of data-driven methods in literary research revolutionises the way the bibliographies are prepared, standardised and published. In conclusion, the paper identifies the gap between data studies and data production, which should be bridged through the interdisciplinary cooperation within digital humanities. Literary bibliographies: from auxiliary subdiscipline to producers of research data Literary bibliographyhas traditionally been an auxiliary subdiscipline in the humanities, and its research goals were relatively limited. The place of bibliography within the larger scientific context – and the very meaning of the term – has shifted in the last decades. Until mid-20th century that meaning was broad and covered a variety of problems of organizing and selecting documents, while more recently these problems have been attributed mainly to disciplines connected to library and information sciences. And the term bibliography narrowed its scope to detailed examination of printed books as physical objects. As a result of this shift, bibliographers came to play a rather passive role in the development of the research potential of data gathered by subject bibliographies in the humanities. Such development could only be driven by the expectations of researchers in the field, and these, until recently have been rather modest, rarely demanding more than reference provision. The rise of data-driven research literary had a far reaching impact on literary bibliographies, redefining the role of bibliographers as data specialists, their core operations, the setup of bibliographical departments, and the modes of gathering and publishing metadata. The way data-driven research uses bibliographic data amounts in fact to redefinition of a bibliographic record itself. While its main purpose used to be to provide reference, i.e. an accurate representation of the described document, now this aura of raw datahave dissipated in favor of a more nuanced understanding, which presents the bibliographic record as already a statement of both cultural meaning and certain research assumptions, which need to be examined in their own right. This is especially relevant in case of literary bibliographies. As texts constitute the quintessential primary source of literary studies, the advances of data-driven research in this field depend on the acute awareness of the possible implications of decisions, methods, tools used by literary bibliographers. And conversely, it depends on whether literary bibliography can open to and assimilate the insights coming from e.g. the sociological and cultural developments within historical bibliography, or the new wave of documentation studies, which examines e.g. theoretical issues of documenting and its implications in contemporary culture. CLB and PBL as scholarly data infrastructures CLB and PBL are examples of scholarly infrastructures that gather rich and large datasets that aim at reconstructing the literary system in a most comprehensive way possible. They are unique resources of open bibliographic metadata, covering respectively 250 and 70 years, each containing app. 0,7 mln records in database form, and app. 1,5 mln records in form of scanned pages/cards of bibliographic information. Departments responsible for their creation have been continuously running for 70 years, consisting of teams of app. 15 bibliographers working full-time. In the last decades they transformed their bibliographic output from the original printed formto the database versions with proprietary system and public interfaces that present large, deeply structured datasets. The information they gather represents an augmented perspective on the complexities of literary culture as a whole. It deals with literary, artistic and scientific, publications, their reception, literary events, and the organization of literary scientific life. The special attention is paid to the actors of the literary life, and the cultural and scientific institutions and their activities. For each year CLB and PBL process thousands of literary books, and hundreds of journals in order to register information on a broad range of literary subjects outlined above. The idea behind literary bibliographies is an idea of recreating the complexities of literary system through bibliographic metadata: - they aim for completeness of information, processing relevant sourcesin totality – books, parts of the books, proceedings, articles from the journals and newspapers, in the print and electronicform; - they deal with the broad cultural environment: register daily journals, presence of literary issues on radio, TV, the Internet; - they aim for methodological stability through decades of processing sources of information, and they rely on stable teams of bibliographers; - they use the same infrastructure for broad range of topics – information about the literary figures, works, organisation, events etc. are interconnected; - they are a long-running projects with stable financial foundation. The missing link between data producers and data researchers While certain researchers – as Bode argues – fail to recognize or describe the limitations of datasets they work on, more thorough and sustained discussion between the researchers in the field of data research and data producers,could lead to both better research and better preparation of bibliographic data. Cooperation in bridging empirical data studies in literary research and regular data production will lead to a better understanding of how certain literary issuescan and should be represented in the form of bibliographic datasets, not only from technological point of view, but from a literary point of view. The paper will expand on the complexities of 5 crucial issues from CLB and PBL databases that should be the subject of such analysis: 1) very basic yet underappreciated issue of readiness of bibliographic databases for quantitative analysis and statistical representation; the bibliographies have not traditionally taken this kind of usage of their data into account; this can be taken into account by the researcher, but this issue is so prevalent, that it needs to be addressed; 2) balancing the stability of metadatawith the sensitivity to changes in terminology; in large databases the subject headings could be changed without the memory of the previous ones which is sometimes needed – in case of mistake on inherent prejudices– but sometimes it might induce ahistoricity; 3) how to account for the incompleteness and heterogeneity in data and the assessment of its impact on data research? Not all incomplete information is retrospectively traceable, as the literary bibliographic databases have not valued possible statistical uses; for example – for different years there might be journals that have not been processed due to very practical reasons, or materials can be processed using different methods; 4) the introduction of new or atypical forms of documents into the literary bibliographic databases – like Internet documents, or grey literature, samizdat, performance, stand-up, video games etc. – with the awareness of limited workforce available; 5) the relative lack of subject analysis of artistic texts in bibliographical databases: How it should be accounted for? May it be supplemented with the results of textual studies? How big of an issue it is for the development of further research? Issues like these cannot be solved only by data producers, but rather through cooperation of data producers and users, especially the researchers. The broader cooperation between both sides can significantly develop the possibilities for the data-driven researchin the field of literary studies. Funding: Activities of Czech Literary Bibliography Research Infrastructure are supported by the Ministry of Education, Youth and Sports of the Czech Republic. "
	},
	{
		"id": 390,
		"title": "Polish Literary Bibliography - New Research Data Portal for Complex Cultural Dataset",
		"authors": [
			"Koper, Beata",
			"Umerle, Tomasz"
		],
		"body": " Polish Literary Bibliography [PBL] is a large literary database containing app. 0,7 mln bibliographic records in a database form, and app. 1,5 mln records in a form of scanned pages of bibliography. It covers the period from 1944 to 2003. It contains data on more than 40000 authors, 100000 works, and records close to 15000 cultural events, more than 10000 theatrical plays and radio plays and literary programmes, and 1000 films. PBL has experienced two comprehensive remediationsin recent decades. First, around 1998, it was transformed from a printed book into an online databasein open access. It was one of the earliest of such cultural databases in Poland, and it provided a stable environment for 20 years of continuous creation of rich bibliographic metadata. Yet, its Oracle-based production environment and its user interface were geared to faithfully represent the structure and layout of the bibliographic data of its printed predecessor, rather than to open for the possibilities of digitally-enabled data exploration, and as a result it did not comply with any of the common standards in terms of record structure or data formats, what eventually lead to serious problems with both preservation and interoperability of collected data. From 2015 PBL has been transformed into a new kind of service, in cooperation between the Centre of Digital Humanities of the Institute of Literary Research of the Polish Academy of Sciences, the Institutes Department of Current Bibliography, and IT specialists - Poznan Supercomputing and Networking Center. The redesign of the service was influenced by the research on users practices via Google Analyticsand their practices, combined with the strong conviction that the efficient search engine of the service should be complemented by the rich data-exploration experience. As Whitelaw argues, it is the rich, browsable interfaces that reveal the scale and complexity of digital heritage collections. The goal of the project team was to produce a service that can display not only the bibliographic records and the contents of authoritative files in the most seamless way possible, but also to open up the database for advanced research uses. The poster presentation will focus on the results of the second remediation of PBL - the redesign of the bibliographic service, and the plans for its development. It will present 1. the main characteristics of the new PBL service and 2. the main challenges that the project team had to face and resolve that resulted from the complexities of the PBL dataset. Design for rich data In order to account for the complexities of PBL data, the new service includes: Many access points that enable exploration of the complexities of the PBL dataset from different perspectives: search engine and advanced search engine, browsable finding aids that include a complex set of filters and facets; user can browse data starting with: subject headings, authoritative files: people, institutions, events, series, works, other categories: books, films, plays, TV programmes etc. Documentation describing PBL dataset in following manner: the scope of dataset, the organization of bibliographical work, the sources used for creating metadata. Provision for the use and reuse of the data in data-driven studies: API module, and possibility of CSV downloads, Statistical module for data visualization: Kibana. The main challenges for the project team The main challenges that the poster presentation will highlight are: finding the best workflow for collaboration between digital humanities researchers, IT specialists and bibliographers, striking the balance between the need of consistent structuring of data and the unstructured nature of annotations provided in specialised bibliography, data processing for the data migration process: introducing modern authoritative control and linked open data mark-uppersonal authority files into one, and restructuring institutional and event files), including the implementation of VIAF IDs, parsing the annotated data fields in order to structure them, retrodigitization of 40000 pages of bibliographic records. "
	},
	{
		"id": 391,
		"title": "BigSense: a Word Sense Disambiguator for Big Data",
		"authors": [
			"Uslu, Tolga",
			"Mehler, Alexander",
			"Schulz, Clemens",
			"Baumartz, Daniel"
		],
		"body": "Abstract We describe BigSense, a neural network-based approach for highly efficient word sense disambiguation. BigSense uses the entire English Wikipedia disambiguation pages to train a model that achieves state-of-the-art results while being many times faster than its competitors. In this way it is possible to disambiguate very large amounts of text data with reference to the largest freely available disambiguation model, while the time complexity of the model remains manageable. Thus, our approach paves the way for large-scale disambiguations in text-related digital humanities. 1. Introduction WSD is an indispensable task in the field of Natural Language Processing. In this paper, we describe BigSense, a neural network based approach for efficient WSD. This approach is based on a previous model, which worked well for the German language. BigSense is extended to English using all disambiguation pages of the English Wikipedia. We improved BigSenses predecessor so that we again achieve state-of-the-art results, despite being much faster than our competitors. In our Wikipedia-based experiment we achieve an F-score of almost 90 %. This enables the use of WSD also for larger corpora and brings us one step closer to the goal of machine reading on the basis of largely disambiguated texts. 2. Related Work The general approach of BigSense was motivated by fastText. This is because fastText is a very efficient way to classify data. We transposed fastText to WSD in order to efficiently determine the meaning of ambiguous words. By this we mean scenarios in which hundreds of thousands of different words are ambiguous.describe similar tools to BigSense since they perform Entity Linking, where they link text segments to knowledge databases such as Wikipedia. BigSense, however, has its focus on the disambiguation of ambiguous words. The creation of the disambiguation corpus, using Wikipedias link structure, was similarly performed by. However, they did not use the corpus to disambiguate all ambiguous words, but compared them with the Senseval 2 dataset using a subset of 30 nouns.utilize graph based algorithms operating on semantic networks to perform WSD.present two WSD algorithms, achieving the best results by means of a semi-supervised algorithm combining labeled sentences with unlabeled ones and propagating labels based on sentence similarity.show that the use of word embeddings achieves an improvement in WSD compared to standard features.define WSD in terms of a sequence learning problem. This is done by means of a bidirectional LSTM-based neural network. Unlike these approaches, we present a method that can handle big data: in terms of the number of senses to be distinguished and in terms of the number of units to be disambiguated. On the one hand, knowledge driven approaches using, for example, WordNet and related resources are limited in terms of the number of senses distinguished by them. On the other hand, approaches that rely on algorithms like PageRank or classifiers like SVMs or LSTMs are limited in terms of their time efficiency. Their runtime can last up to weeks and months when handling the amounts of data used in this paper. Therefore, we need a versatile and efficient method for WSD, as presented in the next section. 3. Model 3.1. Architecture The artificial neural networkused by BigSense builds upon a previous model architecture by adding an additional fully-connected hidden layer and replacing the sigmoid output layer with a dynamically reduced softmax layer. We found out that this additional hidden layer improves performance. Adding more hidden layers did not bring a performance boost anymore, however it slowed down the runtime. Each ambiguous word is assigned a distinct set of senses, which are gathered together for all ambiguous words in a training batch and make up the reduced output layer. This enables fast training, despite relying on the computationally more expensive softmax function. Possible senses are chosen based on the word to be disambiguated and are part of the input of the ANN. Instead of using global averaging pooling to combine embedding vectors, their sum is divided by the square root of the number of input words. We decided against using Dropout, since it did not appear to have any beneficial impact. Figure 1: Model architecture of BigSense. 3.2. Data Training and test corpora are generated using links of Wikipedia articles. Disambiguation page titles are used to find ambiguous words. Each title is simplifiedand assigned as word to a new sense group. Titles of redirects to disambiguation pages are also added. Sense groups combine similar ambiguous words which can assume the same set of senses. Senses are URLs to Wikipedia articles that can refer to full articles or single sections. A sense may only belong to exactly one sense group, to ensure the same subset of senses are always present in a reduced output layer of the ANN. Therefor the same URL may be used for multiple senses. Figure 2: Relationships between words, sense groups, senses and article URLs. Senses are found using two methods: First, links on disambiguation pages are added as senses to the assigned sense group of the page. Links in section See also and links that do not contain the simplified disambiguation page title are ignored. Second, if there is an article or section that is linked at least five times using a link with the same simplified title as a sense group, it is added as sense to that group. Redirects are resolved before counting. Each paragraph in Wikipedia is used as a training example for the senses corresponding to the article or section it is from. In addition, any paragraph that contains links is used as example for senses that are assigned to the link destinations. To improve the quality of examples, paragraphs need to contain at least 5 tokens. Paragraphs that contain HTML or Wikicode tags are discarded, as well as tables. Text generated by most templates will simply be ignored. A training example consists of one tokenized paragraph, a list of possible senses for one ambiguous word and the correct sense. Most paragraphs are assigned to multiple senses in different sense groups and therefor reused multiple times. A sense group needs to have at least one sense or it will be removed as well. 4. Experiment We applied BigSense to our Wikipedia generated corpus as well as to Senseval and SemEval tasks. 4.1. Wikipedia-based Disambiguation The large number of articles in the English Wikipedia allowed us to generate 121,275,847 training examplesfrom 31,842,587 relevant paragraphs. Our approach yielded 549,770 senses for 478,077 article URLs in 168,546 sense groups. On average a sense group contains 3.3 senses. Around 40% of all sense groups only contain one sense, which is reflected in the MIN baseline. A wide range of parameter configurations were tested and were able to achieve an F-score of up to 89.5 %. Instead of using tokens directly, we exclude punctuation marks, build bi-grams and hash them using 10,000,000 buckets. The embedding and hidden layer both have a size of 25. Micro batches of 32 examples each were used to train the network using gradient decent with exponentially decaying learning rateover 8 epochs. Tools 1,000 5,000 all Wikifier 16 min 41 s 1 h 24 min ≈ 302 days Illinois 6 min 53 s 24 min ≈ 77 days IXA 58 min 49 s 4 h 47 min ≈ 3 years Babelfy 1 min 50 s - ≈ 33 days TAGME 5 min 42 s 28 min 40 s ≈ 98 days BigSense 10s 13 s 16 min 49 s Table 1: Runtime-related evaluation regarding similar tools using 1000, 5000 and all test instances. Type F1 micro F1 macro BigSense 0.895 0.779 MFS Baseline 0.591 0.405 MIN Baseline 0.057 0.124 Table 2: Evaluation of BigSense in the Wikipedia experiment. 4.2. Senseval and SemEval related Disambiguation SemCor 3.0was used to train models to evaluate Senseval and SemEval related tasks. We used the same parameters as we did for the Wikipedia data, except for gradient clipping, which we had to enable, because the learning rate of 1.0 was too high. Training duration was also extended, to suit the smaller data set. We evaluated models on the test data after each epoch until the moving average of the cost function did no longer improve. In some of these experiments we had lemma and part-of-speechinformation, which we also considered as parameters. Table 3 lists the results for Senseval 2and Senseval 3. NG Epochs SE2 SE3 Token 1 95 0.718 - Token 2 79 0.710 - Lemma 1 73 0.708 - Lemma 2 88 0.706 - Token + PoS 1 66 0.709 0.688 Token + PoS 2 87 0.705 0.693 Lemma + PoS 1 76 0.709 0.702 Lemma + PoS 2 100 0.708 0.694 Table 3: F1-score for Senseval tests.Table 4 lists results for SemEval-2013 Task 12, and SemEval-2015 Task 13. Some test cases require the classification of senses that are not included in the training data. However, the neural network cannot predict classes it has never seen before. In these cases we proceed by classifying the most frequent sense in WordNet. NG Epochs SE13 SE15 Token 1 64 0.682 0.697 Token 2 76 0.685 0.708 Token + PoS 1 47 0.642 - Token + PoS 2 52 0.623 - Table 4: F1-score for SemEval tests.We also conducted an experiment where we ignored all test classes that did not occur in training set. In this way, we can find out what classification quality we achieve for a subset of the test set for which we have training data available. SE2 SE3 SE13 SE15 BigSense* 0.906 0.889 0.842 0.788 Table 5: F-score of BigSense, when ignoring unknown test cases. 4.3. Discussion We have successfully developed a classifier that can not only efficiently disambiguate huge amountsof data, but can also compete with the state-of-theart. In order to compare with other WSD methods, we have carried out Senseval and SemEval tests. Here we were able to show that we can keep up with state-ofthe-art methods and even surpass them in some fields. In addition, we have shown that we can achieve 90% F-score if we only consider senses that were included in the training set. Model SE2 SE3 SE13 SE15 Iacobacci, 2016 0.634 0.653 0.673 0.715 Yuan, 2016 0.736 0.692 0.670 - Chaplot, 2015 0.605 0.586 - - Raganato, 2017 0.720 0.702 0.669 0.724 Melamud, 2016 0.718 0.691 0.656 0.719 BigSense 0.718 0.702 0.685 0.708 Table 6: Comparison of BigSense to state-of-the-art methods. 5. Conclusion We presented a novel approach called BigSense for disambiguating large amounts of data. In order to present the efficiency and quality of BigSense, we have created a huge disambiguation corpus using the English Wikipedia. Here we have classified almost 550,000 senses with an F-score of 89.5 %. In Senseval and SemEval tests we can keep up with state-of-the-art methods and in some cases even surpass them. In future work we will analyze the influence of topic classifiers on BigSense. "
	},
	{
		"id": 392,
		"title": "DHK12: Open-Access Digital Humanities Curricula for K-12 Schools",
		"authors": [
			"Uyola, Rosalie"
		],
		"body": " DHK12 Short Paper Proposal DHK12is a free, unrestricted, online open-access collaborative network of educators who seek to: use digital tools to make the humanities come to life for students draw on the scholarship of women and people of color to diversify curricula support students as they do the work of historians by creating knowledge. This year, DHK12 is in the process of launching two digital projects -- built by students, the people who will need the tools to deal with the complexities of the future -- that will contribute to public scholarship in digital indigenous studies, digital black studies, Africana and diasporic studies, digital queer studies, and digital feminist studies. With these interdisciplinary and transnational, student-driven digital archive projects, we will build complex models of memory and commemoration, analysing our data with computational methods and communicating the results to a broader public. DHK12 is part of an ethical and political imperative for a growing number of teachers and scholars committed to accountable and reciprocal research practices and knowledge-sharing. As producers of an open-access interdisciplinary curriculum and network, we are organizing academic knowledge production away from the profit motive of textbook publishers. Instead, we use primary source documents and digital archives, and work collaboratively with local cultural institutions to teach DH thinking and skills to primary and secondary school students. DHK12 develops projects to teach students and teachers how to use computational text analysis, digital mapping and timelines, image processing, and 3D modeling to develop new epistemologies, ontologies, and ways of knowing and understanding public humanities and societal engagement. DHK12 Student Project #1Our digital indigenous studies project, The Red Atlantic, explores internal settler colonialism versus external imperial colonialism. Questions that we explore include: Is there a different representation or story about Indigenous people and what happened to them produced by the American Museum of Natural Historyversus the Natural History Museum? How do different types of colonialisms tell different stories about the encounter or the colonial project in 3 dimensions? Using computational text analysis, we will analyze internal records to ask: What are the internal debates? What do the debates around changing exhibits sound and look like? What are the issues? This year, K-12 students in NYC photographed dioramas and mounted exhibits at the American Museum of Natural Historyand we built a digital archive that places the museums narrative side by side with historical and artistic representations of culture created by indigenous scholars. Our objective is to use digital tools to decolonize the museum and open up spaces for indigenous scholars and artists within locations traditionally identified with dominant representations of colonialism. As a local and international public space, AMNH is a powerful producer of historical narratives and our digital archive seeks to re-center the voices and ontologies of indigenous scholars within the epistemological origins story of the American continent. For next year, we plan to partner with a K-12 school in London to build phase two of our archive, in which we decolonize the representations of indigeneity at the Natural History Museum by placing current exhibitions and collections in conversations with contemporary work created by indigenous scholars and artists. DHK12 Student Project #2Rainbow is Enufis a digital archive documenting the remarkable tenacity of black women, trans women, and femmes visual, cultural, and political influence on American history. This black joy and black excellence archive draws on and contributes to digital black studies, digital queer studies, and digital feminist studies. Currently, in public and independent American schools, children learn about black American life by studying enslavement, the 13th Amendment of the U.S. Constitution, Jim Crow Laws, redlining/blockbusting of housing, and most recently, the prison industrial complex. To offer students an opportunity to understand the complexities of these histories more completely, our project combines digital research and digital production that allows students to explore the joy and excellence of black labor and resistance through the study of music, film, dance, art, comedy, theatre, and food. Students will utilize digital resources such as Digital Public Library of Americaand Stanfords Tooling up for DH http://toolingup.stanford.edu . The historiography of the project highlights and centers contributions of black women as creators of knowledge. By presenting students with new information using primary sources and giving voice and volume to marginalized histories, we seek to decolonize U.S. History curricula. Our contribution to digital humanities in general and to the arts, Africana and diasporic studies, and social justice research in particular is a digital archive that is free, unrestricted, and open access, for use by researchers, teachers, students, writers, curators, community organizers, and activists from around the world who are dedicated to the interdisciplinary study of U.S. culture and history in a global context. As a K-12 teacher and junior scholar in American Studies, I hope this short paper presentation will enable me to build partnerships with DH scholars worldwide to continue to develop interdisciplinary K-12 digital humanities curricula that use and contribute to interdisciplinary scholarship, teaching, and creative student-centered work. The presentation will have a lessons learned recommendation section for others who wish to utilize digital humanities for pedagogy within the K-12 systems, as well as students reflections about their experience. "
	},
	{
		"id": 393,
		"title": "Publishers, Printers and Booksellers - Implications of Properly Structured Metadata for Digital History",
		"authors": [
			"Vaara, Ville",
			"Hill, Mark",
			"Tolonen, Mikko"
		],
		"body": " There is a critical point to be made about proper use of metadata in digital history. With any computational analysis of a large historical dataset, there is a strong temptation to approach the dataset as holistic representation of the language and intellectual landscape of its era. Digital history projects are often rightly criticised for having a naive approach to sources resulting in simplification of complex phenomena. In this paper we demonstrate a way to avoid this, and how proper use of metadata is necessary for serious corpus control and digital source criticism. This work makes two contributions to the history of the book and digital history. First, we present a methodological approach for creating a historical biographical database from a bibliographical catalogue. Second, we demonstrate solutions for forming a uniform dataset from a noisy and heterogenous starting point. This opens new opportunities which earlier historical research using bibliographical data has missed due to problems of data quality and coverage. For example, while publisher networks had a greater impact on the distribution of ideas in early modern period than has been realised, publisher information as a source has not previously been extracted at scale, despite its potential to change the way we study intellectual history. Additionally, as this work is part of wider intellectual history research project, and the dataset produced here is combined with other bibliographical research strands, there are more general claims with regard to the utility of proper metadata in quantitative computational book history. Data The English Short-Title Catalogueis a standard source for analytical bibliographic researchholding close to half a million titles with varying but substantial coverage of printed items published in English from 1473 to 1800. Following MARC 21 guidelines, the data in the catalogue closely matches that found in the original published titles: names, years, publisher imprints, etc. are documented as they appeared on the title pages of the printed originals. The raw data presents major challenges for computational approaches to analysis, however. Bibliographical data is compiled around published titles, and all data points are handled as simple strings of text, instead of independent and unique objects. Thus a new, more structured, data model is required. To this end, we implemented a relational model, where each actor is an independent and unique object connected to all the titles they were involved with. The names of entities in the ESTC which this study focuses onare hidden in the text of the full publisher statements, and thus had to be identified in the imprint. As a first step, a machine learning based named entity recognitionpackage offered by the Stanford NLP Groupwas used for entity name identification. Following extraction, various heterogeneous textual representationsof the same entity were identified, linked, and collected to create objects with unique identifiers for each historical actor. A data matching process was applied to test if names detected in the publisher statements could be linked with distinct entities found in external databasesand Virtual International Authority File). In the cases where such entries did not exist, similar entities were grouped with rules based logic. Previous efforts to distill the publisher information from the ESTC have not incorporated these essential linking and unification steps, or approach the problem with labour intensive solutions, Map of Early Modern Londonand ImprintDB). The end result of the unification process is a bibliographic database of 900,000 non-unique records harmonized into fewer than 200,000 actors, of which 30,000 are identified as being part of the book trade. Between these actors we have been able to make roughly 800,000 individual connections. These relations are documented in linking tables similar to a linked data database. The benefit of a flexible linked data model is that it allows natural extension and modification as research based needs arise. We claim that this is currently the state of the art dataset covering the early modern English book trade. Discussion Multiple previous historical hypotheses, based on both geographically and chronologically limited sources, can now be tested at a scale that covers all the publications in the ESTC. Questions of location, spread of the trade outside London and the importance of networks and connections to that process, and questions of authors and publishers relationscan all benefit from a statistical re-examination. Previously unidentified niches, subgroups, and structures in the social networks of the book trade can be discovered through quantitative data exploration. Another potential for this type of book trade metadata can be identified with regard to corpus control for researchers utilizing large full text collections, or Eighteenth Century Collections Online). While large scale historical text corpora strive to encompass everything, by their they nature introduce multiple layers of statistical bias into the data. By making use of linked metadata one can focus text mining efforts on historically meaningful subsections of large text corpora. In fields such as historical computational linguistics, the standard solution has been to limit the corpus to a relatively small manually curated one. Large historical text collections do not typically come supplied with the kind of metadata that would allow properly subsetting them on a large scale, but with the methods presented in this paper, that becomes possible. This work demonstrates a general method for generating a linked biographical database from library metadata catalogue, and shows the benefit of using this as starting point for historical research. While at this stage the primary users of the data are the historians in the research group, as the project progresses the tools and data will be published following good practices for open science, such as adhering to a tidy data model, proper code documentation, and open repositories. Additionally, the methods can be adapted to a variety of existing national and transnational bibliographical metadata resources. "
	},
	{
		"id": 394,
		"title": "Former aux Methodes en Sciences Humaines et Sociales avec Bequali",
		"authors": [
			"Vandenbunder, Jeremie",
			"Bendjaballah, Selma",
			"Garcia, Guillaume",
			"Cadorel, Sarah",
			"Groshens, Emilie",
			"Fromont, Emilie",
			"Juillard, Emeline"
		],
		"body": " Lenseignement des méthodes qualitatives et ethnographiques occupe une place prédominante dans les filières universitaires en SHS, favorisant un apprentissage par et pour la recherche. Dans ce domaine, les façons de faire sont diverses, que ce soit en termes de méthodes enseignées, de formes denseignementou bien encore de données utiliséesSur ce point précis, disposer de données qualitatives fiables, pertinentes, éthiquement et juridiquement sûres représente une gageure pour les enseignants. Cette communication vise justement à apporter un éclairage particulier sur cette question des données en présentant la banque denquêtes qualitatives beQuali et ses apports pour la formation aux méthodes en SHS. En effet, beQuali répond aux besoins spécifiques des enseignants en mettant à disposition des données anonymisées, issues denquêtes reconnues et couvrant un large éventail de thématiques et de méthodes. De plus, beQuali permet dutiliser ces données non seulement comme des exemples illustratifs mais aussi comme des corpus manipulables en tant que tels. Plus généralement, beQuali est une banque denquêtes qualitatives développée au sein du Centre de données socio-politiqueset fait partie de léquipement dexcellence DIME-SHS, qui offre des outils numériques pour la production et la réutilisation denquêtes en sciences sociales. Elle met à disposition de la communauté académique des données issues denquêtes qualitatives en sociologie et en science politique. beQuali sinscrit pleinement dans la perspective des humanités numériques dans la mesure où elle participe de la diffusion des savoirs par le biais des outils numériques. Le catalogue de beQuali compte à lheure actuelle une douzaine denquêtes dans les domaines de la sociologie et des sciences politiques. Les objets détudes sont donc particulièrement divers, tout comme les méthodologies mises en œuvre. De fait, la banque denquête a pour but de mettre en lumière le pluralisme des méthodes qualitatives et propose ainsi des enquêtes par entretiens, observations ethnographiques ou entretiens collectifs. Dans le but dapporter le plus déléments possibles sur le contexte de lenquête comme sur son déroulement, les corpus mis à disposition pour chaque enquête comprennent, outre les données proprement dites, lensemble des documents relatifs à la préparation de lenquête comme à son analyse. Pour donner un exemple concret, le dernier ajout au catalogue est une enquête réalisée par Anne Gotman intitulée « Dilapidation et prodigalité ». Le corpus mis à disposition est composé de 327 fichiers, dont 68 transcriptions dentretien et 36 questionnaires. Pour cette communication, une présentation générale de loutil beQuali sera proposée en premier lieu. Il sagira ainsi de donner quelques éléments de contexte concernant la banque denquêtes et de décrire ses différentes activités. Surtout, une présentation du site internet de beQualisera réalisée, notamment des différents outils dexploration des enquêtes. Ensuite, nous souhaitons mettre en lumière les usages possibles de beQuali pour la formation méthodologique et les diverses problématiques quils soulèvent en termes de pédagogie, daccès aux données et de contextualisation de ces dernières. En offrant un point de vue privilégié sur la recherche « en train de se faire », beQuali constitue de fait un moyen original de transmettre des savoirs et des savoir-faire. beQuali offre en effet aux étudiants lopportunité de faire connaître toute la complexité dune recherche qualitative en sciences sociales, complexité souvent invisibilisée dans les travaux finaux des chercheursLa mise à disposition des enquêtes sur internet, comme lutilisation de nombreux outils numériques, permettent de plus une adaptation pertinente aux différentes demandes des enseignants. Cest notamment le cas des opérations de numérisation des transcriptions dentretien et de lutilisation de la TEI. Lédition en TEI permet de décrire la structuration du texteet un affichage dynamique sur le site beQuali. Cet outil permet ainsi de faire travailler en ligne les étudiants sur la relation entre enquêteur et enquêté, par exemple, sur la nature des échanges ou le niveau de distance entre ces derniers. De plus, nous proposons de faire la démonstration dun nouvel outil de recherche dans les corpus denquête dédié aux enseignants et étudiants. Ce « kit denseignement » facilite la découverte des matériaux utiles et de leur contenu en réponse à des thématiques denseignement prédéfinies. Pour finir, la présentation sefforcera dintégrer des retours dutilisateurs de beQuali, notamment des enseignants. En définitive, cette présentation courte vise à présenter, via lexemple de beQuali, les possibilités ouvertes par le numérique pour explorer et exploiter les corpus complexes que sont les enquêtes qualitatives dans un contexte de formation universitaire. "
	},
	{
		"id": 395,
		"title": "Tracing People, Places And Dates In An Early Modern Context",
		"authors": [
			"van Berchum, Marnix",
			"Bosse, Arno"
		],
		"body": " The accurate identification of people, places, and dates is fundamental to historical research. The letter records in Early Modern Letters Online and their metadata rely on the correct identification of these entities. In practice each of these identifications raises considerable difficulties. Dating early modern letters accurately requires systems for mastering a complex landscape in which places transition between different calendars at different times. Accurately recording early modern places requires capturing data that describes changes over time in how places are both named and nested within larger entities. Confidently identifying letter writers and recipients requires the development of authority files for early modern individuals who are not found in national biographical dictionaries or library catalogues. To facilitate this process, the Cultures of Knowledge project at Oxford University and the Humanities Clusterat the Netherlands Academy of Sciencesare developing three Linked Open Data resources for people, places, and dates. This poster presents the current development of EM Places – a collaboratively curated, historical geo-gazetteer for the sixteenth to eighteenth century – and EM Dates, an early modern calendar resource and conversion service. A prosopographical name authority, Early Modern People, is being planned for development after these tools. It will be populated initially by the c. 25,000 unique early modern biographical and prosopographical person records collected thus far by Cultures of Knowledge. The identification of places includes capturing data describing changes both in how places are named and how these are related hierarchically to other place-related entities, such as polities. But in practice, large-scale geo-gazetteers such as GeoNames can capture very little of this complexity. In particular, they lack data on the different contexts a given place has occupied throughout its history. Recently, greater attention has been paid to enriching and integrating gazetteers. Specialized gazetteers, conceived from the outset not just for human readers but also for computation, have helped to establish standards for querying and exchanging datasets, while other projects are preparing data models capable of representing temporal entities. Inspired by these developments, the Cultures of Knowledge project and the HuC are preparing a collaboratively populated, Linked Open Data geo- gazetteer for early modern scholars. EM Places has four goals: 1) to be a resource for identifying early modern places by means of their current and historical name variants, 2) to provide means for researchers to contribute additional historical contexts and place name attestations, 3) to fully credit and source contributions to the gazetteer by individual researchers and projects, and 4) to make the EM Places datasets and software infrastructure freely accessible and easily reusable, including via a Timbuctoo Graph QL API and asan OpenRefine reconciliation service. EM Places will also be able to share its data in the Linked Places Interconnection Format and participate in the Pelagios and World-Historical Gazetteer networks. The precise dating of documents is complicated by the simultaneous use in the early modern period of a number of different calendar systems. Worse, the dates of transitions between calendars varied from place to place: most Catholic countries adopted the Gregorian calendar in 1582. Until 1752, some Protestant countriescontinued to use the old style Julian calendarwhile others used the new style Julian with the New Year on January 1. However more specific data on when a particular historical polity such as a duchy switched from one calendar to another is often hard to identify and has never been collected in one location. EM Dates was designed to address this problem by providing a central resource for inferring the correct historical calendar for early modern place entities and carrying out the necessary date conversions. Given an ISO 8601 input date and an authority ID of a known place entity, it will attempt to infer the appropriate calendar for the requested date conversion. To accomplish this, it first queries the EM Places API for the dateat which the historical polity of which the place is a parttransitioned from one calendrical systemto another, flagging uncertaininstances for review. Users are given an opportunity to view the inferred dates and calendars and make corrections as needed. EM Dates then converts and exports the converted dates in tabular format together with the necessary provenance metadata. The first release of EM Dates will support conversion to the Gregorian from the Julian and Roman calendars. Further calendarsare planned for future releases. EM People and EM Places are both built on the Timbuctoo RDF datastore platform developed by the KNAW HuC and will be available in pilot form in Autumn 2019. More information may be found at their respective GitHub repositories: and . "
	},
	{
		"id": 396,
		"title": "The Literary Pepsi Challenge: intrinsic and extrinsic factors in judging literary quality",
		"authors": [
			"van Cranenburgh, Andreas",
			"Koolen, Corina"
		],
		"body": " Introduction The project The Riddle of Literary Quality http://literaryquality.huygens.knaw.nl aimed to find correlations between texts of novels and judgments of their literary quality. In other words: is the literariness of novels associated with or even explained by text-intrinsic properties? The 2013 National Reader Surveycollected a wealth of information on perceptions of literary quality of contemporary novels. It turns out that a machine learning model can predict the literary judgments based on the texts to a substantial extent; based on word frequencies and syntactic patterns, 61% of the variation in ratings of novels is predictable from purely textual features. This demonstrates that the text contains enough clues to distinguish literary texts from non-literary texts. However, we do not know to what extent humans rely on textual features when rating how literary a text is, since we collected judgments on whole novels by presenting the participants with the title and author of each novel. For the same reason it was not possible to identify the contribution and influence of particular aspects of the text. What we need is a blind experiment in which literariness is judged purely on the basis of text, without revealing any other information. We therefore propose a new survey, based on fragments from the novels used in the NRS, to collect evidence that text-intrinsic characteristics play a role in ratings of literary quality, and investigate exceptions where we suspect various biases may play a role. The results will tell us more about how perceptions of literariness are formed and which particular textual aspects play a role. They will also enable a direct comparison between the performance of humans and a computer model on this task. Motivation The NRS made clear that genre plays a role in judging literary quality. In the survey, Dutch respondents were asked to rate recently published novels on a scale of literary qualityand asked to motivate one of their ratings by an answer to the question Why did you rate this book with the score for literariness as you did? Respondents gave roughly three types of response, exemplified by Examples 1–3. It is suspenseful, the storyline is perfect, but in a literary novel I expect a deeper layer. Its chicklit Too light, simple, chicklit reads easily, but does not amount to much. First, as expected, style and narrative structure are important. But in explaining why they found a novel not to be literary, respondents also often found it sufficient to refer to genre, without referring to textual qualities. It is possible that those textual qualities are implied. Some respondents did elaborate and explained low ratings in terms of both genre and style. However, genre exclusion may also point to bias. If a novel with a pink cover is excluded from a high rating without further explanation, what does that mean? Are we judging the text or repeating common sense ideas on literary quality without questioning? The first indication that extrinsic factors play a role are large gaps between the prediction of the computer model and reader judgments. The translation of The sense of an ending, for instance, received the highest average rating, 6.6, whereas the model predicted 5.4. This novel was awarded the Man Booker Prize the year before, which has probably influenced respondents. For Eat, Pray, Love, this was the other way around: the computer predicted 4.7, while readers gave it a 3.5. A preliminary survey, conducted at a meeting of the KNAW Computational Humanities Program, showed that bias might play a role. We offered a handful of visitors five fragments, extracted from novels surveyed in the NRS. Respondents were asked: does this fragment originate from a novel with a high or low rating in the NRS? We anonymized the text by abbreviating names as initials. Remarkably, a fragment from Elizabeth Gilberts Eat, Pray, Love was the only fragment that all respondents picked as a highly rated novel—which it was not. Simkinconducted an online quiz, showing that average readers perform no better than chance at distinguishing a canonicalfrom a non-canonicalauthor. However, the fragments were shortand participants were not selected to have affinity with literature. Given these results, it is interesting to test the influence of text and bias on literariness in a carefully designed survey. Survey setup The two most important questions for the survey setup are who the participants will be, and what they will rate. We aim to select participants with literary affinity or expertise. To prevent the influence of author prestige, respondents should not see any metadata; nor do we want to cherry pick fragments. A double-blind setup with anonymized fragments will allow for this—we will set up a computer program to select equally sized fragments at fixed or random points from several novels. A trade-off needs to be made for fragment length; several sentences is too short, but more than a few pages takes too much time. Instead of a 7-point Likert scale, as in the National Reader Survey, we will present pairs of fragments, and ask the rater which is the more literary one. This has the advantage of forcing the rater to make a concrete comparison, instead of expecting each rater to have an existing, well-calibrated scale. Rankings can be computed with the Elo rating system, the same system used to rank chess players. In addition, we can ask for a motivation. We intend to run two experiments. The first experiment tests whether participants pass the challenge and measures how humans perform at the task of recognizing literariness from unmodified text fragments. The second experiment introduces manipulations of fragments to confirm the influence of particular features, e.g., protagonist gender, sentence construction, topic. This approach is followed by Blohm et al., who present an experiment on lines of poetry rated for poeticity and grammaticality. "
	},
	{
		"id": 397,
		"title": "A Transcription Portal for Oral History Research and Beyond",
		"authors": [
			"van den Heuvel, Henk",
			"Draxler, Christoph",
			"van Hessen, Arjan",
			"Corti, Louise",
			"Scagliola, Stefania",
			"Calamai, Silvia",
			"Karouche, Norah"
		],
		"body": " Background and Introduction Over the past 2 years a number of researchers from various backgrounds have been working on the exploitation of digital techniques and tools for working with oral historydata. The result of a CLARIN workshop in Arezzo https://oralhistory.eu/workshops/arezzo , May 2017, was the idea of a so called Transcription Chain as a webportal where researcher could upload their audio files, have them transcribed by Automatic Speech Recognitionand could edit/correct the text results with a speech editor. The Transcription Chaincan be considered as a couple of concatenated different software tools that ingest Audio and or Video documents and output Time-stamped TranscriptionsA Timed Transcription is a transcription of the spoken content where each transcribed objecthas a start-time and a duration or end-time. . A TC can be a set of software packages stored and run on a personal computer, but in this proposal, we see a TC as a set of web based tools, running on one or more computer servers in the internet. A TC typically uses different tools that run on different servers in different countries. The TC as defined in the Arezzo-workshop contains two basic elements: Transcription and Alignment. Transcription of thecontent of an AV-document can be done in two ways: Automatically by an ASR-engine eventually followed by manual checking and correcting the recognition results Manually, eventually followed by a forced alignment to receive a TT with the start- and end-times of all spoken words. Afterediting of a transcription it needs to bealigned with the speech signal. Several alignment tools can be used for this operation. The TC was implemented as a OH Transcription portal by developers of the Bavarian Archive for Speech Signalsin Munich. In this contribution we address the implementation of the portal, the first experiences as reported in a follow-up CLARIN workshop in Munich, and our future plans with the portal. Implementation of the Portal A prototype implementation of the OH portal has been set up at the Institute of Phonetics and Speech Processing. It can be accessed via the following URL: https://www.phonetik.uni-muenchen.de/apps/oh-portal/ The portal works like a dynamic spreadsheet: the columns represent files and processes, the rows are individual files. Files enter the leftmost column, and then proceed from left to right through the spreadsheet. This way, one can monitor the progress of ones data through the transcription chain. At every step in the processing can intermediate results be downloaded to the local machine. This transcription chain currently consists of four steps: Data upload and verification Automatic speech recognitionManual verification and correction of the recognised text Automatic word and phoneme alignment and segmentation The upload and verification step transfer the audio data to the server and check the file format, e.g. convert stereo files to two mono files. Then, the user is asked to select the ASR language. Currently, the portal supports English, Dutch, Italian and German. ASR is performed by academic partners such as the University of Twente, Radboud University Nijmegen, Sheffield University, or European Media Lab, or commercial service providers such as Google. Note that most ASR service providers store the audio data and to use them to improve their services – this is a severe problem for privacy reasons. Steps 2, 3 and 4 are mandatory: if the results of the ASR are known to be very reliable, then the manual verification can be omitted. On the other hand, if for some reason ASR does not work for the given files, one can skip ASR and proceed to the manual transcription of the file directly. In some cases, fine-grained word alignment is not needed, and hence it can be switched off. Figure 1: Main screen of the TC portal showing the four processing steps This is indicated by the checkbox in the column head. Figure 2: Output formats of the web interface It is the nature of a portal that many sites can access the portal server simultaneously, and that the portal does not perform the services itself. Instead, it calls external service providers, e.g. ASR services, passes the data to these servers, and processes the results. In the current beta version of the portal, queuing of incoming requests is a bottleneck, because the portal has to wait until one job has been processed before starting the next. Hence, the portal cannot estimate how long processing will take, it cannot inform its users about estimated time of termination, and it cannot reorder the queue to optimise throughput. The verification and correction of the ASR outcome is done using the Octra editor. Octra features three different graphical user interfaces for efficient transcription. The innovative 2D editor displays longer signal fragments on the screen with good time resolution. Human transcribers set boundaries in signal pauses and then transcribe the signal fragments between these pauses. Figure 3: OCTRA interface for manual correction of the transcriptions Octra is fully integrated into the portal, so that files opened with Octra will automatically be sent back to the portal for the subsequent processing steps. User Experiences Most OH researchers indicated that they prefer flawless transcriptions because they use the textual results for the final analyses. Only a few indicated that they used the transcriptions to quickly find the audio passage in question. Another issue is that most of the recorded speech is not grammatically correct. However, solving this problemis impossible because it would be tantamount to interpreting the text. Nonetheless, to increase the readability of the text, we tried to make sentences by adding a full stop after a pause of 400 msec or more. This isnt a perfect solution but it made the text more readable according to the scholars in the Munich-workshop. The disadvantage is that you get a lot of short sentences when people speak hesitantly. Finally, we most recordingscontain multiple speakers. This is solved by speaker clustering: indicating when someone else was speaking. As a result, we get a transcript where a new paragraph is started each time the speaker changes. Diarization is not the same as speaker recognition, so we do not get a name or ID of the speaker but only an indication of the speaker. In general, we get more speakers than there are in reality. Future work The OH portal is currently being updated to improve throughput and to adapt to changes in ASR services. We plan to implement a traffic light system to distinguish ASR services by their privacy policy, file quotas and language support. Furthermore, an authentication mechanism will be installed. In the first official releasethe commercial engines were removed but the participating scholars were informed about the inclusion of commercial engines in the previous releases. To our surprise, some scholars argued that they had no objection at all to the use of commercial software because they had already posted their OH-interviews on YouTube. So, in the next release we will include them again and offer the scholars the option to use them or not. Furthermore, we will extend the service to more languages. Contacts are established for Polish, Czech, Swedish and Finnish. Finally, scholars in Munich asked us if it will be possible in the near future to add their own vocabularies. At the moment it is not, but hopefully it will be possible in one of the next versions. "
	},
	{
		"id": 398,
		"title": "Durchdruck im Fokus: Visualising the Spatiality of Articles in Historical Newspapers",
		"authors": [
			"Van Galen, Quintus",
			"Hall, Mark",
			"Nicholson, Bob"
		],
		"body": " Dealing with the complex data challenges offered by newspapers has never been easy, and for years historians have struggled to delve deeper into an archive that had only received the minimum metadata – title and date. However, since digital archives, historical research of newspapers has become easier than it ever was. For that, they deserve full credit. Yet when archives change their medium or storage, certain aspects of the documents they hold inevitably get lost. Similarly, when modes of access change, certain aspects of a source, particularly the aspects that are more complex than the interface offers easy access to, drift from the mind. In a physical newspaper, the researcher would become intimately familiar with the spatiality of the texts they read from them. Front page or tucked in the back, set apart or clustered with articles about similar topics, these were all aspect that a researcher could use to gauge the importance of an article to the editor of the paper. Not so much with a digital archive. Keyword searches deliver the historian right to the text they requested, dropped directly on target without needing to consider a texts surroundings. To address the question of textual spatiality, we developed a tool that allowsvisualisation of the location of articles on newspaper pages in the form of a set of heatmaps. It relies on metadata included in the archives to deliver the researcher directly to the article they request, mapping image coordinates to the article. The coordinates of the area covered by these bounding boxes can be extracted at large volumes, tidied, and plotted on a page-by-page basis. This further improves upon existing tools, which estimate article size based on word count. These visualisations allow researchers to contextualise the space in which relevant articles appeared, by showing what a typical newspapers pages looked like, and how the articles under investigation fit into that model. This paper proofs this technique on a case study on Imperial sentiment. By investigating the spatiality of the articles within the paper as a whole, it is possible to see the context in which British readers encountered the Empire, compare it to context of appearance for foreign news, and explore change over time. For example, we noted a prevalence of Imperial keywords on page 7, which continues through the 35 years under review – a total of 1800 issues. Closer reading of the paper reveals this to be the section for adverts and notices. Just this brief application shows the ways in which a transformative tool can be employed in conjunction with more traditional research methods. These visualisations allow not only individual article sets to be plotted, but also allows a relative comparison between two sets of newspaper pages. The placement of articles mentioning Calcutta or Australia within Reynolds Newspaper, a national weekly paper, can easily be compared with the placement of articles in, say, the Birmingham Daily Post, which only distributed regionally. Such a comparison shows both papers place articles on the Empire in very similar contexts, with patterns of appearance closer to national news rather than international or foreign news. Additionally, visualisations can be integrated with other textual analysis tools, such as topic modelling, as a way to allow more intuitive understanding of the contents of each topic, or even a comparative analysis of the places within a paper where certain topics occur. This technique allows an extra handle during the arduous process of discovering the semantic meaning of a topic. Reducing the complex data to a two-dimensional graphic is non-trivial, as a multitude of aspects need to be considered, such as scaling pages to the same size, Binning per pixel or binning per column, and rough versus precise binning. It also needs to draw data from various parts and indices in the archive to produce the kinds of meaningful visualisations that facilitate new and more substantive research questions. This kind project underlines the need for digital archives to allow researchers access to the data raw, not only limited access through a web interface. In the context of newspaper research, these digital tools enable a whole field of study in newspapers as objects, with articles having their own spatiality made accessible for research on a larger scale than previously possible. In the context of the Digital Humanities, these tools show how digital tools can drive methodological innovation in other fields, and how computers can, while on the one hand disconnecting us from the complexities of our sources, also serve as vehicle for making those complexities understandable. "
	},
	{
		"id": 399,
		"title": "A Multidisciplinary Approach To The Use Of Technology In Research: The Case Of Interview Data",
		"authors": [
			"van Hessen, Arjan",
			"Scagliola, Stefania",
			"Corti, Louise",
			"Calamai, Silvia",
			"Karrouche, Norah",
			"Beeken, Jeannine",
			"Daxler, Christoph",
			"van den Heuvel, Henk"
		],
		"body": " Summary As increasingly sophisticated new technologies for working with numbers, text, sound and images come on stream, there is one type of data that begs to be explored by the wide array of available digital humanities tools, and that is interview data.,. A stream of concerted activities by the authors of this workshop based on their engagement with interview data from different perspectives, led to a series of 4 workshops from 2016 to 2018, held in Oxford, Utrecht, Arezzo and München, that were funded by CLARIN. This workshop intends to present the motivation for this DH2019-workshop by sketching the complex design and results of these series of multidisciplinary workshops. The premise was that the multimodal characterand multidisciplinary potential of interview datais rarely fully exploited, as most scholars focus on the analysis of the textual representation of the interviews. This might change by getting acquainted with scholarly approaches and conventions from other disciplines. Aim of this workshop When considering research processes that involve interview data, we observe a variety of scholarly approaches, that are typically not shared across disciplines. Scholars hold on to engrained research practices drawn from specific research paradigms and they seldom venture outside their comfort zone. The inability to reach across methods and tools arises from tight disciplinary boundaries, where terminology and literature may not overlap, or from different priorities placed upon digital skills in research. We believe that offering accessible and customized information on how to appreciate and use technology can help to bridge these gaps. This workshop aims to break down some of these barriers by offering scholars who work with interview data the opportunity to apply, experiment and exchange tools and methods that have been developed in the realm of Digital Humanities. Previous work As a multidisciplinary group of European scholars, tools and data professionals, spanning the fields of speech technology, social sciences, human computer interaction, oral history and linguistics, we are interested in strengthening the position of interview data in Digital Humanities. Since 2016 we have organized a series of workshops, supported by CLARIN on this topic. Our first concrete output was the development of the T-Chain, a tool that supports transcription and alignment of audio and text in multiple languages. Second, we developed a format for experimenting with a variety of annotation, text analysis and emotion recognition tools as they apply to interview data. The workshop The half-day workshop will provide a fruitful cross-disciplinary knowledge exchange session. It will: Use presentations and hands-on sessions to explore annotation, text analysis and emotion extraction tools with interview data; Test the T-Chain with participants own audio-clips; Attract open source Speech-to-Text software developers to expand the number of languages that could be integrated into the T-Chain. Scope and organisation of the series of workshops The organising team created a community of experts from The Netherlands, Great Britain, Italy and Germany who actively scoped and assembled invitations to scholars to the workshops. The countries were selected on the basis of the availability of mature open source speech recognition software, as the first goal was to develop a portal for automatic transcription and alignment of interview data for different languages. Scholars and archivists who participated were those who work/teach with interview data, and who were interested in the use of technology to facilitate transcription and annotation of interview data and explore cross disciplinary analysis and interpretation of data. They represented the following communities: Historians and social science users who undertake research with recorded interview data sources; Linguists who use spoken language sources; Software tools specialists who develop support data processing and analysis tools. Figure 1: schematic view of the T-Chain: processing of spoken documents from recording/digitization into transcribed digital files including metadata The first three workshops thus focused on exploring user requirements and testing the performance of various speech-to-text software on interview data that was provided by researchers and data curators. This led to the development of the Transcription Chaina portal for automatic transcription and alignment of interview data in English, Dutch, German and Italian. This tool is meant to support the first phase of the research process, the transcription of interviews, and to create an open source easy to access web resource with different choices for the format of the output, anticipating the onward import into a variety of tools. Cross disciplinary overtures in München During the fourth workshop in München the scope was broadened to the subsequent phases of the research process: the annotation and analysis of the data. The presumption was that the multimodal characterand multidisciplinary potential of interview datacould be better exploited by bringing diverse approaches together and encouraging the uptake of digital tools. Anticipating that this diversity of participants and tools would made the organisation of the workshop complex, a careful design of the workshop was key to ensuring satisfying experiences and countering disorientation. To this end the following principles were applied: gathering detailed information on the participants prior to the workshop to tailor the sessions to their level of digital savviness, collecting and preparing data that was familiar to the participants in both a common languageand in their native language, building on homework assignments to install and become familiar with a number of tools, making sure that during the workshop a participant with advanced digital skills was represented in each of the language groups, and eliciting and recording feedback on use of the tools directly after the session exercises through group interviews. The first session of the 3-day workshop was devoted to testing the first version of the T-Chain with German, Dutch, English and Italian data. In the subsequent three hands‐on sessions participants worked with proprietary and open source annotation tools, common among social scientists, with text mining tools used by computational linguists, and with emotion recognition tools used by computer scientists. Prior to starting the hands-on sessions, it was deemed necessary to introduce, using jargon-free language, the range of diverse research profiles of the disciplines represented. A parade of research trajectories Each of these disciplines uses a different approach when working with recorded interview data, and within every discipline there exist distinct sub-disciplines. Rarely is there a unanimous voice on methods, analysis and use of tools. Even speaking of linguistics is an over-simplification, just as the term oral history is a broad term for a variety of approaches to interpreting interviews on peoples life stories. For example, whereas an oral historian will typically approach a recorded interview as an intersubjective account of a past experience, another historian might consider the same source as a factual testimony. From a different perspective, a social scientist is likely to try to discover common themes and similarities and differences across a whole set of interviews. Thus, disciplinary approaches represent distinct analytical frameworks that might make use of tools in different ways. To illustrate the variety of landscapes, we invited workshop participants to consider 1-2 research trajectories that reflected their own approachto working with interview data. This enabled us to come up with a high-level simplified journey and to identify how and where the digital tools might fit into the researchers own workflow. The diversity of practices of course implies that there are many variations of this trajectory. Figure 2: High-level simplified journey of working with interview data Tools for Transcription, annotation, linguistic analysis and emotion recognition The researchers were invited to work in four language groups of 5 to 6 peoplein hands‐on sessions, using step‐by‐step worksheets and pre‐prepared interview extracts. A distinction can be made between types of tools that support the research process, in the sense that technology substitutes manual labour, and those that have actual impact on the interpretation of the data. The T-Chain, developed with CLARIN support, with its speech to text and alignment software, can partly substitute the cumbersome transcription of interviews, a practice that is common to anyone working with interviews. The need for transcription is commonly understood across disciplines. Then there are tools that aid the annotation of text and audio-visual data by offering a structured system for attributing meaning to passages. At this point the common needs tend to decrease, as the choice of a particular annotation tool leads to an engrained practice of research, that requires time to build up and cannot be easily traded for an alternative. For this purpose, the open source tool ELAN was used, and compared with the proprietary qualitative data software NVivo. In the following two sessions the experimental component increased, as the same interview data was explored with computational linguistic tools of increasing complexity. The two tools used were Voyant and NLPCore, web-based tools, that allowed the processing of transcripts on the fly, enabling a whole set of specific language features to be directly identified. The second tool, TXM, had to be installed and allowed for a more granular analysis of language features, requiring the integration of a specific language model, the splitting of speakers, the conversion of data into computer readable XML language, and the lemmatization of the data. The last session was the most daunting one, illustrating what the same data yielded when processed with the acoustic tool PRAAT, and the facial recognition tool Open Smile. User experience and evaluation A first analysis of the user experience seems to suggest that scholars are very much open to cross-fertilization, that the likelihood of a digital tool being used by a scholar significantly increases when the tool is transparent, and finally, that legal and ethical concerns are paramount in deciding whether or not to use a digital tool. First, there is great potential to cross‐fertilize approaches. Oral historians and social scientists generally enjoyed trying out each others methods, in particular analytic tools that might help complement their own content‐driven approaches to working with interview data, for instance to elucidate features of spoken language. Second, knowing what a digital actually does behind the scenes increased participants sense of the tools usability. Following that, tools must be flexible. Scholars are only willing to integrate a digital tool into their existing research practice and methodological mindset, if the tool can easily be used or even adapted to fit needs. Limited functionality of the free easy‐to‐use tools, and the observed methodological and technological complexity and jargon‐laden nature of the dedicated downloadable tools, despite the availability of clear documentation, were both seen as significant barriers to use in everyday research practice. Specifically complex in this regard was the experience with the linguistic tools, especially those that required the data to be pre-processed. These have a high learning curve. While output triggers fascination and curiosity, it appears difficult to translate insights into the structure of language in an entire corpus, or in meaningful comparisons of subsets, to ones non-computational practice of interpreting individual personal accounts. The same applies to the emotion recognition tools. The messy data that pre-processing interviews about migration yielded, led to sifting out possible hypothesis, but not to a deeper understanding of the experience of migration. The real challenge lies in being able translate insights with regard to scale, frequency and paralinguistic features into the classic interpretation of the interview data. Often this means looking at other things, for instance the amount and character of silences within an entire corpus. This may reveal a pattern that is typical for this group of respondents or for this type of spoken data in general but is not related to migration. The same question about whether a shift in focus is needed to discover new practices applies to using speech to text software and alignment of audio and text. The most salient conclusion of the exploration could be that the traditional practice of interpretation of interviews can be enriched by considering digital tools as purely heuristic instruments, not instruments that connect directly to their existing ways of interpretation of data, but that should be considered at the very beginning of the research process, when one is still considering what collections to reuse, or how the characteristics of ones own data relate to open source data. Besides raising these epistemological concerns, the workshop participants pointed to some more mundane concerns. Explanation of linguistic approaches would be better appreciated in more lay terms, following a step-by-step pathway from meta to concrete level, following the way in which the introduction of linguistic tools was prepared for the workshop. Participants, many of whom admitted to a lack of technical proficiency, felt that dejargonising the approaches, and preparing a simplified layer of information would make the tools more appealing to unfamiliar users and make getting started easier. Third, users are also concerned about where are data are going when online tools are used for processing their interview data, prompting thinking about how these tools might be better aware of legal and ethical issues, for example adding explicit GDPR‐compliant data processing agreements to allay worries. Finally, users pointed to a great need for contextual information about, and metadata for, the data collection and processing, when interview data sources are used. Inviting language resources and scholars across languages and different disciplines certainly enriched the meeting experience. "
	},
	{
		"id": 400,
		"title": "Tantrums and Traitors: a Diachronic Analysis of Emotions in Parliamentary Debates on War Criminals and Collaborators",
		"authors": [
			"van Lange, Milan Mikolaj"
		],
		"body": " In this study discussions about the punishment of collaborators and war criminals in the Netherlands are investigated by analysing the verbatim minutes of parliament. The application of text mining techniques to this digitised historical text corpus allows for a diachronic perspective on a historical case study. With this paper, I present the historical case, the materials and techniques used, and some insights based on preliminary results. I will also address general advantages and limitations of using text mining in historical research. Aim of this investigation is to explicitly investigate and discuss the validity of the use of emotion lexicons in diachronic historical research. Historical case After the Netherlands was liberated in May 1945, the issue of the punishment of collaborators and war criminals became acute. At peak, during the summer of 1945, more than hundred thousand suspected people were kept in custody within the Netherlands. For contemporary politicians, this issue was pressing and demanded quick action. Heated debates in Dutch parliament followed. This was, however, not the end of the story. For the next five decades, the question caused recurring heated political controversies. The debates in Dutch parliament about the punishment, penalty reduction, or release of these people are not only among the longest debates in Dutch parliamentary history, but are generally considered to have been the most emotionally charged. Controversies kept recurring in 1952, 1966, 1972, and 1989. The 1952 controversy started when Queen Juliana proposedto turn the death sentence of German war criminal Willy Lages into life imprisonment. This proposal caused lengthy debates in parliament and mass demonstrations in Amsterdam, attended by approximately 15.000 people. This manifestation of public anger stands in apparent contrast with the common assumption in Dutch historiography about the silence of the 1950s. This silence refers not only to talking about past war experiences, but also toexpressing related emotions. Decades later the issue proved to still have the potential to cause controversies. By 1972 only three foreign war criminals were still incarcerated in the Dutch city of Breda. Public unrest rose when the potential release of these last three German war criminals in Dutch custody was discussed in parliament. Newsreels show the manifestations of sadness and anger of the – again – more than 15.000 people that demonstrated in- and outside the parliamentary building in The Hague. This Breda Three controversy became one of the best-known emotional Second World War-related affairs in the Netherlands. The Minister of Justice at the time, Dries van Agt, called this debate the most emotional episode of his entire political career. Questions This presumed high emotional charge of well-known controversies seems to be taken for granted. This leaves historians with not only substantive but also epistemological questions. To start with the latter: how can we observe, interpret, and compare emotionality in the context of historical research? Investigating emotionality is full of complexities. First, emotions are volatile and thus difficult to grasp – especially in retrospect. Next, it is to be expected that historians reading parliamentary proceedings, trying to identify elusive emotionality, are influenced by their own preconceptions. This is especially the case in ethically charged controversies in the relative recent past. By approaching a well-researched case historically with an innovative text mining approach, I hope to be able to also investigate these personal judgments of emotion in historiography. This study aims to answer the following substantive historical questions: how emotional were these debates? Which emotions were distinctive? How does the 1952 parliamentary discussion and the uproar it caused relate to the generally assumed silence of the 1950s? How do the differentdiscussions relate to each other? How exceptional are the war criminal debates when they are compared to other contemporary issues? Methodology This investigation uses emotional word use to investigate emotions present in the discussions. I rely on generic emotion lexicons that are produced outside the field of history. They are derived from the fields of computational linguistics and psychology. The emotion lexicons used are generic in the sense that that their creation is not based on a singledataset. Using lexicons to identify, analyse, and evaluate manifestations of emotions, can be considered as a rather crude method. On the level of a single particular sentence, these lexicons may not always be reliable in determining whether a certain emotion is manifest. However, when these lexicons are applied to large corpora, they are reliable in determining whether a certain part of the corpus has moremanifestations of a certain emotion when compared to others. Taking into account the ambition to address the influence of personal judgment in this investigation, I consider it an important advantage that the lexicons are from outside the specific domain of this study, and not influenced by personal judgments of the investigatortowards the historical sources or themes under scrutiny. The use of emotion mining as a method in historical research forces researchers to formalize what they want to know, and formalize how their analytical process leads to their answers and conclusions. The historical parliamentary debates are analysed by using open-source text mining packageswithin the R-programming environment. The digitised parliamentary debates are pre-processed: they are lemmatised and stopwords and interpunctions are removed. Next, the corpora are transformed into Document Term Matrices. The lexicons are used to score the parliamentary records on manifestation of words relating to different categories of emotion. In this process, a Term Frequency – Inverse Document Frequencyweighting is assigned to each unique word in the corpus. The DTMs are used to measure how distinctive words from an emotion categoryare for a specific document relative to all other documents. The output is then turned into boxplots and distribution graphs that are used for statistical evaluation and comparison of the results. This output generates insights in which different emotional words were distinctive, and in which proportions they were manifest in the parliamentary discussions. Outputs are compared over time to come up with a long-term perspective on historical developments in discussing collaborators and war criminals in Dutch parliament. These quantitative results are also confronted with more traditional historical analysis of both primary and secondary sources. "
	},
	{
		"id": 401,
		"title": "Workshop: From Manuscript to Text Analytics",
		"authors": [
			"van Lit, Cornelis",
			"van Peursen, Wido",
			"Popović, Mladen",
			"Van Hecke, Pierre",
			"Roorda, Dirk",
			"Vlaardingerbroek, Hannes",
			"Dhali, Maruf",
			"Coeckelbergs, Mathias"
		],
		"body": " In one day, we take participants through the entire workflow from having real manuscripts in your hands to performing complicated database computations on the texts these manuscripts contain. Examples are drawn from ancient Oriental manuscript cultures, because the specific complexities in these resources highlight the strength of applying new computer technologies. The focus of the workshop is on the underlying questions applying to the use of digital techniques in the study of material culture, languages, texts and literature. Therefore, the workshop is aimed at anyone dealing with manuscripts and texts, from Akkadian cuneiform economic texts to manuscripts of Classical Greek and Latin authors, and from Hebrew and Aramaic Dead Sea Scrolls to Middle Dutch devotional literature. The workshop will provide a birds eye view on the entire workflow that starts from the concrete physical carriers of text, through methods of character recognition, grammatical parsing, syntactic annotation, up to advanced methods of text-analytics e.g. topic modelling, linked data, and stylometry. The workshop is divided into four stages, separated by breaks. Because of the hands-on component of the workshop we can accommodate up to 30 participants. We will only require standard technical supportand participants need to bring their own laptops with them. Instructions for the hands-on parts will be given at the beginning of the workshop. Approximate Schedule: 09.00-09.30 Introduction, aim and agenda of the workshop Stage 1: From physical manuscript to digital manuscript 09.30.10.00 Explanation on the variety of digitization technologies 10.00-10.30 Practicum on evaluating digitized manuscripts 10.30-11.00 Break Stage 2: From digital manuscript to text extraction 11.00-11.30 Explanation on possibilities for Hand Writing Recognition 11.30-12.00 Explanation on pattern recognition and deep learning 12.00.12.30 Practicum on pattern recognition 12.30-13.30 Lunch Stage 3: From text extraction to database 13.30-14.00 Explanation on preparing texts in a uniform manner 14.00-14.30 Practicum on preparing texts according to a schema 14.30-14.45 Break Stage 4: From database to text analysis 14.45-15.15 Explanation on text analysis useful for texts from manuscripts 15.15-15.45 Practicum on applying automated text analysis 15.45-16.00 Conclusion; sharing of contact information and planning for future events "
	},
	{
		"id": 402,
		"title": "Is There Anything It Is Like To Be a Text?",
		"authors": [
			"Van Zundert, Joris J.",
			"Neill, Iian D.",
			"Andrews, Tara L.",
			"Andreas, Kuczera"
		],
		"body": " This paper reflects on the problem of the ontological status of text in the digital environment. Referring to Briet, Van Zundert and Andrewsdrew on Briets fluid concept of documentation and pointed to a similar fluidity in the concept of text. Here we draw a parallel to debates in the field of philosophy of consciousness to examine how this may help us to understand the specific textuality of digital text. In his 2003 article, P.M.S. Hacker confronted Thomas Nagels reasoning in What Is It Like to Be a Bat?. Nagel had argued that consciousness has a unique individual subjective quality that defies reduction to the materiality of body and mind. Hackers refutation of Nagels reasoning is almost exclusively based on the observation that Nagels argumentation in key places is syntactically faulty. For Hacker—a Wittgensteinian philosopher of language—erroneous syntax implies faulty logic and thus incorrect argument. His counter-argument implicitly reduces consciousness to that which can be understood and explained linguistically. But this move ignores thereby the evidence and arguments put forward in fields beyond linguistics and philosophy that speak against the idea of consciousness as a solely linguistic construct: empirical evidence from the neurosciences, theories on consciousness that relate consciousness to mind and body such as came forward from integrated information theory, and ideas surrounding embodied cognition. Sources from many disciplines thus seem to suggest we cannot understand consciousness solely as a linguistic construct, or one that must be explained exclusively by linguistic means. A concrete understanding of consciousness is likely to integrate types of knowledge and experiences that are distinctly non-linguistic and hard to properly linguistically express. Arguably then, mind and consciousness are better understood as emergent properties of a body orchestrating multiple types of sensations and information. Explaining consciousness through linguistic properties alone would be akin to trying to explain plastic arts by words solely. As early 20th century artist Vernon Blakeput it in his phenomenological treatise on art: By means of words one cannot hope to attain to precise transmission of plastic thoughts. The plastic arts exist because they are the natural and only way to transmit that species of thought from one human being to another. They necessarily deal in thought factors which are inexpressible in words. We observe a parallel in digital textual scholarship, which is the fields preoccupation with the digital mimetic reproduction of printed text. This is the prevalent paradigm, and most digital scholarly editions that have been produced follow it meticulously. This mimetic philosophy has historically been expounded through the socio-technical system known as TEI-XML and its supporting community. The vocabulary and syntax of the TEI is geared exclusively towards the structural, documentary, and content description of non-digital texts. Its philosophy and technology confine text to an existence as a book or as a digital reproduction thereof; itsvocabulary and syntax treat textuality only by means of the non-digital idiom of print, manuscript, score, script et cetera. The application of this idiom consequently leads to the reification of text as that which was contained in a document. Just as it is an illusion to think that consciousness can be explained entirely by means of linguistics, so it is an illusion to think that text and a full understanding of it can be achieved by digital means that reduce the text to a print paradigm. Barring some eccentric exceptions there are, to our knowledge, no scholarly editors that hold that only what is on the page is the text, that the text exists solely as a semiotic representation. It is in reading that the text becomes to exist. Umberto Ecoregarded a written text as a series of instructions for the reader to create meaning. The meaning of a text is a co-creation that emerges from the interaction between the text and the situated knowledge and embodied cognition of the reader. The uniqueness of this interaction—that stems from two uniquely situated sites of knowledge, namely the text and the reader, mingling—gives rise to the indeterminacy Jerome McGanncalls the textual condition. Like consciousness cannot be reduced to linguistics, textuality cannot be reduced to documentary description. We may never be able to experience the uniqueness of another readers mind. Consequently we may never be able to fully explain what text is. But the salient point is that we can apply digital technology to understand text beyond its being as a sign on the page. We contend that digital technologies canbe used to explore the properties of these different beings of text. Analogous to what Alan Kaysaid about the computer revolution, we may perhaps say that the digital text has not happened yet. The medium which differentiates the digital from linear, non-interactive text is still in the process of being imagined. So we textual scholars must become more imaginative, and ask ourselves: is there anything it is like to be a text? For us this question serves to displace an understanding of digital text as an inanimate series of electronic signs with an understanding of text as a more dynamic and interactive agent. Being a text in a digital environment involves formal complexities and computational procedural elements that are alien to print or manuscript text. Furthermore, being digital allows a text to transgress the boundaries of the document and to connect to other texts and information in ways that reify connections usually only present in human cognition. Lastly there is an element of code to the existence of text in the digital environment. Software code is texts animated computational twin. Text will often interact with code, to the point of symbiosis. Our use cases demonstrate these properties of digital text. Our first use case focuses on Codex, a system for combining text annotation with graph database networks. A standoff property text editor is a central tool allowing users to create multiple overlapping annotations. The graph network can be constructed out of entities derived from within texts as well as managed independently of the texts themselves. The aspiration of Codex is not so much the reproduction of a manuscript within a document, but the emergence of a multi-cellular network of texts linked by the connective tissue of the graph database. The goal of Codex is to enable scholars to create a kind of meta-text formed from out of the relations existing within texts across the corpus. Our second use case focuses on the creation of a digital critical edition as a computational model of the text and its witnesses, where a documentary model of edition was considered insufficient to represent not only the individual texts, but the relationship between the text witnesses themselves as well as to the information the text carries. The process of transcription, transformation to valid TEI-XML, collation of the chosen layers of witness text, and analysis of the variation is done with available tools, such as tpen2teiand CollateX, that are as general-purpose as possible. Even so, the editors had to write custom code plugins in order to facilitate this process, for example to specify a normalization string for collation, to handle the expansion of abbreviations, or to convert certain elements of TEI markup to HTML display. We consider that this custom software code, insofar as it carries an editors interpretation of the text and the significance of its parts, is itself part of the edition that is produced, and thus part of the edited text. Our third use case considers the object-oriented modeling of a text using a general-purpose computer language. The objective in this case is to demonstrate that adequate transcription as the scholarly community has performed it up to now does not necessarily encompass adequate re-mediation of that text. Computer languages facilitate the meticulous modeling not only of the linguistic layer of a text, but also of many dimensions of the text beyond that. We may actually program a digital scholarly edition so that, when the code is run, the result is the scholarly edition. But we may also model the objects, events, and relations that exist in the narrative world of the text, in which case running the code becomes a re-enactment of the texts narrative. In this way digital technology calls into question the limits of transcription and how digital transcription may add to understanding in novel ways. These use cases show that to imagine what it is like to be a digital text invites us to reconceive of text in a specifically digital fashion that points to affordances we fail to see and use were we to regard digital text as a mere mimesis of print and manuscript texts. This, we contend, adds to our understanding of what being a text is, and ultimately to understanding what text is. "
	},
	{
		"id": 403,
		"title": "Intellectual Networks and Cultural Networks: Kinomatics and the complex cultural geometry of cinema",
		"authors": [
			"Verhoeven, Deb",
			"Moore, Paul S.",
			"Zemaityte, Vejune",
			"Loist, Skadi",
			"Samoilova, Evgenia"
		],
		"body": " Introduction This panel considers how Cinema Studies and Digital Humanities can inform each other through examining the different ways that film distribution analysis extends aspects of critical infrastructure studies. Every discipline has metaphorical geography that delineates the shape and course of its intellectual itineraries. Cinema Studies is no exception. In reflecting on the ways in which our scholarly explorations might be experienced as expressions of, and interactions with, surfaces, spaces, situations, and speeds, we might ask, what are the particular perspectives and projections that delineate our own paths to knowledge. And, more importantly, how might they be reshaped or redirected through their engagement with new objects of study, new questions, new forms of evidence, and innovative disciplinary tools. The metaphors used to make sense of the industrial infrastructure of global cinema and that describe the movement of films through segmented times and spaces already suggest an approach that is premised on the recognition of a connection between Cinema Studies and Digital Humanities. Both film distribution circuits and festival networks gesture to a foundational electrical infrastructure that in turn expresses the way power is mobilised and distributed through a cultural infrastructure such as the cinema. Cinema Studies has been a mainstay of humanities and media studies programs for decades and yet was slow to take up the methodological innovation of the Digital Humanities. This changed when attention in the discipline turned to new methods of analysis based on the observation that the cinema was primarily a social experience rather than a textual projection. The advent of New Cinema History heralded the development of innovative online archives. More recently, digitisation of printed and audio-visual assets has expanded the findability and accessibility of historical resources for cinema researchers. At the same time, crowdsourced, openly accessible databases like IMDb have opened vast information pools for contextualizing analysis. Whilst the proliferation of these digital case studies has recently produced a great deal of methodological innovation in Cinema Studies, this disjointed approach was most often achieved through urban and regional case studies, which has also resulted in a significant deficit in our understanding of the global nature of the cinema despite technological reproducibility allowing films to circulate in principle to any audience without respect to locality. Most existing distinct computational platforms are not yet capable of addressing the global, elastic, and networked nature of the contemporary international film industry currently producing and exploiting huge quantities and varieties of data. As disciplinary Cinema Studies has contemplated its own diffusion, scholarly interest in the distribution of its object of study, films, has also risen. Influential in the circulatory turnin Cinema Studies is the Kinomatics project, an international, interdisciplinary study of the cultural geometry of cinema. The study of film distribution has sat in the background of cinema studies, a cultural and economic engine humming along functionally, connecting filmmakers and their audiences. Rather than viewing film distribution as a transparent mediation, Kinomatics invites consideration of the social and relational aspects of distribution that draws on studies of critical infrastructure in Digital Humanities: How can distribution, as a process of connection, support complex non-binary understandings of infrastructure? How can we mobilise a greater attentiveness to uneven acts of cultural distribution to give insight into their redistribution? In reflecting on the ways in which our own scholarly explorations might be experienced as expressions of, and interactions with, surfaces, spaces, situations, and speeds, what are the particular perspectives and projections that delineate our own paths to knowledge? And how might they be reshaped? As a globally-distributed network of scholars whose shared problematic is the global distribution of cinema, we are less direct collaborators than a community of affiliated scholars learning from each other through articulated problem-solving. We work variously on local and international cases, with a distinct interest in the scaled relation between regional and the global but are committed to regular networking sessions and events that reinforce our shared concerns while supporting our individuated endeavours. The panel reflects the scaled approach we take to our collaboration. It will take the form of a combination of the panel- and multiple-paper session. Within each of its three themes, the panellists responsible for a particular subtopic will open a discussion with short statements of up to 10 minutes, followed by responses from other panel members of up to 5 minutes in total, followed by an invitation to the audience to continue the discussion jointly with us. What is proposed, therefore, is not a series of 4 papers, but a set of interweaving engagements with each other and our audience that will make clear the connections between the 3 component topics outlined below. This panel will profile new cases and methods for histories of film distribution, to bring the networked aspect of the circulation of cinema culture to the foreground. Together, the authors aim to supplement existing approaches to film distribution by including elements of Cinematic Cartographyand Digital Humanities, alongside careful archival work. We aim to expand the purview of what has been called New Cinema Historiesof exhibition and reception, to focus on the circulation and movements of cinema practice and culture—not just its local forms—from early showmen through the studio period, to the present of global digital distribution. New techniques for data visualisation, newly opened archival records, new case studies, and new methods from Digital Humanities all unite on this panel to spotlight the potential for research on distribution to complement the corpus of work on film exhibition and reception. This panel builds a critical theory of film distribution by also examining the various modes and models of academic and disciplinary circulation. The work of Digital Humanities scholars focussed on critical infrastructure studiesoffers a useful touchpoint for extending theories of cinema infrastructure. Scale: Critical infrastructure studies and networked cinema studiesPaper 1 will serve as an introduction to this scaled approach in Cinema Studies enabled by computational analysis, an approach the Kinomatics team has been honing for more than 10 years. This paper will look at the mutual value of cinema distribution studiesand Digital Humanities. By addressing scale as both an object of studyand an approach to analysis itself, this paper proposes to set the scene for the following papers. Event: Circuits of cinema and ephemeral archivesThe public for a film is not merely the aggregation of its audiences, nor is the space marked by a films circulation the area mapped by its aggregated screenings. For Kinomatics, however, these essential concepts are achieved through imputation across films empirical paths of distribution. A particular screening exists always in networked relation to prior instances and is publicized with anticipation, advertised in advance. Connectivity and mobility constitute novel genres of modernism, and the culture of cities and entertainment can be taken as central to media and modernity. Social network analysis charts the relations between communicating actors, and new work data-mining historical newspapers has revealed the structure of recirculation, in figures like the networked author. A cinema circuit is somewhat distinct in reconstituting its network continually, rearranging shows in new venues on a daily basis, to produce such entities over time as territories, seasonsand scenes. Tracking and mapping the circuit or territory of an entire season of film screenings is an important means of studying historical audiences and cultural spaces, but the complexities of networked case studies must confront particular challenges. Recent born-digital forms of film promotion allow data science and quantitative approaches to the global circulation of contemporary cinema; two Kinomatics case studies in this realm follow, described below. For the first century of cinema, however, data about historical film screeningsare typically embedded in highly ephemeral documents such as newspapers, collections of handbills and flyers, or ledgers of corporate records. Digitalised archives of film listings and advertising are captured by the passing of print into searchable databases in historical newspapers and other ephemeral archives. Data-mining for circuits of cinema thus shares the same concerns as any Digital Humanities study reliant on historical ephemera: OCR reliability, archival accessibility and copyright, and imputing across samples of relatively insignificant and partial instances. The data of publicity for film screenings, however, is doubly embedded in advertising: a designed aesthetic object within these print ephemera, which follows norms but hardly ascribes to standardized form, rarely printed as a well-structured directory, not often easily transformed into a database. How, then, to reduce the complexities of amusement advertising to the norms of event listings, toward digital mapping of historical cultural circuits? Moore will spotlight historical case studies on early 20th-century travelling circuits of cinema in North Americaand the model of Verhoevens TUGG database of The Ultimate Gig Guide compiled for rock and pop music in late 20th-century Australia. Markets, territories, and seasons can be discerned as temporal, spatial, and programming patterns when data from entertainment advertising and event listings is aggregated, with potential to link to demographic data to understand long-dispersed, historical audiences and publics. Interpretation: Global data and analytics at scaleTwo case studies both address the global flow of contemporary cinema from different perspectives:The first case study employs big data to investigate the global nature of theatrical film distribution. Despite some unfounded beliefs that the importance of cinema is declining in the digital age, the theatrical sector remains a global billion-dollar business that continues to grow. However, the digital revolution has changed the composition of the global cinema landscape with the yearly increases in international revenues more than compensating for the shrinking prominence of the US/Canadian market. Nonetheless, the global aspect of the contemporary movie exchange, especially with regards to the distribution of non-Hollywood films and low earning titles, remains empirically understudied. This case study showcases the results of the first big data examination of global cinema that analyses the theatrical distribution of diverse new release movies across 40 countries to build an understanding of the current international industry trends. This study uses unique film-related data to be visually analysed, interpreted, and displayed in a way not previously explored: the Kinomatics showtime database. Its inclusive nature allows us to capture a rich sample of all movies screened at the cinema regardless of their origin and box office. We select feature films that received an international commercial release in 2013 and track their global theatrical runs through mid-2015. The sample contains 3,424 movies from 124 production origins, amounting to a total of 130,455,277 showtimes. Data from IMDb and Rentrak databases complement the sample with information on the release date, country of origin, and box office. This study brings together data visualisation and computational techniques to analyse the global diffusion of films in both temporal and spatial terms in order to gain an understanding of the global trends. Employing a data-driven quantitative approach and harnessing the capabilities of big data, it considers international cinema distribution at three distinct scales: globally, nationally, and internationally. The study also examines rarely used measures of distribution to unpack the complex nature of global movie exchange, including the volume of screenings, the number of visited countries, the length of the theatrical run, and international release delay. We find that by moving beyond the localised case studies and engaging with multivariate cinema data from different viewpoints, it is possible to produce new insights into the geographic and temporal patterns of global distribution and relationships present in the data.The second case study will focus on a very different kind of film circulation. While cinema is seen to hinge on commercial distribution, niche cinema and much of global art cinema are distributed on global screens via the film festival circuit. Since the establishment of the first festivals in the 1930s, the sector has proliferated and diversified into more than 6,000 festivals taking place in every region, size and thematic specialization. The festival circuit runs are significant not only because they present an understudied distribution window, but also because there is a strong assumption that for some films they are the only place for global circulation and exhibition. The Toronto International Film Festival reports that ca. 20% of their films screen only at festivals, i.e., do not have any other type of distribution deals. This case study presents results of the project Film Circulation in the International Film Festival Network and the Impact on Global Film Culture which is the first empirical study concerned with complex temporal and spatial patterns of film movement within festival circuitsand the first to employ Digital Humanities methods on the subject. The project collected data from the complete programs of six major film festivals within one festival season of 2013. Since the average length of a festival run is approximately three years, this ensured good coverage of completed festival runs for the sampled films. The chosen festivals constitute three major international A-list festivalsas well as three leading specialized festivals, the short film festivals in Clermont-Ferrand, and the LGBTQ* festival Frameline). The resulting sample includes 1,727 films of all genres from 102 countries produced between 1990 to 2014. The paper also examines if films had any distribution other than festivals by linking our sample to the commercial showtime data of the Kinomatics dataset. This project charts new territory not only by considering different kinds of distribution but also by collecting not readily available data from a variety of sources in pursuit of answering new questions which arise from the study of the complex networked structures of the festival circuits. In order to achieve this, we collected data from festival catalogues and used web-scraping and text mining tools to analyse data from the IMDb film database. Using multiple data sources makes it possible to go beyond binary categories and provide a more complete picture of the festival run patterns. Since IMDb data have been criticized for their bias, we will also talk about our approaches to addressing data quality problems. Conclusion This panel places film distribution historically on top of infrastructures of transportation, communication, and civic institutions; and conducts methods for researching film distribution on top of emerging geographic and cultural-economic approaches to the Digital Humanities and archival work. How might the opportunities presented by an unprecedented proliferation of networked data, for example, also challenge the unspoken assumptions and ordinary practices of conventional film studies research? And how might the computational turn in Cinema Studies present opportunitiesat the intersection of qualitative historiographiesand quantitative research approaches such as empirical analysis and digital visualisations? Altogether, juxtaposing cases and methods, our panel proposes a reflective study of film distribution as networked relations, so that seemingly trivial bits of public knowledge collectively gain significance, and case studies become part of the complex scaled inter-relations. The results show how films find their audience within local and regional routines but are also embedded in transnational cultural networks. "
	},
	{
		"id": 404,
		"title": "Overcoming the Challenges of Optical Music Recognition of Early Music with Machine Learning",
		"authors": [
			"Vigliensoni, Gabriel",
			", ",
			"Calvo-Zaragoza, Jorge",
			", ",
			"Baxter, Noah",
			", "
		],
		"body": " Abstract Several centuries of manuscript music sit on the shelves of libraries, churches, and museums around the globe. On-line digitization programs are opening these collections to a global audience, but digital images are only the beginning of true accessibility since the musical content of these images cannot be searched by computers. In the SIMSSAprojectwe aim at teaching computers to read music and assemble the data on a single website. However, the automatic retrieval and encoding of music from score images has many complexities. In this paper, we describe our current workflow to perform end-to-end optical music recognitionof early music sources. 1 Introduction The process of reading, extracting, and encoding the content from digitized images of music documents is called optical music recognition. Despite more than 50 years of research, OMR is still a complex problem. Some characteristics of music notation—such as the graphical properties of musical objects and the layout and superimposition of musical features—make OMR a remarkably difficult problem. The slow development in OMR, particularly when dealing with early music, also lies in the variability of documents. Since most OMR systems have been developed using heuristic approaches, they usually do not generalize well to documents of a different type. 2 End-to-end OMR workflow for medieval and renaissance music For high scalability, we are taking a machine learning-based approach to OMR. The computer is trained with a large number of examples for each category of musical element to be classified and creates a model. Once a model is created, it is used to classify new examples that the computer has not yet seen. We have implemented this approach in a workflow that performs OMR in medieval and renaissance music scores images. The workflow is divided into four stages: document analysis, symbol classification, music reconstruction and encoding, and symbolic score generation and correction. The entire workflow is depicted in Figure 1. Figure 1. End-to-end optical music recognition workflow for early music. Boxes indicate the software applications on each step. Human symbols indicate interactive, adaptive stages. 2.1 Document analysis Digitized music scores are the input to the system and document analysis is applied to segment the music document into layers. We developed Pixel.js, an open source, web-based, pixel-level classification application to label pixels into their corresponding category or to correct the output of other image segmentation processes. We use this tool interactively with a convolutional neural networkto segment the document into a number of user-defined layers. After a few iterations of training and classification for optimizing the classifier, we obtain a number of image files corresponding to the segmented layers of the original score. For example, these layers may contain notes, staff lines, lyrics, annotations, or ornamental letters. The recognition of the music symbols and the analysis of their relationship is achieved once the symbols are isolated and classified in the found layers. 2.2 Symbol classification The application we developed for the symbol classification stage is called Interactive Classifier. IC is a web-based version of the Gamera classifier. We use it to automatically group the connected components of a specific layer into glyphs. Then, we manually label a series of these musical glyphs into classes. For neume music notation we implement neume-based and neume component-based classification. In either case, IC will extract a set of features for describing each of the neume or neume component classes and will model a classifier. With this model, new glyphs will be classified based on k-nearest neighbours. Once the symbols of the score are classified, we proceed to add their musical context and encode them into a symbolic music format. 2.3 Music reconstruction and encoding We obtain the pitches of neumes or neume components by finding their absolute position in the corresponding staves and use the recognized clef of each system to assign a relative pitch. The output of IC conveys the position and size of each musical element in the original image, and so we add this information to the estimated pitch as well as the staff number to which each neume belongs. Finally, this musical information is encoded into the Music Encoding Initiativemachine-readable symbolic music format. 2.4 Symbolic score generation and correction The last two stages of our OMR workflow, music reconstruction and encoding and symbolic score generation have a common interactive breakpoint for visualizing and correcting the output of the automatized OMR process. This human-driven checkpoint is embedded as a web-based interface called Neon. Neon overlays the original music score image and the rendered version of the output of the OMR process. By visual inspection, the user can assess the differences between the layers, and manually add, edit, or delete music symbols in the browser. So far, however, corrections entered by the user are not fed back into the learning system, but they change the encoded music file output. 2.5 Workflow management system All the constituent parts of our OMR workflow are handled by Rodan, a distributed, collaborative, and networked adaptive workflow management system that allows specifying interactive and non-interactive tasks. 3 Future work In future iterations of the project we will focus on:implementing a non-heuristic, machine learning-based approach for pitch finding);appending neumes to syllables; anddevising a way of feeding back into the workflow the corrected output in Neon. We hope that this infrastructure, in combination with the proper teaching strategies and tactics developed by human teachers in the interfaces for training OMR systems, will enable the end-to-end recognition and encoding of music from music score images. "
	},
	{
		"id": 405,
		"title": "Mapping Fascist Repression, Following The Italian Resistance",
		"authors": [
			"Vitali, Giovanni Pietro"
		],
		"body": " This paper is the first official report on a Marie Curie project entitled Last Letters from the World Wars: Forming Italian Language, Identity and Memory in Texts of Conflict, which started in September 2018. This project deals with a linguistic and thematic analysis of the last letters of people sentenced to death during the First and the Second World Wars, conducted with digital humanities tools. In this very first part of the project, I am preparing the lexicon analysis that will be the focus of my methodology. I am also creating a geographical representation of the corpus because this project is intrinsically geographical in its approach. Indeed, mapping appears necessary considering the linguistic particularities of the Italian territory, which is characterised by a linguistic diversity on the local level. In this part of my research activities, I am developing some tools that are specific to the analysis of the Second World War. Notably, I have already collected and georeferenced four datasetsregarding the Italian Resistance against fascism and the Nazi occupation. I have reworked or created data on this topic thanks to my collaboration with the staff of several Italian institutions and archives: DS_1 - The network of all the massacres committed by Nazis and Fascists since the beginning of the Italian dictatorship. This data was originally collected in the framework of a previous project, The Atlas of the massacres of Nazis and Fascists, by the Central Institute of the Resistance in Milan. The original project led to the creation of a basic database, managed by a SQL system that communicates with Googles API. I added dates and a new style to this project. DS_2 - The political and racial arrests and deportations committed by Nazis and Fascists against civilians and the partisan army. This data is contained in the database of the Italian Documentation Centre of Jewish Culture, with which I have started a collaboration in order to create a mapping of their very rich material. DS_3 - The entire corpus of letters written by partisans, generally Italians, before their execution. This data is directly connected with the project Last Letters. The letters are georeferenced and analysed so that an historical and linguistic analysis could be applied. DS_4 - The movements of the Italian fighters who participated in the Spanish Civil War beside the Spanish Republic against Franco. This data is collected by the Ferruccio Parri Institute, which asked me to exploit it digitally. These four datasets present a huge amount of datathat perfectly display the repression acted by Nazis and Fascists on members of the Resistance and populations, but also the activities of Italian partisans in reaction to this oppression. Indeed, the letters were usually written in the areas where Fascists had crushed their opponents through military attacks. This research has already appeared that a digital mapping approach is essential in dealing with this historical data. This method has never been applied before, and can highlight new connections and differences between Nazi and Fascist abuses. This map, which I have already built, https://goo.gl/BZSo3L, shows how a digital visualisation of the massacres can help to understand in what parts of the country repression was led by Germans or Italians. These peculiarities highlight the dualism of the Italian Resistance, which can be considered both as a phenomenon of resistance and a civil war. This is a very debated problem in historiography, which is linked with ideological positions. Indeed, admitting that Italy was involved in a civil war means that partisans were soldiers of a regular army, as opposed to a group of patriots who fought against foreign invasion. If partisans were construed as a regular army, their war against the Italian Social Republic could be considered a war against other Italians and not against Fascism, which was defeated on 8 September 1943 with the armistice signed between the Italian army and the Allies. The data I have collected so far will be presented using a combination of digital tools such as Gephi, Carto and Google Earth. I am particularly interested in the construction of networks in order to understand what categories of people were mainly repressed. I have georeferenced the data with R and Openrefine and, after correcting coordinates by checking on the maps the potential errors every ten records, I have built datasets divided into several categories regarding politics, social background and gender. Through the use of network analysis combined with spatial analysis, I would like to shed light on the differences between Nazi and Fascist repression, insofar as different categories of people were targeted by one or the other. The idea is to compose the very first atlas of Fascist and Nazi repression in Italy using only open source tools. The mapping is thought and coded mainly with the use of R, without the use of Shiny, but also in JavaScript for some special layers, especially the chronological ones. The meaning of this work is to shed a new light on this period considering that, even in the archives, the actions of Nazis and Fascists were often covered for political reasons after the war. For instance, at the end of the Second World War, politicians probably understood that it was impossible to punish everyone for their crimes and therefore, they often had files disappear from the dossiers. The case of the cupboard of shame is a very famous example of these tendencies: in 1994, a wooden cabinet was discovered inside a large storage room in the Palazzo Cesi-Gaddi, in Rome which, at the time, housed the chancellery of the military attorneys office. The cabinet contained an archive of 695 files documenting war crimes perpetrated on Italian soil under Fascist rule and during Nazi occupation after the armistice on September 8, 1943, between Italy and the Allied armed forces. The cupboards doors were locked and facing the wall when they were discovered. As this case shows, Italy still has trouble dealing with its past. The Fascist regime represents an uneasy heritage for todays political parties and citizens. Mapping the Fascist repression of that time is a way to display the historical responsibilities of Fascism through the violence expressed against racial and political categories, and the use of a digital map enables us to visualize the phenomenon under a new light. Connecting those historical events with the geographical places where they happened facilitates the emergence of patterns in the repression of populations in the course of the most tragic event of twentieth-century Italian history. "
	},
	{
		"id": 406,
		"title": "Toward a Theory of Editorialization",
		"authors": [
			"Vitali-Rosati, Marcello"
		],
		"body": " The notion of editorialization has a fundamental place within the francophone scientific community as a key-concept for understanding and interpreting the digital culture. The concept has been at the center of theoretical works of the academic community for the last ten years. It has been recently the subject of a book in English. In my communication I will present the result of ten years of work on this concept and explain how the theory of editorialization can help DH scholars to think about the architectural space of the digital environment, and how it impacts knowledge production, circulation and legitimation. More specifically, I will argue that this theory can be a very powerful theoretical framework to take into account the political implications of our practices as dh scholars. What is Editorialization? The word editorialization, in the sense that it is used here, is a neologism in English. It comes from the French éditorialisation. If, in English the word is a derivative of editorialize, which means – according to most dictionaries – to express an opinion in the form of an editorial or to introduce opinion into the reporting of facts, in French it has acquired a broader meaning and is related in particular to digital culture and to digital forms of producing knowledge. This shift in meaning, from an idea that denotes the expression of opinion to one that suggests the production of knowledge in the digital age, is actually quite useful; as we will see, the French version of the term retains its association with the notion of opinion in that it refers to the production of content that expresses a kind of opinion or that offers a better way to see or interpret the world. In 2008 Gérard Wormserused the term editorialization in a broad sense, to describe any digital editorial activity and to signify how knowledge is produced in the digital age in general. If the digital is not only about tools but in fact refers to a whole cultural environment, then editorialization – the way of producing contents in digital environments – must have a cultural dimension as well. In other words, the difference between publishing and editorialization is not only a difference of tools, but rather signifies a broader cultural difference: editorialization is not the way we produce knowledge using digital tools; it is the way we produce knowledge in the age of the digital, or, better, in digital society. In this sense, the term editorialization expresses an idea quite close to notions like knowledge designor information architecture. According to a restrictive definition, editorialization is a set of technical devices, structures, and practicesthat produces, organizes, and enables the circulation of content on the web. In other words, editorialization is the process of producing and diffusing content in a digital environment. We could say that, in this sense, editorialization is what publishing becomes under the influence of digital technologies. Clearly, this has an impact on the content itself: the concept of editorialization tries to stress how technology shapes content. The obvious limitation of this first definition is that it considers the digital environment as a discrete, separated space. In this sense it is a web-centered definition that does not take into consideration the fluidity that exists between digital and pre-digital space. Further analysis shows that we can take all the acts of structuring content online – on the web or on other forms of the connected environment, like mobile apps – and consider these acts in their function of shaping our whole reality. In this sense, we can define editorialization as a set of individual and collective actions that take place in a digital online environment and that aim to structure the way we understand, organize, and judge the world. These actions are shaped by the digital environment in which they take place, and so editorialization, just as the first definition makes clear, is not only about what people do but also how their actions are shaped and oriented by a particular environment. But the emphasis needs to be put not only on how we produce content, but also on the fact that these contents are actually the world in which we live. It is important to stress that, if we consider the word digital in a cultural sense, digital space is our primary space, the space in which we live. With this in mind, we can make a distinction between various digital environments – for instance, the web and other forms of connected environment – and digital space as a hybridization of these environments with the totality of our world. These considerations allow us to arrive at a final definition: Editorialization is the set of dynamics that produce and structure digital space. These dynamics can be understood as the interactions of individual and collective actions within a particular digital environment. The object of editorialization is not content, but the world itself: we editorialize things, or, better, we editorialize the space in which we live. We could say that the encyclopedic project as it was conceived during the 18th century by Diderot and DAlambert was realized with the world wide web: the totality of our knowledge has been organized and linked in a unique and huge architectural framework. But it goes further: with editorialization we are not only structuring the knowledge, we are structuring the world itself. We can say that editorialization is not only an architecture of knowledgebut more precisely an architecture of being. This is why the concept of space becomes so crucial: editorialization is a way of organizing space not in a metaphorical sense: editorialization is an actual architectural action, it organizes our actual space. DH scholars as architects Now, the fact that editorialization is a way of producing the space we inhabit raises an important problem: who are the actors of editorialization? Who is producing this hybrid space? This problem is the main political issue when we analyse our digital culture; addressing it is the first step towards developing what Geert Lovink calls Net criticism. As Morozof points out, digital space seems to be completely driven by a small number of private companies. Morozovs analysis of the role of Silicon Valley corporations starts from the idea that what should be public on the web is actually private. The domains of public interest are owned by big companies, and the services that should be granted by public authorities are instead organized and provided by private corporations. The problem is thus twofold: private corporations own most of digital space and they impose their ideologies on the whole social space, leaving no room for a public sphere. Private corporations decide what the structure of digital space is and what values it holds. But according to the theory of editorialization, what we do as digital humanists is also a way of producing and organizing the space. In this sense, as digital humanists, we play the role of architects of our actual world. Our ways of organizing and structuring contents and information should be interpreted as an architectural gesture. Editorialization is the action of organizing the world, and this is why our academic practices have a huge political implication. Producing tools, platforms, data visualizations, and proposing different epistemological models to produce, organize and disseminate knowledge means to allow the existence of a plurality of spaces and to avoid the risk of a monolithic space produced and managed by a small number of companies who share the same values and the same visions of the world. As DH scholars we should be the producers of a public sphere - heterogeneous and plural- a space where commonsare possible and where democracy can take place. The theory of editorialization can be a useful theoretical framework to drive our DH practices and projects, because it allows us to understand that dealing with digital objectsmeans also being an architect who organizes and structure a living space. My presentation will be structured as follows: A history of the term editorialization. Different definitions of the concept. Editorialization as a way of producing space. DH scholars as architects. Some examples of DH projects as forms of editorialization "
	},
	{
		"id": 407,
		"title": "Determining And Visualizing Genesis: A Digital Edition of Goethe’s Faust",
		"authors": [
			"Vitt, Thorsten",
			"Brüning, Gerrit"
		],
		"body": " Since October 2018, version 1.0rc of a digital genetic edition of Goethes Faust is publicly accessible online. The poster visualizes and explains the editions most specific features and discusses recent research about modelling macrogenesis as a graph. The archive is the editions heart piece with detailed multi-view representations of about 550 manuscripts spanning a period from the early 1770s until 1832. Additionally, it contains printed editions and testimonies about the genesis of Faust. For each of the manuscripts there are the following views, some of which are combined on screen: visualization of the quire structureand manuscript descriptionplus, see fig. 2: high-resolution digital facsimiles diplomatic transcription overlay separate diplomatic rendering of inscriptional phenomena textual rendering for a more interpretive account of alterations 2 H, collation visualisationwith preview imagesand textual manuscript description; extract. 2 H, verse line 11579f. Views: digital facsimile – text image overlay – separate diplomatic transcript – diplomatic rendering with tooltip information about inscriptional phenomena – textual rendering with tooltip information about alterations Additionally, each manuscripts final textual stage is rendered in way that closely corresponds to the rendering of the critically established text of the work. All but the diplomatic views are also available for the printed editions. The editions genesis section comes with navigable graphic visualizations of how witnesses relate to each other. Bar diagramsindicate the textual content of each witness by length of and gaps within bars. Their order is based on a global chronological ordering expressed in a directed acyclic graph. Bar diagram for a scene of Faust II. Witnesses are chronologically ordered. Ochre colored bars do not contain elaborate versions, but are semantically connectable to passages of the final text, the canonic numbering of serves as x-axis. The editions text was critically established on the basis of the complete textual transmission. By clicking on a line of text all other versions appear synoptically, showing its development within and across all other witnesses in graph based chronological order. All sigils are links that lead to the corresponding passage of the respective witness in the archive. In addition to the synopsis, there are critical apparatus entries that report about editorial decisions. Synoptical view of the development of V. 11580. Line 11578 has additional apparatus entry The views are generated from TEI-encoded source data, the poster will illustrate the process. The chronologicalorder of witnesses is far from trivial: Only a few witnesses can be precisely dated. There are many relative chronologies that only span a few witnesses each. In an experimental part of the edition, all relevant chronological information has been extracted from scholarly literatureand recorded in a simple data model that basically recognizes relative and absolute datings, together with the bibliographic source and optional comments. This information has been modeled as a directed graph, as illustrated in fig. 5: E.g., the path 1825-05-25 → 2 III H.5 → 1825-04-06 means that 2 III H.5 has been written not before February 26th and not after April 4th, 1825. From this graph, additional information can be inferred, e.g., limits for the absolute dating of 2 III H.8. Subgraph around 2 III H.8 Contradictions in model or sources lead to cycles in the graph. In the recorded information, there is one strongly connected componentcomposed of 477 witnesses and 2136 assertions, too many to check manually. Identifying and removing all contradictions with the smallest number of interventions means solving the NP-completeminimum feedback arc set problem, for which we can calculate an exact solution using a method by Baharev et al.. The procedure can be manually influenced by placing weights on sources or individual edges or by marking obviously wrong edges as to ignore first. All three dashed-red edges must be removed to make this component acyclic From the cycle-free results, a topological ordering of the witnesses can be createdthat is then used everywhere the edition requires a chronological ordering. Subgraph visualisations created using NetworkXand GraphVizhelp to explain the ordering. "
	},
	{
		"id": 408,
		"title": "Implementing the Assertive Edition for Historians – Some samples",
		"authors": [
			"Vogeler, Georg"
		],
		"body": " The assertive edition Historians have since long considered historical documents as an information carrier. Editing documents for historical research thus often meant to create modernized or abridged texts and offered a wide range of tools to access the content of the texts. In the digital world this approach has ended in a type of digital scholarly edition which is still lacking an accepted term: drawing from the Semantic Web it could be named semantic edition, in the context of fact-checking it might be termed factual edition, or putting it into logical reasing the term could be assertive edition. Scholarly editions in this field range from early adopters of OWL like the Henry III Fine Rolls, extensive RDF representations of indices like in sandrart.netor burkhard-source http://burckhardtsource.org/ , or infrastructures based on the integration of text and database as in Symogih. http://symogih.org/ Projects like Old-Bailey-Onlinehttps://www.oldbaileyonline.org/ conceptualised the information part as relational databases, recent activities in the context of the Historical Atlas of Poland project created a resource labelled edition as database of annotations to images of the original documentshttps://atlas.ihpan.edu.pl/gis/agad_wschowa_i2_pub/index.php . The main change in the recent years in these digital editions is the extension of the concept of database representation from annotations similar to printed indicesto representations of full information conveyed. In particular, the simple representation of information in RDF statements is attractive and supports this effort. RDF additionally has the advantage to be an accepted data exchange standard, which is flexible and expressive enough to cover a wide range of projects. On the other hand, the TEI has been extended to include formal descriptions of persons, places, names and basic knowledge information organisation. Classic relational database systems are powerful tools to create datasets representing complex knowledge, and the recent discussion on graph technologies is moving the discussion about the best suited technical solutions forward. The paper will give an insight in a group of projects trying to integrate the wide range of technologies around. The DH repository and publication platform GAMSin Graz holds several projects which can be considered semantic/factual/assertive editions: the scholarly editions of the annual accounts of the City of Basel, the Urfehdebuch Basel, or the Cantus-Network. Others are in preparation. The tasks to be solved with these resources are multiple: Which assertion made by the texts should be represented? How good the XML structure helps to extract assertions? How to link between data representation, text, and image? How to organise an efficient workflow? Which content should be represented? All projects come from specific domains: accounting, criminal history, history of liturgy, taxation, political institutions, and dietary history. This leads to the development of individual ontologies. In the case of accounts the projects could benefit from an international activity in the development of an ontology for historical accounting. The criminal records used a local ontology which should be harmonised in the wider range of criminal history research. The Cantus Network reused identifiers from liturgical studyhttp://cantus.uwaterloo.ca/ and started to work on an ontology for liturgical feasts based on the reference work by Hermann Grotefend. Cooking receipts can reuse food ontologies for modern purposes or generic data from wikidata. For the imperial diet the historians in the project suggested to work on an ontology representing forms of communication as they are considered core in the interpretation of the political event. The experiences with these ontologies demonstrate that a core functionality in the implementation of assertive editions it he possibility to model information which goes beyond the flat text, i.e. information that can hardly be extracted with standard information extraction methods. How to link between data representation, text, and image? The GAMS system integrates TEI transcriptions with a IIIF image server. This makes it easy to link between text and images. The relationship between assertions and text is handled by a local ontology considering the data entries as a kind of factoid, which carries a link to the source. In practice the textual fragments supporting the formal assertions carry identifiers by which they can be stored together with the data with a local <g2o:textualContent> property. http://gams.uni-graz.at/o:gams-ontology/#textualContent This is an approach similar to Web Annotation so it seems suitable to reuse the W3C Web Annotation Vocabulary. https://www.w3.org/TR/annotation-model/ This would even allow the full range of relationships between the main representations of the text. How to organise an efficient workflow? Centring the technical solution on well-established standards helps to create a framework of different tools. While Oxygen-XML is a main tool for XML encoding and plug-ins like Ediarum www.bbaw.de/en/telota/software/ediarum help in transcription and basic markup, tools like Transkribus https://transkribus.eu/ are used in the Cooking Receipt project to create topographically precise transcriptions. In the Cantus project a workflow based on the conversion of word files to TEI was chosen. The TEI offers with the @ana attribute a powerful method to extract assertions with XSLT. For the criminal records, the accounts and the hearth tax documents, customized TEI are used, which combine the text structurewith the interpretative level. XSLT-Transformations during ingest in the GAMS convert extracts the RDF representation of the text. The DEPCHA project demonstrates that this workflow can be used as a hub between various input forms. Conclusion The experiences in the projects show that DH provides methods for historians to create rich editions including formal representations of the information conveyed. The GAMS infrastructure does this by combining a set of tools around standards like IIIF, TEI and RDF. Still, the standards for data exchange in many humanities domains do not exist. Future work will have to focus on creating reused domain ontologies, which the data-for-history initiative is promoting. http://dataforhistory.org/ It has to be evaluated, if W3C Web annotation can become the major link between the various representations of texts in digital scholarly editions. All of this can form the assertive edition many historians dream of. "
	},
	{
		"id": 409,
		"title": "IncipitSearch: a guide to collaboration",
		"authors": [
			"Neovesky, Anna",
			"von Vlahovits, Frederic"
		],
		"body": " A centralized access to sources, editions, and further kinds of publications facilitates the research process and provides a comprehensive overview of existing information. To connect musicological collections and repositories, we created a metasearch for annotated music: IncipitSearch.It is a tool and a service specifically tailored for research on music incipits, the initial sequences of notes that characterize a work. IncipitSearch is a service to interconnect musical pieces via metadata. It is also a tool that can be reintegrated into existing digital research platforms. By connecting some of the largest digital collections of music metadata it already offers access to around 1 million incipits. In four comprehensible steps, this poster will be a guide explaining how data owners can add their data to IncipitSearch and how the reimplementation of the search functionality can be carried out. What kind of digital collections can be integrated? Firstly, the poster focuses on the process of implementing data and elaborates what preconditions must be taken care of to get the musical sources ready for IncipitSearch. To these key questions answers will be given: What kind of music can be implemented? What kind of digital collections can be integrated? How do they have to be licensed? What exactly is the IncipitSearch definition of an incipit? How must collections be prepared if they are not already encoded? IncipitSearch uses a specific metadata format that aims at making the content highly semantic and interoperable. Our approach uses RDFa and schema.orgto add fundamental metadata to existing markup in a semantically structured way. Plaine & Easienotation is used to encode the music incipits. The format is also used to semantically enrich the output format for the aggregated data. An easy understanding and usage of the format is necessary to create a low-threshold approach to inspire the integration of further resources to the IncipitSearch platform. In order to achieve that we needed a format that could serve two use cases: 1) Augmenting already existing collections with incipit information. 2) Annotating completely new content not available online yet. Data owners will ideally provide their content in JSON-LD serialization. The poster will demonstrate how the metadata format is structured and explain each step in producing the metadata file. How must collections be prepared if they are already encoded? At the moment, IncipitSearch aggregates the incipit data of the catalogue of Glucks complete works, the SBN OPAC, the RISM OPAC and includes a sample data set of the thematic Breitkopf Catalogo delle Sinfonie 1762. The underlying data of these repositories is quite diverse. Therefore different crawlers have been programmed to transform and aggregate such already existing data. The poster will offer help for projects that already came to an end or dont have resources for their own data engineering. The importance of update routines will also be touched. How to implement incipit search functionality in other repositories? Fourthly, the guide will demonstrate how easy it is to implement an incipit search functionality in individual projects such as work catalogues or library catalogues by making use of the IncipitSearch API. An example of this is the Incipitsuche of the Gluck-Gesamtausgabes digital work catalogue. By making the API basically an elasticsearch endpoint it is naturally very well documented. The poster will present some fundamental parameters of an IncipitSearch query and give insights into the making of the Gluck reintegration. The basic structure of an IncipitSearch service script will be provided as well. From its very beginning the main idea of IncipitSearch was bringing very different types of musicological resources together and to make them accessible in an open source environment. This proposal emphasizes openness, transparency and collaboration as central aims of IncipitSearch by delivering a manual for involvement. "
	},
	{
		"id": 410,
		"title": "Towards Multilingualism In Digital Humanities: Achievements, Failures And Good Practices In DH Projects With Non-latin Scripts",
		"authors": [
			"Lee, Martin",
			"Wagner, Cosima"
		],
		"body": " Exposé The one day workshop responds to the call for multilingualism and multiculturalism in the Digital Humanitiesand discusses achievements, failures and good practices in DH projects with non-latin scripts. We want to provide hands-on insight into Dos and Donts in NLS context and identify possible transferable practices to other languages and disciplines in the sessions, building upon lessons learned in the workshop NLS in multilingualenvironments 1 held in 2018 at Freie Universität Berlin. The main goal was and is to strengthen an international network of NLS practitioners and experts who develop, maintain and distribute specific NLS knowledge, regardless of their working affiliation in academia, libraries, museums, or elsewhere. While Digital Humanities scholarship is a vibrant and growing field of research in many humanities and social sciences disciplines, it has also been criticized as being culturally and technologically biased. As a result, there is a lack of DH infrastructure suited for processing non-latin scripts. This is not only a cultural problem of the representation of DH research from non-anglophone countries but also for area studies disciplines within the so called Western academic world. In their call for papers to the DH Asia conference 2018 at Stanford University the organisers stated that when we look at DH in Western Europe and the Americas, we find a vibrant intellectual environment in which even college and university undergraduates – let alone more advanced researchers – can download off-the-shelf analytical platforms and data corpora, and venture into new and cutting-edge research questions; while, in the context of Asian Studies, we find an environment in which many of the most basic elements of DH research remain underdeveloped or non-existent. This is not only true for Asian Studies but also for academic disciplines like Egyptology, Arabic Studies, Jewish Studies and other disciplines conducting DH research in or with non-latin scripts. While there have been recent activities to strengthen the collaboration and networking within the NLS-DH community 2 there is still a strong need for knowledge exchange on NLS suited DH tools, best practices and networking events. For example, how can we raise the awareness for NLS specific aspects in dominant standardization committees like the Unicode Consortiumor innational authority files? How can we establishstandards for multilingual metadata in NLS and how rigid or how flexible do they have to be? How can the recognition rate of non-standard characters with OCR be improved? How can multilingual/multiscript data from different sources be integrated and processedin collaborative research platforms? Furthermore, in line with the discourse on the digital transformation of academic research and teaching, the need for stronger collaboration rises. Especially in the case of externally funded research projects, specific knowledge on DH and NLS can seldom be held within the organisation and projects tools and platforms often cannot be maintained longer than the duration of the project. Instead, these responsibilities should be part of the service portfolio of research infrastructure institutions like libraries or data centers. However, these institutions are normally not equipped to support all languages and disciplines. The workshop tackles these issues by presentations with challenges, answers and recommendations of 15 experts conducting DH projects with Arabic, Chinese, Japanese, Koreanscript sources and ancient Egyptian hieratic signs, by providing time for group discussions and a wrap-up session for securing modes of documentation, future collaboration of NLS DH tools best practice and their transmission to research infrastructure institutions. The presentations will address the following subjects and methods of NLS DH: digital representation of manuscripts: OCRdigital representation of NLSdigital research infrastructures and virtual research environments for NLS DH projectsmultilingual metadata and metadata in NLSsemantic web and linkeddata in NLStext encoding and mark-up languages and NLSdata mining / text mining in NLSNER, machine translation, annotation for NLSWorkshop Format The first part of the workshop will give developers and researchers the chance to present their challenges, solutions and tools for NLS DH related problems and questions. In the second part of the workshop, we will develop an organisational strategy for cooperation and collaboration among the NLS DH community through a working group session. To aid organisation, we will provide a Wiki that participants can use during and after the workshop to organise collaboration. We envision the results of this workshop to be a cornerstone for building an institutionalized network of NLS DH practitioners with a joint knowledge management systemand communication channels. Furthermore, we have initiated a NLS DH handbook and want to develop this into a living handbook to be maintained by the aforementioned NLS DH network. Finally, contributions to the workshop are planned to be published in a special issue on NLS and DH in an open access journal. List of presentations and presenters: For updated information on the workshop and a list of presenters with abstracts of their presentations please refer to the following Link . Arabic script: Arabic Script in Digital Humanities Research Software Engineering Presenters: Oliver Pohl, Jonas Müller-LaackmannTowards a Versatile Open-Source Ecosystem for Computational Arabic Literary Studies Presenters: Mahmoud Kozae and Dr. Jan Jacob van GinkelAncient Egyptian hieratic script: „Ancient Egyptian Hieratic Script – Aspects of Digital Paleography for a NLS Presenters: Svenja Gülden, Susanne Gerhards, Tobias Konrad; Akademie der Wissenschaften und der Literatur / Johannes Gutenberg University Mainz; Project Altägyptische KursivschriftenCJK scripts: Chinese SHINE: A Novel API Standard & Data Model to Facilitate the Granular Representation and Cross-referencing of Multi-lingual Textual Resources Presenters: Pascal Belouin, Sean WangNo text - no mining. And what about dirty OCR? Training, optimizing, and testing of OCR/KWS-Methods for Chinese Scripts Presenter: Amir MoghaddassMultilingual research projects: Challengesfor making use of standards, authority files, and character recognition Presenter: Matthias ArnoldJapanese Expectations and reality: developing an English-Japanese semantic web environment for the Late Hokusai research project Presenter: Stephanie SantschiKorean Curation Technologies for a Cultural Heritage Archive. Analysing and transforming the „Project Tongilbu data set into an interactive curation workbench Presenter: Peter BourgonjeCreating, Linking, Visualizing and Interpreting Chinese and Korean datasets with MARKUS Environment Presenters: Jing Hu, Leiden University, The Netherlands; Ba-ro Kim, Chung-Ang University, South Korea Notes [1] For a workshop report in English see https://blogs.fu-berlin.de/bibliotheken/2019/01/18/workshop-nls2018/; the German version was published at DHd Blog. [2] e.g. annual DH Asia conferences at Stanford University, see http://dhasia.org/; a Summer School in June 2019 on right2left issues at the Digital Humanities Summer Institute, see http://www.dhsi.org/events.php; a Workshop on Nicht-lateinische Schriften in multilingualen Umgebungen: Forschungsdaten und Digital Humanities in den Regionalstudienin June 2018 at Freie Universität Berlin/Campus Library. "
	},
	{
		"id": 411,
		"title": "SemAntic: A Semantic Image Annotation Tool For The Humanities",
		"authors": [
			"Wagner, Simon",
			"Christoforaki, Maria",
			"Donig, Simon",
			"Handschuh, Siegfried"
		],
		"body": " Annotation, in computer science, is the act of creating associations between distinct pieces of information. The annotated material can be multimodalwhile annotations can be informal, formal, and semantic,. The concept of annotation, both as a process and an outcome, has long been debated in the Humanities. For a historical viewpoint and an attempt to derive key features for a digital system design, see. In this paper we present SemAntic, a web-based application for semantically annotating images. It accepts a variety of formats, enables the user to mark parts of the image using circular, rectangular and polygonal regions and to associate them with user loaded RDF ontology classes, and finally, export the resulting annotations in JSON according to the Web Annotation Data Model, a W3C Recommendation. SemAntic was developed in the context of Neoclassica, where the automatic image classification componentrequired an image corpus annotated according to the specifically developed Neoclassica domain ontology. The review of the available image annotation tools revealed a lack of up-to-date applications meeting our needs. Requirements like built-in support for ontologies, polygonal annotations, and export format were not met by most, while the most sophisticated required a steep learning curve and/or or a login at the developers servers, which we could not use since some of our images do not have a permissive license, i.e., they are either proprietary, or forbid redistribution. The architecture of SemAntic comprises of a server component written in Java and a web front-end, based on the popular Bootstrapand VueJSframeworks. The component used for drawing the actual annotations makes heavy use of the Fabric.jsHTML5 canvas library. Data persistence for storing the ontology and annotation data is realized through a MongoDB database. The current high-level server architecture is illustrated in Figure 1. There is an ongoing effort to migrate the existing codebase over to the Java Spring frameworkto reduce complexity. Figure 1.SemAntic high-level server architecture Figure 2. illustrates the webinterface. In addition to basic CRUDannotation features, the frontend also supports undo and redo. It furthermore allows users to browse ontologies in a tree view supporting class labels in multiple languages, as well as,a search function covering class names, labels in all languages used, and class definitions. Figure 2. SemAntic web interface Additionally, SemAntic provides an import function for annotations created using other software where the already assigned labels are mapped to the ontology classes presently loaded in the tool. In case that is not possible,the Levenshtein distance, is used to determine which ontology class is more likely to be an appropriate match ., while at the same time we are improving the stability of the backend and the ergonomics of the user interface. SemAntic has not yet been given to external users for evaluation as it has primarily been developed to service the need of the Neoclassica project. As of now, it was mainly used by domain experts. However, it is designed as a generic tool that can admit any kind of ontology. We plan to conduct an evaluation in the near future. As soon as the evaluation stage is finished, we intend to release SemAntic under an open source license. "
	},
	{
		"id": 412,
		"title": "Complexity And Uncertainty In DH Projects: A Co-design Approach Around Data Visualization",
		"authors": [
			"Wandl-Vogt, Eveline",
			"Senabre Hidalgo, Enric",
			"Theron, Roberto"
		],
		"body": " Introduction In the recent years, with the pervasiveness of computers and a great variety of electronic devices connected to the Internet, Digital Humanitiesas a research field has experienced a great transformation that has permitted the completion of very ambitious projects with large impact in the society beyond the academia. This has resulted in a major economic impact in the cultural and creative industry. A number of new and powerful ICT have made possible the exploitation of a wealth of datathat have, on the other hand, changed enormously the practice in DH, and exposed novel challenges that must be faced in order to complete any of the said projects. From the creation to the consumption of digital resources, there are new stakeholders, contexts and tasks to consider. The amount of digital resources produced, stored, explored, and analysed in any DH project is immensely vast, so the traditional humanities tools have to be either substituted or aided with ancillary tools in the form of interactive visualisations or novel user interfaces. Furthermore, during the whole lifecycle of any DH project -from the data preparation to the actual analysis or exploration phase-, many decisions have to be made in order to yield the desired results that depend on the uncertainty pertaining to both the datasets and the models behind them. The PROVIDEDH projectaims to provide visual interactive tools that convey the degree of uncertainty of the datasets and computational models used behind, designed to progressively adapt the visualizations to incorporate the new, more complete or more accurate data. The project not only takes into account scholars, since it is most relevant in DH the fact that the role of citizens has changed enormously. We live in a society that has democratized science, and the number of projects in which the contribution of citizens, either producing or using digital resources, has exploded. The experience gained in other areas of science in which the intervention of computing has been much deeper and constant can be analysed and adapted to the case of humanities. Specially, regarding infrastructures, frameworks, models and tools that can be standardized for the different disciplines in the humanities. Taking into account these key issues of complexity in data visualization for DH, the workshop will develop through knowledge design methodsand Open Innovationa hands-on sequence around specific software needs and features. More specifically, via participatory designwith participants as domain experts of their own needs and experience. Design thinking, and co-design as its more participative dimension, represents a set of practical approaches for the creative definition and solving of problems, as well as for generating different types and forms of design knowledge. It offers a great variety of visual methods, procedures and techniques for designing new projects in complex and uncertain circumstances, as well as the simultaneous exploration of scenarios, user-centered and participatory approaches and the integration of many possible points of view. Workshop phases Each stage of the workshop focuses on specific ways to generate and discuss visual information, in accordance with participatory design practices. The different co-design and discussion phases will be based on research toolkit materials adapted expressly for the session and the DH2019 conference theme. Phase 1 - DIY accreditation and general introduction: We begin by offering participants a series of ways to think about research perspectives and backgrounds in DH, customizing a session badge to identify approaches from a set of investigator roles and profiles. This way, we facilitate personal introductions and sharing of motivations for the session, followed by an introduction to the PROVIDEDH project goals, around data visualization and decision making in DH, as well as the rationale of the workshop phases. Phase 2 - Presentation and discussion about exemplary visualizations in DH projects: Eliciting from participants experience and awareness about relevant DH projects, via rounds of short presentations this phase will focus on relevant aspects related to complexity in DH datasets and data visualization, following a convergence sequence. That is, in order to generate a discussion about visualization and complexities in DH in a participatory wayand a later coming-together to select optionsthrough idea-sharing and decision taking mechanisms. Phase 3 - Comparison of complexity-related issues and matrix of uncertainty: Based on results from the previous phase, at this stage participants will discuss guided by a visual canvasthe different criteria for assessing complexity and uncertainty in DH projects in relation to data sharing and visualization. The technique will allow for discussions oriented to software features and also set a shared understanding for the next iterations. Phase 4 - Identification of user personas for prototype evaluation: Adapting the User Experiencetechnique of personas from the field of user-centred systems design, this part of the workshop will allow participants to generate and discuss different perspectives around potential end-users of the PROVIDEDH platform, as well as for other technological systems related to DH visualizations. Phase 5 - PROVIDEDH prototype and DH-related user stories: Based on a selection of personas generated in the previous phase, this final part of the workshop will be twofold. On the one hand, it will allow participants to provide and discuss feedback on data visualizations features of the PROVIDEDH prototype. On the other hand, we will formulate in a series of new potential user storiesin relation to DH complexity challenges around data visualization. "
	},
	{
		"id": 413,
		"title": "\"A Project Review Under The Focus Of Complexities On The Example Of ExploreAT!\"",
		"authors": [
			"Dorn, Amelie",
			"Wandl-Vogt, Eveline",
			"Palfinger, Thomas",
			"Theron, Roberto",
			"Way, Andy",
			"Abgaz, Yalemisew",
			"Benito, Alejandro",
			"Losada, Antonio"
		],
		"body": " In recent years cross-disciplinary, cross-organisational and cross-sectoral research has been widely practised, linking across digital tools, research fields and actor groups. As our world is becoming increasingly more complex, this phenomenon is also particularly visible in academic fields, where complexities have arisen at several levels. In the field of Digital Humanitiesthe benefit of bringing together different disciplines and research areas is the opportunity to work with complex data structures, thus providing a springboard for further scientific work. In this context, digital humanists not only have on an important role in the Humanities themselves, but also in relation to society. The processing of complex data provides increased opportunities to access Humanities knowledge across actor groups, and also enables more interested citizens to engage with it. As a result, DH have the opportunity to prove their importance to society, but at the same time participation of further actors groups yet again increases the complexity regarding the interaction with the society. In this paper, we provide insights into the Digital Humanities project exploreAT! - exploring austrias culture through the language glass, against the complexities it has addressed and for which we introduce our solutionsby drawing on cross-disciplinary efforts in an international framework. The project has addressed complexities on several levels and from several perspectives. To exemplify our approach, we here report on three complex aspects that were tackled: the focal topic food, the interaction with society within a open innovation framework and the use of existing and new technologies. These three aspects are met within a cultural analysis framework, in line with one of the definitions of culture in the literature. As a general background, the project has been based on a digitised German non-standard language collection from the time of the former Austro-Hungarian monarchy, counting around 3.5 million digitised entries. The project has aimed to rethink dialect lexicography, by connecting a language resource to real-world objects for cultural exploitation, bring it into European networks and infrastructures and open it for sustainable use and re-use for various actor groups such as scholars and citizens. Through the setup of complex data models, the linking to and enrichment with other datasets and frameworkswas enabled and visualisation of this multi-modal content and underlying complex system has been achieved by different prototypes. Through participatory methods, e.g. design thinking and agile research, community groups have been connected to these scientific efforts by looking into links to cultural and societal challenges. In addition, it has also given rise to exploration space , an experimental Open Innovation Research Infrastructurefor the Humanities, which has been listed as a best practice example by the Austrian Federal Ministry of Science, Research and Economy, and is also as an example in the citizen science policy report. Citizen science has provided the project the possibility to engage non-experts in the scientific process, to raise e.g. their understanding of scientific work, but also to raise the understanding of social needs of the involved scientists. With the widespread availability and use of the internet and especially smartphones, the past decade has seen a remarkable increase in successful web-based citizen science projectsand showed the potential of these approaches. While these have highlighted the advantages of citizen involvement in science, they have also triggered debate. Legal and ethical questions have been raised regarding data security, the risk of harming participating citizensor using them only as a free labour for scientific research projects. Our project aimed to tackle these issues by implementing citizen science within an Open Innovation framework, giving participating citizens the possibility to engage with the project from the beginning and to deliver their inputs and ideas with low barriers. Learning to understand each other is a complex undertaking that requires time and, above all, physical space. For this reason, the exploration space was established, which in addition to the digital aspects of the project also provided physical space for interaction with citizens. In this room, among other things, workshops were held in which where a crucial part to interact with interested citizens. This physical space offered the opportunity to interact directly, to jointly answer sensitive and difficult questions and to develop concrete ideas for solutions. In this context, citizens and researchers engaged through participatory formats such as co-creation and co-design workshops in different scenarios around the cultural topic of food, which provides a wide field with numerous perspectives of engagement across actor groups and also with different community groups. With food being treated as either a service, a product or an emotion, the complexity of the subject offered also several possibilities for new cultural research questions with insights for lexicographic analysis. By applying different technological tools and methods, combining state-of-the artand novel technologies, the processing of these complex data was enabled to access, structure and analyse the derived cultural content. Finally, we aim to share our learnings from this project with communities and future projects with similar interests and challenges. On the one hand, we share our processes and learnings in face-to-face workshops, but also the applied methodologies are made available online and can thus be readily applied to datasets or projects who wish to follow a similar approach. In our endeavours we follow the FAIR data principles and aim to make our data and insights better findable, useable and reusable for the benefit of scholars and actor groups beyond. "
	},
	{
		"id": 414,
		"title": "RISE and SHINE: A Modular and Decentralized Approach for Interoperability between Textual Collections and Digital Research Tools",
		"authors": [
			"Wang, Sean",
			"Belouin, Pascal",
			"Ho, Hou Ieong",
			"Chen, Shih-Pei"
		],
		"body": " Digital humanitiesas a field has been grappling with the significant issue of interoperability. In response, many have proposed that DH needs basic infrastructures behind research projects to ensure its long-term success. In Europe, for instance, CLARIN and DARIAH are two such large-scale research infrastructures for humanities. While they have done a tremendous job in centralizing available digital resources, much of their infrastructures remain at the administrative level, and their generic coverage across the entire humanities meant that their utility for specificdisciplines and non-European languages is limited. Furthermore, while their focus on open-access resources should be lauded, many textual resources—especially in the Asian context—remain licensed and protected. How can we, as scholars in DH and Asian studies, design a digital research infrastructure fit for our specific needs, taking past experiences with these large-scale infrastructural projects into consideration? In this paper, we present our technical answers to this question. RISE stands for Research Infrastructure for the Study of Eurasia. Formerly known as Asia Network, it is a pioneering approach for resource dissemination and emerging data analyticsin the humanities, developed by the Max Planck Institute for the History of Science. We developed RISE and SHINE to facilitate the secure linkage between third-party research tools to various third-party textual collections. Our goal is to revolutionize how scholars can work with textual sources via existing research tools that have already emerged within DH. Although many research tools for analyzing and visualizing texts are already available, it is usually difficult for humanities scholars to download texts from various databases and to prepare them in formats that individual research tools require before analyses. By designing a set of standardized APIs to link texts to digital research tools, we allow scholars to apply digital research tools on texts, regardless of their locations or formats. When it comes to licensed texts, a common situation for resources in many Asian languages, it is impossible for scholars to use digital research tools to analyze them without illegally downloading or scraping the full texts. By securely linking these licensed texts to digital research tools, we allow scholars to work in a legal manner and ensuring commercial publishers the safety of their collections under a secured virtual research environment. Such flexible, networked approach to e-infrastructure development avoids re-creating silos of resources in the digital realm and allow scholars to fully leverage the potential of material digitization and digital research tools. To do so, we at MPIWG developed a range of tools: SHINE is a set of standardized APIs for exchanging textual resources, both open-access ones and protectedones that require authentication and authorization. Inspired by the success of IIIF, we aim to develop a similar standard for a wide range of communities working with textual resources. Resource providers and research tool developers will find SHINE useful because it supports interoperability among resource repositories and research tools in a decentralized manner. SHINE adopts a generic data model based on metadata from resource providers lists of collections and texts. Given a resource ID, SHINE supports API calls to obtain metadata and full-texts in a hierarchical organization. Researchers will find SHINE useful because they will gain unprecedented access to textual resources in a machine-readable format, so that textual resources can be analyzed in research tools in a seamless and legal research workflow. Here you can find a full list of API endpoints currently implemented by SHINE. RISE is a middleware that protects resource exchanges via SHINE. It authenticates and authorizes these exchanges, especially for protectedresources. It connects with authentication servers from research organizations to authenticate their users and to authorize the access right for users to access certain resources based on their organizational affiliation. It also includes a browser-based administrative interface for resource providers, research tool developers, and research organizations to regulate protocols for specific resource exchanges. It is worth noting that SHINE, as an exchange format, could be adopted independently to facilitate interoperability without RISE. RISEs architecture overview To demonstrate the benefits of adopting RISE and/or SHINE and to encourage third-party development of SHINE-compatible technical solutions, a suite of open-access software modules linked to RISE have already been developed and will be made available for others to freely adopt and adapt for their own purposes. They are available on our GitHub. RISE-RP is a reference implementation for resource providers to publish their resources via SHINE. By installing RISE-RP, a resource providercan easily publish its texts via the SHINE API protocol for other scholars to use via SHINE-compatible research tools. RISE-Catalog is a browser-based user interface that research organizations could customize and implement to facilitate resource discovery and resource-tool linkage for their internal researchers. For example, a research library can customize RISE-Catalog so that it only shows the texts collections that the library has access to. A library user could then browse it and open the selected texts in specific research tools. RISE-JS-Library is a JavaScript module for research tool developers to enable their research tools resource consumption via SHINE. After adopting RISE-JS-Library, a research tool can immediately provide to its users browse-and-search function for all resources in RISE-Catalog within its own user interface. At the moment, RISE-Catalog has used SHINE to link to several collections of varying licenses, with resources in Chinese, Arabic, Latin, Greek, and English. Open-access collections include Kanseki Repository, Corpus DB, Oxford Text Archive, Perseus Digital Library, and other miscellaneous repositories. Licensed collections include the Chinese Buddhist Electronic Texts, Chinese Text Project, and the Taiwan History Digital Library. It also provides the option for users to directly open selected texts in MARKUS, a popular research tool for semi-automatic tagging historical Chinese texts. A user in MARKUS could also browse-and-search the RISE-Catalog directly, as MARKUS had implemented RISE-JS-Library in a beta version. We are eager for connect to more resources and research tools in order to make RISE and SHINE more useful. Implementation of RISE-JS-Library within digital research tools Despite its current name, this suite of products can handle resources in all languages and can be customized to fit existing IT and content management systems. Many projects and infrastructures have proposed similar ideas, creating complex new initiatives that largely centralize resources in digital silos. Yet centralization does not necessarily address interoperability or the challenges with licensed texts. Ours, by comparison, is a modular solution that works, allowing for interoperability between collections and research tools without centralizing resources. Scholarly research and structural design therefore remain intimately connected. This demonstrates the significant returns from our early investment into DH research. While we promote open access whenever possible, the reality is that many digitized resources in the humanities are still sold by publishers or private vendors. We have had to navigate this complex licensing terrain during our everyday work, and RISE and SHINE is now a prototype primed for a model of decentralized e-infrastructure for humanists. We believe that this approach is a fruitful one that would provide a sustainable and interoperable research environment for humanist scholars. "
	},
	{
		"id": 415,
		"title": "A Graph Database of Scholastic Relationships in the Babylonian Talmud",
		"authors": [
			"Waxman, Joshua"
		],
		"body": " Introduction With the rise of the Internet, it has been pointed out that the Babylonian Talmud, with layers of texts commenting and referring to other texts, was an ancient hyper-textNow, with the rise of social media, we can start to think of the Talmud as an ancient social network. True, Abaye and Rava did not use Facebook or Twitter. Rava cites Rav Nachman rather than retweeting him or sharing his status update. Although Abaye didnt friend Rava, they studied under a shared teacher, Rabba, both headed the academy at Pumpedita, and they frequently take opposing positions and argue with one another. We set out to create a social network graph of the Babylonian Talmud. Using Named Entity Recognition of a statement-aligned Aramaic / English bitext, we found nodesand edges. We induced formal scholastic relationships from these interactions, and propagated biographical information across the graph. We highlighted rabbis in the Talmudic text, using color to indicate scholastic generation, and displayed relevant subgraphs to show how the participants on a folio relate to one another both locally and globally. This biographical and scholastic information can aid a student of Talmud in understanding the dynamics of the discourse and why specific rabbis say what they say. For instance, the modern Revadim approach to Talmudstudy makes great use of which generation a Tanna or Amora lived, which academy he belonged to, and so on. Related Work The study of social network graphs of scholastic relationships has occurred in other, non-Talmudic, contexts. See for example Kofia et al.. For the Babylonian Talmud, many printed works and a few existing digital resources can prove useful to identifying rabbis and their interactions. Satlowcreated a spreadsheet with approximately 5000 rabbinic names listed by Aaron Hyman in Toldot Tanaʾim ṿe-Amoraʾim, as an intended first step to performing social network analysis of rabbinic literature. Parkerassembled a database of approximately 250 Tannaim and Amoraim, their scholastic generations, and teacher / student relationships between them. Sefaria is an open and free online library of rabbinic literature. Among the items in their database is a digital version of the Koren Noé Talmud, with the Aramaic and the English translation aligned statement by statement or paragraph by paragraph, and with linked commentary. Our Approach We performed Named Entity Recognition on the statement-aligned bitext corpus to mark up the scholars and their interactions, in both the Hebrew and English text. Such NER is challenging on purely Hebrew text, but we exploited the aligned text and a transliteration model to attain fairly accurate results. We used customized fuzzy string matching to match these names to those in the Parker database, when the names are spelled differently or have a patronymic. The Talmud consists of 5,894 folio pages, of which we have processed 4965, to recognize approximately 145,000 named entities and 12,000 interactions. Testing on a small tractate consisting of 59 folio pages, for just the English named entities, we achieve 81% precision, 94% recall, and an F1 score of 87%. For pairs ofin the literal text, performance is slightly lower. We built two graphs. The first was based on Parkers data. The second graph was populated from the NER- extracted names and interactions. Rabbis are represented as nodes, and weighted edges exist for each type of interaction. Where possible, we transfered generational information from the Parker data, which sometimes required disambiguation based on context. For instance, Rabbi Eleazar would refer to Rabbi Eleazar ben Shamua in a Mishna or brayta but usually Rabbi Eleazar ben Pedat, an Amora, otherwise. Because many rabbis are not encoded in Parkers database, our graph includes nodes not marked for scholastic generation. We are exploring a few approaches to bootstrap off our initial generational knowledge, using our detected relationships. As an example, if known scholar A, of generation 3, cites unknown scholar B, then we can assume that B is probably of generation 1 or 2, that is, a near but previous generation. If known scholar A speaks to unknown scholar B, and B replies, they are probably within a generation of one another. By considering these constraints and iterating, we can propogate generational knowledge across the graph. We have begun to summarize interactions as scholastic relationships. Other identifiable relationships are primary teacher, colleague, and frequent disputant. Results and Conclusions The current release version of the system is available at www.mivami.org. Here, we include a few figures to illustrate the system output, drawn from Menachot 2b. Figure 1 is a single Talmudic statement. The literal translation is bolded while the gloss text is not. Rabba is colored red and Abaye green, matching their Amoraic generation as given in a legend. Rabbi Shimon is colored as a 5th generation Tanna. Hover text presents patronymic and generation. Figure 2 is the teacher/student relationship graph of that statement, again color coded. Circles with outlines are rabbis who appear on the page. Circles without outlines are shared teachers. Thus, we see that Abaye and Rava are contemporariesand were both students of Rav Yosef and Rav Nahman. Figure 3 displays local interactions; the bidirectional arrow shows Rabba and Abaye speaking to one another. Finally, Figure 4 shows the rabbis local to the page, as they interact across the entire Talmud. Rava both speaks to and cites Rabba, making it apparent that they interacted face-to-face with Rabba as teacher. Together, these graphs provide valuable insight into the meaning of the texts and dynamics of Talmudic discussions by highlighting relevant biographical information that is not readily accessible on the page of the Talmud. "
	},
	{
		"id": 416,
		"title": "Mediating Research Through Technology @ NEP4DISSENT",
		"authors": [
			"Wciślik, Piotr",
			"Maryl, Maciej",
			"Edmond, Jennifer",
			"Wieneke, Lars",
			"Labov, Jessie",
			"van Bree, Pim",
			"Kessels, Geert"
		],
		"body": " With this poster, the EU-funded COST Action 16213 New Exploratory Phase in Research on East European Cultures of Dissentwishes to invite collaboration in facilitating the integration of DH methods and tools by the multidisciplinary community built around the study and curatorship of the cultural legacy of resistance and dissent in former socialist countries in comparative and transnational perspective. Resistance and dissent in former socialist Europe 1945-1989 constitutes a remarkable chapter of Europes recent past, which not only informs in a decisive way the identities of post-socialist societies, but has also reshaped the continent as a whole and still provides an important reference for contemporary social movements worldwide. The main aim of NEP4DISSENT, an international scholarly network formed by around 200 participants from 38 countries to date, is to trigger the next discovery phase of this legacy by forging a new, reflexive approach, and by providing a platform for incubating networked, transnational, multidisciplinary and technology-conscious research with creative dissemination capacities. NEP4DISSENT creates a valuable interface for communication between three communities of practice. Facilitated by IT experts with humanities and social sciences expertise, the network enables participant researchers to train with cutting-edge digital tools, and to increase their capacities for creative dissemination through engaging in productive dialogue with art and cultural heritage curators, in order for future research to be technologically advanced and better disseminated. The research and capacity building agenda of NEP4DISSENT represents a complex and original challenge for the marketplace of digital research infrastructures. That is due to its multidisciplinary character, the uneven propagation of DH research practices between disciplines and national scholarly communities East and West, the uneven digital readiness of the often very ephemeral sources, as well as its multilinguality that makes the comparative and transnational perspectives difficult to apply. On the other hand, DH approaches, in particular methods of data aggregation and federation, techniques of text, image and layout recognition, as well as tools for network-graph and spatio-temporal visualisation and analysis, are uniquely qualified to explore in full scope that comparative and transnational dimension of the dissident networks of solidarity, which has been one of the most extraordinary aspects of that legacy. NEP4DISSENT Working Group 5 Mediating Research through Technology , involving representatives of projects such as CENDARI, COURAGE, NODEGOAT, IMPRESSO, as well as research infrastructures: DARIAH, or CLARIN, has been set up to tackle the complexity of this scenario. The mission of WG5 is to map the specific needs of the networks participants against the available digital research infrastructures, to facilitate DH capacity building through training and research collaboration. While WG5 does not create new infrastructures, its aim is to offer recommendations for new or improved services that would better respond to the specific problems of infusing the digital at various stages of the life-cycle of scholarly and curatorial projects in this domain, and most broadly, to promote and deepen our understanding of the role of the digital technologies in research on contemporary history. Workshops and summer schools organised by WG5 are the principal avenue for engaging both communities in that regard. The poster will feature the conclusions drawn from the exploratory activities of WG5 conducted thus far, characterised by the multi-channelled, ethnography-inspired approach to understanding the place of technology in the existing workflows of the NEP4DISSENT participants, and based on a variety of inputs, including two surveys circulated in December 2017 and summer 2018 among the network participants supplying a broad baseline understanding of the network and the concerns of its contributors; structured group interviews conducted in February-May 2018 with key network members, whose profiles and interests indicated that they occupy research positions at the interface between analogue and digital sources and approaches; and informal conversations during the NEP4DISSENT assemblies in Brussels, Warsaw and Belgrade. Further, the poster will stimulate discussion about training schools as a fulcrum of DH methods and tools propagation, welcoming insight about best practices which will help to better prepare for the course Cultures of Dissent in Eastern Europe: Research Approaches in the Digital Humanities organized by NEP4DISSENT in the framework of a Central European University Summer School immediately after the DH2019 conference. Finally, the poster will create a meeting point for both digital research infrastructure service providers who would like to expand their user base towards this intriguing domain of contemporary history scholarship, and for experts interested in broader methodological, technical and social aspects of penetration of DH approaches in both similar and different scholarly communities of practice. "
	},
	{
		"id": 417,
		"title": "How to Better Find Historical Photographs in an Archive - Geographic Driven Reverse Search for Photographs",
		"authors": [
			"Weinfurtner, Anne",
			"Dorner, Wolfgang",
			"Graf, Simon"
		],
		"body": " Historical photographs are important sources for documenting the past of humanity. They can serve as evidence of our societys history but are also objects of research themselves. Today, the standard format for describing the geographical origin of a photograph is to assign a geoname or point coordinate of the location of the photographer to the image. In view of historical photographs, we are dealing within this operation with spatial uncertainty. In most cases it is not possible to define the exact location of the photographer in form of coordinates in e.g. regions that afflicted strong environmental changes over time. The aspect of spatial uncertainty is also present, when describing the location of the photograph`s origin by a so-called geoname. A geoname can be comprehended to different extents. For example, it can name a region, a city or just a small part of a city. Therefore the question of the exact spatial origin cannot be answered by solely using a geoname. Such aspects of uncertainty in the localization of historical photographs complicate the retrievability in archives for spatially oriented search. Since the photographs cannot be found by entering search queries in the archive, their potential to serve as a data source for research cannot be fully exploited. From a technical and mathematical perspective, photographs are a two-dimensional representation of information from a three-dimensional space due to its central perspective character. Each object, such as a building or a landscape element, has two-dimensional coordinates in the image and corresponding three-dimensional coordinates in the world coordinate system. This approach describes a reverse geographical search of historical photographs based on this underlying principle. This means, that spatial information will not be defined by one geoname with various possible spatial extents describing where the picture has been taken, but instead we allow to assign global coordinates to each depicted building and object in the respective photograph for a better georeferencing of the whole picture. This method counteracts to the given issue of uncertainty in spatial metadata of historical photographs. Map based retrieval techniques will be able to better satisfy the information demand of spatially oriented and object centric scientific disciplines such as archaeology, monument conservation, architecture or landscape planning. The presentation demonstrates how an object centric search approach could help to better find photographs for the purpose of e.g. spatially oriented humanities. A web-mapping system allows to use web based tools, to georeference objects depicted in a photograph on a dynamic web map. Users can place markers of different types to define objects such as a building, person or landscape elementon historical photographs and can then assign them to a real location on a world map. The more items have been located on the map, the more accurate the image origin can be determined and the more information can be assigned to the photograph stored in the archive. Figure 1: Visualization of the principle of pasting elements in a photograph and their assignment to world coordinates in a map based on OpenStreetMap data. An overview map allows to access photograph collections via spatial interests. Photographs shown there can be filtered by entering metadata search queries such as geonames or assigned tags in a search bar. Figure 2: Screenshot of the overview map with georeferenced photographs, villages and components in the Bavarian-Czech border areaAn additional automated computer-vision-based pre-classification of photographs, helps to filter search results for image categories like e.g. landscape, building, group of persons, portrait. This machine-based image analysis allows a rough categorization of photographs in great archival collections, that are not at all or not well documented due to missing manpower or financial resources. As an outlook, an application in archaeology will be presented. Archaeologists from the University of South Bohemia České Budějovice use the web-mapping tool for historical photographs in order to reconstruct abandoned settlements that were destroyed after the eviction of the German population in the Czech countryside near the national border to Bavaria. The data collected via the online tool will be used for the orientation of the photographs in three-dimensional space. Further on, the oriented images are used for creating 3D reconstructions for the digital preservation of abandoned sites. The online tool has been developed in 2018 in order to serve crowd-sourcing methods to the public in the middle of 2019. The procedure is supported by the European Union within the Cross-Border Cooperation Program Freistaat Bavaria - Czech Republic Objective ETZ 2014-2020. "
	},
	{
		"id": 418,
		"title": "Agent-Based Modeling in Art History: Simulating an Insane Asylum",
		"authors": [
			"Wendell, Augustus",
			"Ozludil, Burcak"
		],
		"body": " This short paper reports on the development of a system that incorporates agent-based modelingin art/architecture historical research and scholarship. ABM is a computational process simulating agents and their behaviors; the relationships between agents; and the interaction between agents and their environments. ABM has been used in a range of fields including predicting the spread of epidemics, behavior in economic systems, movement within the built environment, egress modelingand many more.The development of this ABM system is supported by the Getty Foundation as part of the Advanced Topics in Digital Art History: 3D andspatial Networks Summer Institute, Venice, 2018-2019. In art/architectural history, ABM enables simulating inhabitants in addition to space itself and its formal qualities. Agents, programmed with basic rules or data to move autonomously within space, are modeled to recognize and sense their environment. This approach can expand both art historical questions and narratives by observing emergent movement in space and interactions between inhabitants. As a highly iterative computational process ABM allows for experimentation and new outcomes emerging from slight variations. In our prototype of the Istanbul Toptasi Insane Asylum, we model medical and daily routines of the asylum inhabitants.This setting presents both a computational and philosophical challenge as ABM agents are typically assumed to be active, autonomous individuals with decision-making capability in a non-restricted environment. In contrast, the asylum is a highly-regulated environment with unpredictableagents. Based on scattered evidence on life in the asylum and the scripted strict schedule, we model interactions between various agent types, as well as exposure of patients to natural light, air, and ventilation, all of which are measurable with the autonomous sensing of agents. The asylum presents a productive case study of applying ABM to art/architectural history as the movement of agents provides insight into the functioning of a nineteenth century imperial medical facility. Objectives The specific objective of the ABM in this case study is analyzing whether or not the scripted rules of daily life in the asylum could be implemented given the number and types of admitted patients in the year 1911. We iterate the process by adjusting the attributes and detailing the rulesto produce various potential scenarios. While the objective here is limited to the simulation of the routines of patients, we do believe that the experiment provides some insight into the daily experience of patients, in the absence of textual evidence produced by them, such as letters, diaries. Our claim is not that this will be an all-encompassing understanding of the patient experience, but that it has the potential to open a window into these now-lost lives and, at a larger scale, to allow us to elaborate theoretical implications in question. Method Agents in the Toptasi Asylum simulation adhere to a strict set of rules. These rules allow the agents to behave autonomously over a long simulation time using only their initial programmed instructions. A central simulation clock is referenced for choreographing daily routines, instructing agents to find the shortest path to their next itinerary location using staircases and walkable surfaces. Below are three tables showing the attributes, rules, and the model timing that govern one agent type, the patient, chosen for this paper.Figure: Screen captures from the 3D model in SpatioScholar and historical photographs showing a male patient ward and the female patient courtyard. Left bottom image shows the ABM system within Unity3D. Sources: Right top: Ergin, Müessesat-ı Hayriye-yi Sıhhiye Müdiriyeti, 1911; right bottom: Mazhar Osman, Sıhhat Almanakı, 1933. Toptasi PatientAttributes Each agent represents an asylum inhabitant that belongs to one typeEach agent belongs to one ward in the asylumEach agent belongs to one sex that ties into the block and ward that they are admittedEach agent is in one stateToptasi PatientSimulation Rules An agent starts the day, cleans self and makes their bed at 07:00 An agent proceeds to have breakfast in a dining hall at 08:00 An agent needs to be in their ward between 09:00-11:00 for daily medical visits An agent proceeds to have lunch in a dining hall at 11:00 An agent has time dedicated to a mixture of airing, exercise, socializing, education etc. between 12:00-16:00 An agent proceeds to have dinner in a dining hall at 16:00 An agent proceeds to their ward for down time and eventually sleep at 17:00 Toptasi Agent Simulation Model Timing A single clock increments each minute of simulated time. This clock broadcasts the current time to all agents for their internal itinerary movement. The simulation may run in either real time, where a minute within the simulation equates to a minute outside the simulation, or in a compressed timeline where the simulation clock calculates at a higher rate. The Platform The ABM system is built within the Unity3D application as a series of prefabricated objects and C# scripts. Agents are scripted objects that use the existing Unity3D codebase to path find and navigate within any imported 3D historical model. A natural language text file defines the attributes and daily itinerary of each agent cohort, allowing alternative and multiple agent setups to be quickly loaded from outside the simulation. An example itinerary line from this file, 1200: Target DiningHall defines a change in location by time and target object. he simulation agents continuously test a number of conditions using iterative and highly accurate raycasting methods: the proximity and visual connection to other agents, exposure to natural light and intervisibility to specific architectural and programmatic features. The outcome of these conditions are stored as data within each agent and are downloaded into a master external CSV file. The output CSV file makes the agents data available for data processing and visualization. "
	},
	{
		"id": 419,
		"title": "Advertising Gender - Using Computer Vision to Trace Gender Displays in Historical Advertisements, 1920-1990",
		"authors": [
			"Wevers, Melvin",
			"Smits, Thomas"
		],
		"body": " This study applies computer vision techniques to examine the representation of gender in historical advertisements. Using information on the relative size, position, and gaze of men and women in thousands of images, we chart gender displays in Dutch newspaper adverts between 1920 and 1990. In the 1925 edition of Psychology in Advertising psychologist Alfred Poffenberger encouraged ad makers to short-circuit the consumers mind through vivid, pictorial appeals to fundamental emotions. The images and visual clichés tap into a representational system that produces meaning outside the realm of the advertised product. In the late 1970s, sociologist Erving Goffman examined the semiotic content in advertisements printed in contemporary newspapers and glossy magazines. He noted that displays of gender are often conveyed and perceived as if they were somehow natural, deriving, like temperature and pulse, from the way people are and needful, therefore, of no social or historical analysis. Of course, almost the exact opposite is true. Goffman conceived of five different categories to study the depictions of, and the relations between men, women, and children in advertisements: relative size, feminine touch, function ranking, ritualization of subordination, and licensed withdrawal. Drawing from this approach, Jonathan Schroeder, contends that in advertisements the male regularly represents the the active subject, the business-like, self-assured decision maker, while the female occupies the passive object, the observed sexual/sensual body, eroticized and inactive. Studies like that of Goffman and Schroeder relied on a limited number of advertisements, and they also do not take the historicity of ads into account. For example, Goffman looked at around 400 different advertisements, which he more-or-less randomly selected from newspapers and magazines easy to hand - at least to my hand. As Kangnotes, he was often criticized for this method. Instead of relying on a random sample, he purposefully selected images that mirrored gender differences. Goffman emphasized that he choose his categories partly on the basis of the fact that he could find almost no images that disproved his categories. For example, images in which women were relatively larger and had the executive role. In an earlier project, we developed methods to extract visual material from historical newspapers. Using a large-scale data set of digitized historical advertisements extracted these methods, we examine continuity and change in displays of gender using computational means. By applying state-of-the-art computer vision methods to a diachronic set of digitized Dutch newspapers advertisements published between 1920-1990, we can test existing hypotheses and contribute to existing replication studies about gender displays in advertisements. The data is kindly provided by the National Library of the Netherlands. We will start our research after the First World War, because Dutch advertising practice changed rapidly in the Interbellum. Technological advances made it cheaper to print images in newspapers. Influenced by American ad agencies, Dutch ad makers saw the potential of using visual material to increase sales and to convey particular brand identities. In this short paper, we operationalize Erving Goffmans theory on gender displays in two ways. First, Goffman argues that differences in size will correlate with differences in social weight. Using facial recognition software, we select adverts that include people, and then train a gender detection algorithm using a convolutional neural network to estimate whether men or woman were represented in the images. This allows us to visually represent the changing faces of men and women in advertisements. Part of the process also entails a reflection on the inherent bias in these algorithms. Second, Goffman contends that body postures and the engagement in social settings show that men are often portrayed in executive roles and that women are often depicted as being removed from the social situation gazing off into the distance. Using information extracted in the first step, we quantify whether how people were represented on image, e.g. how was position where on the image? In Figure 1, for example, we see a social gathering of four people with the central figure drinking a bottle of Coca-Cola. Using the described computer vision techniques, we can extract faces and determining where on the images these are located. This reveals a man as a central figure with two women and man surrounding the central man. By extracting these features from a large set of advertisements and applying clustering methods, we can detect patterns in advertisements over time that can be compared to findings by Goffman and others.. Coca-Cola advertisements. Nieuwsblad van het Noorden, August 14, 1959. We argue that it is not only possible to test existing theories on displays of gender in a more extensive database, but also to shed light on their historicity: the fact that these displays seem natural and constant, but can, and will, change, sometimes rapidly, over time. In line with the conference theme of complexity, we are aware that it is problematic to reduce displays of gender to binary categories. In this study, we are operationalizing displays of gender to a limited set of features, thereby reducing the complexity of gender. However, the coarse-graining of displays of gender into features that are comprehensible and computable allows us to extract trends from collections of advertisements. These trends offer context to particularities and can reveal the use of visual clichés in advertising discourse. Moreover, in the paper, we evaluate and reflect on the use and performance of models trained on contemporary training data as well as relying on probabilities to define gender. "
	},
	{
		"id": 420,
		"title": "A Mobile Website To Support Teachers In Discussing Terrorism In The Classroom",
		"authors": [
			"Wiering, Frans"
		],
		"body": " Introduction In the TerInfo project, students and experts from Pedagogy, Religious Studies, Psychology, History and Information Sciencecollaboratively aim to increase the societal resilience against terrorism in Dutch primary and secondary education. The project formally started in July 2017 and is supported by the city of Utrecht. Project leader is Beatrice de Graaf, professor of History of International Relations & Global Governance. Utrechts population has a very diverse background, which is reflected in the classrooms. Especially when terrorist incidents occur, fragmented and unverified information reaches the pupils very quickly though different channels. Such information may have immediate negative consequences, for example it may increase anxiety with children or create tensions between them. Teachers who consequently feel an urge to comfort, provide perspective or de-escalate, often experience a reluctance to act. Therefore, TerInfo aims to support teachers by providing them with: reliable and compact information about terrorism and radicalisation practical ideas and assignments for discussing these issues in class in an inclusive manner quick and helpful interpretation of recent developments expert support tailored to the schools specific needs. The project is supported by a mobile websitecontaining a growing collection of carefully prepared materials. This paper describes the creation of this site and the design philosophy behind it. Much effort has gone into designing it to optimally match the requirements of the teachers and to empower them in their work. The website was developed, in several iterations, by a team of seven bachelor students of Information Science, six following the honours programme. This programme offers students a range of learning opportunities beyond the standard programme, which they can tailor to their own interests. All six honours students decided to spend a significant part of their programme working on the Ter Info website. Prototype The first iteration of website development took place in April-June 2017 during the introduction project, a group project that completes the first year of the programme. The assignment for the introduction project is to design and prototype an innovative interactive system that answers a societal need. Students are expected to apply all relevant knowledge and skills they have acquired so far, but are otherwise given considerable freedom in the choice of topic, methods and organisation. All projects, prototypes in particular, are publicly presented during a symposium. When this student team embarked on this project, the general problem outlined above was available to them, but beyond this they had very little information to go on. Taking a human-centred design approachthey decided to first investigate the teachers experiences and needs, and to familiarise themselves thoroughly with the topics of terrorism and radicalisation. Next, they created a set of draft articles, providing information on various terrorist attacks and organisations as well as interpretation, and supporting materials for use in class. The design of the website started from a set of personas and scenarios based on the interviews with the teachers and went through several iterations of lo-fi and hi-fi prototypes. The final prototype holding the created materials was realised in PHP using the Laravel frameworkand includes a backend for the creation of new content, based on AsgardCMS. The prototype was evaluated both by school teachers and through heuristic evaluation by team members. Pilot Once the project formally started in summer 2017, the platform was further developed by the Information Science students, with domain experts and students in Pedagogy and History taking responsibility for the content of the site. The content is structured in five levels: About Ter Info Terrorist attacks Terrorism in a nutshell Terrorists Discussing terrorism in class Articles typically take 2-4 minutes to read, contain a FAQ section, and provide links to further materials on the Internet and to academic sources. There are also links to the relevant articles on the site itself: for example, the article What is terrorism links to What terrorists want to achieve and How scared should we be of terrorism. In March-June 2018, a pilot study was done with five schools from the city of Utrecht participating. These schools display a large diversity both in leveland ethnic background. At the beginning and end, seminars were organised with expert presentations and discussion sessions. Between seminars, workshops were held at each school in order to fine-tune the pilot to the specific circumstances. Overall, participants were quite satisfied with the project, yet they had a great deal of feedback on all aspects of the project. It was observed for example that article length was good and the FAQs were often used, but that academic sources were never consulted. Many suggestions were made for adding pages. There is a clear wish for personalisation based on type of school and for a push notification system to announce new materials and developments. Next steps Phase 2 of TerInfo is currently in full swing, with 19 schools participating. In parallel, a next version of the site is being developed by a new team of students, from the University of Applied Sciences Utrecht. In my presentation I will discuss the evaluations and experiences on which this next version will be based. I will also present a long-term vision of the website, with particular attention to the issue of scale. Currently the university experts provide intense individual support to the schools. In the long run this is not sustainable. A likely avenue thus seems to be to reshape the site as an online community, where participants gradually develop expertise themselves and share their experiences with their peers. Acknowledgement TerInfos website was developed by Information Science students Mirkan Davarci, Max Groot, Lisa Hordijk, Mitchell Klijs, Hessel Laman, Herman Nelissen and Govert Vermeer. We greatly appreciate their contribution to the project. "
	},
	{
		"id": 421,
		"title": "Encoding Early Modern English Drama: Embedding Digital Approaches In Undergraduate Literature Courses.",
		"authors": [
			"Williamson, Elizabeth"
		],
		"body": " There has been greater inclusion of teaching in DH debates in recent years, yet in practice, UK digital humanities teaching tends to be found in explicitly DH postgraduate courses or as specific skills training for self-selecting staff orstudents; it also appears less in English literature teaching than in history, classics, media studies or linguistics. This paper offers a case study where TEI XML encoding was introduced to first-year undergraduates in an established course on Shakespeare, to facilitate discussion on how digital humanists can enable integration of digital skills into traditional English literature teaching by capitalizing on affinities between DH, book history, and textual scholarship. Integrating digital approaches into undergraduate teaching of English literature enables us to introduce students early on to reflecting critically on the digital, and lets us embrace the complexities of encoding as editing, rather than allowing undergraduates to only interact with the digital through a GUI. Early modern studies provides an ideal context in which to teach text encoding and digital literacy skills. In part this is due to the availability of encoded early modern texts for analysis and reuse, for example in the massive release into the public domain of lightly encoded EEBO-TCP texts in January 2015, and in part to fewer issues around copyright than with more modern texts. More specifically, there is a synergy between the existing concerns of the Shakespeare course under discussion and those of digital publication, where the latter finds a natural fit in conversations on book history, text technologies, and editorial agency. Olin Bjork, in an article comparing American composition and computing classes to the new media studies side of digital humanities, suggests that A weakness of digital humanities is that it undertheorizes the transformation of material objects into digital objects. The materiality and the instability of the text are complexities that go to the heart of early modern literature studies: these are issues that our students grapple with when they consider the nature of the early modern play-text, yet rarely do we recognise that the digital text must be part of this conversation. Accordingly, in a course that unsettles the Shakespearean play as single authoritative text, there is a real need to push conversations on early print and textual scholarship into the realm of the digital, to unsettle and critique the digital texts the students encounter more often than they open their assigned course book. The course under discussion is a first year module with c.200 students enrolled, and the digital element ran as a successful pilot in 2018 and was expanded in 2019. The digital element is introduced in a guest lecture by a digital humanist; issues raised are debated in seminar groups, and existing digital texts and projects are introduced to the students, including A Digital Anthology of Early Modern English Drama, Internet Shakespeare Editions, the Queens Men Editions, Digital Renaissance Editions, and Folger Digital Texts, where students use the Folger API to explore the possibilities opened up by text encoding. The students then have the opportunity to create their own digital edition of the ending of King Lear, opting in to an additional workshop on TEI XML. They can elect to do their final assessment as a TEI-encoded play excerpt coupled with traditional essay, which allows them to reflect on the differences between early print, modern printed editions, and digital media. Writing about the medium and the choices it encourages or enforces allows them to critically reflect on digital texts at a crucially early point in their university careers. In the encoding part of the assignment, by taking ownership of a digital play text, they come to see both the scholarly edition and the digital medium as less an unquestionably authoritative black box and more something that they themselves can have agency over and interrogate. By pulling back the curtain on the scholarly text and the digital medium, they are more able to critique both text and textual manifestation. DH in the undergraduate classroom often tends towards GUI publication, with wikis, Wordpress sites, or platforms like the Alliance for Networking Visual Cultures Scalar. While that is a valuable part of the picture, this paper explores what happens when students are introduced to the complexities of XML encoding as an editing practice, and how exposure to the component parts of a digital publication opens up that black box. This reveals to the students the variety of people involved in publication, in modern as well as early modern times, and demonstrates that encoders are editors and developers are intellectual partners who have concrete influence over the resulting output. Significantly, this message is also imparted to fellow academic staff teaching on the course. This approach thus invites fellow academic staff into the digital humanities, developing by proxy their understanding of digital resource creation and consequently their ability to critique this growing area of scholarly production. The aim of this paper is to share experiences and create discussion around the natural affinity between early modern studies and digital publication and digital critical literacy, especially in an undergraduate context. A parallel can be seen between the instinct to view the digital object as something that appears from thin air and the unquestioning acceptance of a particular critical edition as the immutable authoritative text. It is too easy to ignore the digital provenance of a text online or the multiple agents involved in producing anytext. To do this is to remove those who construct the text, and obscure the encoding as well as editorial choices made at every stage of its creation. To direct attention towards these is to put them back. This is a conversation that we need to begin early on in students academic career, to situate digital critical literacy within an existing tradition of literary criticism: we need to teach students to close-read the material digital object at the same time as the literary text and its early print origins. "
	},
	{
		"id": 422,
		"title": "The CHQL Query Language for Conceptual History Relying on Google Books",
		"authors": [
			"Willkomm, Jens",
			"Schmidt-Petri, Christoph",
			"Schäler, Martin",
			"Schefczyk, Michael",
			"Böhm, Klemens"
		],
		"body": " Introduction The digitization of large time-labeled bibliographies has resulted in corpora such as the Google Ngram data set. Such corpora extremely accurately reflect how individual words are used over time. They are expected to reveal novel insights into the evolution of language and society, provided adequate analysis systems are available. In this context, developing a comprehensive query algebra that allows domain experts to formalize complex hypotheses would be a major contribution to successfully unlock this potential. The case of conceptual history serves as our example from the humanities. In conceptual history, researchers examine the evolution of concepts represented by words such as peace or freedom. In exploring the history of a concept, scholars commonly make use of, but are not restricted to, word-usage frequencies, word contexts, sentiment analysis, how words refer and relate to and contrast with each other, or they look for word pairs or word families whose usage is correlated. Consider our example: how the words East and West change from merely cardinal directions to politically charged concepts after 1945. In this paper, we present a query algebra for empirical analyses of temporal text corpora, the Conceptual History Query Language. A temporal text corpus in our sense is a set of words and word chains, i.e., ngrams, together with their usage frequency at various points of time. Our query language is meant to be useful for domain experts, i.e., be descriptive and complete, and bear optimization potential to allow fast query processing on large data sets. We focus on an algebra inspired by the German tradition of Begriffsgeschichte, as exemplified by the work of Reinhart Koselleck. Related Work Existing query algebras, like the one for the Structured Query Language, do not feature specific support for analyses of the kind we envisage. Other approaches from the literature, e.g., the Contextual Query Language, the Corpus Query Language, or the ANNIS Query Language, have similar issues. The common relational algebra, does not contain sufficiently specific operators, e.g., temporal or linguistic operators. Extensions exist to add temporal operators, but not linguistic operators. To query relations between words, there are special-purpose query languages. For example, SQWRL is a language to query an ontology. Querying word relations, e.g., from an ontology, does not include all required linguistic relationships. Further, ontologies do not provide temporal information. SQWRL does not contain any temporal operator. All of these algebras have in common that they do not cover both linguistic and temporal operators required for research on conceptual history. Related work in the digital humanities mainly consists of data processing and the analysis of text corpora. Some frameworks focus on linguistic and reflective properties as well as their evolution such as. Respective systems cannot output the required information to conduct research on conceptual history in a comprehensive way. In addition, such systems do not provide a sufficiently abstract interface, a reason why experts are reluctant in using them. Concept Types and Operators This section shows in the abstract how the operators of CHQL allow searching for concept types. A formal definition of all of our operators is given inand will be presented at DH2019. Conceptual history claims that pragmatic properties of historical, cultural and economic relevance are incorporated in concepts, irrespectively of whether individual users are aware of this or not. It attempts to track changes of particular conceptsover time to determine how their pragmatic relevance changes. Thus, concepts will be categorized as belonging to a particular concept type at a particular moment in time. Conceptual historians typically read and interpret large masses of texts which provide a variety of information types) which help to determine the concept type. Because we want to do the same using Distant Reading techniques, these information types need to be translated into observable data characteristics for which individual operators in the query language are defined. Finding an adequate number of helpful information types, structuring them and converting them into computable and combinable items is the main challenge of our project. Since there is no accepted formal specifications of information types, we describe an interpretation of Kosellecks information types in order to map them on to data characteristics. Data characteristics are quantitative feature either directly present in our data, or a derived piece of information. We describe which data characteristics are needed to simulate Kosellecks information needs and explain our realization of all data characteristics and their implementation as operators. The relationship between concept types, information types, data characteristics and operators to hypothesize concept types One of Kosellecks implicit assumptions is that each concept type has specific characteristics. In our terminology: any concept type can be described using a specific combination of information types. For example, Koselleck may plausibly be read as claiming that words that form a parallel conceptwould have similar word frequencies and have a significant number of identical surrounding words. By contrast, counter concepts would also have similar word frequencies yet their surrounding words would behave differently. For instance, if enlightenment and reason are parallel concepts for a particular period, their relative word frequencies should be similar, and if emancipation occurs near enlightenment, it should occur near reason too, and both concepts should be endorsed rather than criticized. By contrast, if East and West are counter concepts, their word contexts should contain different words, and there should be some sort of contrast in attitude between them. If every concept type has its own specific linguistic and pragmatic properties and hence should be representable by a specific combination of information types, it should be possible to develop a system that finds these information types in large corpora that are not amenable to conventional close reading. To this end, we need a formal definition of any information type which is observable and quantifiable. We present a selection of some of the data characteristics with the information type they are intended to represent: Individual Context: This requires two data characteristics: a set of surrounding words for a target word, i.e., the linguistic context, and the sentiment for this context, by summing up the sentiment values of the words of the context. Our surroundingwords operator and sentiment operator implement this. Topic Grouping: Using topic modeling, groups of words may be classified as belonging to a particular topic. Sentence Structure: This again requires two data characteristics: the function of a word, i.e., differentiate between parts of speech, and completing phrases, i.e., search for missing words in a phrase. The first data characteristic is implemented by our operator pfilter. We implement the second one as a pattern-matching operator which we call textsearch. Frequency Data: Neologisms, which might be evidence for radical changes, would display abrupt increases in word-usage frequency over time. To find this and similar characteristics, we propose an operator time series-based selection that compares the time-series values with a constant. To allow for a temporal restriction, we also provide a subsequence operator that limits the selection to an arbitrary time interval. The combination of both operators facilitates the search for neologisms. Sentiment Analysis: Using well-proven resources such as LIWCor customized dictionaries, our sentiment operator represents the emotions associated with a concept, relying on the words in its context. Results Using CHQL, we have tested the hypotheses thatEast and West have acquired a political context after 1945, whereas North and South havent, and thatthe former have turned into counter concepts in the political sphere, their contexts expressing diverging attitudes, whereas the latter have remained parallel concepts in the geographical sphere. The operator trees 1 and 2 shown in Figures 2 and 4 illustrate how CHQL allows combining the operators mentioned to perform a single search, yielding the results shown in Figures 3 and 5. Formalisation of hypothesis 1 in CHQL The result of Query 1 on the Google Books Ngram Corpus Formalisation of hypothesis 2 in CHQL The result of Query 2 on the Google Books Ngram Corpus "
	},
	{
		"id": 423,
		"title": "Using Ngrams to Develop a Query Algebra for Conceptual History",
		"authors": [
			"Willkomm, Jens",
			"Schmidt-Petri, Christoph",
			"Schäler, Martin",
			"Schefczyk, Michael",
			"Böhm, Klemens"
		],
		"body": " The digitization of large time-labeled bibliographies has resulted in corpora such as the Google Ngram data setwhich are expected to reveal novel insights into the evolution of language and society. We present our project of developing a query algebra to unlock this potential, the Conceptual History Query Language. It is inspired by the German tradition of Begriffsgeschichte as exemplified by the work of Koselleck. Conceptual history claims that pragmatic properties of historical, cultural and economic relevance are incorporated in concepts and attempts to track any changes over time to determine how their pragmatic relevance changes. Thus, concepts will be categorized as belonging to a particular concept type at a particular moment in time. To determine the type of a particular concept, scholars commonly make use of word-usage frequencies, word contexts, sentiment analysis, how words refer and relate to and contrast with each other, or they look for word pairs or word families whose usage is correlated etc.. They closely read and interpret large masses of texts, but because we want to do the same using Distant Reading techniques, these information types need to be translated into observable data characteristics for which individual operators in the query language are defined. Finding these information types and building computable items is the main challenge of our project. Since there is no accepted definition of information types, we describe an interpretation of them in order to map them on to data characteristics. Data characteristics are quantitative feature either directly present, or a derived piece of information. We describe which data characteristics are needed to simulate Kosellecks information needs and explain our realization of all data characteristics and their implementation as operators. The relationship between concept types, information types, data characteristics and operators to hypothesize concept types The assumption is that each concept type has specific characteristics, that is, any concept type can be described using a specific combination of information types. For example, Koselleck may plausibly be read as claiming that word pairs that form parallel conceptswould have similar word frequencies and have a significant number of identical surrounding words. By contrast, counter concepts would also have similar word frequencies yet their surrounding words would behave differently. For instance, if enlightenment and reason are parallel concepts for a particular period, their relative word frequencies should be similar, and if emancipation occurs near enlightenment, it should occur near reason too, and both concepts should be endorsed rather than criticized. By contrast, if East and West are counter concepts, their word contexts should contain different words, and there should be some sort of contrast in attitude between them. We are developing a system that finds these information types in large corpora that are not amenable to conventional close reading. We present a selection of some of the data characteristics with the information type they are intended to represent: Individual Context: Our surroundingwords and sentiment operators search for surrounding words and sum the associated sentiment, using sentiment analysis) Topic Grouping: Using topic modeling, groups of words may be classified as belonging to a particular topic. Sentence Structure: The function of a word, i.e. various parts of speech, and completing phrases, i.e., search for missing words in a phrase, are implemented by our operator pfilter and by a pattern-matching operator called textsearch. Frequency Data: Abrupt increases in word-usage frequency over time and similar characteristics are implemented with the operator time series-based selection, a subsequence operator may limit the selection to an arbitrary time interval. Using CHQL, we have tested the hypotheses thatEast and West have acquired a political context after 1945, whereas North and South havent, and thatthe former have turned into counter concepts in the political sphere, their contexts expressing diverging attitudes, whereas the latter have remained geographical parallel concepts. The operator trees 1 and 2 shown in Figures 2 and 4 illustrate how CHQL allows combining the operators mentioned to perform a single search, yielding the results shown in Figures 3 and 5. Formalisation of hypothesis 1 in CHQL The result of Query 1 on the Google Books Ngram Corpus Formalisation of hypothesis 2 in CHQL The result of Query 2 on the Google Books Ngram Corpus "
	},
	{
		"id": 424,
		"title": "Digital Humanities in European Research Libraries - a Survey",
		"authors": [
			"Wilms, Lotte",
			"Degkwitz, Andreas",
			"Derven, Caleb",
			"Lefferts, Marian",
			"Lingstadt, Kirsty",
			"O'Dwyer, Liam",
			"Verbeke, Demmy"
		],
		"body": " The European Association for Research Librarieshas more than 400 members – national, university and other libraries. Since 2018 digital humanities is part of the official strategy plan of the organisation and former LIBER president Kristiina Hormia-Poutanen mentioned it in the Foreword as a priority are[a..] where libraries are positioned as a hub around which researchers, collections and service development come together. However, it is unclear how and if the LIBER libraries are engaging with the digital humanities. They might be positioned as a hub, but are they also using this position? This poster will present the outcomes of a survey amongst 57 LIBER members about their engagement with digital humanities. Library and DH surveys Other surveys have been conducted outside of Europe, such as the reports Digital Humanitiesand Supporting Digital Scholarshipof the Association of Research Libraries, and the Australasian survey by McKenzie and Ross. In Europe, digital humanities activities have seen a similar growth in academic libraries as in other continents, but no special attention has been given to the libraries as a whole. Only in the summer of 2017 has LIBER launched a digital humanities working group, even though the topic of DH has been part of their annual conference call for proposals since 2014. In Europe only the UK and Ireland have conducted similar surveys, also partly based on those done in the US. The Research libraries UK published their report - The role of Research Libraries in the creation, archiving, curation, and preservation of tools for the Digital Humanities by Christina Kamposiori – in July 2017, which investigated the activities of 27 UK-based libraries. The CONUL Research Group survey on Digital Scholarship surveyed their 11 member libraries of the Consortium of National and University Libraries of Ireland. Most other European publications of libraries and digital humanities are anecdotal and focused on Western Europe, such as blog posts from the Digital Scholarship Unit of the British Library on projects they run using their digital collections, the blog posts of the Dutch KB Lab on their researcher-in-residence projects, or the report of the CORPUS project of the Bibliotheque National de France. However, other parts of Europe are also embracing digital humanities in libraries, which can be seen for example by the involvement of national libraries in Estonia and Spain in local DH initiatives on which no publications have been written so far,. These Europe-wide activities have not yet been researched in connection to each other, but are, when described, solely published about as individual case studies. This project will change this and examine the DH activities in European research libraries on a larger scale and place them in context with the global library community. Content of the survey To ensure a complete overview of issues related to digital humanities and libraries, the survey has been designed around the following themes: General information about the library Organisation of DH in library Services/support Staffing/skills SpacesFunding Collections Impact Awareness Partnerships Future Selection of themes and questions The themes have been selected by gathering input from participants at the Digital Humanities and Digital Cultural Heritage workshop at the LIBER conference in July 2018. Participants were invited to share the questions they wanted to ask their colleagues. Their suggestions were subsequently arranged into topics and placed alongside other DH in library-surveys conducted earlier to ensure all themes were sufficiently covered. This survey specifically adds a theme on collections as that was underrepresented in surveys done on digital humanities and libraries, even though it is a crucial aspect of working in this field. Attention is given to copyright issues, physical opposed to digital collections and accessibility of the digital collections. A shorter exploratory survey was conducted in a smaller group of participants to test a number of questions and answer models, resulting in a general overview of DH activities amongst members of the LIBER working group. Next to that, several questions were added into the survey to facilitate further research into the topics discussed. The survey was subsequently coded into SurveyMonkey and tested by a group of ten people - librarians and social scientists – after which the final changes resulted in a list of 83 questions. Respondents All 400 member libraries of LIBER have been invited to partake in the survey from 1 February to 15 March 2019. Even those that do not have a Humanities faculty were asked to contribute, given that their collections might be of interest to those outside of their localresearch community. In these cases the survey also served to create awareness of other potential users of theircollections. After six weeks, 57 participants completed the 83-question survey. Future research This poster will present the outcomes of the survey and will therefore present an overview of the activities of European research libraries in the digital humanities. The full landscape analysis will be shared in an open access report, and is part of a PhD-project on the role of digital humanities in European research libraries with the research question: What are the new roles and responsibilities of the European academic library since the computational turn in humanities research and how do these organisations deal with this change?. This survey marks the beginning of this research and subsequent case studies will follow from the survey results. The presenter of this poster welcomes input on themes to be addressed in the cases selected for more in-depth research. "
	},
	{
		"id": 425,
		"title": "ISEBEL an Intelligent Search Engine for Belief Legends",
		"authors": [
			"Ding, QiQing",
			"Meder, Theo",
			"Windhouwer, Menzo"
		],
		"body": " Distributed around the globe more databases of folktales, including belief legends, have come into existence. Combining them might open up new and exciting research possibilities. ISEBELis a project aiming to create a search engine that makes exactly this possible by providing unified search over the participants database, while dealing intelligently with the various languages. The following databases are currently providing their stories to the project: the Dutch Folktale Database, the Evald Tang Kristensen Collection, and the WossiDia archive. We are looking out to and working on including other interesting archives and languages throughout Europe - especially a few more Nordic countries already have shown interest, i.e., Iceland, Norway, Sweden and Estonia. In the project a set of metadata elements about the stories in the databases have been identified - apart from the text, there is information about narrator, time and place of narrating, keywords/tags and the URL leading to the story and metadata in the original database. These elements are part of a core metadata schema, which already provides hooks to be easily extended with moreinformation. The databases use this to provide metadata about their stories. The central catalogue of ISEBELruns a harvester periodically, which collects this metadata. The technical basis of this communication is provided by the OAI-PMH protocol. The collected metadata is loaded and indexed by CKAN, an open source data portal. The harvested data from each data provider are stored as individual XML files by the harvester. The XML files, which contain single stories in each one of them, are imported using local API calls to CKAN. Researchers can then search for the data through CKAN and its underlying indexer, namely Solr. The project is still under heavy development to reach its design goals, e.g., to provide researchers with cross-language search results rather than in only one language. In order to achieve this goal, the translation and keywords extraction must work together with keywords mapping to interconnect the stories and keywords in different languages. The domain specific keywords are the keywords specific to one language, which are either manually attached to the stories by the data provider, difficult to translate using machine translation or even not available in other language. Those keywords will be manually maintained and mapped to the computer generated keywords, discussed next, to make the mapping complete. The computer generated keywords are those keywords extracted from the English story translated from the local language of the data provider. According to the test runs, keywords quality extracted from translated stories are much better than translated keywords. The main reason is that the entire story provides more context for an adequate translation. At the moment ISEBEL is focussing on the specific genre of the traditional belief legend, mainly because all three databases have this folktale genre in common. These legends mainly deal with traditional folk belief in the supernatural, like ghosts, hauntings, devils, witches, wizards, spells, werewolves, nightmares, giants, trolls, goblins etc., as well as stories about hidden treasures, famous robbers, underground passages and sunken castles. The information on folktales collected from the initial three databases make it possible to analyze folktales and traditional folk belief in a large coastal region of the North Sea and the Baltic Sea, where especially Migratory Legends are interesting. Research can include geographic dispersion. For instance, the dispersion of legends about mermaids will most likely show narratives in the direct coastal areas. Another possible research could be gender related: what is the difference between male and female repertoires? How close do the legends play around the own home, and is there a difference here between male and female narrators? "
	},
	{
		"id": 426,
		"title": "Madgwas: a Database of Ethiopian Binding Decoration",
		"authors": [
			"Winslow, Sean M.",
			"Schneider, Gerlinde"
		],
		"body": " Ethiopia is home to the only remaining continuous tradition of widespread Christian scribal production, but the manuscripts produced by that tradition are little-studied and the resources for dating and describing Ethiopian manuscripts are few and poorly-developed compared to their European relations. Ethiopian manuscripts are an understudied but cognate part of the wider European/Mediterranean Christian manuscript tradition. Madgwas is a database for the identification, cataloguing, and dating of Ethiopian binding tools and decoration. It leverages European and international libraries increasing sharing of manuscript images through the International Image Interoperability Frameworkmanifests to produce a catalogue that links binding decoration, scribal tools, and individual manuscripts in a way that will serve a versatile set of researcher needs and drive research output for the project team. This poster will present the results of the first stage of project development, the ingest of the Ethiopian manuscripts hosted by the British Librarys Endangered Archives Program. "
	},
	{
		"id": 427,
		"title": "Migrating Charters into the TEI P5",
		"authors": [
			"Winslow, Sean M.",
			"Bürgermeister, Martina",
			"Vogeler, Georg"
		],
		"body": " Research projects often create domain-specific annotation vocabularies. This poster will present approaches to the modelling and migration of encoded charter data that arose during the migration of the Charters Encoding Initiativeto be compliant with the current version of the Text Encoding Initiative. It is part of a project to migrate and enhance encoded charter descriptions from the virtual charter platform monasterium.net in order to provide a well documented, reusable environment that prolongs the data life cycle. The migration follows established principles of data sustainability and interoperability. It relies on the ingest of data from Monasterium.net into the GAMS, a Fedora-Commons-based certified trusted digital repository at the University of Graz that handles preservation and publication, and also provides benefits like data visibility, unique handle references, and the provision of interoperable data via OAI-PMH. To achieve this, a new data model extension had to be developed in order to support both scholarly needs and the careful curation of data. The project evaluated which concepts from the charter domain are of wider importance. The new TEI P5 extension for charter-specific data, based upon the existing CEI, has to structure the data in a context-neutral manner that supports encoding diverse historical periods and regions using diplomatic TEI markup, including Ethiopian royal acts, Nepalese charters from Mustang, and early modern grants of arms fromMarburg. This justifies a data model extension in order to support both scholarly needs and the careful curation of data. It includes elements new to the TEI to model: Authenticating features like signatures, fingerprints, and notarial signa. Span-level features describing the conventional elements of documentary instruments. Person/Organization level legal actors. Status of documents as originals or copies and their relationship with other textual witnesses. Domain-specific annotation can be achieved additionally through the creation of structured ontologies, e.g. describing methods of authentication, types of manuscript additions, and heraldic blazonry to support the typing and subtyping of data. This enhances the possibility of semantic use in the principle of Linked Open Data. Faced with heterogeneous data from a variety of sources, the project involves a series of transformations where charter encoding is re-imagined and the CEI is re-modelled and transformed to the TEI P5 in a context-sensitive manner. This entails: Cleaning and rationalizing existing data in a more interoperable and cross-project comparable manner. Mapping of existing data to new standards and transformation into current, standards-compliant TEI P5 structure. The new data model will be tested through the development of facet-based search and predefined queries and visualization based upon scholarly needs of target audience of diplomatics, legal history, and art history scholars. All of this is part of a long-term project to develop tools that enable end users such as archives and individual scholars, as well as the repository monasterium.net, to describe historic legal data in a structured manner that is semantically interoperable with other historical data. "
	},
	{
		"id": 428,
		"title": "The Complexities of Video Games and Education: In the Library, the Museum, Schools and Universities",
		"authors": [
			"Wisdom, Stella",
			"Burn, Andrew",
			"Bushell, Sally",
			"Butler, James",
			"Zeiler, Xenia"
		],
		"body": " Introduction This panel explores several research projects that use video games and digital game making tools as methods for engaging learners of all ages with digitised collections from libraries, archives and museums to facilitate new understandings of historical and cultural events, or create new media adaptations and interpretations of classic literary works. The panel includes presentations from collaborative research projects, which investigate how video game software can be used in different learning environments to actively engage children, young people and adults with social issues, history and literature. These projects include a presentation on British Library digital initiatives, such as the Off the Map game design competition, online interactive fiction writing jams and their interactive fiction writing summer schools. There are then two speakers who have worked collaboratively with the British Library on educational video game projects. Playing Beowulf from University College Londons Institute of Education, is a project that has produced their own digital game authoring tool for schools, MissionMaker, which enables users to make their own adaptations of classic literary texts. Whereas educational Minecraft project Litcraft, led by Lancaster University, uses the popular Minecraft gaming platform to build accurate scale models of authorial maps from classic works of literature; which are then used as virtual environments for classroom learning activities set in locations of the book narratives. The fourth speaker examines educational videogames in India and includes an introduction to a new game project from the University of Helsinki. Who are collaborating with an Indian game development studio, to create a game based on the Indian festival Durgapuja, which is designed for international audiences and for use by numerous academic disciplines, for teaching aspects of contemporary Indian society and culture. [280 words] Places of inspiration; digital interactive writing and literary game making in libraries Interactive fiction and literary game making encourage writers and readers to explore new ways of engaging with stories and narratives. Furthermore, digitised collections from galleries, libraries, archives and museums can be used to create new media adaptations and interpretations of classic literary works. There are a growing number of easy to use digital tools offering new ways for library users to curate and tell stories beyond the printed book and the spoken word. This paper is an overview and evaluation of experiments and collaborations, which have used British Library digital collections in creative projects. It covers the Off the Map game design competition, which set UK higher education students the task of creating video games, text adventures and virtual interactive environments using digitised British Library assets including maps, views, texts, illustrations and recorded sounds as creative inspiration. These competitions were themed to coincide with British Library exhibitions on gothic literature, William Shakespeare and Alices Adventures in Wonderland. Submitted entries offered completely new interpretations of the Librarys collections and for the Alices Adventures in Wonderland theme a playable version of the winning game was included in the physical exhibition in the Library. This paper will also discuss the British Librarys involvement with online interactive fiction writing jams, ran in partnership with Surrey Libraries and Read Watch Play : a global online reading group that has monthly themes. In 2017 Odyssey Jam was held for the water theme and in 2018 a Gothic Novel Jam celebrated the 200th year anniversary of the publication of Mary Shelleys Frankenstein . UK game development company, Media Molecule, created an entry with their new game development engine Dreams . The Bitsy game development community also engaged, using their software to create 1980s retro style 2D games. Gothic Novel Jam received 46 entries from all around the world including the UK, Australia, America and France. Most entries were interactive fiction, 2D and 3D explorer games, but other types of submissions included a digital headstone generator, an atmospheric audio soundscape and a table-top role playing storytelling game based on a Victorian séance. Participants were encouraged to use the British Librarys vast collection of public domain images available on Flickr Commons. Often the images, like the gothic novel genre, was used as inspiration. However, some works incorporated the images as an integral part of the story; As a Glow Brings Out a Haze is an example of an entry that used the Librarys digitised book illustration images as a key element of the storytelling in the game. Finally, this paper will cover interactive fiction writing summer schools held at the British Library in 2017 and 2018. Using Twine, an open-source programme for writing interactive narratives, over five intense days, adult participants learned how to build non-linear digital story games with expert tuition from specialists in interactive storytelling. [465 words] Playing Beowulf: gaming the library This paper will present outcomes from two research projects, which developed a Unity-based games authoring tool, MissionMaker, to allow users to make their own adaptations of classic literary texts. One of these, in collaboration with the British Library, was the Anglo-Saxon poem Beowulf; the other, still in progress, features the Shakespeare play Macbeth. The talk will draw on examples from a range of participants of different ages, arguing that game-making can transform engagement with such texts, in terms of understanding of narrative, literary value and the processes of reading and interpretation. The projects also suggested the value of games as a way of animating the archive: of inserting historical fictions into contemporary landscapes of play. Finally, the conjunction of literary adaptation, game design and rule-making, also suggested how games allow a bridging of the notorious and persistent arts-science divide: how literary narrative can become computable, and how computer science can craft poetic forms. [153 words] Litcraft: re-engaging children with literature through Minecraft This paper will be about a project undertaken in partnership with The British Library and The Wordsworth Trust as part of the Chronotopic Cartographies project. The paper is centred on Litcraftwhich uses the popular Minecraft gaming platform to build accurate scale models of authorial maps from classic works of literature. Impact is achieved by re-engaging childrenwith literature in a model of positive reinforcement that makes works accessible in entirely new ways, combining the textual and the digital. Reading and writing are integrated with an immersive experience of the literary world. The paper will be in two parts, relating to our work with museums and libraries. The first part articulates the original development of the Litcraft project out of a pilot project called Lakescraft, which used an Ordnance Survey Minecraft map of the Lake District to embed interdisciplinary activities into the landscape. The Litcraft project further builds on this model directly in work undertaken for The Wordsworth Trust, Grasmere. As part of the re-design of the museum site for 2020 we are creating a digital resource that maps Wordsworths autobiographical poetic memoriesin the landscape of the lakes. This resource will then be integrated in a central corridor between museum spaces to engage children with the landscape in an imbedded way through Litcraftbut will also connect children actively to items in the museum itself. The second part of the paper will be on our work with libraries through the Litcraft in Libraries scheme. As well as having the British Library as a major partner we also work with five different regions in the UK to deliver the resource to children through the library space. The paper will describe the resource created for Robert Louis Stevensons Treasure Island. In preparation for the immersive activity, children read and study the character of Jim Hawkins; they read the chapter My Shore Adventure in which Jim hides in the boat and runs ashore against the wishes of the adults. Children then enter the Minecraft worldand themselves run all over it finding various objects and using Stevensons map to help them negotiate the digital world. Textual and immersive worlds work together to increase empathetic and spatial understanding and motivate both reading and writing. We will talk through the resources themselves as well as the range of ways they have been delivered across libraries. Litcraft has the most powerful impact on pedagogy and on individual readers – potentially changing reluctant readers into lifelong readers. But it also has an impact on use of, and numbers of, visitors to libraries and museums and on larger accessibility to and understanding of literary culture and heritage. [497 words] The complexities of video games as teaching tools: educational game development for teaching indian culture and society While DH is often understood to first and foremost develop and provide digital tools and methods to support Humanitiesinformed research, in a wider understanding, the discipline also comprises the researching of the interconnections of digital technology and culture at large. This includes video games, as today, games actively contribute to construct perceptions of norms, values, identities, and society in general. Games are increasingly complex, interactive virtual worlds which, among other things, can be used for transmitting information or knowledge about certain subjects. Especially educational games do so in a very conscious and straightforward way. They are developed specifically to either teach or, in a more subtle way, to draw attention to and offer background knowledge on certain topics. A number of studies highlight the additional benefits of the immersion and emotional factors which educational games offer, as an additional value in education as compared to traditional teaching. For instance, currently and within the larger video game boom in India, we find an innovative development: some pioneering games from especially so-called indiestudios produce educational games, which take up specific issues in society and try to bring them to the awareness of a broader audience – to educate a larger audience about a specific problem or issue. The game MISSING: A GAME FOR A CAUSE is a perfect example for this. The game is part of an art project, which was initiated to draw attention to the abduction of girls in India. In India today, about 3 million girls are trafficked every year. This game attests for the changing landscape in Indian game development, and for the increasing awareness of game developers just how much games can interrelate with society. Recently, we find additional efforts also in the academic sector. This paper will introduce to the first educational video game for teaching contemporary Indian society and culture, which is in the final development phase. The joint effort of the speaker and an Indian game development studio produced a game introducing to selected core aspects of the highly popular Indian festival Durgapuja. This game version is targeted at university students with little or no background knowledge on the subject yet; that is, it is designed as an introduction to contemporary Indian culture and society. The game will be available through open access and can be used internationally and in numerous disciplines. [393 words] "
	},
	{
		"id": 429,
		"title": "E-Lexicography Between Digital Humanities And Artificial Intelligence: Complexities In Data, Technologies, Communities",
		"authors": [
			"Wissik, Tanja",
			"P. McCrae, John",
			"Buitelaar, Paul",
			"Tasovac, Toma",
			"Tonra, Justin",
			"Zaytseva, Ksenia"
		],
		"body": " Lexicography is currently embracing rapid change as the traditional methods of publishing dictionaries are replaced by the ubiquity of lexical information on the Web. Furthermore, the application of computational techniques to the processes of lexicography is revolutionizing how dictionaries can be constructed. In this context, the recently established ELEXISproject aims to develop an infrastructure for eLexicography across Europe, that builds a virtuous cycle of lexicography where lexicographic resources are linked across languages, in order to build improved natural language processing tools, which can then aid in the construction of novel resources and retro-digitization of dictionaries, thus driving the cycle. The project is multilingual, covering 15 European countries, and has a strong interest in driving lexicography for under-resourced and minoritized languages. The event will be the second iteration of a highly successful workshop first run before the EADH Conference in Galway on 6 December 2018. On that occasion, the Workshop was the highest-subscribed of the ten pre-conference workshops, with 33 regular registrations and approximately fifty attendees from a range of backgrounds in lexicography, computer science, and the humanities. Specific emphasis in this edition of the workshop will be on complexity in regard of data, technologies and community aspects of lexicography. Complexity in data concerns issues in access to lexicographic data across stakeholders, representation formats, linguistic assumptions, underlying theories and scope of analysis and representation, legal restrictions and licenses, multilingual, cross-lingual, comparative and typological issues, as well as advanced aspects in multimodality, concerning audio-visual representation. Complexity in technologies concerns challenges in creating and expanding novel lexicographic resources, which require a combination of many technologies, including natural language processing tools as well as machine learning approaches and AI methods in general, for data linking and data management, in order to identify and represent words, their senses and definitions. Complexity in communities, in part, lies in differences related to stakeholders type and status, status of the language in question, and involvement in networking activities. In addition, the complexity is related to several academic disciplines involved in eLexicographic research such as linguistics, natural language processing, digital humanities, artificial intelligence, computational linguistics, computer science, etc. This constitutes a challenge to provide professionals with training opportunities and ensure knowledge exchange among all stakeholders. The workshop has three main aims: firstly, we will invite speakers from existing major lexicographic projects to give insights into the complexity in regard of data, technologies and community aspects of lexicography. Secondly, we will provide a hands-on tutorial with the ELEXIS infrastructure to enable participants to become familiar with the technologies being developed in the project. Finally, we will make an open call for posters, which will provide an overview of new projects in the area of electronic lexicography, with a special focus on papers that tackle the topic of complexity. The poster presenters will give a 5 minute lightning talk on their topic in addition to their contribution to the poster session. Topics of Interest Lexicography and Digital Humanities Complexity in Data for Lexicography Complexity in Technologies for Lexicography Complexity across Lexicography and Related Communities Access and usage of dictionaries on the Web Retro-digitization of lexicographic resources Lexicography for language learning Use and applications of NLP for lexicography Lexicography for under-resourced languages Lexicography for terminology and translation Linked Data for lexical resources AI for Lexicography Summary of the Call We welcome submissions of abstracts of up to 500 words that will be presented as posters at the workshop. Submissions should present methodologies, experiments, use cases, descriptions of ongoing or planned research projects and position papers on topics related to the topics of interest. Furthermore, we especially welcome papers describing interdisciplinary research combining research in lexicography, linguistics, computer science and digital humanities approaches and giving insights into complexity in regard of data, technologies and community aspects of lexicography. Please submit abstracts by May 6th 2019 in any language. Submissions will be reviewed by at least 3 reviewers and will be made available online prior to the workshop. Papers should be submitted via EasyChair. Notifications will be sent by end of May and final versions of abstracts will be required by end of June. More information on the workshop can be found on the workshop website. Tentative Schedule 9:00-9:30 Invited talk: Unity in Variety: Observation and Lexicographic Treatment of the German Standard Variety used in South Tyrol Andrea Abel9:30-10:00 Invited talk: Framing in the Dutch Language: from structured data to text and back from text to structured data on situations Piek Vossen10:00-10:30 Introduction to ELEXIS infrastructure and technology 10:30-11:00 Coffee break 11:00-12:00 Lightning Talks 12:00-13:00 Poster session Organizing Committee John P. McCrae is a lecturer above-the-bar at the National University of Ireland Galway in the school of information technology. His work has focussed on the application of linked data to language resources. In particular, he is the original developer of the lemon-OntoLex model, which has become a de-facto standard for representing lexicons on the Web. In addition, he is a board member of the Global WordNet Association. He has also organized many eventsAddress: Insight Centre for Data Analytics, National University of Ireland Galway, Ireland Email: john.mccrae@insight-centre.org Paul Buitelaar is a Senior Lecturer at the National University of Ireland Galway, vice-director of the Insight Centre for Data Analytics at NUIG and head of the Insight Unit for Natural Language Processing. His main research interests are in the development and use of Natural Language Processing methods and solutions for semantic-based information access. He has been involved in a large number of national and international funded projects in this area. In recent years he was involved in the development of the Saffron framework for knowledge extraction and the definition and implementation of lemon, a vocabulary for Linguistic Linked Data. Address: Insight Centre for Data Analytics, National University of Ireland Galway, Ireland Email: paul.buitelaar@insight-centre.org Toma Tasovac is Director of the Belgrade Center for Digital Humanitiesand Director of the Digital Research Infrastructure for the Arts and Humanities. His areas of interest include lexicography, data modeling, TEI, digital editions and research infrastructures. Toma was previously a Steering Group member of the European Network for eLexicography, and is currently also affiliated with the European Lexicographic Infrastructure. Address: Belgrade Center for Digital Humanities, Belgrade, Serbia Email: ttasovac@humanistika.org Justin Tonra is Lecturer in Englishat the National University of Ireland Galway. His areas of research interest include digital approaches to literary studies, book history, textual studies and bibliography, scholarly editing, and literature of the Romantic period. He is currently joint National Coordinator for DARIAH Ireland, and a working-group leader for COST Action CA16204 Distant Reading for European Literary History. Address: English Department, National University of Ireland Galway, Ireland Email: justin.tonra@nuigalway.ie Tanja Wissik is a senior researcher at the Austrian Centre for Digital Humanitiesof the Austrian Academy of Sciences and teaches at the University of Graz. She holds a PhD from the University of Vienna in translation studies with a specialization in the field of terminology and corpus linguistics. She has been working in numerous research projects related to language resources and language technologies and is involved in outreach and network activities. Address: Austrian Academy of Sciences, Austria Email: Tanja.Wissik@oeaw.ac.at Ksenia Zaytseva is data analyst at the Austrian Centre for Digital Humanitiesof the Austrian Academy of Sciences. She is primarily involved in development of tools and services for DH projects in archaeological and linguistic domains. Her main research interests are Semantic Web technologies, LinkedData, controlled vocabularies and reference data services. She is also interested in scientific Python programming, machine learning and web application development. Address: Austrian Academy of Sciences, Austria Email: ksenia.zaytseva@oeaw.ac.at The workshop is supported by the two EU projects ELEXISand Prêt-à-LLOD, and DARIAH-IE. Programme Committee Fahad KhanMonica Monachini* Rute CostaFrancesca FrontiniAndrea BellandiChristophe RocheBolette Sandford PedersenChristiane Fellbaum* Philipp CimianoSimon KrekVera HildenbrandtKarlheinz Mörth* Thierry DeclerckKatrien Depuydt* Christian ChiarcosMonika Rind-Pawlowski* Alexander Geyken* * Awaiting confirmation "
	},
	{
		"id": 430,
		"title": "Active Learning from Scratch in Diverse Humanities Textual Domains: Optimizing Annotation Efficiency for Language-Agnostic NER",
		"authors": [
			"Erdmann, Alexander",
			"Wrisley, David Joseph",
			"Joyeux-Prunel, Béatrice"
		],
		"body": " Interest among Digital Humanitiespractitioners in semantic annotation of corpora continues to grow, while at the same time linguistic resources for niche domains or languages underperform, or are simply unavailable. Since DH research often involves multilingual and multi-domain corpora—drawing on different genres, periods and cultures, and ranging in type from unstructured prose to semi-structured text—annotation can require significant time and language skills that not all contributors in research groups have. Annotation in collaborative projects is a case in point. Furthermore, off the shelf tools for annotation, despite being trained with language agnostic architectures, only process one language in one domain at a time. Since available tools do not perform well for niche domains, new semi-automatic semantic annotation solutions must be sought for creating data for downstream DH tasks. For annotating diverse corpora, we propose a language-agnostic, robust and customizable named entity recognitionresource. It can identify any type of user-specified entities in any textual domain, although here we have largely focused on the annotation of place names for use in the spatial humanities. Instead of relying on pre-existing NER taggers not tailored to the domain of interest, our resource enables humanists to build their own customized NER taggers. Such taggers require manually annotated data to learn to identify named entities automatically. However, since annotation is costly, we propose an active learning pipeline in which the most informative sentences in a corpus are identified prior to annotation. Our approach is to optimize performance while minimizing the time and energy of exclusively manual annotation. To this end, we developed the Humanities Entity Recognizer, which uses the conditional random fieldmachine learning architecture, both to identify sentences containing named entities most crucial for annotation and to identify named entities once sufficient manual annotation is completed. The system is designed to work with unstructured texts in any language, especially humanities texts where entities are unevenly distributed. It assumes no language resources beyond a tokenizer. Our inspiration comes from related NER research that demonstrates that feature-based architectures like CRFs can, in fact, be language agnostic. However, such systems do not anticipate noisy DH domains. A promising active learning strategy for low resourced neural NER exists, although it requires computational resources we do not assume to be available to most humanities researchers. Finally, a transfer learning solution has been proposed, whereby a model for the under-resourced Uyghur language leverages better resourced NER models for two related languages. This pipeline however, requires access to multiple specific sources of data beyond the average humanities data scenario. Our design merits particular attention for its careful consideration of the workflows of the humanist. Using limited entity lists, we delexicalize features and introduce other factors to encourage the algorithm to generalize rapidly in identifying new entities with high recall, sorting sentences based on likelihood to contain frequent, previously unannotated named entities. The system learns on its own that capitalization is important in languages where it is, but can ignore capitalization in other languages. Since the textual scholar working with the HER system is actively involved in the iterative annotation and correction, a choice was made to favor recall over precision for the simple reason that it is easier to remove, or hand correct, an inaccurate entity than to lose one in the black box. The annotation process on the initial seed and successive batches resembles a close reading of the corpus from the perspective of its potential named entities. At present the scripts work on the command line and subsequent annotation is carried out in a text editor. Integration into community-based, social annotation interfaces is a desirable next step, but beyond the scope of this paper. Our choice of CRF machine learning stems from a commitment to under-resourced domains, since neural models are notoriously data hungry. Nonetheless, we tested neural modelsextensively both for active learning and/or for performing the final NER tagging, with the result that CRFs outperform them until the amount of manually annotated data exceeds about 30,000 words. Should the user exceed this threshold, HER also supports use of neural models. Additionally, the interpretability of CRFs allowed us to maximize multiple criteriawhen choosing the best sentences to annotate, whereas the neural models performed poorly for active learning, only capable of predicting sentences uncertainty. We also introduce the notion of an inclusive evaluation framework whereby the accuracy of the model is determined by both manual and automatic annotations, rather than exclusive frameworks that look only at the final models prediction on a held out test set. Figure 1: Inclusive and Exclusive Evaluation of Learning Architectureswith the Humanities Entity RecognizerThe code and documentation for the Humanities Entity Recognizerare freely available at http://github.com/alexerdmann/HER. Our research stems from a session of a 2018 NYU-PSL Global Alliance funded workshop devoted to named entities and spatial humanities research. We began with non-English materials already annotated for named entities: the FranText corpus. We chose six sample texts across very different domains—a travel narrative, a gastronomic treatise, novels, an autobiography and a memoir—of varying named entity densities and distributions. The texts proper names were further annotated to distinguish place and person names. Performance of models trained using the HER system for annotation increased significantly. After annotating just 40,000 words, error was reduced 68.6% as compared to annotating 40,000 words from randomly selected sentences. In addition to this previously annotated corpus—not the norm for most DH research—we worked with three other unannotated corpora of significant typological and structural difference: A German corpus of approximately 1M wordsfrom the Weimar period, partially sourced from Project Gutenberg; A medieval French corpus composed of 1.1M words in both prose and verse for which a pre-existing placeography was available; A Portuguese corpus consisting of approximately 250K words from catalogs of the São Paolo exhibitionscontaining a significant amount of structured informationfor which a personography and placeography were available from the Artl@s Global Exhibition Catalogues Database. We discuss results using these corpora, as well as challenges encountered using the Humanities Entity Recognizeracross diverse domains and different kinds of entities. We report performance over a learning curve of quantities of manual annotation and qualitatively evaluate predictions in the absence of gold-standard data. We conclude with our key contributions in the development of this system: whiteboxing the NER process, handling totally unresourced domains and messy corpora in a language agnostic manner, and designing complete flexibility to granularity and types of entities. "
	},
	{
		"id": 431,
		"title": "Complexities in the Use, Analysis, and Representation of Historical Digital Periodicals",
		"authors": [
			"Wulfman, Clifford Edward",
			"Rusinek, Sinai",
			"Segal, Zef",
			"Rißler-Pipka, Nanette",
			"Ketchley, Sarah",
			"Roeder, Torsten",
			"Bunout, Estelle",
			"Düring, Marten"
		],
		"body": " Panel introduction The theme of this conference is complexities, and there are few printed media as complex as newspapers and magazines. They are, generally, serial publications, frequently with miscellaneous content and complex page layout, and often entailing complex publication relationships among authors, editors, and publishers. They have generally been poorly served by conventional research libraries, which have often discarded covers and advertising in the desire to conserve space and whose catalogues seldom represent the actual nature of their holdings. The digital remediation of periodicals has also been complex. Digitization is a complex term with a variety of meanings and consequently a variety of products, each with different stakeholders. Librarians want simple formats that can be produced, catalogued, stored, and delivered in standard ways. Scholars of visual arts want pages images of very high resolution and chromatic fidelity; many linguists and textual scholars dont care about pages at all but are keenly invested in machine-readable transcriptions of complete texts that are often spread across different pages and different issues. Scholars of print need information about paper types and typography. Repositories need information about copyright. Although most text-based scholarsseldom venture beyond the downloaded PDF and token search, some have begun to engage digital textuality in more complex and sophisticated ways, and these engagements have exposed some of the limitations of standard approaches to digitization, which often focus on graphical user interfaces to page images and the production of efficient inverted indexes at the expense of the periodical corpus. This panel follows a session at the 2014 Digital Humanities Conference in Lausanne, Switzerland, entitled Remediating 20th-Century Magazines of the Arts: Approaches, Methods, Possibilities, which stimulated an ongoing series of discussions and calls for the formation of an ADHO SIG devoted to Periodicals. Since then, projects large and small–Europeana Newspapers, the Chronicling America Data Challenge, Traslantis, the Viral Texts Projectand, more recently, Impressoand Newseye–have begun to go beyond the retrieval paradigm to explore other methods and approaches to newspapers. This panel showcases the complexity of periodical studies and the many ways digital technologies enable them today. „Horizontal reading in a corpus of 19th century German music magazines and daily newspapers: analysing the debates on Verdis Messa da Requiem Perhaps even more than today, musical concerts in the 19th century were most relevant social and cultural events. Whenever a new musical piece was performed, detailed reviews were published in local newspapers, discussing musical ideas as well as the quality of composition and performance. The discussions were usually published in the feuilleton and in many cases even on the title page. Music critique was part of a general cultural discourse, connected to political opinions, religious beliefs and social imprints, and newspapers were a relevant forum for these debates. In the late 1860s, the number of German newspapers increased immensely, due to less restrictive press laws and to advanced printing techniques as well. It was in the mid-1870s when the newspapers announced a new piece by the Italian opera composer Giuseppe Verdi: The grand Messa da Requiem, a commemorative work for orchestra, choir and soloists, for the deceased Italian writer Alessandro Manzoni. The first performances in Austria and Germany induced a discussion about theappropriateness of operatic style in the genre of sacred music, and if church music should ideally be free of realism and overwhelming or untrue emotion. The research described here focuses on analyzing the ideals of requiem music from the perspective of German music critique and compares them on regional and local levels. The analysis was supported by TEI-encoded transcriptions of all relevant German articles between 1874 and 1878, resulting in a corpus of 330 texts and approximately one million characters. The markup focuses on semantics: About 8,000 entities and relations, amongst them more than 500 individual persons, 140 performances, 100 geographic items and 135 musical works, and 100 cases of text reuse were identified. The corpus was then analyzed from different perspectives. Based on the semantic markup, each partial analysis started from one single entry point, extracts all occurrences of one semantic unit, including some context, and compares all these in respect to publication journal, place and date and possibly authors background. A delicate matter is the extent of context, which operates sufficiently on phrase segments in most cases. This method of extracting and parallel reading the contexts of one single semantic unit, or metaphorically: »horizontal« reading, helps identifying patterns in the reception. For example, the part Agnus Dei, a very popular piece which was mentioned in more than 60 texts, was strikingly often compared with other known opera numbers for its tune, which suggested that Verdi could have copied the melody. While musicological research has so far been limited to individual critics, the new method can be used to compare different receptions to each other without requiring background information on the single authors, which is often not available. However, this method requires accessible digital copies of newspapers, reliable metadata, high quality OCR or transcriptions, phrase segmentation, and tagged semantic entities. Re-discovering the „hidden women of 19th century Egyptology through thematic analysis of historical newspapers The end of the 19th and beginning of the 20th centuries saw great archaeological activity in Egypt, a period that came to be known as the Golden Age of Egyptology. My research seeks to re-examine the ambiguous and liminal position of women attached to these archaeological digs and expeditions. The starting point was the unpublished travel journals of Mrs. Emma B. Andrews, who for over two decades between 1889 and 1914 traveled the Nile with the millionaire lawyer turned archaeologist, Theodore M. Davis. Andrews was present when Davis discovered eighteen of the forty-two tombs now known in the Valley of the Kings. and her writings provide a detailed record of excavation often lacking in contemporary publications. My initial work included transcribing and encoding the diaries in TEI XML, tagging named entities that have formed the basis of a research database and a digital edition. The research corpus has expanded to include material written by the wives, relatives, and friends of expedition directors, archaeologists, artists, and photographers working in Egypt at that time, and comprises diaries, correspondence, historical ephemera and a large number of historical newspapers. These documents give the historian an overview of the social, geographical and political history of Egypt at the time within the broader context of the histories of archaeology and Egyptology, of gender studies, and of the social, cultural and political history of the Victorian era. Working with a large corpus of digitized newspapers has shifted the work away from granular markup towards broader thematic analysis, including topic modeling and ngram analysis. This paper focuses on the process and challenges of extracting significant information from collections of digitized newspapers of varying quality and content using computational methods, using material drawn from archives including Chronicling America, Gale Primary Sources, Europeana Newspapers, and the British Library. Relevant newspaper articles, advertisements, and editorials have been compiled from keyword searches, based on lists generated from the named entities captured during the markup of diaries and correspondence. This has been a simple yet effective way to create relevant datasets - a task that would otherwise have been slow and unwieldy given the scope of the source material. The main challenge has been poor-quality OCR, which has affected returned search results. This has led to the development of strategies for cleaning bad OCR programmatically using NLTK and RegEx, as well as with digital tools developed for the purpose, including Lexos and Gales Digital Scholar Lab. Work beyond text cleaning has focused on analyzing curated newspaper datasets in an effort to identify prevalent cultural and historical themes using topic modeling tools and ngram analysis. The aim is to explore Egyptological womens writing within this cultural context, to develop a picture of everyday life from multiple historical primary source material, a topic that is often obscured from an historians view. The periodical as a geographical space: the 19th century Hebrew HaZefirah as a case study A historical periodical, just like any historical event or phenomenon, is defined in space and time. Its institutions operate from actual buildings; it is circulated via transportation routes; and through its reader network it connects between places and people. However, the relation between the two concepts, geographical space and periodical, is not one-way. Just as much as the periodical is within a geographical space, one can and should explore the ways in which geographical spaces are within periodicals, and most importantly, are created by them. Many periodicals produce an internal geographical space, whose attributes and connections result from the text and inner conventions of the periodical, which might include the distinction between different pages, articles, and sections, or the distinction between places that appear in the headlines and places that appear in the body of the text. Geographical places, such as cities, mountains, rivers, states and continents, are located on paper rather than on the globe; their sizesare defined by the frequency of their references, and the distances between the places are defined by proximity on the line, page or article. These geographical spaces that emerge from the text are mediated geographical spaces with completely different shapes and topographies than the normal world map. They are influenced by cultural, political, ideological and economic perceptions of writers and readers of the periodical, but they also reinforce such perceptions and legitimize them. Recent advances in digitization, automatic annotation, morphological and lexical analysis, as well as geographic and network mapping, have made the exploration of such uncharted and chaotic spaces more feasible than before. This paper will use the nineteenth century Hebrew newspaper HaZefira as a case study for different geographical spaces that originate from and within the newspaper. By alternating between various ways of representing the geographical data in the journalistic text, I will show that spaces are constantly being deterritorialized and reterritorialized. Hazefira, which operated between 1862 and 1931 was established in order to educate its readers with worldly knowledge. Its articles discussed global news, travel stories, and scientific discoveries, all of which involved spatial references. This paper will discuss the different ways of exploring the relations between textual spacesand their spatial references, the digital methodologies that assist the unveiling of these spaces, and the problems that are still left to be dealt with. Reconceptualizing resources: transforming Blue Mountain into TEI-encoded editions and a bibliographic knowledge base This paper presents work being done by the Blue Mountain Project to create TEI-based editions of its magazines and a bibliographic knowledge base of avant-garde publication that leverages its detailed bibliographic metadata and the resources of the semantic web. The Blue Mountain corpus of European and North American Avant-Garde periodicals currently contains full runs of thirty-four titles in eleven languages, with full text encoded in METS/ALTO and descriptive metadata in MODS. Blue Mountains rich and extensive metadata are not well represented in the content models predominant in digital library repositories, models based on page images of books and manuscripts. Furthermore, traditional bibliographic metadata often fails to capture important information about periodicals, including periodicity, issuance, and republication. The paper describes the reconception of Blue Mountains digital objects from library resources to digital editions, and of its metadata from descriptive catalog records to a bibliographic knowledge base. It describes our transformation of METS/ALTO documents into TEI-encoded XML and IIIF manifests, as well as our conversion of Blue Mountains MODS data to PRESSoo, part of a family of models that enable analysts to capture the temporal nature of publication as well as the inter-relationships among a variety of agents and entities, allowing researchers to ask questions not easily answered before. A generous design of interfaces as an answer to the complexity of digitized newspaper collections The digitization of historical sources does not automatically translate into accessibility for historical research. The publication of digitized newspapers and periodicals still leaves researchers willing to use them, hamstrung by interfaces that offer little interaction other than search functionalities. The insufficient interactions offered by current interfaces derive from their technical and material background but also from the limited information collected so far on the actual needs of the users, especially for research. Research in the humanities is characterized by iteration, discovery and exploration, especially given the huge volume and inherent complexity of historical newspapers. Digitized newspapers offer opportunities for enrichment which in turn should allow the development of novel research workflows which go beyond mere content retrieval by means of keyword searches and get us closer to humanist research practices. The ambition to offer room for more flexible and open interaction with digitized cultural heritage collections has been captured by Whitelawunder his call for more generous interfaces. The interdisciplinary project impresso - Media Monitoring of the Past aims at materializing this call with the development of a methodologically reflected technological framework to enable new ways of engaging with multilingual digitized historical newspapers, mainly from Switzerland and Luxembourg. In other words, impresso works on the conception of a generous interface by fostering synergies between computational linguists, designers, humanist researchers and institutions holding the collections. Our proposition for the implementation of generosity is to enable users to interact with primary sources in a digital environment using all possible entry points resulting from this transformation. In our case, such interactions are made possible by OCR improvement, annotations of named entities, topics and text reuse detection as well as close and persistent cooperation with historians and designers. We make use of newspaper metadata provided by the partnering libraries, the metadata produced during the compilation of digital collections e.g. the quantity of words, pages, issues, as well as annotations such as the segmentation of the document, named entities, topic modeling and text reuse. We integrate indicators of the quality of the digitization, e.g. OCR quality, missing issues, pages. Each of these items opens a distinct entry for search, exploration and analysis. These added layers of information require a suitable interface to be usable for researchers. This interface can facilitate the navigation of large-scale collections, both to get an overview of the general collection made available but also to get an overview of the content of a particular query or a particular collection of articles, curated by a user. The impresso interface provides space for a visual exploration of the collections, using filters to customize the visualization based on the mentioned metadata, as well as automatically extracted named entities, topics and text similarities. This push towards the design of a generous interface should encourage novel research workflows and a more reflective use of historical newspapers. Opportunities and limitations of research on historical periodicals in the era of digitization: usability of digital collections in the light of different modes of reception This paper reflects the initial discussions of the newspaper and magazine working group in the Digital Humanities in the German-Speaking World Association. Before the new opportunities of digitization emerged in libraries all over the world, historical periodicals were a rare and difficult field for both researchers and librarians. In cultural and media history, only few titles were in focus of research. In literary studies, periodicals were only consulted for first editions by canonical authors. In economics, media studies and social sciences, historical periodicals were observed for reader and consumer behavior. For linguistics, historical periodicals showed changes in non-fictional language though building up a corpus was very difficult before digitization. Today, we can observe two ways of dealing with mass digitization. For libraries, there is always the question of emphasizing the quantity or the quality of digital editions. Both strategies have advantages: emphasizing quantity yields access to a wider range of titles; emphasizing quality leads to better access, better searchability, and greater re-usability. Likewise, researchers take advantage of the opportunities offered by digitization in two ways. Those simply looking to read content in hard-to-find titles are happy with the quantitative approach: a badly scanned page image is sufficient for their needs. Researchers in a DH context, however, whose research entails establishing author networks, layout analysis, or topic modeling, are frustrated by the quantitative approach and dependent upon it at the same time. Still other researchers are hampered by the loss of materiality inherent in digital editions. It is time to open up a discussion about how the different players in the process of digitization could work together to overcome these obstructions. We need to establish and compare the requirements of the different stakeholders as well as the status of the current landscape of digitized historical periodicals. At the moment there is a very active movement in research on historical periodicals and some are already considering digital collections as a knowledge systems and not only as containers of text, images, and other pieces of information. "
	},
	{
		"id": 432,
		"title": "The PARTHENOS Training Suite: Empowering eHumanities and eHeritage Research(ers) with essential Knowledge and Skills",
		"authors": [
			"Wuttke, Ulrike",
			"Neuroth, Heike",
			"Laura, Rothfritz",
			"Jennifer, Edmond",
			"Vicky, Garnett",
			"Frank, Uiterwaal",
			"Marie, Annisius"
		],
		"body": " Brief Summary In this long paper the main principles, insights, and results from the development of the PARTHENOS Training Suite will be presented to the international Digital Humanities community in order to stimulate the discussion of Digital Humanities pedagogy and to stipulate the further uptake of these resources. PARTHENOS develops educational resources that focus on fundamental, interdisciplinary knowledge and skills that facilitate successful engagement and use of digital research infrastructures for digital humanities and cultural heritage scholarship in an increasingly complex, networked, and open environment. These resources are brought to the target groups and audiences in appropriate ways based on didactic and practical insights, using up-to-date means of communicating knowledge and information. Background Digital Research Infrastructures play an increasing role in the Humanities and in Cultural Heritage Studies2017). While some definitions of research infrastructures focus resources and tools provided, that is their hard, rather technical aspects, more and more their soft aspects, that is their function as networks of knowledge and people come into focus, with both aspects being in the service of aggregating resources to make us better connected and more informed. Digital research infrastructures are relatively recent additions to the humanities and cultural heritage landscape; therefore, their transformative presence needs to be embedded in university curricula and beyond because the ongoing digital transformation of research affects not onlyresearchers, but also cultural heritage practitioners in the sense of lifelong learning. Those who use digital research infrastructures for their research and those who contribute to their development and their extension alike, need to acquire additional theoretical knowledge and practical skills to fully harvest the fruits of their resource-intensive development and sustenance and to make use of their potentials to advance research. This need for a broad implementation of data skills into curriculum frameworks and training for all disciplines was only recently underlined by the European Commission Expert Group on FAIR data. Before the start of PARTHENOS, a cluster project that consists of important European eHumanities and eHeritage Research Infrastructures and institutions and is funded by the European Commission, eHumanities and eHeritage infrastructures already offered training events and contributed to the development, extension, and sustenance of platforms for online training materials or course overviews, e.g. #dariahteach or the DH Course Registry. However, many of these training materials and events are highly project specific or thematically or methodologically specialised and require prior knowledge that most potential new users often lack. This knowledge gap not only prevents them from successfully engaging with the material, it often forms a barrier that prevents them from taking interest into infrastructural themes in the first place. PARTHENOS aimed to fill these gaps by developing educational resources that focus on fundamental, interdisciplinary knowledge and skills that facilitate successful engagement and use of digital research infrastructures for digital humanities and cultural heritage scholarship in an increasingly complex, networked, and open environment. These resources are brought to the target groups and audiences in appropriate ways based on didactic and practical insights, using up-to-date means of communicating knowledge and information. Developing PARTHENOS Training The development of the cross-disciplinary PARTHENOS training materials is spearheaded by the PARTHENOS Training Teamwith additional input from the User Requirements Team. Conforming to the results of the analysis of the user requirements carried out in the first phase of the project, PARTHENOS Training focuses on online-materials for self-study and for reuse by educators for teaching purposes in different contextsusing the PARTHENOS Training Suite as its main carrier. Additionally, the training-portfolio encompassesevents to test the educational materials against users, such as the PARTHENOS eHumanities and eHeritage Webinar Series. PARTHENOS Training develops on the basis of the wealth of knowledge inherent in the project through its project partners introductory, cross-disciplinary learning resources that address infrastructural metatopics and are suited to foster the skills and knowledge that empower and unleash the potentials of eHumanities and eHeritage research. The focus is on knowledge transfer about the roles, functions, and potentials of eHumanities and eHeritage Research Infrastructures for researchers, practitioners, developers, data and computing centre staff, policy makers, and managers. During the first phase of PARTHENOS Training the focus was on more generic levels of information. The first modules were centred around the creation of general knowledge, skills, and abilities that foster the understanding what eHumanities and eHeritage Research Infrastructures are, how they can be of benefit for different communities, and what kind of knowledge and skills are needed to successfully work with them. After an intensive assessment exercise, the focus was shifted in the second phase to more specialized areas, especially highlighting how outputs and products of Research Infrastructures and particularly of PARTHENOS can help users to navigate successfully through the increasingly complex Digital Humanities and Cultural Heritage research landscape. The modules created in the second phase address topics such as research data management, creating and assessing research impact, the use of community standards and ontologies, and how to develop research questions using Digital Humanities methods and tools from a broader Humanities and Cultural Heritage perspective. Additionally, all the materials of the PARTHENOS eHumanities and eHeritage Webinar Seriescan be accessed via the PARTHENOS Training Suite for reuse. Guiding Principles and Modes of Delivery The educational resources produced by PARTHENOS address two levels of user needs: the need to know aboutand the need to know howand are presented online mainly via the PARTHENOS Training Suite Website, a WordPress-based eLearning platform. In contrast to print-based learning and teaching resources, the modules of the PARTHENOS Training Suite make use of up-to-date means of communicating knowledge and information, and technologies, allowing multimedia content and multimodal forms of acquisition and delivery of knowledge and skills by learners and teachers. Within the modules the range of materials consists of video-lectures, interviews, shortexplanatory clips, presentation slides, exercises and explanations of basic principles, case studies, collections of further links and reading suggestions, and brochures. The Training Suite consists of several modules that allow flexible access. The users are guided through the modules in a linear fashion for better orientation, but can also access specific points of interest via the side navigation menu. Through this they have full control over the learning process and can chose which materials they want to explore in more detail as well as different modes of mediation. PARTHENOS provides its training materials as Open Educational Resources. Thus, it takes up one of the foundational principles of the culture of the Digital Humanities that is based on sharing and reuse. It also adheres to the principles of co-creation during the development process, another principle of the Digital Humanities. There is an ongoing process of exchange with the PARTHENOS partners and beyond about the further development of the training materials. This intensive exchange aims to ensure the uptake of new developments in the ever-changing complex world of digital infrastructures and eResearch methods and tools in the Humanities and Cultural Heritage field. It ensures the direct integration of research communities into the development processand enhances the innovation potential and quality. PARTHENOS partners not only to develop materials, but also to facilitate their uptake, either by inclusion into other educational contexts, but also by using our partners networks to spread the news about available new resources. For PARTHENOS training is an ideal means of communicating project outcomes and to reach its disciplinary and geographically diverse communities. It is thus closely connected to the projects outreach and dissemination activities. For example, to extend the reach of the PARTHENOS Training materials, a short movie featuring two aliens who talk about standards, taking a rather light stance to a seemingly difficult topic, and reflecting the linguistic diversity of the potential users by voices in different languages, has been developed. In this context, PARTHENOS has also launched in 2018 the highly successful PARTHENOS eHumanities and eHeritage Webinar Series. In this series of online seminars, international experts from PARTHENOS and beyond took the learners on a journey along the research life cycle, highlighting how using and contributing to eHumanities and eHeritage research infrastructures empowers research. Outline and Proposal for a Long Paper In this long paper the main principles, insights, and results from the development of the PARTHENOS Training Suite will be presented to the international Digital Humanities community in order to stimulate the discussion of Digital Humanities pedagogy and to stipulate the further uptake of these resources. Acknowledgements: PARTHENOS Training is a cross PARTHENOS effort and includes input from external experts. We especially express our gratitude to past and present members of PARTHENOS Training that are not authors of this abstract: Elizabeth Burr, Stefanie Läpke, Rebecca Sierig, Helen Goulis, Jenny Oltersdorf. "
	},
	{
		"id": 433,
		"title": "Visualizing the Temporal Space of Narratives",
		"authors": [
			"Yeager, Sean A."
		],
		"body": " Introduction This paper studies the graphs which are created when a narratives fabulais plotted along the vertical axis of a coordinate plane and its syuzhetis plotted along the horizontal axis. Such graphs were recently dubbed time maps by William Nelles and Linda Williams, and though time maps offer a new means for the study of narrative, they have received minimal scholarly attention. To remedy this, I survey the relevant theoretical landscape, outline a methodology for generating time maps, and theorize on the myriad forms they may take. Time maps are particularly relevant for temporal narratologists because they visualize the ordering framework Gérard Genette developed in Narrative Discourse. But instead of simply tracking the order of a scenes position in the fabula, as Genette and others have done, I incorporate numerical values from the text to create time maps of unrivaled accuracy. Theory Many contemporary studies visualize literary complexity. Their most direct forerunner is Kurt Vonneguts framework for graphing the emotional arcs of stories. Vonnegut also plots syuzhet along the horizontal axis, but plots emotional valence along the vertical axis. Vonnegut created qualitative sketches that captured the essential shapes of several stories and predicted that most narratives could be described by a few patterns. His work inspired independent studies by Matthew Jockers and Andrew Reagan which implement automated machine learning techniques to quantitatively construct this same structure for a large number of texts. In sync with Vonneguts prediction, Reagans group found that six basic shapes account for the emotional arcs of approximately 95% of the texts in Project Gutenberg. Franco Moretti and the Stanford Literary Lab have also generated a number of papers in this vein. I highlight one, written by Maria Kanatovas group. They studied patterns of anachrony in recent mystery films to conclude that a new subgenre has emerged, which features heavy use of analeptic flashbacks that are external to the fabulas principal reach. Complementing these studies of time, last years Digital Humanities conference also featured a number of projects which digitally documented the spatial worlds within texts. Of particular note is Randa El Khatibs survey of Paradise Losts moral geography, as well as the anotational mapping that Gimena del Rio Riande, Romina De León, Nidia Hernández, and Leif Isaksen have done with Recogito 2. Other researchers have more directly studied time maps. Perhaps most famously, Steve Aprahamian published a time map of Memento to the films Wikipedia page. Daniel Aureliano Newman created a time map of Aldous Huxleys Eyeless in Gaza, using it to interpret the novel in terms of the biological concept of neoteny. Elisa Pezzotta created similar graphs for several science fiction films featuring time-travel. David Wittenberg has also performed a detailed study of time travel narratives which included a time map of Heinleins By His Bootstraps. The most comprehensive study of time maps to date, though, was the aforementioned analysis by Nelles and Williams. Yet their work is still limited in scope, merely concluding that narratives are inherently anachronous. My findings are in accordance with Nelles and Williams, yet I reach further. I formulate a general theory of time maps, outline a systematic methodology for creating them, and use them to perform interpretive work. Methodology Genettes scheme for organizing the scenes of disordered stories is a precursor to my work. He assigns each scene a letter, corresponding to its placement in the syuzhet, and a number, corresponding to its order in the fabula. The key difference is that I represent each scene as a line segment, instead of a point. This adds many layers of difficulty to data collection, since each scene now requires four pieces of information instead of two, but also allows for distinguishing scenes of varying durations. The n-th line segment has its origin atwhere s ni denotes the scenes initial location in the syuzhet and f ni denotes the scenes initial location in the fabula. The terminus of this line segment is atwhere s nf denotes the scenes final location in the syuzhet and f nf denotes the scenes final location in the fabula. A narratives time map is simply the complete set of these line segments and, consequently, the study of time maps is a geometric approach to narratology. Each line segment will have its own values for s ni , f ni , s nf , and f nf . In my data sets, each scene corresponds to a row containing all of these values, along with secondary columns which track the temporal cues essential to the compilation process. The works in my corpus were chosen because they contain a large number of temporal cues. My data sets for works such as Infinite Jest , A Visit from the Goon Squad , and House of Leaves are several hundred lines each. Machine learning techniques may eventually be implemented to collect this information, but entry currently requires a large amount of human input due to the fluid nature of fabula timeflow in narrative and the myriad ways it may be represented. Analysis I begin my analysis by examining the time map of Rashomon , which is shown in Figure 1. This film is particularly interesting because it simultaneously demonstrates two kinds of parallel storytelling. The first features three storylines being simultaneously told in the syuzhet, though they are nested in the fabula. These storylines manifest as three groups of line segments vertically displaced from one another: the red corresponds to the framing narrative of the monk and the woodcutter stranded on a rainy day; the green corresponds to their recollections of a trial; and the remaining colors sync up with the witnesss accounts of the crime. The second form of parallel storytelling in Rashomon is what Genette called repetition, which occurs when a single scene is revisited. Repetition is central to Rashomon because each witness recalls the same crime, but narrates the events differently. This film highlights the capacity for time maps to track narratological complexity for viewers. It also serves as a theoretical model: stories with multiple timelines will be vertically offset from one another, while those with repeated events will contain line segments which are horizontally offset. Hitchcocks Rope , portrayed in Figure 2, is a perfectly linear narrative and stands in contrast to Mrs. Dalloway , which is approximated in Figure 3 by plotting the singular tolls of Big Ben that occur throughout the novel. The latters time map is nearly linear, but thrown off by Woolfs extended discussion of the characters actions at lunchtime. This demonstrates a key difference between the camera and the written word: cameras can easily produce purely linear stories through the use of real-time filming, but this is extremely difficult to achieve in writing, due to the slippery nature of language. Figure 4 displays the time map for Citizen Kane , which is very nearly a triangle. Its sides are the vertical green dots on the left, the horizontal black line on top, and the multicolored line segments below. These respectively correspond to the opening newsreel, the reporters interviews, and the events of Kanes life as recounted by the interviewees. The film notably features several narrators, whose spans of syuzhet are demarcated by vertical gray lines. Yet because these narrators recall Kanes life in almost exactly the order it unfolded, they produce a hypotenuse which is very nearly linear. As a result, almost every scene from Citizen Kane lies along the perimeter of this triangle, with one glaring exception: when Kane is forced to sell controlling interest of the Inquirer to Thatcher. This loss quite literally lies at the center of the film — and, thus, the viewers understanding of Kanes life — albeit in a complex way which can only be recognized by plotting its time map. The interpretive inroads multiply when the corners of the triangle are taken into consideration. The film opens in the top left corner with Kanes last word, the bottom left corner depicts an early memory of him sledding as a child, and the top right corner shows that same sled being thrown into a furnace. Just as a triangle is geometrically defined by its three endpoints, Kanes life — or at least the audiences view of it — is defined by Rosebud. Inverting the trope of having an inner child, Kanes core is failed ambition which is padded by memories of simpler times. Conclusion In short, time maps reveal many types of complexity, ranging from interpreting individual texts to formulating narratological theory. In addition to the works discussed in this abstract, I have created and studied many others. Their elaborate structures require more space to unpack than is available here. In the future, natural language processing may be used to algorithmically generate time maps, but my experiences unpacking the ambiguities of temporal language lead me to believe that this would be a herculean undertaking. Figure 1: Time map of Rashomon Figure 2: Time map of Rope Figure 3: Time map of Mrs. Dalloway Figure 4: Time map of Citizen Kane "
	},
	{
		"id": 434,
		"title": "Digital History and the Politics of Digitization",
		"authors": [
			"Zaagsma, Gerben Zaagsma"
		],
		"body": " Much has been, and is, made of the transformative potential of digital resources and data for humanities and historical research. Historians are flooded with digital and digitized materials and tend to take them for granted, grateful for the opportunities they afford. As the late Roy Rosenzweig observed in 2003, historians may be facing a fundamental paradigm shift from a culture of scarcity to a culture of abundance. Yet, if we accept that we do indeed live in a culture of abundance, that abundance is still rarely questioned and qualified, let alone contextualized in time and space. To put it simply: the question of why, where and how we can access what we can access is rarely posed. Digitization, however, is far from neutral. In an academic world that increasingly privileges what is online, where analogue archives are sometimes even referred to as hidden archives, we need to start imagining what a world of historical scholarship based upon digital resources looks like. The online documentary record affects historical research and we need to understand how and in what ways our online evidentiary basis is constituted, a question that goes without saying for historians when dealing with analogue archives but is often ignored when digital resources are used. Indeed, there is a marked discrepancy between the use of digital resources by many historians and their lack of interest in how these are created and constituted. Digitization first and foremost means selection. Archives, libraries, museums and other heritage institutions select materials to be digitized on the basis of a variety of criteria such as the preservation of fragile materials, easy access to collection highlights and/or often-used material, and the research value of certain collections. Legal and ethical questions can play a role too and, given the costs involved, the availability of funding, public or private, plays a key role in enabling digitization projects in the first place. Funding is not only influenced by the aforementioned criteria and concerns but also by memory politics and the way in which a given countrys or groups past, or aspects thereof, resonate in public discourses and debates. That is, if funding is available at all. Retro-digitization of heritage can be a luxury that many countries cannot afford. In this respect Barringer et al. have noted the political and economic inequality between North and South, which has shaped not only the form and content of digital libraries, but also access on the continent to material about the continent. The point is well demonstrated by contrasting the European Unions policies of promoting and financing digital cultural heritage Taking as a starting point the Commission Recommendation of 27 October 2011 on the digitization and online accessibility of cultural material and digital preservation: eur- lex.europa.eu/LexUriServ/LexUriServ.do?uri=OJ:L:2011:283:0039:0045:EN:PDF . See also: ec.europa.eu/digital-single-market/en/policies/digital-cultural-heritage . with the more limited resources that states on the African continent have, where privately funded endeavors such as the British Librarys Endangered Archives program acquire a relatively more important role. See: eap.bl.uk/. In view of all this, the question becomes: what are the politics of digitization and what are its implications for historical research? Is the often-lauded democratizing potential of digitization also offset by risks, inherent in selection processes that might privilege the digitization of heritage corresponding to existing national master narratives, the availability of funding and/or the priorities set by certain research agendas? How does transnational heritage fit into this picture when most digitization projects are, in one way or another, nationally framed? And how does all of this play out globally? When answering these questions, two things need to be taken into account. The first is the broader historical and technological context of heritage preservation and its politics. For instance, there are remarkable parallels to be drawn between the era of microfilming, as a new means of reproduction, and the current phase of digitization, from processes of selection, metadating practices and questions of accessibility, to the reasons for preserving analogue materials and utopian claims as to how research practices could, and would, change. Unlike the current era of digitization, however, the microfilm never succeeded in penetrating and saturating scholarly practices to the extent that digitization does. The second is the role of the nation in processes of digitization and selection. As in the era of the microfilm, the role of the nation matters, and it matters a lot, contrary to some earlier scholarly predictions. This is not to suggest that digitization, especially when it comes to heuristics, cannot facilitate transnational history approaches, as recently argued by. Around 15 years ago, for instance, Rosenzweig rhetorically asked: If national archives were part of the projects of state-building and nationalism, then why should states support post- national digital archives?. Yet, even a cursory glance at the about pages and mission statements of many digital libraries and archives demonstrates that national concerns have far from disappeared when it comes to efforts to digitize the past. The existence of a supra-national digital resource like Europeana might seem to contradict this point, but it is very clear that part of Europeanas mission is to promote and create a sense of common European heritage. This paper will present the first findings of an ongoing research project. It is about the digital resources we work with as historians; in the first place with regard to what is being digitized, the sources and data, and to a lesser degree the metadata, notwithstanding the latters profound political aspects and effects, e.g. with regard to access. To avoid misunderstandings: it is not about the politics of digital humanities more broadly conceived; e.g. about addressing claims that it is incumbent upon DH to fulfil a political mission or become a more self-critical discipline by consciously investigating its own gender, diversity and other biases. The paper explores the question of the politics of digitization by focusing on one specific dimension: the question of digitization and selection, and its implications for historical research, by using the example of the digitization of Jewish heritage. It combines a theoretical, critical-reflexive approach with concrete examples and is structured as follows: Introducing the politics of digitization: why is this an important question? What politics are we speaking about? And how does this affect historical research? Scholarly context: A very short overview of the state of the art in DH literature, with a particular focus on what insights can be drawn from debates in the fields of heritage studies, information, archival and library science. Spatial context: How can we conceive of digitization within a global context? As an example, I will briefly discuss the state of digitization of European versus African cultural heritage and pinpoint the major issues involved. For Europe, see the eNumerate reports on the state of cultural heritage digitization available at: www.egmus.eu/ . Statistical information on Africa has to be compiled from various sources. For a general discussion of digitization and the nation in relation to Africa see. Analysis of the political dimensions of digitization of Jewish heritage and selection criteria for digital preservation: As mentioned, this paper derives from a bigger project with deals with various dimensions and contexts relevant to the politics of digitisation, not only the issue of selection. As for the latter, this main project will provide a systematic overview of how digital libraries/archives in a number of countries advertise themselves online and what selection criteria for digital preservation they use, which will be contextualised with information and data from these countries from, inter alia , the Compendium of Cultural Policies and Trends . For the purposes of the paper, I will outline this approach and then provide a couple of brief examples. The Compendium of Cultural Policies and Trends is a web-based and permanently updated information and monitoring system of national cultural policies and related development. See: www.culturalpolicies.net/web/compendium.php . Concluding remarks on the politics of digitization and its implications for historical research: Outlook: what can we learn from the examples provided, and how should we conceive of this question within a global context? "
	},
	{
		"id": 435,
		"title": "Global Language Justice in the Digital Sphere: The Ethiopic Case",
		"authors": [
			"Zaugg, Isabelle Alice"
		],
		"body": " Considering the stunning advancements in communication technology, some may find it hard to imagine that at the outset of the digital age 50%to 90%of languages are at risk of extinction this century. This dire forecast relates to a complex amalgamation of factors including the history of colonization, oppression of minority and Indigenous communities, migration, innovations in global transport, international media consolidation, and urbanization. Digital technologies also appear to be contributing to a pattern of mass language extinction. This may be particularly surprising considering the rhetoric around digital technologies potential to connect the world and democratize access to information and communication. When the diversity of human languages and their written scripts are considered, the challenge of achieving these aspirations without undermining language and script diversity comes into focus. While there exists a potential for digital technologies to support linguistic diversity in all its richness, this trajectory is by no means fated. This paper addresses key factors and efforts required to achieve such a future as well as the roadblocks that may prevent many languages from surviving the digital age. Digital extinction will be the fate, according to Rehm, of languages that suffer from insufficient technology support. This is a three-pronged process. First, if digital technologies make the use of a language impossible or inconvenient, a language community will suffer a loss of function as other languages take over tasks such as email, texting, search, e-commerce, etc. Second is a simultaneous loss of prestige associated with the absence of a language within the high-prestige technological realm. Digital technologies were first developed in English-speaking contexts and spread internationally during a period of increasing English dominance after the Cold War; thus, English has gained prestige in the digital sphere while the prestige of many other languages has declined. The third prong of digital extinction is the loss of competence which occurs as it becomes increasingly difficult to raise a digital native competent in the language. With digital technologies playing an increasing role in human communication, the lack of digital use of a language is likely to become an increasingly central factor in language extinction more generally. Considering that Kornaipredicts that less than 5% of languages will achieve full vitality in the digital realm, the scope of this concern becomes apparent. Ultimately, what does it matter if global communication is achieved at the expense of language diversity? Language is interconnected with identity, culture, and an intergenerational sense of belonging. Therefore, the users of under-resourced languages clearly stand to lose the most from digital extinction. However, the loss of language diversity impacts us all. Language is tied up with human knowledge, and when we lose one we lose the other. Harrisonargues that the extinction of ideas we now face has no parallel in human history, while Hale states that losing a language is like dropping a bomb on a museum, the Louvre. While the digital age promises us global access to a grand storehouse of knowledge, we may lose more than we gain if digital extinction goes unchecked. Technological support for full digital viability of a language consists of a variety of factors. Foundational are digital standards, or the protocols that determine which languages written scripts are supported by digital devices, software, and platforms. As such, digital standards play a key role in determining which languages are included or excluded from digital communication. While more and more languages are supported by foundational standards, language inclusiveness has primarily taken place through the process of technology companies targeting new, profitable markets of language users. Left behind are language communities too small or too poor to be considered viable target markets. The central research questions are: How is language diversity threatened or bolstered by digital communication technologies? What can be done to ensure languages survive the digital age? I identify the primary hurdles in terms of technological design and governance of digital technologies that disadvantage minority and Indigenous languages. Last, I identify best practices for how these barriers can be overcome, including policy best practices for digital governance institutions to meet the needs of digitally-disadvantaged language communities. The impact of digital technologies on language diversity has been radically understudied because of the complex and cross-disciplinary nature of the research area. While linguists and anthropologists study language shifts, they typically lack the technical expertise to understand how digital design and governance impact language choices in the digital sphere. Similarly, tech designers and computer scientists frequently lack awareness of the implications of their work on language diversity. This interdisciplinary research responds to the conferences call to build complex models of complex realities, analyze them with computational methods, and communicate the results to a broader public. I utilize an instrumental case study method, suited to exploratory and cross-disciplinary research. I focus on the case of the digitization and standardization of the Ethiopic script culminating in its inclusion in the dominant character encoding standard Unicode as well as ISO/IEC 10646, which is kept in-synch with Unicode. I also consider other forms of digital support developed for the Ethiopic script and its languages, including Ethiopias national language Amharic. The case of Ethiopic is uniquely informative in that it was the first indigenous African script to be included in Unicode and ISO/IEC 10646. This case also demonstrates many common challenges that affect other digitally-disadvantaged languages and scripts, such as Mongolian and NKo. Research methods include in-depth qualitative interviews with key actors, including Ethiopic digital pioneers who built early word-processing programs, keyboards, and other basic digital supports for the script and its languages. I also interviewed members of the Unicode Technical Committee and the ISOs Subcommittee 2/Working Group 2 on character encoding, observed their ongoing work to support digitally-disadvantaged scripts and languages, and did research in their archives about inclusion of the Ethiopic script in their standards. Furthermore, in order to determine to what extent foundational supports for Ethiopic have allowed for use of the script and its languages in the digital sphere, I conducted a non-traditional content analysis of comments on popular Ethiopian-themed Facebook pages. While rates of utilization of the Ethiopic script have seen increases over the last decade, there are still more Amharic comments written in the Latin script than those written in Ethiopic. This indicates ongoing barriers in support for the script, since transliteration of Amharic into Latin is uncommon outside of the digital sphere. Last, I chronicle ongoing work, for which I am a participant-observer, to develop layout and formatting rules for Amharic in partnership with the digital governance institution World Wide Web Consortium. These rules will allow software and platforms to accurately support the unique characteristics of the Ethiopic script and Amharic language. Challenges include collecting Ethiopic publishing expertise from stakeholders unfamiliar with the W3C, some of whom are not online. The instrumental nature of the Ethiopic case means that throughout I situate this history in the context of larger trends affecting digitally-disadvantaged scripts and languages as a group. Despite concerning trends in terms of digital support for language diversity, the digital age is still young. The digital technologies we use today were shaped by forces in the recent past, but these are all subject to change. People write code. And as Russell, DeNardis, and Osbornassert, we have an opportunity and a responsibility to shape our technologies to support the future we wish to inhabit. If we wish to preserve and revitalize the diversity of human language, and the wealth of knowledge it contains, digital technologies can help us do that. If we, and particularly those who design and govern digital technologies, prefer to let the market decide, digital technologies will contribute heavily to global linguistic homogenization and the mass extinction of minority and Indigenous languages. Recommendations include a focus on support for language diversity as the corporate social responsibility of global tech giants, an issue of accessibility and equity. This can be promoted by advocates reaching out to companies, voicing needs and connecting them with language expertise as necessary. Digital governance organizations should lower barriers of entry for digitally-disadvantaged language communities to participate and voice their concerns. This may include working with trusted third-party intermediaries, such as the Script Encoding Initiative, which bridges script communities needs with the technical requirements of Unicode proposals. Governments can support academia to build digital tools for non-market languages, as well as purchasing digital tools that support national and local languages, creating market incentives to develop them. Collaborations between linguists and technologists are also essential. This presentation will present how we can shape the future of language diversity by closing the linguistic digital divide through advocacy, digital design, and governance of the digital sphere. "
	},
	{
		"id": 436,
		"title": "Materia on the Move: Trade & Colonisation of Collections - Digital Studies in Provenance",
		"authors": [
			"Zervanou, Kalliopi",
			"Pieters, Toine"
		],
		"body": " Description: The transfer of ideas, knowledge and culture is linked with the transfer and interpretation of objects, be it art objects, tools or natural specimens, such as plants, minerals, and animals. These objects have been removed from their natural cultural and environmental context into collections. Being mediums of knowledge transfer and evolution, objects move in both space and time. In time, as objects are relocated and re-studied, new interpretations of the objects and relations between them are formed in accordance to scientific and political, ethical and societal developments. In space, they move along trade networks and are discussed in communication networks. Currently, digitised cultural heritage data, such as art collections, letters, digitised manuscripts, archival material, and natural history specimens, present an unprecedented opportunity for humanities researchers investigating various aspects of such object trajectories: - by connecting and integrating disparate data sources to re-contextualise lost historical and cultural contexts; - by extraction of information from historical and modern document sources; - by developing metadata that capture the evolution of knowledge and the information content of available digitised sources; - by developing algorithms for combining available information This workshop aims to bring together researchers from various humanities disciplines, such as history, ethnography and archaeology, with the established guardians of collections, namely researchers in archival, library, museum studies and information science professionals and stakeholders to present and discuss approaches in tracing and documenting provence, be it geographical or cultural and ideological. Topics of interest include: • Use of linked data for data integration • Approaches to metadata and knowledge resource development • Historical information modelling • Use of geographical information & historical location resolution • Processing of textual sources for information related to transferred objects • Tracing knowledge through scientific and/or commercial networks Workshop leaders: Dr. Kalliopi Zervanou is a computational linguist and currently a lecturer in Information Systems at the Industrial Engineering & Innovation Sciences department in Eindhoven University of Technology. Since 2009, she has been actively involved in digital humanities in a series of projects and as a leading member of the computational linguistics digital humanities community, both as programme chair of the LaTeCH workshop series, as well as founding member of the respective ACL Special Interest Group, SIGHUM, which she also serves as elected secretary for three consecutive terms, since 2013. Her current research focuses on historical data modelling issues and event extraction from texts. Prof. Toine Pieters is Professor of the History of Pharmacy and Allied Sciences at Descartes Centre for the History and Philosophy of the Sciences and the Humanities and the Freudenthal Institute/HPS, Faculty Department of Science at Utrecht University. He has published extensively on the history of the production, distribution and consumption of therapeutic drugs and the interwovenness with economy, science and the public sphere. Currently he is working on the history of medicinal plants and plant based drugs. His broader interests include digital humanities and cultural heritage. The multidisciplinary nature of his research has opened paths for fruitful collaborations in the promising and fast growing field of digital humanities. He has been project leader and research coordinator of multiple projects in this fieldand cultural heritage stakeholders interested in developing and implementing digital methods for tracing the provenance of materials in collections and re-creating the historical or cultural context of artefacts or natural materials in cultural heritage collections. Expected number of participants: 20-30 participants Special requirements for technical support: no special requirements apart from standard presentation projector and internet access Intended length and format of the workshop: half day workshop including research & system demo presentations followed by discussion Special requirements for attendees: no special requirements Participation: There will be a call for papers and the organisers aim to publish the workshop proceedings at CEUR-WS.org Program Committee: Tinde van Andel, Naturalis Biodiversity Center, The Netherlands Gudrun Bucher, SUB Göttingen, Germany Thierry Declerck, DFKI GmbH, Germany Katherine McDonough, The Alan Turing Institute, UK Rik Feiken, Cultural Heritage Agency – RCE, The Netherlands Maarten Heerlien, Rijksmuseum, The Netherlands Inge van der Jagt, Cultural Heritage Agency – RCE, The Netherlands Wouter Klein, Utrecht University, The Netherlands Martijn Kleppe, National Library - KB, The Netherlands Marijn Koolen, Huygens ING, The Netherlands Piroska Lendvai, Georg-August-Universität Göttingen, Germany Ioanna Lykourentzou, Utrecht University, The Netherlands Julianne Nyhan, University College London, UK Petya Osenova, Bulgarian Academy of Sciences, Bulgaria Michael Piotrowski, Université de Lausanne, Switzerland Marijn Schraagen, Utrecht University, The Netherlands Kiril Simov, Bulgarian Academy of Sciences, Bulgaria Caroline Sporleder, Georg-August-Universität Göttingen, Germany Andreas Vlachidis, University College London, UK Lianne Wilmink, Cultural Heritage Agency – RCE, The Netherlands "
	},
	{
		"id": 437,
		"title": "Data in Museums: Digital Practices and Contemporary Heritage",
		"authors": [
			"Zuanni, Chiara"
		],
		"body": " This short paper will examine notions of, and practices in relation to, digital data in museums. On the one hand, it will discuss how digital data are conceptualized, identified, and considered in museums; on the other hand, it will explore how they are produced, collected, curated, shared, and preserved within the heritage sector. Digital data have now a liminal position in museums, where they are increasingly being recognized as part of 21 st century heritage. Despite the UNESCO Charter on the Preservation of Digital Heritage dating to 2003, and a CIDOC Digital Preservation Working Group active since 2006, museums have begun to recognise the value of preserving digital data as sources and representations of contemporary heritage only in recent years. For example, MoMA announced its first collection of videogames in 2012; at the Museum of London, in 2017, the installation Pulse tracked social media and displayed live what Londoners were tweeting; in 2018, a major exhibition on videogames was held also at the Victoria and Albert Museum in London; and, outside of the museum sector, the Daily Show programmeorganised the pop up The Donald J. Trump Presidential Twitter Library, which has now visited a few North American cities and it is the subject of a virtual museum. However, this recent attention for digital data as potential museum objects implies also a reconsideration of their position within museum taxonomies, where in the past they have mostly been treated as auxiliary information and interpretative support. In addition, user generated content related to the museum and its collections, and generated by visitors during museum visits and by online audiences on social media, will be considered also for its potential in expanding object biographies. Previous research has focused on participatory practices and digital engagementand the evaluation of digital programmes, although there are not yet clear frameworks and methodologies for harvesting, managing, and using this data. Less work has been done on the exploration of objects online biographies, as emerging from the museum digitisation practices and its collection management systems, and continuing through social media photos and digital engagement material, and this paper will highlight some of the emerging challenges. While the inclusion of digital data in collections might prompt a redefinition of the values and position of digital heritage in a museum and might provoke new questions on the digital lives of museum objects, this data causes also a series of curatorial and methodological challenges. Firstly, this data will come in different forms, each one presenting different technical challenges in relation to their collection, archiving, and representation. Secondly, it might even be part of assemblages of digital and analogue items which, together, represent the heritage of 21st century events. The acquisition of both physical and digital material within a museum system poses a series of additional challenges to its ontologies and vocabularies, which are not prepared to acquire born digital materials in the same digital infrastructure as that of more traditional museum objects. Throughout the discussion, the paper will observe how museum approaches to digital collecting and to digital preservation diverge or complement existing practices in parallel fields. The idea that history and heritage are now increasingly shared and produced online, and thus we ought to preserve digital outputs and research the circulation of news, opinions, and debates in the digital sphere has a longer story in the field of digital humanities, and web archiving in particular. Hence, the paper will observe how discussions and projects in the digital humanities field could be productively inspire new forms of curating in museums, in order to improve practices in relation to the acquisition, recording, management, and preservation of this data. Similarly, the field of digital ethnographyand research in social sciences have emphasised the values of collecting, analysing, and eventually preserving our online lives. For the purposes of this paper, it is particularly the ethical discussions around the collection of contemporary data which are of interest. In conclusion, the paper will focus on the one hand on questions on the collection, management, and use of digital data, which are increasingly crucial for future museum curating practices. On the other hand, the paper will discuss how the repositioning of digital data as heritage raises new questions in relation to the materiality and authenticity of the digital, to the politics and impact of co-production and knowledge creation online, to the management of digitised and born-digital content, and to the ethics of collecting digital data and its consequent implications. "
	}
]